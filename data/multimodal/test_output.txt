--- Page 2 ---
F I F T H E D I T I O N
Linear Algebra
and Its Applications
David C. Lay
University of Maryland‚ÄîCollege Park
with
Steven R. Lay
Lee University
and
Judi J. McDonald
Washington State University
Boston Columbus Indianapolis New York San Francisco
Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montreal Toronto
Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singa pore Tai pei Tokyo
REVISED PAGES


--- Page 3 ---
Editorial Director: Chris Hoag
Editor in Chief: Deirdre Lynch
Acquisitions Editor: William Hoffman
Editorial Assistant: Salena Casha
Program Manager: Tatiana Anacki
Project Manager: Kerri Consalvo
Program Management Team Lead: Marianne Stepanian
Project Management Team Lead: Christina Lepre
Media Producer: Jonathan Wooding
TestGen Content Manager: Marty Wright
MathXL Content Developer: Kristina Evans
Marketing Manager: Jeff Weidenaar
Marketing Assistant: Brooke Smith
Senior Author Support/Technology Specialist: Joe Vetere
Rights and Permissions Project Manager: Diahanne Lucas Dowridge
Procurement Specialist: Carol Melville
Associate Director of Design Andrea Nix
Program Design Lead: Beth Paquin
Composition: Aptara¬Æ, Inc.
Cover Design: Cenveo
Cover Image: PhotoTalk/E+/Getty Images
Copyright ¬© 2016, 2012, 2006 by Pearson Education, Inc. All Rights
Reserved. Printed in the United States of America. This publication is
protected by copyright, and permission should be obtained from the
publisher prior to any prohibited reproduction, storage in a retrieval
system, or transmission in any form or by any means, electronic,
mechanical, photocopying, recording, or otherwise. For information
regarding permissions, request forms and the appropriate contacts within
the Pearson Education Global Rights & Permissions department, please
visit www.pearsoned.com/permissions/.
Acknowledgements of third party content appear on page P1, which
constitutes an extension of this copyright page.
PEARSON, ALWAYS LEARNING, is an exclusive trademark in the U.S.
and/or other countries owned by Pearson Education, Inc. or its afÔ¨Åliates.
Unless otherwise indicated herein, any third-party trademarks that may appear in this work are the property of their
respective owners and any references to third-party trademarks, logos or other trade dress are for demonstrative or
descriptive purposes only. Such references are not intended to imply any sponsorship, endorsement, authorization, or
promotion of Pearson‚Äôs products by the owners of such marks, or any relationship between the owner and Pearson
Education, Inc. or its afÔ¨Åliates, authors, licensees or distributors.
This work is solely for the use of instructors and administrators for the purpose of teaching courses and assessing student
learning. Unauthorized dissemination, publication or sale of the work, in whole or in part (including posting on the internet)
will destroy the integrity of the work and is strictly prohibited.
Library of Congress Cataloging-in-Publication Data
Lay, David C.
Linear algebra and its applications / David C. Lay, University of Maryland, College Park, Steven R. Lay, Lee University,
Judi J. McDonald, Washington State University. ‚Äì Fifth edition.
pages cm
Includes index.
ISBN 978-0-321-98238-4
ISBN 0-321-98238-X
1. Algebras, Linear‚ÄìTextbooks. I. Lay, Steven R., 1944- II. McDonald, Judi. III. Title.
QA184.2.L39 2016
5120.5‚Äìdc23
2014011617
REVISED PAGES


--- Page 4 ---
About the Author
David C. Lay holds a B.A. from Aurora University (Illinois), and an M.A. and Ph.D.
from the University of California at Los Angeles. David Lay has been an educator
and research mathematician since 1966, mostly at the University of Maryland, College
Park. He has also served as a visiting professor at the University of Amsterdam, the
Free University in Amsterdam, and the University of Kaiserslautern, Germany. He has
published more than 30 research articles on functional analysis and linear algebra.
As a founding member of the NSF-sponsored Linear Algebra Curriculum Study
Group, David Lay has been a leader in the current movement to modernize the linear
algebra curriculum. Lay is also a coauthor of several mathematics texts, including In-
troduction to Functional Analysis with Angus E. Taylor, Calculus and Its Applications ,
with L. J. Goldstein and D. I. Schneider, and Linear Algebra Gems‚ÄîAssets for Under-
graduate Mathematics, with D. Carlson, C. R. Johnson, and A. D. Porter.
David Lay has received four university awards for teaching excellence, including,
in 1996, the title of Distinguished Scholar‚ÄìTeacher of the University of Maryland. In
1994, he was given one of the Mathematical Association of America‚Äôs Awards for
Distinguished College or University Teaching of Mathematics. He has been elected
by the university students to membership in Alpha Lambda Delta National Scholastic
Honor Society and Golden Key National Honor Society. In 1989, Aurora University
conferred on him the Outstanding Alumnus award. David Lay is a member of the Ameri-
can Mathematical Society, the Canadian Mathematical Society, the International Linear
Algebra Society, the Mathematical Association of America, Sigma Xi, and the Society
for Industrial and Applied Mathematics. Since 1992, he has served several terms on the
national board of the Association of Christians in the Mathematical Sciences.
To my wife, Lillian, and our children,
Christina, Deborah, and Melissa, whose
support, encouragement, and faithful
prayers made this book possible.
David C. Lay
REVISED PAGES


--- Page 5 ---
Joining the Authorship on the Fifth Edition
Steven R. Lay
Steven R. Lay began his teaching career at Aurora University (Illinois) in 1971, after
earning an M.A. and a Ph.D. in mathematics from the University of California at Los
Angeles. His career in mathematics was interrupted for eight years while serving as a
missionary in Japan. Upon his return to the States in 1998, he joined the mathematics
faculty at Lee University (Tennessee) and has been there ever since. Since then he has
supported his brother David in reÔ¨Åning and expanding the scope of this popular linear
algebra text, including writing most of Chapters 8 and 9. Steven is also the author of
three college-level mathematics texts: Convex Sets and Their Applications, Analysis
with an Introduction to Proof , and Principles of Algebra.
In 1985, Steven received the Excellence in Teaching Award at Aurora University. He
and David, and their father, Dr. L. Clark Lay, are all distinguished mathematicians,
and in 1989 they jointly received the Outstanding Alumnus award from their alma
mater, Aurora University. In 2006, Steven was honored to receive the Excellence in
Scholarship Award at Lee University. He is a member of the American Mathematical
Society, the Mathematics Association of America, and the Association of Christians in
the Mathematical Sciences.
Judi J. McDonald
Judi J. McDonald joins the authorship team after working closely with David on the
fourth edition. She holds a B.Sc. in Mathematics from the University of Alberta, and
an M.A. and Ph.D. from the University of Wisconsin. She is currently a professor at
Washington State University. She has been an educator and research mathematician
since the early 90s. She has more than 35 publications in linear algebra research journals.
Several undergraduate and graduate students have written projects or theses on linear
algebra under Judi‚Äôs supervision. She has also worked with the mathematics outreach
project Math Central http://mathcentral.uregina.ca/ and continues to be passionate about
mathematics education and outreach.
Judi has received three teaching awards: two Inspiring Teaching awards at the University
of Regina, and the Thomas Lutz College of Arts and Sciences Teaching Award at
Washington State University. She has been an active member of the International Linear
Algebra Society and the Association for Women in Mathematics throughout her ca-
reer and has also been a member of the Canadian Mathematical Society, the American
Mathematical Society, the Mathematical Association of America, and the Society for
Industrial and Applied Mathematics.
REVISED PAGES
iv

--- Page 6 ---
Contents
Preface viii
A Note to Students xv
Chapter 1 Linear Equations in Linear Algebra 1
INTRODUCTORY EXAMPLE: Linear Models in Economics and Engineering 1
1.1 Systems of Linear Equations 2
1.2 Row Reduction and Echelon Forms 12
1.3 Vector Equations 24
1.4 The Matrix Equation AxDb 35
1.5 Solution Sets of Linear Systems 43
1.6 Applications of Linear Systems 50
1.7 Linear Independence 56
1.8 Introduction to Linear Transformations 63
1.9 The Matrix of a Linear Transformation 71
1.10 Linear Models in Business, Science, and Engineering 81
Supplementary Exercises 89
Chapter 2 Matrix Algebra 93
INTRODUCTORY EXAMPLE: Computer Models in Aircraft Design 93
2.1 Matrix Operations 94
2.2 The Inverse of a Matrix 104
2.3 Characterizations of Invertible Matrices 113
2.4 Partitioned Matrices 119
2.5 Matrix Factorizations 125
2.6 The Leontief Input‚ÄìOutput Model 134
2.7 Applications to Computer Graphics 140
2.8 Subspaces of Rn148
2.9 Dimension and Rank 155
Supplementary Exercises 162
Chapter 3 Determinants 165
INTRODUCTORY EXAMPLE: Random Paths and Distortion 165
3.1 Introduction to Determinants 166
3.2 Properties of Determinants 171
3.3 Cramer‚Äôs Rule, V olume, and Linear Transformations 179
Supplementary Exercises 188
REVISED PAGES
v

--- Page 7 ---
viContents
Chapter 4 Vector Spaces 191
INTRODUCTORY EXAMPLE: Space Flight and Control Systems 191
4.1 Vector Spaces and Subspaces 192
4.2 Null Spaces, Column Spaces, and Linear Transformations 200
4.3 Linearly Independent Sets; Bases 210
4.4 Coordinate Systems 218
4.5 The Dimension of a Vector Space 227
4.6 Rank 232
4.7 Change of Basis 241
4.8 Applications to Difference Equations 246
4.9 Applications to Markov Chains 255
Supplementary Exercises 264
Chapter 5 Eigenvalues and Eigenvectors 267
INTRODUCTORY EXAMPLE: Dynamical Systems and Spotted Owls 267
5.1 Eigenvectors and Eigenvalues 268
5.2 The Characteristic Equation 276
5.3 Diagonalization 283
5.4 Eigenvectors and Linear Transformations 290
5.5 Complex Eigenvalues 297
5.6 Discrete Dynamical Systems 303
5.7 Applications to Differential Equations 313
5.8 Iterative Estimates for Eigenvalues 321
Supplementary Exercises 328
Chapter 6 Orthogonality and Least Squares 331
INTRODUCTORY EXAMPLE: The North American Datum
and GPS Navigation 331
6.1 Inner Product, Length, and Orthogonality 332
6.2 Orthogonal Sets 340
6.3 Orthogonal Projections 349
6.4 The Gram‚ÄìSchmidt Process 356
6.5 Least-Squares Problems 362
6.6 Applications to Linear Models 370
6.7 Inner Product Spaces 378
6.8 Applications of Inner Product Spaces 385
Supplementary Exercises 392
REVISED PAGES


--- Page 8 ---
Contents vii
Chapter 7 Symmetric Matrices and Quadratic Forms 395
INTRODUCTORY EXAMPLE: Multichannel Image Processing 395
7.1 Diagonalization of Symmetric Matrices 397
7.2 Quadratic Forms 403
7.3 Constrained Optimization 410
7.4 The Singular Value Decomposition 416
7.5 Applications to Image Processing and Statistics 426
Supplementary Exercises 434
Chapter 8 The Geometry of Vector Spaces 437
INTRODUCTORY EXAMPLE: The Platonic Solids 437
8.1 AfÔ¨Åne Combinations 438
8.2 AfÔ¨Åne Independence 446
8.3 Convex Combinations 456
8.4 Hyperplanes 463
8.5 Polytopes 471
8.6 Curves and Surfaces 483
Chapter 9 Optimization (Online)
INTRODUCTORY EXAMPLE: The Berlin Airlift
9.1 Matrix Games
9.2 Linear Programming‚ÄîGeometric Method
9.3 Linear Programming‚ÄîSimplex Method
9.4 Duality
Chapter 10 Finite-State Markov Chains (Online)
INTRODUCTORY EXAMPLE: Googling Markov Chains
10.1 Introduction and Examples
10.2 The Steady-State Vector and Google‚Äôs PageRank
10.3 Communication Classes
10.4 ClassiÔ¨Åcation of States and Periodicity
10.5 The Fundamental Matrix
10.6 Markov Chains and Baseball Statistics
Appendixes
A Uniqueness of the Reduced Echelon Form A1
B Complex Numbers A2
Glossary A7
Answers to Odd-Numbered Exercises A17
Index I1
Photo Credits P1
REVISED PAGES


--- Page 9 ---
Preface
REVISED PAGES
The response of students and teachers to the Ô¨Årst four editions of Linear Algebra and Its
Applications has been most gratifying. This Fifth Edition provides substantial support
both for teaching and for using technology in the course. As before, the text provides
a modern elementary introduction to linear algebra and a broad selection of interest-
ing applications. The material is accessible to students with the maturity that should
come from successful completion of two semesters of college-level mathematics, usu-
ally calculus.
The main goal of the text is to help students master the basic concepts and skills they
will use later in their careers. The topics here follow the recommendations of the Linear
Algebra Curriculum Study Group, which were based on a careful investigation of the
real needs of the students and a consensus among professionals in many disciplines that
use linear algebra. We hope this course will be one of the most useful and interesting
mathematics classes taken by undergraduates.
WHAT'S NEW IN THIS EDITION
The main goals of this revision were to update the exercises, take advantage of improve-
ments in technology, and provide more support for conceptual learning.
1.Support for the Fifth Edition is offered through MyMathLab. MyMathLab, from
Pearson, is the world‚Äôs leading online resource in mathematics, integrating interac-
tive homework, assessment, and media in a Ô¨Çexible, easy-to-use format. Students
submit homework online for instantaneous feedback, support, and assessment. This
system works particularly well for computation-based skills. Many additional re-
sources are also provided through the MyMathLab web site.
2.TheFifth Edition of the text is available in an interactive electronic format. Using
the CDF player, a free Mathematica player available from Wolfram, students can
interact with Ô¨Ågures and experiment with matrices by looking at numerous examples
with just the click of a button. The geometry of linear algebra comes alive through
these interactive Ô¨Ågures. Students are encouraged to develop conjectures through
experimentation and then verify that their observations are correct by examining the
relevant theorems and their proofs. The resources in the interactive version of the
text give students the opportunity to play with mathematical objects and ideas much
as we do with our own research. Files for Wolfram CDF Player are also available for
classroom presentations.
3.TheFifth Edition includes additional support for concept- and proof-based learning.
Conceptual Practice Problems and their solutions have been added so that most sec-
tions now have a proof- or concept-based example for students to review. Additional
guidance has also been added to some of the proofs of theorems in the body of the
textbook.
viii

--- Page 10 ---
Preface ix
4.More than 25 percent of the exercises are new or updated, especially the computa-
tional exercises. The exercise sets remain one of the most important features of this
book, and these new exercises follow the same high standard of the exercise sets from
the past four editions. They are crafted in a way that reÔ¨Çects the substance of each
of the sections they follow, developing the students‚Äô conÔ¨Ådence while challenging
them to practice and generalize the new ideas they have encountered.
DISTINCTIVE FEATURES
Early Introduction of Key Concepts
Many fundamental ideas of linear algebra are introduced within the Ô¨Årst seven lectures,
in the concrete setting of Rn, and then gradually examined from different points of view.
Later generalizations of these concepts appear as natural extensions of familiar ideas,
visualized through the geometric intuition developed in Chapter 1. A major achievement
of this text is that the level of difÔ¨Åculty is fairly even throughout the course.
A Modern View of Matrix Multiplication
Good notation is crucial, and the text reÔ¨Çects the way scientists and engineers actually
use linear algebra in practice. The deÔ¨Ånitions and proofs focus on the columns of a ma-
trix rather than on the matrix entries. A central theme is to view a matrix‚Äìvector product
Axas a linear combination of the columns of A. This modern approach simpliÔ¨Åes many
arguments, and it ties vector space ideas into the study of linear systems.
Linear Transformations
Linear transformations form a ‚Äúthread‚Äù that is woven into the fabric of the text. Their
use enhances the geometric Ô¨Çavor of the text. In Chapter 1, for instance, linear transfor-
mations provide a dynamic and graphical view of matrix‚Äìvector multiplication.
Eigenvalues and Dynamical Systems
Eigenvalues appear fairly early in the text, in Chapters 5 and 7. Because this material
is spread over several weeks, students have more time than usual to absorb and review
these critical concepts. Eigenvalues are motivated by and applied to discrete and con-
tinuous dynamical systems, which appear in Sections 1.10, 4.8, and 4.9, and in Ô¨Åve
sections of Chapter 5. Some courses reach Chapter 5 after about Ô¨Åve weeks by covering
Sections 2.8 and 2.9 instead of Chapter 4. These two optional sections present all the
vector space concepts from Chapter 4 needed for Chapter 5.
Orthogonality and Least-Squares Problems
These topics receive a more comprehensive treatment than is commonly found in begin-
ning texts. The Linear Algebra Curriculum Study Group has emphasized the need for
a substantial unit on orthogonality and least-squares problems, because orthogonality
plays such an important role in computer calculations and numerical linear algebra and
because inconsistent linear systems arise so often in practical work.
REVISED PAGES


--- Page 11 ---
xPreface
PEDAGOGICAL FEATURES
Applications
A broad selection of applications illustrates the power of linear algebra to explain fun-
damental principles and simplify calculations in engineering, computer science, mathe-
matics, physics, biology, economics, and statistics. Some applications appear in separate
sections; others are treated in examples and exercises. In addition, each chapter opens
with an introductory vignette that sets the stage for some application of linear algebra
and provides a motivation for developing the mathematics that follows. Later, the text
returns to that application in a section near the end of the chapter.
A Strong Geometric Emphasis
Every major concept in the course is given a geometric interpretation, because many
students learn better when they can visualize an idea. There are substantially more
drawings here than usual, and some of the Ô¨Ågures have never before appeared in a linear
algebra text. Interactive versions of these Ô¨Ågures, and more, appear in the electronic
version of the textbook.
Examples
This text devotes a larger proportion of its expository material to examples than do most
linear algebra texts. There are more examples than an instructor would ordinarily present
in class. But because the examples are written carefully, with lots of detail, students can
read them on their own.
Theorems and Proofs
Important results are stated as theorems. Other useful facts are displayed in tinted boxes,
for easy reference. Most of the theorems have formal proofs, written with the beginner
student in mind. In a few cases, the essential calculations of a proof are exhibited in a
carefully chosen example. Some routine veriÔ¨Åcations are saved for exercises, when they
will beneÔ¨Åt students.
Practice Problems
A few carefully selected Practice Problems appear just before each exercise set. Com-
plete solutions follow the exercise set. These problems either focus on potential trouble
spots in the exercise set or provide a ‚Äúwarm-up‚Äù for the exercises, and the solutions
often contain helpful hints or warnings about the homework.
Exercises
The abundant supply of exercises ranges from routine computations to conceptual ques-
tions that require more thought. A good number of innovative questions pinpoint con-
ceptual difÔ¨Åculties that we have found on student papers over the years. Each exercise
set is carefully arranged in the same general order as the text; homework assignments
are readily available when only part of a section is discussed. A notable feature of the
exercises is their numerical simplicity. Problems ‚Äúunfold‚Äù quickly, so students spend
little time on numerical calculations. The exercises concentrate on teaching understand-
ing rather than mechanical calculations. The exercises in the Fifth Edition maintain the
integrity of the exercises from previous editions, while providing fresh problems for
students and instructors.
Exercises marked with the symbol [ M] are designed to be worked with the aid of a
‚ÄúMatrix program‚Äù (a computer program, such as MATLAB¬Æ,MapleTM,Mathematica¬Æ,
REVISED PAGES


--- Page 12 ---
Preface xi
MathCad¬Æ, or DeriveTM, or a programmable calculator with matrix capabilities, such as
those manufactured by Texas Instruments).
True/False Questions
To encourage students to read all of the text and to think critically, we have devel-
oped 300 simple true/false questions that appear in 33 sections of the text, just after
the computational problems. They can be answered directly from the text, and they
prepare students for the conceptual problems that follow. Students appreciate these
questions‚Äîafter they get used to the importance of reading the text carefully. Based
on class testing and discussions with students, we decided not to put the answers in the
text. (The Study Guide tells the students where to Ô¨Ånd the answers to the odd-numbered
questions.) An additional 150 true/false questions (mostly at the ends of chapters) test
understanding of the material. The text does provide simple T/F answers to most of
these questions, but it omits the justiÔ¨Åcations for the answers (which usually require
some thought).
Writing Exercises
An ability to write coherent mathematical statements in English is essential for all stu-
dents of linear algebra, not just those who may go to graduate school in mathematics.
The text includes many exercises for which a written justiÔ¨Åcation is part of the answer.
Conceptual exercises that require a short proof usually contain hints that help a student
get started. For all odd-numbered writing exercises, either a solution is included at the
back of the text or a hint is provided and the solution is given in the Study Guide ,
described below.
Computational Topics
The text stresses the impact of the computer on both the development and practice of
linear algebra in science and engineering. Frequent Numerical Notes draw attention
to issues in computing and distinguish between theoretical concepts, such as matrix
inversion, and computer implementations, such as LU factorizations.
WEB SUPPORT
MyMathLab‚ÄìOnline Homework and Resources
Support for the Fifth Edition is offered through MyMathLab ( www.mymathlab.com ).
MyMathLab from Pearson is the world‚Äôs leading online resource in mathematics, inte-
grating interactive homework, assessment, and media in a Ô¨Çexible, easy-to-use format.
MyMathLab contains hundreds of algorithmically generated exercises that mirror those
in the textbook. Students submit homework online for instantaneous feedback, support,
and assessment. This system works particularly well for supporting computation-based
skills. Many additional resources are also provided through the MyMathLab web site.
Interactive Textbook
The Fifth Edition of the text is available in an interactive electronic format within
MyMathLab. Using Wolfram CDF Player, a free Mathematica player available from
Wolfram ( www.wolfram.com/player ), students can interact with Ô¨Ågures and experiment
with matrices by looking at numerous examples. The geometry of linear algebra comes
alive through these interactive Ô¨Ågures. Students are encouraged to develop conjectures
REVISED PAGES


--- Page 13 ---
xii Preface
through experimentation, then verify that their observations are correct by examining
the relevant theorems and their proofs. The resources in the interactive version of the
text give students the opportunity to interact with mathematical objects and ideas much
as we do with our own research.
This web site at www.pearsonhighered.com/lay contains all of the support material
referenced below. These materials are also available within MyMathLab.
Review Material
Review sheets and practice exams (with solutions) cover the main topics in the text.
They come directly from courses we have taught in the past years. Each review sheet
identiÔ¨Åes key deÔ¨Ånitions, theorems, and skills from a speciÔ¨Åed portion of the text.
Applications by Chapters
The web site contains seven Case Studies, which expand topics introduced at the begin-
ning of each chapter, adding real-world data and opportunities for further exploration. In
addition, more than 20 Application Projects either extend topics in the text or introduce
new applications, such as cubic splines, airline Ô¨Çight routes, dominance matrices in
sports competition, and error-correcting codes. Some mathematical applications are
integration techniques, polynomial root location, conic sections, quadric surfaces, and
extrema for functions of two variables. Numerical linear algebra topics, such as con-
dition numbers, matrix factorizations, and the QR method for Ô¨Ånding eigenvalues, are
also included. Woven into each discussion are exercises that may involve large data sets
(and thus require technology for their solution).
Getting Started with Technology
If your course includes some work with MATLAB, Maple, Mathematica, or TI calcula-
tors, the Getting Started guides provide a ‚Äúquick start guide‚Äù for students.
Technology-speciÔ¨Åc projects are also available to introduce students to software
and calculators. They are available on www.pearsonhighered.com/lay and within
MyMathLab. Finally, the Study Guide provides introductory material for Ô¨Årst-time
technology users.
Data Files
Hundreds of Ô¨Åles contain data for about 900 numerical exercises in the text, Case
Studies, and Application Projects. The data are available in a variety of formats‚Äîfor
MATLAB, Maple, Mathematica, and the Texas Instruments graphing calculators. By
allowing students to access matrices and vectors for a particular problem with only a few
keystrokes, the data Ô¨Åles eliminate data entry errors and save time on homework. These
data Ô¨Åles are available for download at www.pearsonhighered.com/lay and MyMathLab.
Projects
Exploratory projects for Mathematica,TMMaple, and MATLAB invite students to dis-
cover basic mathematical and numerical issues in linear algebra. Written by experi-
enced faculty members, these projects are referenced by the icon
WEB at appropriate
points in the text. The projects explore fundamental concepts such as the column space,
diagonalization, and orthogonal projections; several projects focus on numerical issues
such as Ô¨Çops, iterative methods, and the SVD; and a few projects explore applications
such as Lagrange interpolation and Markov chains.
REVISED PAGES


--- Page 14 ---
Preface xiii
SUPPLEMENTS
Study Guide
A printed version of the Study Guide is available at low cost. It is also available electron-
ically within MyMathLab. The Guide is designed to be an integral part of the course. The
icon
SG in the text directs students to special subsections of the Guide that suggest how
to master key concepts of the course. The Guide supplies a detailed solution to every
third odd-numbered exercise, which allows students to check their work. A complete
explanation is provided whenever an odd-numbered writing exercise has only a ‚ÄúHint‚Äù
in the answers. Frequent ‚ÄúWarnings‚Äù identify common errors and show how to prevent
them. MATLAB boxes introduce commands as they are needed. Appendixes in the Study
Guide provide comparable information about Maple, Mathematica, and TI graphing
calculators (ISBN: 0-321-98257-6).
Instructor‚Äôs Edition
For the convenience of instructors, this special edition includes brief answers to all
exercises. A Note to the Instructor at the beginning of the text provides a commentary
on the design and organization of the text, to help instructors plan their courses. It also
describes other support available for instructors (ISBN: 0-321-98261-4).
Instructor‚Äôs Technology Manuals
Each manual provides detailed guidance for integrating a speciÔ¨Åc software package or
graphing calculator throughout the course, written by faculty who have already used
the technology with this text. The following manuals are available to qualiÔ¨Åed instruc-
tors through the Pearson Instructor Resource Center, www.pearsonhighered.com/irc and
MyMathLab: MATLAB (ISBN: 0-321-98985-6), Maple (ISBN: 0-134-04726-5),
Mathematica (ISBN: 0-321-98975-9), and TI-83 C/89 (ISBN: 0-321-98984-8).
Instructor‚Äôs Solutions Manual
The Instructor‚Äô s Solutions Manual (ISBN 0-321-98259-2) contains detailed solutions
for all exercises, along with teaching notes for many sections. The manual is available
electronically for download in the Instructor Resource Center ( www.pearsonhighered.
com/lay ) and MyMathLab.
PowerPoint¬ÆSlides and Other Teaching Tools
A brisk pace at the beginning of the course helps to set the tone for the term. To get
quickly through the Ô¨Årst two sections in fewer than two lectures, consider using
PowerPoint¬Æslides (ISBN 0-321-98264-9). They permit you to focus on the process
of row reduction rather than to write many numbers on the board. Students can receive
a condensed version of the notes, with occasional blanks to Ô¨Åll in during the lecture.
(Many students respond favorably to this gesture.) The PowerPoint slides are available
for 25 core sections of the text. In addition, about 75 color Ô¨Ågures from the text are
available as PowerPoint slides . The PowerPoint slides are available for download at
www.pearsonhighered.com/irc . Interactive Ô¨Ågures are available as Wolfram CDF Player
Ô¨Åles for classroom demonstrations. These Ô¨Åles provide the instructor with the oppor-
tunity to bring the geometry alive and to encourage students to make conjectures by
looking at numerous examples. The Ô¨Åles are available exclusively within MyMathLab.
REVISED PAGES


--- Page 15 ---
xiv Preface
TestGen
TestGen ( www.pearsonhighered.com/testgen ) enables instructors to build, edit, print,
and administer tests using a computized bank of questions developed to cover all the
objectives of the text. TestGen is algorithmically based, allowing instructors to create
multiple, but equivalent, versions of the same question or test with the click of a but-
ton. Instructors can also modify test bank questions or add new questions. The soft-
ware and test bank are available for download from Pearson Education‚Äôs online catalog.
(ISBN: 0-321-98260-6)
ACKNOWLEDGMENTS
I am indeed grateful to many groups of people who have
helped me over the years with various aspects of this book.
I want to thank Israel Gohberg and Robert Ellis for
more than Ô¨Åfteen years of research collaboration, which
greatly shaped my view of linear algebra. And it has been a
privilege to be a member of the Linear Algebra Curriculum
Study Group along with David Carlson, Charles Johnson,
and Duane Porter. Their creative ideas about teaching linear
algebra have inÔ¨Çuenced this text in signiÔ¨Åcant ways.
Saved for last are the three good friends who have
guided the development of the book nearly from the
beginning‚Äîgiving wise counsel and encouragement‚ÄîGreg
Tobin, publisher, Laurie Rosatone, former editor, and
William Hoffman, current editor. Thank you all so much.
David C. Lay
It has been a privilege to work on this new Fifth Edition
of Professor David Lay‚Äôs linear algebra book. In making this
revision, we have attempted to maintain the basic approach
and the clarity of style that has made earlier editions popular
with students and faculty.
We sincerely thank the following reviewers for their
careful analyses and constructive suggestions:
Kasso A. Okoudjou University of Maryland
Falberto Grunbaum University of California - Berkeley
Ed Migliore University of California - Santa Cruz
Maurice E. Ekwo Texas Southern University
M. Cristina Caputo University of Texas at Austin
Esteban G. Tabak New York Unviersity
John M. Alongi Northwestern University
Martina Chirilus-Bruckner Boston University
We thank Thomas Polaski, of Winthrop University, for his
continued contribution of Chapter 10 online.
We thank the technology experts who labored on the
various supplements for the Fifth Edition , preparing thedata, writing notes for the instructors, writing technology
notes for the students in the Study Guide , and sharing their
projects with us: Jeremy Case (MATLAB), Taylor Univer-
sity; Douglas Meade (Maple), University of South Carolina;
Michael Miller (TI Calculator), Western Baptist College;
and Marie Vanisko (Mathematica), Carroll College.
We thank Eric Schulz for sharing his considerable tech-
nological and pedagogical expertise in the creation of in-
teractive electronic textbooks. His help and encouragement
were invaluable in the creation of the electronic interactive
version of this textbook.
We thank Kristina Evans and Phil Oslin for their work in
setting up and maintaining the online homework to accom-
pany the text in MyMathLab, and for continuing to work
with us to improve it. The reviews of the online home-
work done by Joan Saniuk, Robert Pierce, Doron Lubinsky
and Adriana Corinaldesi were greatly appreciated. We also
thank the faculty at University of California Santa Barbara,
University of Alberta, and Georgia Institute of Technology
for their feedback on the MyMathLab course.
We appreciate the mathematical assistance provided by
Roger Lipsett, Paul Lorczak, Tom Wegleitner and Jennifer
Blue, who checked the accuracy of calculations in the text
and the instructor‚Äôs solution manual.
Finally, we sincerely thank the staff at Pearson Edu-
cation for all their help with the development and produc-
tion of the Fifth Edition : Kerri Consalvo, project manager;
Jonathan Wooding, media producer; Jeff Weidenaar, execu-
tive marketing manager; Tatiana Anacki, program manager;
Brooke Smith, marketing assistant; and Salena Casha, edi-
torial assistant. In closing, we thank William Hoffman, the
current editor, for the care and encouragement he has given
to those of us closely involved with this wonderful book.
Steven R. Lay and Judi J. McDonald
REVISED PAGES


--- Page 16 ---
A Note to Students
This course is potentially the most interesting and worthwhile undergraduate mathe-
matics course you will complete. In fact, some students have written or spoken to us
after graduation and said that they still use this text occasionally as a reference in their
careers at major corporations and engineering graduate schools. The following remarks
offer some practical advice and information to help you master the material and enjoy
the course.
In linear algebra, the concepts are as important as the computations. The simple
numerical exercises that begin each exercise set only help you check your understanding
of basic procedures. Later in your career, computers will do the calculations, but you
will have to choose the calculations, know how to interpret the results, and then explain
the results to other people. For this reason, many exercises in the text ask you to explain
or justify your calculations. A written explanation is often required as part of the answer.
For odd-numbered exercises, you will Ô¨Ånd either the desired explanation or at least a
good hint. You must avoid the temptation to look at such answers before you have tried
to write out the solution yourself. Otherwise, you are likely to think you understand
something when in fact you do not.
To master the concepts of linear algebra, you will have to read and reread the text
carefully. New terms are in boldface type, sometimes enclosed in a deÔ¨Ånition box. A
glossary of terms is included at the end of the text. Important facts are stated as theorems
or are enclosed in tinted boxes, for easy reference. We encourage you to read the Ô¨Årst
Ô¨Åve pages of the Preface to learn more about the structure of this text. This will give
you a framework for understanding how the course may proceed.
In a practical sense, linear algebra is a language. You must learn this language the
same way you would a foreign language‚Äîwith daily work. Material presented in one
section is not easily understood unless you have thoroughly studied the text and worked
the exercises for the preceding sections. Keeping up with the course will save you lots
of time and distress!
Numerical Notes
We hope you read the Numerical Notes in the text, even if you are not using a computer
or graphing calculator with the text. In real life, most applications of linear algebra
involve numerical computations that are subject to some numerical error, even though
that error may be extremely small. The Numerical Notes will warn you of potential
difÔ¨Åculties in using linear algebra later in your career, and if you study the notes now,
you are more likely to remember them later.
If you enjoy reading the Numerical Notes, you may want to take a course later in
numerical linear algebra. Because of the high demand for increased computing power,
computer scientists and mathematicians work in numerical linear algebra to develop
faster and more reliable algorithms for computations, and electrical engineers design
faster and smaller computers to run the algorithms. This is an exciting Ô¨Åeld, and your
Ô¨Årst course in linear algebra will help you prepare for it.
REVISED PAGES
xv

--- Page 17 ---
xvi A Note to Students
Study Guide
To help you succeed in this course, we suggest that you purchase the Study
Guide (www.mypearsonstore.com ; 0-321-98257-6). It is available electronically within
MyMathLab. Not only will it help you learn linear algebra, it also will show you how to
study mathematics. At strategic points in your textbook, the icon
SG will direct you to
special subsections in the Study Guide entitled ‚ÄúMastering Linear Algebra Concepts.‚Äù
There you will Ô¨Ånd suggestions for constructing effective review sheets of key concepts.
The act of preparing the sheets is one of the secrets to success in the course, because
you will construct links between ideas. These links are the ‚Äúglue‚Äù that enables you to
build a solid foundation for learning and remembering the main concepts in the course.
TheStudy Guide contains a detailed solution to every third odd-numbered exercise,
plus solutions to all odd-numbered writing exercises for which only a hint is given in the
Answers section of this book. The Guide is separate from the text because you must learn
to write solutions by yourself, without much help. (We know from years of experience
that easy access to solutions in the back of the text slows the mathematical development
of most students.) The Guide also provides warnings of common errors and helpful hints
that call attention to key exercises and potential exam questions.
If you have access to technology‚ÄîMATLAB, Maple, Mathematica, or a TI graph-
ing calculator‚Äîyou can save many hours of homework time. The Study Guide is
your ‚Äúlab manual‚Äù that explains how to use each of these matrix utilities. It intro-
duces new commands when they are needed. You can download from the web site
www.pearsonhighered.com/lay the data for more than 850 exercises in the text. (With
a few keystrokes, you can display any numerical homework problem on your screen.)
Special matrix commands will perform the computations for you!
What you do in your Ô¨Årst few weeks of studying this course will set your pattern
for the term and determine how well you Ô¨Ånish the course. Please read ‚ÄúHow to Study
Linear Algebra‚Äù in the Study Guide as soon as possible. Many students have found the
strategies there very helpful, and we hope you will, too.
REVISED PAGES


--- Page 18 ---
1Linear Equations in
Linear Algebra
INTRODUCTORY EXAMPLE
Linear Models in Economics
and Engineering
It was late summer in 1949. Harvard Professor Wassily
Leontief was carefully feeding the last of his punched cards
into the university‚Äôs Mark II computer. The cards contained
information about the U.S. economy and represented a
summary of more than 250,000 pieces of information
produced by the U.S. Bureau of Labor Statistics after two
years of intensive work. Leontief had divided the U.S.
economy into 500 ‚Äúsectors,‚Äù such as the coal industry,
the automotive industry, communications, and so on.
For each sector, he had written a linear equation that
described how the sector distributed its output to the other
sectors of the economy. Because the Mark II, one of the
largest computers of its day, could not handle the resulting
system of 500 equations in 500 unknowns, Leontief had
distilled the problem into a system of 42 equations in
42 unknowns.
Programming the Mark II computer for Leontief‚Äôs 42
equations had required several months of effort, and he
was anxious to see how long the computer would take to
solve the problem. The Mark II hummed and blinked for 56
hours before Ô¨Ånally producing a solution. We will discuss
the nature of this solution in Sections 1.6 and 2.6.
Leontief, who was awarded the 1973 Nobel Prize
in Economic Science, opened the door to a new era
in mathematical modeling in economics. His effortsat Harvard in 1949 marked one of the Ô¨Årst signiÔ¨Åcant
uses of computers to analyze what was then a large-
scale mathematical model. Since that time, researchers
in many other Ô¨Åelds have employed computers to analyze
mathematical models. Because of the massive amounts of
data involved, the models are usually linear ; that is, they
are described by systems of linear equations .
The importance of linear algebra for applications has
risen in direct proportion to the increase in computing
power, with each new generation of hardware and
software triggering a demand for even greater capabilities.
Computer science is thus intricately linked with linear
algebra through the explosive growth of parallel processing
and large-scale computations.
Scientists and engineers now work on problems far
more complex than even dreamed possible a few decades
ago. Today, linear algebra has more potential value for
students in many scientiÔ¨Åc and business Ô¨Åelds than any
other undergraduate mathematics subject! The material in
this text provides the foundation for further work in many
interesting areas. Here are a few possibilities; others will
be described later.
Oil exploration. When a ship searches for offshore
oil deposits, its computers solve thousands of
separate systems of linear equations every day .
SECOND REVISED PAGES
1

--- Page 19 ---
2CHAPTER 1 Linear Equations in Linear Algebra
The seismic data for the equations are obtained
from underwater shock waves created by explosions
from air guns. The waves bounce off subsurface
rocks and are measured by geophones attached to
mile-long cables behind the ship.
Linear programming. Many important management
decisions today are made on the basis of linear
programming models that use hundreds of variables.
The airline industry, for instance, employs linearprograms that schedule Ô¨Çight crews, monitor the
locations of aircraft, or plan the varied schedules of
support services such as maintenance and terminal
operations.
Electrical networks. Engineers use simulation
software to design electrical circuits and microchips
involving millions of transistors. Such software
relies on linear algebra techniques and systems of
linear equations.
WEB
Systems of linear equations lie at the heart of linear algebra, and this chapter uses them
to introduce some of the central concepts of linear algebra in a simple and concrete
setting. Sections 1.1 and 1.2 present a systematic method for solving systems of linear
equations. This algorithm will be used for computations throughout the text. Sections 1.3
and 1.4 show how a system of linear equations is equivalent to a vector equation and to a
matrix equation . This equivalence will reduce problems involving linear combinations
of vectors to questions about systems of linear equations. The fundamental concepts of
spanning, linear independence, and linear transformations, studied in the second half of
the chapter, will play an essential role throughout the text as we explore the beauty and
power of linear algebra.
1.1 SYSTEMS OF LINEAR EQUATIONS
Alinear equation in the variables x1; : : : ; x nis an equation that can be written in the
form
a1x1Ca2x2C    C anxnDb (1)
where band the coefÔ¨Åcients a1; : : : ; a nare real or complex numbers, usually known
in advance. The subscript nmay be any positive integer. In textbook examples and
exercises, nis normally between 2 and 5. In real-life problems, nmight be 50 or 5000,
or even larger.
The equations
4x1 5x2C2Dx1and x2D2 p
6 x1
Cx3
are both linear because they can be rearranged algebraically as in equation (1):
3x1 5x2D  2and 2x1Cx2 x3D2p
6
The equations
4x1 5x2Dx1x2and x2D2px1 6
are not linear because of the presence of x1x2in the Ô¨Årst equation andpx1in the second.
Asystem of linear equations (or a linear system ) is a collection of one or more
linear equations involving the same variables‚Äîsay, x1; : : : ; x n. An example is
2x1 x2C1:5x 3D8
x1  4x3D  7(2)
SECOND REVISED PAGES


--- Page 20 ---
1.1Systems of Linear Equations 3
Asolution of the system is a list .s1; s2; : : : ; s n/of numbers that makes each equation a
true statement when the values s1; : : : ; s nare substituted for x1; : : : ; x n, respectively. For
instance, .5; 6:5; 3/ is a solution of system (2) because, when these values are substituted
in (2) for x1; x2; x3, respectively, the equations simplify to 8D8and 7D  7.
The set of all possible solutions is called the solution set of the linear system. Two
linear systems are called equivalent if they have the same solution set. That is, each
solution of the Ô¨Årst system is a solution of the second system, and each solution of the
second system is a solution of the Ô¨Årst.
Finding the solution set of a system of two linear equations in two variables is easy
because it amounts to Ô¨Ånding the intersection of two lines. A typical problem is
x1 2x2D  1
 x1C3x2D3
The graphs of these equations are lines, which we denote by `1and`2. A pair of numbers
.x1; x2/satisÔ¨Åes both equations in the system if and only if the point .x1; x2/lies on both
`1and`2. In the system above, the solution is the single point .3; 2/ , as you can easily
verify. See Figure 1.
2
3x2
x1
/lscript1/lscript2
FIGURE 1 Exactly one solution.
Of course, two lines need not intersect in a single point‚Äîthey could be parallel, or
they could coincide and hence ‚Äúintersect‚Äù at every point on the line. Figure 2 shows the
graphs that correspond to the following systems:
(a) x1 2x2D  1
 x1C2x2D3(b) x1 2x2D  1
 x1C2x2D1
/lscript1/lscript22
3x2
x1
(a)
/lscript12
3x2
x1
(b)
FIGURE 2 (a) No solution. (b) InÔ¨Ånitely many solutions.
Figures 1 and 2 illustrate the following general fact about linear systems, to be
veriÔ¨Åed in Section 1.2.
SECOND REVISED PAGES


--- Page 21 ---
4CHAPTER 1 Linear Equations in Linear Algebra
A system of linear equations has
1.no solution, or
2.exactly one solution, or
3.inÔ¨Ånitely many solutions.
A system of linear equations is said to be consistent if it has either one solution or
inÔ¨Ånitely many solutions; a system is inconsistent if it has no solution.
Matrix Notation
The essential information of a linear system can be recorded compactly in a rectangular
array called a matrix . Given the system
x1 2x2Cx3D0
2x2 8x3D8
5x1  5x3D10(3)
with the coefÔ¨Åcients of each variable aligned in columns, the matrix
2
41 2 1
0 2  8
5 0  53
5
is called the coefÔ¨Åcient matrix (ormatrix of coefÔ¨Åcients ) of the system (3), and
2
41 2 1 0
0 2  8 8
5 0  5 103
5 (4)
is called the augmented matrix of the system. (The second row here contains a zero
because the second equation could be written as 0x1C2x2 8x3D8.) An augmented
matrix of a system consists of the coefÔ¨Åcient matrix with an added column containing
the constants from the right sides of the equations.
Thesizeof a matrix tells how many rows and columns it has. The augmented matrix
(4) above has 3 rows and 4 columns and is called a 34(read ‚Äú3 by 4‚Äù) matrix. If mand
nare positive integers, an mnmatrix is a rectangular array of numbers with mrows
andncolumns. (The number of rows always comes Ô¨Årst.) Matrix notation will simplify
the calculations in the examples that follow.
Solving a Linear System
This section and the next describe an algorithm, or a systematic procedure, for solving
linear systems. The basic strategy is to replace one system with an equivalent system
(i.e., one with the same solution set) that is easier to solve .
Roughly speaking, use the x1term in the Ô¨Årst equation of a system to eliminate the
x1terms in the other equations. Then use the x2term in the second equation to eliminate
thex2terms in the other equations, and so on, until you Ô¨Ånally obtain a very simple
equivalent system of equations.
Three basic operations are used to simplify a linear system: Replace one equation
by the sum of itself and a multiple of another equation, interchange two equations, and
multiply all the terms in an equation by a nonzero constant. After the Ô¨Årst example, you
will see why these three operations do not change the solution set of the system.
SECOND REVISED PAGES


--- Page 22 ---
1.1Systems of Linear Equations 5
EXAMPLE 1 Solve system (3).
SOLUTION The elimination procedure is shown here with and without matrix notation,
and the results are placed side by side for comparison:
x1 2x2Cx3D0
2x2 8x3D8
5x1  5x3D102
41 2 1 0
0 2  8 8
5 0  5 103
5
Keep x1in the Ô¨Årst equation and eliminate it from the other equations. To do so, add  5
times equation 1 to equation 3. After some practice, this type of calculation is usually
performed mentally:
 5¬åequation 1¬ç
C¬åequation 3¬ç
¬ånew equation 3¬ç 5x1C10x 2 5x3D0
5x1  5x3D10
10x 2 10x 3D10
The result of this calculation is written in place of the original third equation:
x1 2x2C x3D0
2x2 8x3D8
10x 2 10x 3D102
41 2 1 0
0 2  8 8
0 10  10 103
5
Now, multiply equation 2 by1
2in order to obtain 1 as the coefÔ¨Åcient for x2. (This
calculation will simplify the arithmetic in the next step.)
x1 2x2C x3D0
x2 4x3D4
10x 2 10x 3D102
41 2 1 0
0 1  4 4
0 10  10 103
5
Use the x2in equation 2 to eliminate the 10x 2in equation 3. The ‚Äúmental‚Äô‚Äô computation
is
 10¬åequation 2¬ç
C¬åequation 3¬ç
¬ånew equation 3¬ç 10x 2C40x 3D  40
10x 2 10x 3D10
30x 3D  30
The result of this calculation is written in place of the previous third equation (row):
x1 2x2C x3D 0
x2 4x3D 4
30x 3D  302
41 2 1 0
0 1  4 4
0 0 30  303
5
Now, multiply equation 3 by1
30in order to obtain 1 as the coefÔ¨Åcient for x3. (This
calculation will simplify the arithmetic in the next step.)
x1 2x2Cx3D0
x2 4x3D4
x3D  12
41 2 1 0
0 1  4 4
0 0 1  13
5
The new system has a triangular form (the intuitive term triangular will be replaced by
a precise term in the next section):
x1 2x2Cx3D0
x2 4x3D4
x3D  12
41 2 1 0
0 1  4 4
0 0 1  13
5
SECOND REVISED PAGES


--- Page 23 ---
6CHAPTER 1 Linear Equations in Linear Algebra
Eventually, you want to eliminate the  2x2term from equation 1, but it is more efÔ¨Åcient
to use the x3in equation 3 Ô¨Årst, to eliminate the  4x3andCx3terms in equations 2 and 1.
The two ‚Äúmental‚Äù calculations are
4¬åequation 3¬ç
C¬åequation 2¬ç
¬ånew equation 2¬ç4x3D  4
x2 4x3D4
x2 D0 1¬åequation 3¬ç
C¬åequation 1¬ç
¬ånew equation 1¬ç x3D1
x1 2x2Cx3D0
x1 2x2 D1
It is convenient to combine the results of these two operations:
x1 2x2D1
x2D0
x3D  12
41 2 0 1
0 1 0 0
0 0 1  13
5
Now, having cleaned out the column above the x3in equation 3, move back to the x2in
equation 2 and use it to eliminate the  2x2above it. Because of the previous work with
x3, there is now no arithmetic involving x3terms. Add 2 times equation 2 to equation 1
and obtain the system:
x1 D1
x2D0
x3D  12
41 0 0 1
0 1 0 0
0 0 1  13
5
The work is essentially done. It shows that the only solution of the original system is
x2x3
(1, 0, /H110021)
x1Each of the original equations
determines a plane in
three-dimensional space. The
point .1; 0; 1/lies in all three
planes.
.1; 0; 1/. However, since there are so many calculations involved, it is a good practice
to check the work. To verify that .1; 0; 1/isa solution, substitute these values into the
left side of the original system, and compute:
1.1/ 2.0/C1. 1/D1 0 1D0
2.0/ 8. 1/D 0C8D8
5.1/  5. 1/D5 C5D10
The results agree with the right side of the original system, so .1; 0; 1/is a solution of
the system.
Example 1 illustrates how operations on equations in a linear system correspond to
operations on the appropriate rows of the augmented matrix. The three basic operations
listed earlier correspond to the following operations on the augmented matrix.
ELEMENTARY ROW OPERATIONS
1.(Replacement) Replace one row by the sum of itself and a multiple of another
row.1
2.(Interchange) Interchange two rows.
3.(Scaling) Multiply all entries in a row by a nonzero constant.
Row operations can be applied to any matrix, not merely to one that arises as the
augmented matrix of a linear system. Two matrices are called row equivalent if there
is a sequence of elementary row operations that transforms one matrix into the other.
It is important to note that row operations are reversible . If two rows are inter-
changed, they can be returned to their original positions by another interchange. If a
1A common paraphrase of row replacement is ‚ÄúAdd to one row a multiple of another row.‚Äù
SECOND REVISED PAGES


--- Page 24 ---
1.1Systems of Linear Equations 7
row is scaled by a nonzero constant c, then multiplying the new row by 1=cproduces
the original row. Finally, consider a replacement operation involving two rows‚Äîsay,
rows 1 and 2‚Äîand suppose that ctimes row 1 is added to row 2 to produce a new row
2. To ‚Äúreverse‚Äù this operation, add  ctimes row 1 to (new) row 2 and obtain the original
row 2. See Exercises 29‚Äì32 at the end of this section.
At the moment, we are interested in row operations on the augmented matrix of a
system of linear equations. Suppose a system is changed to a new one via row operations.
By considering each type of row operation, you can see that any solution of the original
system remains a solution of the new system. Conversely, since the original system can
be produced via row operations on the new system, each solution of the new system is
also a solution of the original system. This discussion justiÔ¨Åes the following statement.
If the augmented matrices of two linear systems are row equivalent, then the two
systems have the same solution set.
Though Example 1 is lengthy, you will Ô¨Ånd that after some practice, the calculations
go quickly. Row operations in the text and exercises will usually be extremely easy to
perform, allowing you to focus on the underlying concepts. Still, you must learn to
perform row operations accurately because they will be used throughout the text.
The rest of this section shows how to use row operations to determine the size of a
solution set, without completely solving the linear system.
Existence and Uniqueness Questions
Section 1.2 will show why a solution set for a linear system contains either no solutions,
one solution, or inÔ¨Ånitely many solutions. Answers to the following two questions will
determine the nature of the solution set for a linear system.
To determine which possibility is true for a particular system, we ask two questions.
TWO FUNDAMENTAL QUESTIONS ABOUT A LINEAR SYSTEM
1.Is the system consistent; that is, does at least one solution exist?
2.If a solution exists, is it the only one; that is, is the solution unique ?
These two questions will appear throughout the text, in many different guises. This
section and the next will show how to answer these questions via row operations on
the augmented matrix.
EXAMPLE 2 Determine if the following system is consistent:
x1 2x2Cx3D0
2x2 8x3D8
5x1  5x3D10
SOLUTION This is the system from Example 1. Suppose that we have performed the
row operations necessary to obtain the triangular form
x1 2x2Cx3D0
x2 4x3D4
x3D  12
41 2 1 0
0 1  4 4
0 0 1  13
5
SECOND REVISED PAGES


--- Page 25 ---
8CHAPTER 1 Linear Equations in Linear Algebra
At this point, we know x3. Were we to substitute the value of x3into equation 2, we
could compute x2and hence could determine x1from equation 1. So a solution exists;
the system is consistent. (In fact, x2is uniquely determined by equation 2 since x3has
only one possible value, and x1is therefore uniquely determined by equation 1. So the
solution is unique.)
EXAMPLE 3 Determine if the following system is consistent:
x2 4x3D8
2x1 3x2C2x3D1
4x1 8x2C12x 3D1(5)
SOLUTION The augmented matrix is
2
40 1  4 8
2 3 2 1
4 8 12 13
5
To obtain an x1in the Ô¨Årst equation, interchange rows 1 and 2:
2
42 3 2 1
0 1  4 8
4 8 12 13
5
To eliminate the 4x1term in the third equation, add  2times row 1 to row 3:
2
42 3 2 1
0 1  4 8
0 2 8  13
5 (6)
Next, use the x2term in the second equation to eliminate the  2x2term from the third
equation. Add 2times row 2 to row 3:
2
42 3 2 1
0 1  4 8
0 0 0 153
5 (7)
The augmented matrix is now in triangular form. To interpret it correctly, go back to
equation notation:
2x1 3x2C2x3D1
x2 4x3D8
0D15(8)
The equation 0D15is a short form of 0x1C0x2C0x3D15. This system in trian-
gular form obviously has a built-in contradiction. There are no values of x1; x2; x3that
satisfy (8) because the equation 0D15is never true. Since (8) and (5) have the same
solution set, the original system is inconsistent (i.e., has no solution).
x2
x1
x3
The system is inconsistent because
there is no point that lies on all
three planes.
Pay close attention to the augmented matrix in (7). Its last row is typical of an
inconsistent system in triangular form.
SECOND REVISED PAGES


--- Page 26 ---
1.1Systems of Linear Equations 9
N U M E R I C A L N O T E
In real-world problems, systems of linear equations are solved by a computer.
For a square coefÔ¨Åcient matrix, computer programs nearly always use the elim-
ination algorithm given here and in Section 1.2, modiÔ¨Åed slightly for improved
accuracy.
The vast majority of linear algebra problems in business and industry are
solved with programs that use Ô¨Çoating point arithmetic . Numbers are represented
as decimals :d1  dp10r, where ris an integer and the number pof digits to
the right of the decimal point is usually between 8 and 16. Arithmetic with such
numbers typically is inexact, because the result must be rounded (or truncated)
to the number of digits stored. ‚ÄúRoundoff error‚Äù is also introduced when a
number such as 1=3is entered into the computer, since its decimal representation
must be approximated by a Ô¨Ånite number of digits. Fortunately, inaccuracies in
Ô¨Çoating point arithmetic seldom cause problems. The numerical notes in this
book will occasionally warn of issues that you may need to consider later in your
career.
PRACTICE PROBLEMS
Throughout the text, practice problems should be attempted before working the exer-
cises. Solutions appear after each exercise set.
1.State in words the next elementary row operation that should be performed on the
system in order to solve it. [More than one answer is possible in (a).]
a.x1C4x2 2x3C8x4D12
x2 7x3C2x4D  4
5x3 x4D7
x3C3x4D  5b.x1 3x2C5x3 2x4D0
x2C8x3 D  4
2x3 D3
x4D1
2.The augmented matrix of a linear system has been transformed by row operations
into the form below. Determine if the system is consistent.
2
41 5 2  6
0 4  7 2
0 0 5 03
5
3.Is.3; 4; 2/a solution of the following system?
5x1 x2C2x3D7
 2x1C6x2C9x3D0
 7x1C5x2 3x3D  7
4.For what values of handkis the following system consistent?
2x1 x2Dh
 6x1C3x2Dk
SECOND REVISED PAGES


--- Page 27 ---
10 CHAPTER 1 Linear Equations in Linear Algebra
1.1 EXERCISES
Solve each system in Exercises 1‚Äì4 by using elementary row
operations on the equations or on the augmented matrix. Follow
the systematic elimination procedure described in this section.
1. x1C5x2D7
 2x1 7x2D  52.2x1C4x2D  4
5x1C7x2D11
3.Find the point .x1; x2/that lies on the line x1C5x2D7and
on the line x1 2x2D  2. See the Ô¨Ågure.
x2
x1x1 + 5x2 = 7x1 ‚Äì 2x2 = ‚Äì2
4.Find the point of intersection of the lines x1 5x2D1and
3x1 7x2D5.
Consider each matrix in Exercises 5 and 6 as the augmented matrix
of a linear system. State in words the next two elementary row
operations that should be performed in the process of solving the
system.
5.2
6641 4 5 0 7
0 1  3 0 6
0 0 1 0 2
0 0 0 1  53
775
6.2
6641 6 4 0  1
0 2  7 0 4
0 0 1 2  3
0 0 3 1 63
775
In Exercises 7‚Äì10, the augmented matrix of a linear system has
been reduced by row operations to the form shown. In each case,
continue the appropriate row operations and describe the solution
set of the original system.
7.2
6641 7 3  4
0 1  1 3
0 0 0 1
0 0 1  23
7758.2
41 4 9 0
0 1 7 0
0 0 2 03
5
9.2
6641 1 0 0  4
0 1  3 0  7
0 0 1  3 1
0 0 0 2 43
775
10.2
6641 2 0 3  2
0 1 0  4 7
0 0 1 0 6
0 0 0 1  33
775
Solve the systems in Exercises 11‚Äì14.
11. x2C4x3D  5
x1C3x2C5x3D  2
3x1C7x2C7x3D612. x1 3x2C4x3D  4
3x1 7x2C7x3D  8
 4x1C6x2 x3D7
13. x1  3x3D8
2x1C2x2C9x3D7
x2C5x3D  2
14. x1 3x2 D5
 x1Cx2C5x3D2
x2Cx3D0
Determine if the systems in Exercises 15 and 16 are consistent.
Do not completely solve the systems.
15. x1 C3x3 D2
x2  3x4D3
 2x2C3x3C2x4D1
3x1 C7x4D  5
16. x1  2x4D  3
2x2C2x3 D0
x3C3x4D1
 2x1C3x2C2x3Cx4D5
17. Do the three lines x1 4x2D1,2x1 x2D  3, and
 x1 3x2D4have a common point of intersection?
Explain.
18. Do the three planes x1C2x2Cx3D4,x2 x3D1, and
x1C3x2D0have at least one common point of intersec-
tion? Explain.
In Exercises 19‚Äì22, determine the value(s) of hsuch that the
matrix is the augmented matrix of a consistent linear system.
19.1 h 4
3 6 8
20.1 h  3
 2 4 6
21.1 3  2
 4 h 8
22.2 3 h
 6 9 5
In Exercises 23 and 24, key statements from this section are
either quoted directly, restated slightly (but still true), or altered
in some way that makes them false in some cases. Mark each
statement True or False, and justify your answer. (If true, give the
approximate location where a similar statement appears, or refer
to a deÔ¨Ånition or theorem. If false, give the location of a statement
that has been quoted or used incorrectly, or cite an example that
shows the statement is not true in all cases.) Similar true/false
questions will appear in many sections of the text.
SECOND REVISED PAGES


--- Page 28 ---
1.1Systems of Linear Equations 11
23. a.Every elementary row operation is reversible.
b.A56matrix has six rows.
c.The solution set of a linear system involving variables
x1; : : : ; x nis a list of numbers .s1; : : : ; s n/that makes each
equation in the system a true statement when the values
s1; : : : ; s nare substituted for x1; : : : ; x n, respectively.
d.Two fundamental questions about a linear system involve
existence and uniqueness.
24. a.Elementary row operations on an augmented matrix never
change the solution set of the associated linear system.
b.Two matrices are row equivalent if they have the same
number of rows.
c.An inconsistent system has more than one solution.
d.Two linear systems are equivalent if they have the same
solution set.
25. Find an equation involving g,h, and kthat makes this
augmented matrix correspond to a consistent system:
2
41 4 7 g
0 3  5 h
 2 5  9 k3
5
26. Construct three different augmented matrices for linear sys-
tems whose solution set is x1D  2,x2D1,x3D0.
27. Suppose the system below is consistent for all possible values
offandg. What can you say about the coefÔ¨Åcients candd?
Justify your answer.
x1C3x2Df
cx1Cdx2Dg
28. Suppose a,b,c, and dare constants such that ais not zero
and the system below is consistent for all possible values of
fandg. What can you say about the numbers a,b,c, and d?
Justify your answer.
ax1Cbx2Df
cx1Cdx2Dg
In Exercises 29‚Äì32, Ô¨Ånd the elementary row operation that trans-
forms the Ô¨Årst matrix into the second, and then Ô¨Ånd the reverse
row operation that transforms the second matrix into the Ô¨Årst.29.2
40 2 5
1 4  7
3 1 63
5;2
41 4  7
0 2 5
3 1 63
5
30.2
41 3  4
0 2 6
0 5 93
5;2
41 3  4
0 1  3
0 5 93
5
31.2
41 2 1 0
0 5  2 8
4 1 3  63
5;2
41 2 1 0
0 5  2 8
0 7  1 63
5
32.2
41 2  5 0
0 1  3 2
0 3 9 53
5;2
41 2  5 0
0 1  3 2
0 0 0  13
5
An important concern in the study of heat transfer is to determine
the steady-state temperature distribution of a thin plate when the
temperature around the boundary is known. Assume the plate
shown in the Ô¨Ågure represents a cross section of a metal beam,
with negligible heat Ô¨Çow in the direction perpendicular to the
plate. Let T1; : : : ; T 4denote the temperatures at the four interior
nodes of the mesh in the Ô¨Ågure. The temperature at a node is
approximately equal to the average of the four nearest nodes‚Äî
to the left, above, to the right, and below.2For instance,
T1D.10C20CT2CT4/=4; or4T1 T2 T4D30
10¬∞
10¬∞40¬∞
40¬∞20¬∞20¬∞
30¬∞30¬∞12
43
33. Write a system of four equations whose solution gives esti-
mates for the temperatures T1; : : : ; T 4.
34. Solve the system of equations from Exercise 33. [ Hint: To
speed up the calculations, interchange rows 1 and 4 before
starting ‚Äúreplace‚Äù operations.]
2See Frank M. White, Heat and Mass Transfer (Reading, MA:
Addison-Wesley Publishing, 1991), pp. 145‚Äì149.
SOLUTIONS TO PRACTICE PROBLEMS
1.a.For ‚Äúhand computation,‚Äù the best choice is to interchange equations 3 and 4.
Another possibility is to multiply equation 3 by 1=5. Or, replace equation 4 by
its sum with  1=5times row 3. (In any case, do not use the x2in equation 2 to
eliminate the 4x2in equation 1. Wait until a triangular form has been reached and
thex3terms and x4terms have been eliminated from the Ô¨Årst two equations.)
b.The system is in triangular form. Further simpliÔ¨Åcation begins with the x4in the
fourth equation. Use the x4to eliminate all x4terms above it. The appropriate
SECOND REVISED PAGES


--- Page 29 ---
12 CHAPTER 1 Linear Equations in Linear Algebra
step now is to add 2 times equation 4 to equation 1. (After that, move to equa-
tion 3, multiply it by 1=2, and then use the equation to eliminate the x3terms
above it.)
2.The system corresponding to the augmented matrix is
x1C5x2C2x3D  6
4x2 7x3D2
5x3D0
The third equation makes x3D0, which is certainly an allowable value for x3. After
eliminating the x3terms in equations 1 and 2, you could go on to solve for unique
values for x2andx1. Hence a solution exists, and it is unique. Contrast this situation
with that in Example 3.
3.It is easy to check if a speciÔ¨Åc list of numbers is a solution. Set x1D3,x2D4, and
x3D  2, and Ô¨Ånd that
5.3/ .4/C2. 2/D15 4 4D7
 2.3/C6.4/C9. 2/D   6C24 18D0
 7.3/C5.4/ 3. 2/D  21C20C6D5
Although the Ô¨Årst two equations are satisÔ¨Åed, the third is not, so .3; 4; 2/is not a
solution of the system. Notice the use of parentheses when making the substitutions.
They are strongly recommended as a guard against arithmetic errors.
x3
x2x1
(3, 4, /H110022)Since .3; 4; 2/satisÔ¨Åes the Ô¨Årst
two equations, it is on the line of
the intersection of the Ô¨Årst two
planes. Since .3; 4; 2/does not
satisfy all three equations, it does
not lie on all three planes.
4.When the second equation is replaced by its sum with 3 times the Ô¨Årst equation, the
system becomes
2x1 x2Dh
0DkC3h
IfkC3his nonzero, the system has no solution. The system is consistent for any
values of handkthat make kC3hD0.
1.2 ROW REDUCTION AND ECHELON FORMS
This section reÔ¨Ånes the method of Section 1.1 into a row reduction algorithm that will
enable us to analyze any system of linear equations.1By using only the Ô¨Årst part of
the algorithm, we will be able to answer the fundamental existence and uniqueness
questions posed in Section 1.1.
The algorithm applies to any matrix, whether or not the matrix is viewed as an
augmented matrix for a linear system. So the Ô¨Årst part of this section concerns an arbi-
trary rectangular matrix and begins by introducing two important classes of matrices that
include the ‚Äútriangular‚Äù matrices of Section 1.1. In the deÔ¨Ånitions that follow, a nonzero
row or column in a matrix means a row or column that contains at least one nonzero
entry; a leading entry of a row refers to the leftmost nonzero entry (in a nonzero row).
1The algorithm here is a variant of what is commonly called Gaussian elimination . A similar elimination
method for linear systems was used by Chinese mathematicians in about 250 B.C.The process was unknown
in Western culture until the nineteenth century, when a famous German mathematician, Carl Friedrich Gauss,
discovered it. A German engineer, Wilhelm Jordan, popularized the algorithm in an 1888 text on geodesy.
SECOND REVISED PAGES


--- Page 30 ---
1.2Row Reduction and Echelon Forms 13
D E F I N I T I O N A rectangular matrix is in echelon form (orrow echelon form ) if it has the
following three properties:
1.All nonzero rows are above any rows of all zeros.
2.Each leading entry of a row is in a column to the right of the leading entry of
the row above it.
3.All entries in a column below a leading entry are zeros.
If a matrix in echelon form satisÔ¨Åes the following additional conditions, then it is
inreduced echelon form (orreduced row echelon form ):
4.The leading entry in each nonzero row is 1.
5.Each leading 1 is the only nonzero entry in its column.
Anechelon matrix (respectively, reduced echelon matrix ) is one that is in echelon
form (respectively, reduced echelon form). Property 2 says that the leading entries form
anechelon (‚Äústeplike‚Äù) pattern that moves down and to the right through the matrix.
Property 3 is a simple consequence of property 2, but we include it for emphasis.
The ‚Äútriangular‚Äù matrices of Section 1.1, such as2
42 3 2 1
0 1  4 8
0 0 0 5=23
5 and2
41 0 0 29
0 1 0 16
0 0 1 33
5
are in echelon form. In fact, the second matrix is in reduced echelon form. Here are
additional examples.
EXAMPLE 1 The following matrices are in echelon form. The leading entries ( )
may have any nonzero value; the starred entries ( ) may have any value (including zero).
2
664
0 
0 0 0 0
0 0 0 03
775;2
666640 
0 0 0 
0 0 0 0 
0 0 0 0 0 
0 0 0 0 0 0 0 0 3
77775
The following matrices are in reduced echelon form because the leading entries are 1‚Äôs,
and there are 0‚Äôs below and above each leading 1.
2
66410
01
0 0 0 0
0 0 0 03
775;2
6666401 000 0
0 0 0 100 0
0 0 0 0 10 0
0 0 0 0 0 1 0
0 0 0 0 0 0 0 0 13
77775
Any nonzero matrix may be row reduced (that is, transformed by elementary row
operations) into more than one matrix in echelon form, using different sequences of row
operations. However, the reduced echelon form one obtains from a matrix is unique. The
following theorem is proved in Appendix A at the end of the text.
T H E O R E M 1 Uniqueness of the Reduced Echelon Form
Each matrix is row equivalent to one and only one reduced echelon matrix.
SECOND REVISED PAGES


--- Page 31 ---
14 CHAPTER 1 Linear Equations in Linear Algebra
If a matrix Ais row equivalent to an echelon matrix U, we call Uan echelon form
(or row echelon form) ofA; ifUis in reduced echelon form, we call Uthe reduced
echelon form of A. [Most matrix programs and calculators with matrix capabilities
use the abbreviation RREF for reduced (row) echelon form. Some use REF for (row)
echelon form.]
Pivot Positions
When row operations on a matrix produce an echelon form, further row operations to
obtain the reduced echelon form do not change the positions of the leading entries. Since
the reduced echelon form is unique, the leading entries are always in the same positions
in any echelon form obtained from a given matrix. These leading entries correspond to
leading 1‚Äôs in the reduced echelon form.
D E F I N I T I O N Apivot position in a matrix Ais a location in Athat corresponds to a leading 1
in the reduced echelon form of A. Apivot column is a column of Athat contains
a pivot position.
In Example 1, the squares ( ) identify the pivot positions. Many fundamental con-
cepts in the Ô¨Årst four chapters will be connected in one way or another with pivot
positions in a matrix.
EXAMPLE 2 Row reduce the matrix Abelow to echelon form, and locate the pivot
columns of A.
AD2
6640 3 6 4 9
 1 2 1 3 1
 2 3 0 3  1
1 4 5  9 73
775
SOLUTION Use the same basic strategy as in Section 1.1. The top of the leftmost
nonzero column is the Ô¨Årst pivot position. A nonzero entry, or pivot , must be placed
in this position. A good choice is to interchange rows 1 and 4 (because the mental
computations in the next step will not involve fractions).
2
6641Pivot
4 5  9 7
 1 2 1 3 1
 2 3 0 3  1
0
6Pivot column 3 6 4 93
775
Create zeros below the pivot, 1, by adding multiples of the Ô¨Årst row to the rows below,
and obtain matrix (1) below. The pivot position in the second row must be as far left as
possible‚Äînamely, in the second column. Choose the 2 in this position as the next pivot.
2
6641 4 5  9 7
0 2Pivot
4 6 6
0 5 10  15 15
0 3
6Next pivot column 6 4 93
775(1)
SECOND REVISED PAGES


--- Page 32 ---
1.2Row Reduction and Echelon Forms 15
Add 5=2times row 2 to row 3, and add 3=2times row 2 to row 4.
2
6641 4 5  9 7
0 2 4  6 6
0 0 0 0 0
0 0 0  5 03
775(2)
The matrix in (2) is different from any encountered in Section 1.1. There is no way to
create a leading entry in column 3! (We can‚Äôt use row 1 or 2 because doing so would
destroy the echelon arrangement of the leading entries already produced.) However, if
we interchange rows 3 and 4, we can produce a leading entry in column 4.
2
6641 4 5  9 7
0 2 4  6 6
0 0 0  5Pivot
0
0
66 6 Pivot columns0 0 0 03
775General form:2
664   
0   
0 0 0 
0 0 0 0 03
775
The matrix is in echelon form and thus reveals that columns 1, 2, and 4 of Aare pivot
columns.
AD2
6640

Pivot positions
 3 6 4 9
 1 2 1 3 1
 2 3 0 3 1
1
66 6 Pivot columns4 5  9 73
775(3)
Apivot , as illustrated in Example 2, is a nonzero number in a pivot position that is
used as needed to create zeros via row operations. The pivots in Example 2 were 1, 2,
and 5. Notice that these numbers are not the same as the actual elements of Ain the
highlighted pivot positions shown in (3).
With Example 2 as a guide, we are ready to describe an efÔ¨Åcient procedure for
transforming a matrix into an echelon or reduced echelon matrix. Careful study and
mastery of this procedure now will pay rich dividends later in the course.
The Row Reduction Algorithm
The algorithm that follows consists of four steps, and it produces a matrix in echelon
form. A Ô¨Åfth step produces a matrix in reduced echelon form. We illustrate the algorithm
by an example.
EXAMPLE 3 Apply elementary row operations to transform the following matrix
Ô¨Årst into echelon form and then into reduced echelon form:2
40 3  6 6 4  5
3 7 8  5 8 9
3 9 12  9 6 153
5
SOLUTION
STEP 1
Begin with the leftmost nonzero column. This is a pivot column. The pivot
position is at the top.
SECOND REVISED PAGES


--- Page 33 ---
16 CHAPTER 1 Linear Equations in Linear Algebra
2
40 3  6 6 4  5
3 7 8  5 8 9
3
6Pivot column 9 12  9 6 153
5
STEP 2
Select a nonzero entry in the pivot column as a pivot. If necessary, interchange
rows to move this entry into the pivot position.
Interchange rows 1 and 3. (We could have interchanged rows 1 and 2 instead.)
2
43Pivot
 9 12  9 6 15
3 7 8  5 8 9
0 3  6 6 4  53
5
STEP 3
Use row replacement operations to create zeros in all positions below the pivot.
As a preliminary step, we could divide the top row by the pivot, 3. But with two 3‚Äôs in
column 1, it is just as easy to add  1times row 1 to row 2.
2
43Pivot
 9 12  9 6 15
0 2  4 4 2  6
0 3  6 6 4  53
5
STEP 4
Cover (or ignore) the row containing the pivot position and cover all rows, if any,
above it. Apply steps 1‚Äì3 to the submatrix that remains. Repeat the process until
there are no more nonzero rows to modify.
With row 1 covered, step 1 shows that column 2 is the next pivot column; for step 2,
select as a pivot the ‚Äútop‚Äù entry in that column.
2
43 912 9615
0 2Pivot
 4 4 2  6
0 3
6New pivot column 6 6 4  53
5
For step 3, we could insert an optional step of dividing the ‚Äútop‚Äù row of the submatrix by
the pivot, 2. Instead, we add  3=2times the ‚Äútop‚Äù row to the row below. This produces
2
43 912 9615
0 2  4 4 2  6
0 0 0 0 1 43
5
SECOND REVISED PAGES


--- Page 34 ---
1.2Row Reduction and Echelon Forms 17
When we cover the row containing the second pivot position for step 4, we are left with
a new submatrix having only one row:
2
43 912 9615
02 442 6
0 0 0 0 1
Pivot43
5
Steps 1‚Äì3 require no work for this submatrix, and we have reached an echelon form of
the full matrix. If we want the reduced echelon form, we perform one more step.
STEP 5
Beginning with the rightmost pivot and working upward and to the left, create
zeros above each pivot. If a pivot is not 1, make it 1 by a scaling operation.
The rightmost pivot is in row 3. Create zeros above it, adding suitable multiples of row
3 to rows 2 and 1.
2
43 9 12  9 0  9
0 2  4 4 0  14
0 0 0 0 1 43
5Row 1 C. 6/row 3
Row 2 C. 2/row 3
The next pivot is in row 2. Scale this row, dividing by the pivot.
2
43 9 12  9 0  9
0 1  2 2 0  7
0 0 0 0 1 43
5Row scaled by1
2
Create a zero in column 2 by adding 9 times row 2 to row 1.
2
43 0  6 9 0  72
0 1  2 2 0  7
0 0 0 0 1 43
5Row 1 C.9/row 2
Finally, scale row 1, dividing by the pivot, 3.
2
41 0  2 3 0  24
0 1  2 2 0  7
0 0 0 0 1 43
5Row scaled by1
3
This is the reduced echelon form of the original matrix.
The combination of steps 1‚Äì4 is called the forward phase of the row reduction
algorithm. Step 5, which produces the unique reduced echelon form, is called the back-
ward phase .
N U M E R I C A L N O T E
In step 2 above, a computer program usually selects as a pivot the entry in a
column having the largest absolute value. This strategy, called partial pivoting ,
is used because it reduces roundoff errors in the calculations.
SECOND REVISED PAGES


--- Page 35 ---
18 CHAPTER 1 Linear Equations in Linear Algebra
Solutions of Linear Systems
The row reduction algorithm leads directly to an explicit description of the solution set
of a linear system when the algorithm is applied to the augmented matrix of the system.
Suppose, for example, that the augmented matrix of a linear system has been
changed into the equivalent reduced echelon form
2
41 0  5 1
0 1 1 4
0 0 0 03
5
There are three variables because the augmented matrix has four columns. The
associated system of equations is
x1 5x3D1
x2Cx3D4
0D0(4)
The variables x1andx2corresponding to pivot columns in the matrix are called basic
variables .2The other variable, x3, is called a free variable .
Whenever a system is consistent, as in (4), the solution set can be described
explicitly by solving the reduced system of equations for the basic variables in terms of
the free variables. This operation is possible because the reduced echelon form places
each basic variable in one and only one equation. In (4), solve the Ô¨Årst equation for x1
and the second for x2. (Ignore the third equation; it offers no restriction on the variables.)
8
¬à<
¬à:x1D1C5x3
x2D4 x3
x3is free(5)
The statement ‚Äú x3is free‚Äù means that you are free to choose any value for x3. Once
that is done, the formulas in (5) determine the values for x1andx2. For instance, when
x3D0, the solution is .1; 4; 0/ ; when x3D1, the solution is .6; 3; 1/ .Each different
choice of x3determines a (different) solution of the system, and every solution of the
system is determined by a choice of x3.
EXAMPLE 4 Find the general solution of the linear system whose augmented ma-
trix has been reduced to2
41 6 2  5 2 4
0 0 2  8 1 3
0 0 0 0 1 73
5
SOLUTION The matrix is in echelon form, but we want the reduced echelon form
before solving for the basic variables. The row reduction is completed next. The symbol
before a matrix indicates that the matrix is row equivalent to the preceding matrix.
2
41 6 2  5 2 4
0 0 2  8 1 3
0 0 0 0 1 73
52
41 6 2  5 0 10
0 0 2  8 0 10
0 0 0 0 1 73
5
2
41 6 2  5 0 10
0 0 1  4 0 5
0 0 0 0 1 73
52
41 6 0 3 0 0
0 0 1  4 0 5
0 0 0 0 1 73
5
2Some texts use the term leading variables because they correspond to the columns containing leading
entries.
SECOND REVISED PAGES


--- Page 36 ---
1.2Row Reduction and Echelon Forms 19
There are Ô¨Åve variables because the augmented matrix has six columns. The associated
system now is
x1C6x2C3x4D0
x3 4x4D5
x5D7(6)
The pivot columns of the matrix are 1, 3, and 5, so the basic variables are x1,x3, and x5.
The remaining variables, x2andx4, must be free. Solve for the basic variables to obtain
the general solution:8
¬à¬à¬à¬à¬à¬à<
¬à¬à¬à¬à¬à¬à:x1D  6x2 3x4
x2is free
x3D5C4x4
x4is free
x5D7(7)
Note that the value of x5is already Ô¨Åxed by the third equation in system (6).
Parametric Descriptions of Solution Sets
The descriptions in (5) and (7) are parametric descriptions of solution sets in which
the free variables act as parameters. Solving a system amounts to Ô¨Ånding a parametric
description of the solution set or determining that the solution set is empty.
Whenever a system is consistent and has free variables, the solution set has many
parametric descriptions. For instance, in system (4), we may add 5 times equation 2 to
equation 1 and obtain the equivalent system
x1C5x2 D21
x2Cx3D4
We could treat x2as a parameter and solve for x1andx3in terms of x2, and we would
have an accurate description of the solution set. However, to be consistent, we make the
(arbitrary) convention of always using the free variables as the parameters for describing
a solution set. (The answer section at the end of the text also reÔ¨Çects this convention.)
Whenever a system is inconsistent, the solution set is empty, even when the system
has free variables. In this case, the solution set has noparametric representation.
Back-Substitution
Consider the following system, whose augmented matrix is in echelon form but is not
in reduced echelon form:
x1 7x2C2x3 5x4C8x5D10
x2 3x3C3x4Cx5D  5
x4 x5D4
A computer program would solve this system by back-substitution, rather than by com-
puting the reduced echelon form. That is, the program would solve equation 3 for x4in
terms of x5and substitute the expression for x4into equation 2, solve equation 2 for x2,
and then substitute the expressions for x2andx4into equation 1 and solve for x1.
Our matrix format for the backward phase of row reduction, which produces the re-
duced echelon form, has the same number of arithmetic operations as back-substitution.
But the discipline of the matrix format substantially reduces the likelihood of errors
SECOND REVISED PAGES


--- Page 37 ---
20 CHAPTER 1 Linear Equations in Linear Algebra
during hand computations. The best strategy is to use only the reduced echelon form
to solve a system! The Study Guide that accompanies this text offers several helpful
suggestions for performing row operations accurately and rapidly.
N U M E R I C A L N O T E
In general, the forward phase of row reduction takes much longer than the
backward phase. An algorithm for solving a system is usually measured in Ô¨Çops
(or Ô¨Çoating point operations). A Ô¨Çop is one arithmetic operation ( C; ;; =)
on two real Ô¨Çoating point numbers.3For an n.nC1/matrix, the reduction
to echelon form can take 2n3=3Cn2=2 7n=6 Ô¨Çops (which is approximately
2n3=3Ô¨Çops when nis moderately large‚Äîsay, n30/. In contrast, further
reduction to reduced echelon form needs at most n2Ô¨Çops.
Existence and Uniqueness Questions
Although a nonreduced echelon form is a poor tool for solving a system, this form is
just the right device for answering two fundamental questions posed in Section 1.1.
EXAMPLE 5 Determine the existence and uniqueness of the solutions to the system
3x2 6x3C6x4C4x5D  5
3x1 7x2C8x3 5x4C8x5D9
3x1 9x2C12x 3 9x4C6x5D15
SOLUTION The augmented matrix of this system was row reduced in Example 3 to
2
43 9 12  9 6 15
0 2  4 4 2  6
0 0 0 0 1 43
5 (8)
The basic variables are x1,x2, and x5; the free variables are x3andx4. There is no
equation such as 0D1that would indicate an inconsistent system, so we could use
back-substitution to Ô¨Ånd a solution. But the existence of a solution is already clear in
(8). Also, the solution is not unique because there are free variables. Each different
choice of x3andx4determines a different solution. Thus the system has inÔ¨Ånitely many
solutions.
When a system is in echelon form and contains no equation of the form 0Db, with
bnonzero, every nonzero equation contains a basic variable with a nonzero coefÔ¨Åcient.
Either the basic variables are completely determined (with no free variables) or at least
one of the basic variables may be expressed in terms of one or more free variables. In
the former case, there is a unique solution; in the latter case, there are inÔ¨Ånitely many
solutions (one for each choice of values for the free variables).
These remarks justify the following theorem.
3Traditionally, a Ô¨Çopwas only a multiplication or division, because addition and subtraction took much less
time and could be ignored. The deÔ¨Ånition of Ô¨Çopgiven here is preferred now, as a result of advances in
computer architecture. See Golub and Van Loan, Matrix Computations , 2nd ed. (Baltimore: The Johns
Hopkins Press, 1989), pp. 19‚Äì20.
SECOND REVISED PAGES


--- Page 38 ---
1.2Row Reduction and Echelon Forms 21
T H E O R E M 2 Existence and Uniqueness Theorem
A linear system is consistent if and only if the rightmost column of the augmented
matrix is nota pivot column‚Äîthat is, if and only if an echelon form of the
augmented matrix has norow of the form
¬å0  0 b ¬ç withbnonzero
If a linear system is consistent, then the solution set contains either (i) a unique
solution, when there are no free variables, or (ii) inÔ¨Ånitely many solutions, when
there is at least one free variable.
The following procedure outlines how to Ô¨Ånd and describe all solutions of a linear
system.
USING ROW REDUCTION TO SOLVE A LINEAR SYSTEM
1.Write the augmented matrix of the system.
2.Use the row reduction algorithm to obtain an equivalent augmented matrix in
echelon form. Decide whether the system is consistent. If there is no solution,
stop; otherwise, go to the next step.
3.Continue row reduction to obtain the reduced echelon form.
4.Write the system of equations corresponding to the matrix obtained in step 3.
5.Rewrite each nonzero equation from step 4 so that its one basic variable is
expressed in terms of any free variables appearing in the equation.
PRACTICE PROBLEMS
1.Find the general solution of the linear system whose augmented matrix is
1 3 5 0
0 1  1 1
2.Find the general solution of the system
x1 2x2 x3C3x4D0
 2x1C4x2C5x3 5x4D3
3x1 6x2 6x3C8x4D2
3.Suppose a 47coefÔ¨Åcient matrix for a system of equations has 4 pivots. Is the
system consistent? If the system is consistent, how many solutions are there?
1.2 EXERCISES
In Exercises 1 and 2, determine which matrices are in reduced
echelon form and which others are only in echelon form.
1.a.2
41 0 0 0
0 1 0 0
0 0 1 13
5 b.2
41 0 1 0
0 1 1 0
0 0 0 13
5 c.2
6641 0 0 0
0 1 1 0
0 0 0 0
0 0 0 13
775d.2
6641 1 0 1 1
0 2 0 2 2
0 0 0 3 3
0 0 0 0 43
775
SECOND REVISED PAGES


--- Page 39 ---
22 CHAPTER 1 Linear Equations in Linear Algebra
2.a.2
41 1 0 1
0 0 1 1
0 0 0 03
5 b.2
41 1 0 0
0 1 1 0
0 0 1 13
5
c.2
6641 0 0 0
1 1 0 0
0 1 1 0
0 0 1 13
775
d.2
6640 1 1 1 1
0 0 2 2 2
0 0 0 0 3
0 0 0 0 03
775
Row reduce the matrices in Exercises 3 and 4 to reduced echelon
form. Circle the pivot positions in the Ô¨Ånal matrix and in the
original matrix, and list the pivot columns.
3.2
41 2 3 4
4 5 6 7
6 7 8 93
5 4.2
41 3 5 7
3 5 7 9
5 7 9 13
5
5.Describe the possible echelon forms of a nonzero 22
matrix. Use the symbols ,, and 0, as in the Ô¨Årst part of
Example 1.
6.Repeat Exercise 5 for a nonzero 32matrix.
Find the general solutions of the systems whose augmented ma-
trices are given in Exercises 7‚Äì14.
7.1 3 4 7
3 9 7 6
8.1 4 0 7
2 7 0 10
9.0 1  6 5
1 2 7  6
10.1 2 1 3
3 6 2 2
11.2
43 4 2 0
 9 12  6 0
 6 8  4 03
512.2
41 7 0 6 5
0 0 1  2 3
 1 7  4 2 73
5
13.2
6641 3 0  1 0  2
0 1 0 0  4 1
0 0 0 1 9 4
0 0 0 0 0 03
775
14.2
6641 2  5 6 0  5
0 1  6 3 0 2
0 0 0 0 1 0
0 0 0 0 0 03
775
Exercises 15 and 16 use the notation of Example 1 for matrices
in echelon form. Suppose each matrix represents the augmented
matrix for a system of linear equations. In each case, determine if
the system is consistent. If the system is consistent, determine if
the solution is unique.
15. a.2
4  
0  
0 0 03
5
b.2
40   
0 0  
0 0 0 03
516. a.2
4 
0 
0 0 03
5
b.2
4   
0 0  
0 0 0 3
5
In Exercises 17 and 18, determine the value(s) of hsuch that the
matrix is the augmented matrix of a consistent linear system.
17.2 3 h
4 6 7
18.1 3 2
5 h  7
In Exercises 19 and 20, choose handksuch that the system has
(a) no solution, (b) a unique solution, and (c) many solutions. Give
separate answers for each part.
19. x1Chx2D2
4x1C8x2Dk20. x1C3x2D2
3x1Chx2Dk
In Exercises 21 and 22, mark each statement True or False. Justify
each answer.4
21. a.In some cases, a matrix may be row reduced to more
than one matrix in reduced echelon form, using different
sequences of row operations.
b.The row reduction algorithm applies only to augmented
matrices for a linear system.
c.A basic variable in a linear system is a variable that
corresponds to a pivot column in the coefÔ¨Åcient matrix.
d.Finding a parametric description of the solution set of a
linear system is the same as solving the system.
e.If one row in an echelon form of an augmented matrix
is¬å0 0 0 5 0 ¬ç, then the associated linear system is
inconsistent.
22. a.The echelon form of a matrix is unique.
b.The pivot positions in a matrix depend on whether row
interchanges are used in the row reduction process.
c.Reducing a matrix to echelon form is called the forward
phase of the row reduction process.
d.Whenever a system has free variables, the solution set
contains many solutions.
e.A general solution of a system is an explicit description
of all solutions of the system.
23. Suppose a 35coefÔ¨Åcient matrix for a system has three
pivot columns. Is the system consistent? Why or why not?
24. Suppose a system of linear equations has a 35augmented
matrix whose Ô¨Åfth column is a pivot column. Is the system
consistent? Why (or why not)?
4True/false questions of this type will appear in many sections. Methods
for justifying your answers were described before Exercises 23 and 24 in
Section 1.1.
SECOND REVISED PAGES


--- Page 40 ---
1.2Row Reduction and Echelon Forms 23
25. Suppose the coefÔ¨Åcient matrix of a system of linear equations
has a pivot position in every row. Explain why the system is
consistent.
26. Suppose the coefÔ¨Åcient matrix of a linear system of three
equations in three variables has a pivot in each column.
Explain why the system has a unique solution.
27. Restate the last sentence in Theorem 2 using the concept
of pivot columns: ‚ÄúIf a linear system is consistent, then the
solution is unique if and only if .‚Äù
28. What would you have to know about the pivot columns in an
augmented matrix in order to know that the linear system is
consistent and has a unique solution?
29. A system of linear equations with fewer equations than
unknowns is sometimes called an underdetermined system .
Suppose that such a system happens to be consistent. Explain
why there must be an inÔ¨Ånite number of solutions.
30. Give an example of an inconsistent underdetermined system
of two equations in three unknowns.
31. A system of linear equations with more equations than un-
knowns is sometimes called an overdetermined system . Can
such a system be consistent? Illustrate your answer with a
speciÔ¨Åc system of three equations in two unknowns.
32. Suppose an n.nC1/matrix is row reduced to reduced
echelon form. Approximately what fraction of the total num-
ber of operations (Ô¨Çops) is involved in the backward phase of
the reduction when nD30? when nD300?
Suppose experimental data are represented by a set of points
in the plane. An interpolating polynomial for the data is apolynomial whose graph passes through every point. In scientiÔ¨Åc
work, such a polynomial can be used, for example, to estimate
values between the known data points. Another use is to create
curves for graphical images on a computer screen. One method for
Ô¨Ånding an interpolating polynomial is to solve a system of linear
equations.
WEB
33. Find the interpolating polynomial p.t/Da0Ca1tCa2t2
for the data .1; 12/ ,.2; 15/ ,.3; 16/ . That is, Ô¨Ånd a0,a1, and
a2such that
a0Ca1.1/Ca2.1/2D12
a0Ca1.2/Ca2.2/2D15
a0Ca1.3/Ca2.3/2D16
34. [M] In a wind tunnel experiment, the force on a projectile
due to air resistance was measured at different velocities:
Velocity (100 ft/sec) 0 2 4 6 8 10
Force (100 lb) 0 2.90 14.8 39.6 74.3 119
Find an interpolating polynomial for these data and estimate
the force on the projectile when the projectile is travel-
ing at 750 ft/sec. Use p.t/Da0Ca1tCa2t2Ca3t3Ca4t4
Ca5t5. What happens if you try to use a polynomial of degree
less than 5? (Try a cubic polynomial, for instance.)5
5Exercises marked with the symbol [ M] are designed to be worked
with the aid of a ‚Äú Matrix program‚Äù (a computer program, such as
MATLAB, Maple, Mathematica, MathCad, or Derive, or a
programmable calculator with matrix capabilities, such as those
manufactured by Texas Instruments or Hewlett-Packard).
SOLUTIONS TO PRACTICE PROBLEMS
1.The reduced echelon form of the augmented matrix and the corresponding system
x3
x1
x2
The general solution of the
system of equations is the line of
intersection of the two planes.are1 0  8 3
0 1  1 1
andx1 8x3D  3
x2 x3D  1
The basic variables are x1andx2, and the general solution is
8
¬à<
¬à:x1D  3C8x3
x2D  1Cx3
x3is free
Note: It is essential that the general solution describe each variable, with any param-
eters clearly identiÔ¨Åed. The following statement does notdescribe the solution:
8
¬à<
¬à:x1D  3C8x3
x2D  1Cx3
x3D1Cx2Incorrect solution
This description implies that x2andx3areboth free, which certainly is not the case.
SECOND REVISED PAGES


--- Page 41 ---
24 CHAPTER 1 Linear Equations in Linear Algebra
2.Row reduce the system‚Äôs augmented matrix:
2
41 2 1 3 0
 2 4 5  5 3
3 6 6 8 23
52
41 2 1 3 0
0 0 3 1 3
0 0  3 1 23
5
2
41 2 1 3 0
0 0 3 1 3
0 0 0 0 53
5
This echelon matrix shows that the system is inconsistent , because its rightmost
column is a pivot column; the third row corresponds to the equation 0 = 5. There
is no need to perform any more row operations. Note that the presence of the free
variables in this problem is irrelevant because the system is inconsistent.
3.Since the coefÔ¨Åcient matrix has four pivots, there is a pivot in every row of the
coefÔ¨Åcient matrix. This means that when the coefÔ¨Åcient matrix is row reduced, it
willnothave a row of zeros, thus the corresponding row reduced augmented matrix
can never have a row of the form [ 0 0  0 b], where bis a nonzero number. By
Theorem 2, the system is consistent. Moreover, since there are seven columns in
the coefÔ¨Åcient matrix and only four pivot columns, there will be three free variables
resulting in inÔ¨Ånitely many solutions.
1.3 VECTOR EQUATIONS
Important properties of linear systems can be described with the concept and notation
of vectors. This section connects equations involving vectors to ordinary systems of
equations. The term vector appears in a variety of mathematical and physical contexts,
which we will discuss in Chapter 4, ‚ÄúVector Spaces.‚Äù Until then, vector will mean an
ordered list of numbers . This simple idea enables us to get to interesting and important
applications as quickly as possible.
Vectors in R2
A matrix with only one column is called a column vector , or simply a vector . Examples
of vectors with two entries are
uD3
 1
; vD:2
:3
; wDw1
w2
where w1andw2are any real numbers. The set of all vectors with two entries is denoted
byR2(read ‚Äúr-two‚Äù). The Rstands for the real numbers that appear as entries in the
vectors, and the exponent 2 indicates that each vector contains two entries.1
Two vectors in R2areequal if and only if their corresponding entries are equal.
Thus4
7
and7
4
arenotequal, because vectors in R2areordered pairs of real
numbers.
1Most of the text concerns vectors and matrices that have only real entries. However, all deÔ¨Ånitions and
theorems in Chapters 1‚Äì5, and in most of the rest of the text, remain valid if the entries are complex
numbers. Complex vectors and matrices arise naturally, for example, in electrical engineering and physics.
SECOND REVISED PAGES


--- Page 42 ---
1.3Vector Equations 25
Given two vectors uandvinR2, their sum is the vector uCvobtained by adding
corresponding entries of uandv. For example,
1
 2
C2
5
D1C2
 2C5
D3
3
Given a vector uand a real number c, the scalar multiple ofubycis the vector cu
obtained by multiplying each entry in ubyc. For instance,
ifuD3
 1
and cD5; then cuD53
 1
D15
 5
The number cincuis called a scalar ; it is written in lightface type to distinguish it from
the boldface vector u.
The operations of scalar multiplication and vector addition can be combined, as in
the following example.
EXAMPLE 1 Given uD1
 2
andvD2
 5
, Ô¨Ånd 4u,. 3/v, and 4uC. 3/v.
SOLUTION
4uD4
 8
; .  3/vD 6
15
and
4uC. 3/vD4
 8
C 6
15
D 2
7
Sometimes, for convenience (and also to save space), this text may write a column
vector such as3
 1
in the form .3; 1/. In this case, the parentheses and the comma
distinguish the vector .3; 1/from the 12row matrix3 1
, written with brackets
and no comma. Thus 3
 1
¬§3 1
because the matrices have different shapes, even though they have the same entries.
Geometric Descriptions of R2
Consider a rectangular coordinate system in the plane. Because each point in the plane
is determined by an ordered pair of numbers, we can identify a geometric point .a; b/
with the column vectora
b
. So we may regard R2as the set of all points in the plane.
See Figure 1.
x2
x1(2, 2)
(3, ‚Äì1) (‚Äì2, ‚Äì1)
FIGURE 1 Vectors as points.
x2
x1(2, 2)
(3, ‚Äì1) (‚Äì2, ‚Äì1) FIGURE 2 Vectors with arrows.
SECOND REVISED PAGES


--- Page 43 ---
26 CHAPTER 1 Linear Equations in Linear Algebra
The geometric visualization of a vector such as3
 1
is often aided by including an
arrow (directed line segment) from the origin .0; 0/ to the point .3; 1/, as in Figure 2.
In this case, the individual points along the arrow itself have no special signiÔ¨Åcance.2
The sum of two vectors has a useful geometric representation. The following rule
can be veriÔ¨Åed by analytic geometry.
Parallelogram Rule for Addition
IfuandvinR2are represented as points in the plane, then uCvcorresponds to
the fourth vertex of the parallelogram whose other vertices are u,0, and v. See
Figure 3.
vux2
x1u
vu + v
0
FIGURE 3 The parallelogram rule.
EXAMPLE 2 The vectors uD2
2
,vD 6
1
, and uCvD 4
3
are displayed
in Figure 4.
x2
x1u
vu + v
2 ‚Äì63
FIGURE 4
The next example illustrates the fact that the set of all scalar multiples of one Ô¨Åxed
nonzero vector is a line through the origin, .0; 0/ .
EXAMPLE 3 LetuD3
 1
. Display the vectors u,2u, and 2
3uon a graph.
SOLUTION See Figure 5, where u,2uD6
 2
, and 2
3uD 2
2=3
are displayed.
The arrow for 2uis twice as long as the arrow for u, and the arrows point in the same
direction. The arrow for  2
3uis two-thirds the length of the arrow for u, and the arrows
point in opposite directions. In general, the length of the arrow for cuisjcjtimes the
length of the arrow for u. [Recall that the length of the line segment from .0; 0/ to.a; b/
isp
a2Cb2. We shall discuss this further in Chapter 6.]
2In physics, arrows can represent forces and usually are free to move about in space. This interpretation of
vectors will be discussed in Section 4.1.
SECOND REVISED PAGES


--- Page 44 ---
1.3Vector Equations 27
x2
x1
ux2
x1
u0u
2uu
The set of all multi ples of  u Typical multi ples of  u2
3‚Äì ‚Äì
FIGURE 5
Vectors in R3
Vectors in R3are31column matrices with three entries. They are represented ge-
ometrically by points in a three-dimensional coordinate space, with arrows from the
2a
a
x2 x1x3
FIGURE 6
Scalar multiples.origin sometimes included for visual clarity. The vectors aD2
42
3
43
5and2aare displayed
in Figure 6.
Vectors in Rn
Ifnis a positive integer, Rn(read ‚Äúr-n‚Äù) denotes the collection of all lists (or ordered
n-tuples ) ofnreal numbers, usually written as n1column matrices, such as
uD2
6664u1
u2
:::
un3
7775
The vector whose entries are all zero is called the zero vector and is denoted by 0.
(The number of entries in 0will be clear from the context.)
Equality of vectors in Rnand the operations of scalar multiplication and vector
addition in Rnare deÔ¨Åned entry by entry just as in R2. These operations on vectors
have the following properties, which can be veriÔ¨Åed directly from the corresponding
properties for real numbers. See Practice Problem 1 and Exercises 33 and 34 at the end
of this section.
Algebraic Properties of Rn
For all u;v;winRnand all scalars candd:
(i)uCvDvCu (v)c.uCv/DcuCcv
(ii).uCv/CwDuC.vCw/ (vi).cCd/uDcuCdu
(iii) uC0D0CuDu (vii) c.du/D.cd/ u
(iv) uC. u/D  uCuD0, (viii) 1uDu
where  udenotes . 1/u
For simplicity of notation, a vector such as uC. 1/vis often written as u v.
Figure 7 shows u vas the sum of uand v.
x1x2
v
u
‚Äìv
u ‚Äì vFIGURE 7
Vector subtraction.
SECOND REVISED PAGES


--- Page 45 ---
28 CHAPTER 1 Linear Equations in Linear Algebra
Linear Combinations
Given vectors v1;v2; : : : ; vpinRnand given scalars c1; c2; : : : ; c p, the vector ydeÔ¨Åned
by
yDc1v1C    C cpvp
is called a linear combination ofv1; : : : ; vpwith weights c1; : : : ; c p. Property (ii) above
permits us to omit parentheses when forming such a linear combination. The weights in
a linear combination can be any real numbers, including zero. For example, some linear
combinations of vectors v1andv2arep
3v1Cv2;1
2v1.D1
2v1C0v2/;and 0.D0v1C0v2/
EXAMPLE 4 Figure 8 identiÔ¨Åes selected linear combinations of v1D 1
1
and
v2D2
1
. (Note that sets of parallel grid lines are drawn through integer multiples of
v1andv2.) Estimate the linear combinations of v1andv2that generate the vectors uand
w.
3v2
2v22v1
‚Äì2v1‚Äì2v2v1 ‚Äì v2‚Äì2v1 + v23v1
v1
‚Äìv1‚Äìv2w
uv2v1 + v2 3v23
2‚Äì
0
FIGURE 8 Linear combinations of v1andv2.
SOLUTION The parallelogram rule shows that uis the sum of 3v1and 2v2; that is,
uD3v1 2v2
This expression for ucan be interpreted as instructions for traveling from the origin to u
along two straight paths. First, travel 3 units in the v1direction to 3v1, and then travel  2
units in the v2direction (parallel to the line through v2and0). Next, although the vector
wis not on a grid line, wappears to be about halfway between two pairs of grid lines,
at the vertex of a parallelogram determined by .5=2/ v1and. 1=2/ v2. (See Figure 9.)
Thus a reasonable estimate for wis
v1w
‚Äìv22v13v1
0
FIGURE 9wD5
2v1 1
2v2
The next example connects a problem about linear combinations to the fundamental
existence question studied in Sections 1.1 and 1.2.
EXAMPLE 5 Leta1D2
41
 2
 53
5,a2D2
42
5
63
5, and bD2
47
4
 33
5. Determine whether
bcan be generated (or written) as a linear combination of a1anda2. That is, determine
whether weights x1andx2exist such that
x1a1Cx2a2Db (1)
If vector equation (1) has a solution, Ô¨Ånd it.
SECOND REVISED PAGES


--- Page 46 ---
1.3Vector Equations 29
SOLUTION Use the deÔ¨Ånitions of scalar multiplication and vector addition to rewrite
the vector equation
x12
41
 2
 53
5
6
a1Cx22
42
5
63
5
6
a2D2
47
4
 33
5
6
b
which is the same as 2
4x1
 2x1
 5x13
5C2
42x2
5x2
6x23
5D2
47
4
 33
5
and 2
4x1C2x2
 2x1C5x2
 5x1C6x23
5D2
47
4
 33
5 (2)
The vectors on the left and right sides of (2) are equal if and only if their corresponding
entries are both equal. That is, x1andx2make the vector equation (1) true if and only
ifx1andx2satisfy the system
x1C2x2D7
 2x1C5x2D4
 5x1C6x2D  3(3)
To solve this system, row reduce the augmented matrix of the system as follows:3
2
41 2 7
 2 5 4
 5 6  33
52
41 2 7
0 9 18
0 16 323
52
41 2 7
0 1 2
0 16 323
52
41 0 3
0 1 2
0 0 03
5
The solution of (3) is x1D3andx2D2. Hence bis a linear combination of a1anda2,
with weights x1D3andx2D2. That is,
32
41
 2
 53
5C22
42
5
63
5D2
47
4
 33
5
Observe in Example 5 that the original vectors a1,a2, and bare the columns of the
augmented matrix that we row reduced:
2
41 2 7
 2 5 4
 5
666
a1 a2b6 33
5
For brevity, write this matrix in a way that identiÔ¨Åes its columns‚Äînamely,
¬åa1a2b¬ç (4)
It is clear how to write this augmented matrix immediately from vector equation (1),
without going through the intermediate steps of Example 5. Take the vectors in the
order in which they appear in (1) and put them into the columns of a matrix as in (4).
The discussion above is easily modiÔ¨Åed to establish the following fundamental fact.
3The symbol between matrices denotes row equivalence (Section 1.2).
SECOND REVISED PAGES


--- Page 47 ---
30 CHAPTER 1 Linear Equations in Linear Algebra
A vector equation
x1a1Cx2a2C    C xnanDb
has the same solution set as the linear system whose augmented matrix is
a1 a2   an b
(5)
In particular, bcan be generated by a linear combination of a1; : : : ; anif and only
if there exists a solution to the linear system corresponding to the matrix (5).
One of the key ideas in linear algebra is to study the set of all vectors that can be
generated or written as a linear combination of a Ô¨Åxed set fv1; : : : ; vpgof vectors.
D E F I N I T I O N Ifv1; : : : ; vpare inRn, then the set of all linear combinations of v1; : : : ; vp
is denoted by Span fv1; : : : ; vpgand is called the subset of Rnspanned (or
generated )by v 1; : : : ; vp. That is, Span fv1; : : : ; vpgis the collection of all vectors
that can be written in the form
c1v1Cc2v2C    C cpvp
withc1; : : : ; c pscalars.
Asking whether a vector bis in Span fv1; : : : ; vpgamounts to asking whether the
vector equation
x1v1Cx2v2C    C xpvpDb
has a solution, or, equivalently, asking whether the linear system with augmented matrix
¬åv1   vpb¬çhas a solution.
Note that Span fv1; : : : ; vpgcontains every scalar multiple of v1(for exam-
ple), since cv1Dcv1C0v2C    C 0vp. In particular, the zero vector must be in
Spanfv1; : : : ; vpg.
A Geometric Description of Span fvgand Span fu,vg
Letvbe a nonzero vector in R3. Then Span fvgis the set of all scalar multiples of v,
which is the set of points on the line in R3through vand0. See Figure 10.
Ifuandvare nonzero vectors in R3, with vnot a multiple of u, then Span fu;vgis
the plane in R3that contains u,v, and 0. In particular, Span fu;vgcontains the line in
R3through uand0and the line through vand0. See Figure 11.
Span{ v}x3
x2
x1v
FIGURE 10 Spanfvgas a
line through the origin.
Span{u, v}
vu
u /H11545 v
x2
x3
x1
FIGURE 11 Spanfu;vgas a
plane through the origin.
SECOND REVISED PAGES


--- Page 48 ---
1.3Vector Equations 31
EXAMPLE 6 Let a1D2
41
 2
33
5,a2D2
45
 13
 33
5, and bD2
4 3
8
13
5. Then
Spanfa1;a2gis a plane through the origin in R3. Isbin that plane?
SOLUTION Does the equation x1a1Cx2a2Dbhave a solution? To answer this, row
reduce the augmented matrix ¬åa1a2b¬ç:
2
41 5  3
 2 13 8
3 3 13
52
41 5  3
0 3 2
0 18 103
52
41 5  3
0 3 2
0 0  23
5
The third equation is 0D  2, which shows that the system has no solution. The vector
equation x1a1Cx2a2Dbhas no solution, and so bisnotin Span fa1;a2g.
Linear Combinations in Applications
The Ô¨Ånal example shows how scalar multiples and linear combinations can arise when
a quantity such as ‚Äúcost‚Äù is broken down into several categories. The basic principle for
the example concerns the cost of producing several units of an item when the cost per
unit is known:number
of units
cost
per unit
Dtotal
cost
EXAMPLE 7 A company manufactures two products. For $1.00 worth of product
B, the company spends $.45 on materials, $.25 on labor, and $.15 on overhead. For $1.00
worth of product C, the company spends $.40 on materials, $.30 on labor, and $.15 on
overhead. Let
bD2
4:45
:25
:153
5and cD2
4:40
:30
:153
5
Then bandcrepresent the ‚Äúcosts per dollar of income‚Äù for the two products.
a.What economic interpretation can be given to the vector 100b?
b.Suppose the company wishes to manufacture x1dollars worth of product B and
x2dollars worth of product C. Give a vector that describes the various costs the
company will have (for materials, labor, and overhead).
SOLUTION
a.Compute
100bD1002
4:45
:25
:153
5D2
445
25
153
5
The vector 100blists the various costs for producing $100 worth of product B‚Äî
namely, $45 for materials, $25 for labor, and $15 for overhead.
b.The costs of manufacturing x1dollars worth of B are given by the vector x1b, and
the costs of manufacturing x2dollars worth of C are given by x2c. Hence the total
costs for both products are given by the vector x1bCx2c.
SECOND REVISED PAGES


--- Page 49 ---
32 CHAPTER 1 Linear Equations in Linear Algebra
PRACTICE PROBLEMS
1.Prove that uCvDvCufor any uandvinRn.
2.For what value(s) of hwillybe in Span fv1;v2;v3gif
v1D2
41
 1
 23
5; v2D2
45
 4
 73
5; v3D2
4 3
1
03
5;and yD2
4 4
3
h3
5
3.Letw1,w2,w3,u, and vbe vectors in Rn. Suppose the vectors uandvare in Span
fw1,w2,w3g. Show that uCvis also in Span fw1,w2,w3g. [Hint: The solution to
Practice Problem 3 requires the use of the deÔ¨Ånition of the span of a set of vectors.
It is useful to review this deÔ¨Ånition on Page 30 before starting this exercise.]
1.3 EXERCISES
In Exercises 1 and 2, compute uCvandu 2v.
1.uD 1
2
;vD 3
 1
2.uD3
2
;vD2
 1
In Exercises 3 and 4, display the following vectors using arrows
on an xy-graph: u,v, v, 2v,uCv,u v, and u 2v. Notice
thatu vis the vertex of a parallelogram whose other vertices are
u,0, and v.
3.uandvas in Exercise 1 4. u andvas in Exercise 2
In Exercises 5 and 6, write a system of equations that is equivalent
to the given vector equation.
5.x12
46
 1
53
5Cx22
4 3
4
03
5D2
41
 7
 53
5
6.x1 2
3
Cx28
5
Cx31
 6
D0
0
Use the accompanying Ô¨Ågure to write each vector listed in Exer-
cises 7 and 8 as a linear combination of uandv. Is every vector
inR2a linear combination of uandv?
w
xvu
acd
2vb
zy‚Äì2v‚Äìu‚Äìv0
7.Vectors a,b,c, and d
8.Vectors w,x,y, and zIn Exercises 9 and 10, write a vector equation that is equivalent to
the given system of equations.
9. x2C5x3D0
4x1C6x2 x3D0
 x1C3x2 8x3D010.4x1Cx2C3x3D9
x1 7x2 2x3D2
8x1C6x2 5x3D15
In Exercises 11 and 12, determine if bis a linear combination of
a1,a2, and a3.
11.a1D2
41
 2
03
5;a2D2
40
1
23
5;a3D2
45
 6
83
5;bD2
42
 1
63
5
12. a1D2
41
 2
23
5;a2D2
40
5
53
5;a3D2
42
0
83
5;bD2
4 5
11
 73
5
In Exercises 13 and 14, determine if bis a linear combination of
the vectors formed from the columns of the matrix A.
13.AD2
41 4 2
0 3 5
 2 8  43
5;bD2
43
 7
 33
5
14.AD2
41 2 6
0 3 7
1 2 53
5;bD2
411
 5
93
5
In Exercises 15 and 16, list Ô¨Åve vectors in Span fv1;v2g. For each
vector, show the weights on v1andv2used to generate the vector
and list the three entries of the vector. Do not make a sketch.
15. v1D2
47
1
 63
5;v2D2
4 5
3
03
5
16. v1D2
43
0
23
5;v2D2
4 2
0
33
5
SECOND REVISED PAGES


--- Page 50 ---
1.3Vector Equations 33
17. Leta1D2
41
4
 23
5,a2D2
4 2
 3
73
5, and bD2
44
1
h3
5. For what
value(s) of hisbin the plane spanned by a1anda2?
18. Letv1D2
41
0
 23
5,v2D2
4 3
1
83
5, and yD2
4h
 5
 33
5. For what
value(s) of hisyin the plane generated by v1andv2?
19. Give a geometric description of Span fv1;v2gfor the vectors
v1D2
48
2
 63
5andv2D2
412
3
 93
5.
20. Give a geometric description of Span fv1;v2gfor the vectors
in Exercise 16.
21. Let uD2
 1
and vD2
1
. Show thath
k
is in
Spanfu;vgfor all handk.
22. Construct a 33matrix A, with nonzero entries, and a vector
binR3such that bisnotin the set spanned by the columns
ofA.
In Exercises 23 and 24, mark each statement True or False. Justify
each answer.
23. a.Another notation for the vector 4
3
is¬å 4 3 ¬ç.
b.The points in the plane corresponding to 2
5
and
 5
2
lie on a line through the origin.
c.An example of a linear combination of vectors v1andv2
is the vector1
2v1.
d.The solution set of the linear system whose augmented
matrix is ¬åa1a2a3b¬çis the same as the solution
set of the equation x1a1Cx2a2Cx3a3Db.
e.The set Span fu;vgis always visualized as a plane
through the origin.
24. a.Any list of Ô¨Åve real numbers is a vector in R5.
b.The vector uresults when a vector u vis added to the
vector v.
c.The weights c1; : : : ; c pin a linear combination
c1v1C    C cpvpcannot all be zero.
d.When uandvare nonzero vectors, Span fu;vgcontains
the line through uand the origin.
e.Asking whether the linear system corresponding to
an augmented matrix ¬åa1a2a3b¬çhas a solution
amounts to asking whether bis in Span fa1;a2;a3g.
25. LetAD2
41 0  4
0 3  2
 2 6 33
5and bD2
44
1
 43
5. Denote the
columns of Abya1,a2,a3, and let WDSpanfa1;a2;a3g.a.Isbinfa1;a2;a3g? How many vectors are in fa1;a2;a3g?
b.IsbinW? How many vectors are in W?
c.Show that a1is inW. [Hint: Row operations are unnec-
essary.]
26. LetAD2
42 0 6
 1 8 5
1 2 13
5, let bD2
410
3
33
5, and let Wbe
the set of all linear combinations of the columns of A.
a.IsbinW?
b.Show that the third column of Ais inW.
27. A mining company has two mines. One day‚Äôs operation at
mine #1 produces ore that contains 20 metric tons of cop-
per and 550 kilograms of silver, while one day‚Äôs operation
at mine #2 produces ore that contains 30 metric tons of
copper and 500 kilograms of silver. Let v1D20
550
and
v2D30
500
. Then v1andv2represent the ‚Äúoutput per day‚Äù
of mine #1 and mine #2, respectively.
a.What physical interpretation can be given to the vector
5v1?
b.Suppose the company operates mine #1 for x1days and
mine #2 for x2days. Write a vector equation whose solu-
tion gives the number of days each mine should operate in
order to produce 150 tons of copper and 2825 kilograms
of silver. Do not solve the equation.
c.[M] Solve the equation in (b).
28. A steam plant burns two types of coal: anthracite (A) and
bituminous (B). For each ton of A burned, the plant produces
27.6 million Btu of heat, 3100 grams (g) of sulfur dioxide,
and 250 g of particulate matter (solid-particle pollutants). For
each ton of B burned, the plant produces 30.2 million Btu,
6400 g of sulfur dioxide, and 360 g of particulate matter.
a.How much heat does the steam plant produce when it
burns x1tons of A and x2tons of B?
b.Suppose the output of the steam plant is described by
a vector that lists the amounts of heat, sulfur dioxide,
and particulate matter. Express this output as a linear
combination of two vectors, assuming that the plant burns
x1tons of A and x2tons of B.
c.[M] Over a certain time period, the steam plant produced
162 million Btu of heat, 23,610 g of sulfur dioxide, and
1623 g of particulate matter. Determine how many tons
of each type of coal the steam plant must have burned.
Include a vector equation as part of your solution.
29. Let v1; : : : ; vkbe points in R3and suppose that for
jD1; : : : ; k an object with mass mjis located at point vj.
Physicists call such objects point masses . The total mass of
the system of point masses is
mDm1C    C mk
SECOND REVISED PAGES


--- Page 51 ---
34 CHAPTER 1 Linear Equations in Linear Algebra
Thecenter of gravity (orcenter of mass ) of the system is
vD1
m¬åm1v1C    C mkvk¬ç
Compute the center of gravity of the system consisting of the
following point masses (see the Ô¨Ågure):
Point Mass
v1D.5; 4; 3/ 2 g
v2D.4; 3; 2/ 5 g
v3D. 4; 3; 1/ 2 g
v4D. 9; 8; 6/ 1 g
x3
v4
x2
v2v3x1v1
30. Let vbe the center of mass of a system of point
masses located at v1; : : : ; vkas in Exercise 29. Is vin
Spanfv1; : : : ; vkg? Explain.
31. A thin triangular plate of uniform density and thickness has
vertices at v1D.0; 1/ ,v2D.8; 1/ , and v3D.2; 4/ , as in the
Ô¨Ågure below, and the mass of the plate is 3 g.
v2v3
v1
x14
8x2a.Find the .x; y/ -coordinates of the center of mass of the
plate. This ‚Äúbalance point‚Äù of the plate coincides with
the center of mass of a system consisting of three 1-gram
point masses located at the vertices of the plate.
b.Determine how to distribute an additional mass of 6 g
at the three vertices of the plate to move the balance
point of the plate to .2; 2/ . [Hint: Letw1,w2, and w3
denote the masses added at the three vertices, so that
w1Cw2Cw3D6.]
32. Consider the vectors v1,v2,v3, and binR2, shown in the
Ô¨Ågure. Does the equation x1v1Cx2v2Cx3v3Dbhave a
solution? Is the solution unique? Use the Ô¨Ågure to explain
your answers.
0v1v2v3
b
33. Use the vectors uD.u1; : : : ; u n/,vD.v1; : : : ; v n/, and
wD.w1; : : : ; w n/to verify the following algebraic proper-
ties ofRn.
a..uCv/CwDuC.vCw/
b.c.uCv/DcuCcvfor each scalar c
34. Use the vector uD.u1; : : : ; u n/to verify the following alge-
braic properties of Rn.
a.uC. u/D. u/CuD0
b.c.du/D.cd/ ufor all scalars candd
SOLUTIONS TO PRACTICE PROBLEMS
1.Take arbitrary vectors uD.u1; : : : ; u n/andvD.v1; : : : ; v n/inRn, and compute
uCvD.u1Cv1; : : : ; u nCvn/ DeÔ¨Ånition of vector addition
D.v1Cu1; : : : ; v nCun/ Commutativity of addition in R
DvCu DeÔ¨Ånition of vector addition
2.The vector ybelongs to Span fv1;v2;v3gif and only if there exist scalars x1; x2; x3
such that
x12
41
 1
 23
5Cx22
45
 4
 73
5Cx32
4 3
1
03
5D2
4 4
3
h3
5
This vector equation is equivalent to a system of three linear equations in three
unknowns. If you row reduce the augmented matrix for this system, you Ô¨Ånd that
h/H11005 1h/H11005 5
v3v1v2
The points
intersects the plane when h /H11005 5./H110024
3
hlie on a line thath/H11005 9
Span { v1, v2, v3}
SECOND REVISED PAGES


--- Page 52 ---
1.4The Matrix Equation AxDb35
2
41 5  3 4
 1 4 1 3
 2 7 0 h3
52
41 5  3  4
0 1  2  1
0 3  6 h  83
52
41 5  3  4
0 1  2  1
0 0 0 h  53
5
The system is consistent if and only if there is no pivot in the fourth column. That
is,h 5must be 0. So yis in Span fv1;v2;v3gif and only if hD5.
Remember: The presence of a free variable in a system does not guarantee that the
system is consistent.
3.Since the vectors uandvare in Span fw1;w2;w3g, there exist scalars c1,c2,c3and
d1,d2,d3such that
uDc1w1Cc2w2Cc3w3and vDd1w1Cd2w2Cd3w3:
Notice
uCvD c1w1Cc2w2Cc3w3Cd1w1Cd2w2Cd3w3
D .c1Cd1/w1C.c2Cd2/w2C.c3Cd3/w3
Since c1Cd1; c2Cd2, and c3Cd3are also scalars, the vector uCvis in Span
fw1;w2;w3g.
1.4 THE MATRIX EQUATION Ax=b
A fundamental idea in linear algebra is to view a linear combination of vectors as the
product of a matrix and a vector. The following deÔ¨Ånition permits us to rephrase some
of the concepts of Section 1.3 in new ways.
D E F I N I T I O N IfAis an mnmatrix, with columns a1; : : : ; an, and if xis inRn, then the
product of Aand x , denoted by Ax, isthe linear combination of the columns
ofAusing the corresponding entries in x as weights ; that is,
AxDa1a2   an2
64x1
:::
xn3
75Dx1a1Cx2a2C    C xnan
Note that Axis deÔ¨Åned only if the number of columns of Aequals the number of entries
inx.
EXAMPLE 1
a.1 2  1
0 5 32
44
3
73
5D41
0
C32
 5
C7 1
3
D4
0
C6
 15
C 7
21
D3
6
b.2
42 3
8 0
 5 23
54
7
D42
42
8
 53
5C72
4 3
0
23
5D2
48
32
 203
5C2
4 21
0
143
5D2
4 13
32
 63
5
EXAMPLE 2 Forv1;v2;v3inRm, write the linear combination 3v1 5v2C7v3as
a matrix times a vector.
SECOND REVISED PAGES


--- Page 53 ---
36 CHAPTER 1 Linear Equations in Linear Algebra
SOLUTION Place v1;v2;v3into the columns of a matrix Aand place the weights 3,  5,
and 7 into a vector x. That is,
3v1 5v2C7v3Dv1v2v32
43
 5
73
5DAx
Section 1.3 showed how to write a system of linear equations as a vector equation
involving a linear combination of vectors. For example, the system
x1C2x2 x3D4
 5x2C3x3D1(1)
is equivalent to
x11
0
Cx22
 5
Cx3 1
3
D4
1
(2)
As in Example 2, the linear combination on the left side is a matrix times a vector, so
that (2) becomes
1 2  1
0 5 32
4x1
x2
x33
5D4
1
(3)
Equation (3) has the form AxDb. Such an equation is called a matrix equation ,
to distinguish it from a vector equation such as is shown in (2).
Notice how the matrix in (3) is just the matrix of coefÔ¨Åcients of the system (1).
Similar calculations show that any system of linear equations, or any vector equation
such as (2), can be written as an equivalent matrix equation in the form AxDb. This
simple observation will be used repeatedly throughout the text.
Here is the formal result.
T H E O R E M 3 IfAis an mnmatrix, with columns a1; : : : ; an, and if bis inRm, the matrix
equation
AxDb (4)
has the same solution set as the vector equation
x1a1Cx2a2C    C xnanDb (5)
which, in turn, has the same solution set as the system of linear equations whose
augmented matrix isa1a2   anb
(6)
Theorem 3 provides a powerful tool for gaining insight into problems in linear
algebra, because a system of linear equations may now be viewed in three different
but equivalent ways: as a matrix equation, as a vector equation, or as a system of linear
equations. Whenever you construct a mathematical model of a problem in real life, you
are free to choose whichever viewpoint is most natural. Then you may switch from one
formulation of a problem to another whenever it is convenient. In any case, the matrix
equation (4), the vector equation (5), and the system of equations are all solved in the
same way‚Äîby row reducing the augmented matrix (6). Other methods of solution will
be discussed later.
SECOND REVISED PAGES


--- Page 54 ---
1.4The Matrix Equation AxDb37
Existence of Solutions
The deÔ¨Ånition of Axleads directly to the following useful fact.
The equation AxDbhas a solution if and only if bis a linear combination of the
columns of A.
Section 1.3 considered the existence question, ‚ÄúIs bin Span fa1; : : : ; ang?‚Äù Equiva-
lently, ‚ÄúIs AxDbconsistent?‚Äù A harder existence problem is to determine whether the
equation AxDbis consistent for all possible b.
EXAMPLE 3 LetAD2
41 3 4
 4 2  6
 3 2 73
5andbD2
4b1
b2
b33
5. Is the equation AxDb
consistent for all possible b1; b2; b3?
SOLUTION Row reduce the augmented matrix for AxDb:
2
41 3 4 b 1
 4 2  6 b 2
 3 2 7 b 33
52
41 3 4 b 1
0 14 10 b 2C4b1
0 7 5 b 3C3b13
5
2
41 3 4 b1
0 14 10 b2C4b1
0 0 0 b 3C3b1 1
2.b2C4b1/3
5
The third entry in column 4 equals b1 1
2b2Cb3. The equation AxDbisnotconsistent
for every bbecause some choices of bcan make b1 1
2b2Cb3nonzero.
The reduced matrix in Example 3 provides a description of all bfor which the
equation AxDbisconsistent: The entries in bmust satisfy
b1 1
2b2Cb3D0
This is the equation of a plane through the origin in R3. The plane is the set of all linear
combinations of the three columns of A. See Figure 1.
The equation AxDbin Example 3 fails to be consistent for all bbecause the
echelon form of Ahas a row of zeros. If Ahad a pivot in all three rows, we would
not care about the calculations in the augmented column because in this case an echelon
form of the augmented matrix could not have a row such as ¬å0 0 0 1 ¬ç.
In the next theorem, the sentence ‚ÄúThe columns of AspanRm‚Äù means that every bin
Span{ a1, a2, a3}x2
x1x3FIGURE 1
The columns of
AD¬åa1a2a3¬çspan a plane
through 0.
Rmis a linear combination of the columns of A. In general, a set of vectors fv1; : : : ; vpg
inRmspans (orgenerates )Rmif every vector in Rmis a linear combination of
v1; : : : ; vp‚Äîthat is, if Span fv1; : : : ; vpg DRm.
T H E O R E M 4 LetAbe an mnmatrix. Then the following statements are logically equivalent.
That is, for a particular A, either they are all true statements or they are all false.
a.For each binRm, the equation AxDbhas a solution.
b.Each binRmis a linear combination of the columns of A.
c.The columns of AspanRm.
d.Ahas a pivot position in every row.
SECOND REVISED PAGES


--- Page 55 ---
38 CHAPTER 1 Linear Equations in Linear Algebra
Theorem 4 is one of the most useful theorems in this chapter. Statements (a), (b), and
(c) are equivalent because of the deÔ¨Ånition of Axand what it means for a set of vectors
to span Rm. The discussion after Example 3 suggests why (a) and (d) are equivalent;
a proof is given at the end of the section. The exercises will provide examples of how
Theorem 4 is used.
Warning: Theorem 4 is about a coefÔ¨Åcient matrix , not an augmented matrix. If an
augmented matrix ¬åAb¬çhas a pivot position in every row, then the equation AxDb
may or may not be consistent.
Computation of Ax
The calculations in Example 1 were based on the deÔ¨Ånition of the product of a matrix A
and a vector x. The following simple example will lead to a more efÔ¨Åcient method for
calculating the entries in Axwhen working problems by hand.
EXAMPLE 4 Compute Ax, where AD2
42 3 4
 1 5  3
6 2 83
5andxD2
4x1
x2
x33
5.
SOLUTION From the deÔ¨Ånition,
2
42 3 4
 1 5  3
6 2 83
52
4x1
x2
x33
5Dx12
42
 1
63
5Cx22
43
5
 23
5Cx32
44
 3
83
5
D2
42x1
 x1
6x13
5C2
43x2
5x2
 2x23
5C2
44x3
 3x3
8x33
5 (7)
D2
42x1C3x2C4x3
 x1C5x2 3x3
6x1 2x2C8x33
5
The Ô¨Årst entry in the product Axis a sum of products (sometimes called a dot product ),
using the Ô¨Årst row of Aand the entries in x. That is,
2
42 3 43
52
4x1
x2
x33
5D2
42x1C3x2C4x33
5
This matrix shows how to compute the Ô¨Årst entry in Axdirectly, without writing down
all the calculations shown in (7). Similarly, the second entry in Axcan be calculated at
once by multiplying the entries in the second row of Aby the corresponding entries in
xand then summing the resulting products:
2
4 1 5  33
52
4x1
x2
x33
5D2
4 x1C5x2 3x33
5
Likewise, the third entry in Axcan be calculated from the third row of Aand the entries
inx.
Row--Vector Rule for Computing Ax
If the product Axis deÔ¨Åned, then the ith entry in Axis the sum of the products of
corresponding entries from row iofAand from the vector x.
SECOND REVISED PAGES


--- Page 56 ---
1.4The Matrix Equation AxDb39
EXAMPLE 5
a.1 2  1
0 5 32
44
3
73
5D14C23C. 1/7
04C. 5/3C37
D3
6
b.2
42 3
8 0
 5 23
54
7
D2
424C. 3/7
84C07
. 5/4C273
5D2
4 13
32
 63
5
c.2
41 0 0
0 1 0
0 0 13
52
4r
s
t3
5D2
41rC0sC0t
0rC1sC0t
0rC0sC1t3
5D2
4r
s
t3
5
By deÔ¨Ånition, the matrix in Example 5(c) with 1‚Äôs on the diagonal and 0‚Äôs elsewhere
is called an identity matrix and is denoted by I. The calculation in part (c) shows that
IxDxfor every xinR3. There is an analogous nnidentity matrix, sometimes written
asIn. As in part (c), InxDxfor every xinRn.
Properties of the Matrix‚ÄìVector Product Ax
The facts in the next theorem are important and will be used throughout the text. The
proof relies on the deÔ¨Ånition of Axand the algebraic properties of Rn.
T H E O R E M 5 IfAis an mnmatrix, uandvare vectors in Rn, and cis a scalar, then:
a.A.uCv/DAuCAv;
b.A.cu/Dc.Au/.
PROOF For simplicity, take nD3,AD¬åa1a2a3¬ç, and u,vinR3. (The proof of
the general case is similar.) For iD1; 2; 3 , letuiandvibe the ith entries in uandv,
respectively. To prove statement (a), compute A.uCv/as a linear combination of the
columns of Ausing the entries in uCvas weights.
A.uCv/D¬åa1a2a3¬ç2
4u1Cv1
u2Cv2
u3Cv33
5
# # #Entries in uCv
D.u1Cv1/a1C.u2Cv2/a2C.u3Cv3/a3
" " "Columns of A
D.u1a1Cu2a2Cu3a3/C.v1a1Cv2a2Cv3a3/
DAuCAv
To prove statement (b), compute A.cu/as a linear combination of the columns of A
using the entries in cuas weights.
A.cu/D¬åa1a2a3¬ç2
4cu1
cu2
cu33
5D.cu 1/a1C.cu 2/a2C.cu 3/a3
Dc.u 1a1/Cc.u 2a2/Cc.u 3a3/
Dc.u 1a1Cu2a2Cu3a3/
Dc.Au/
SECOND REVISED PAGES


--- Page 57 ---
40 CHAPTER 1 Linear Equations in Linear Algebra
N U M E R I C A L N O T E
To optimize a computer algorithm to compute Ax, the sequence of calculations
should involve data stored in contiguous memory locations. The most widely
used professional algorithms for matrix computations are written in Fortran, a
language that stores a matrix as a set of columns. Such algorithms compute Axas
a linear combination of the columns of A. In contrast, if a program is written in
the popular language C, which stores matrices by rows, Axshould be computed
via the alternative rule that uses the rows of A.
PROOF OF THEOREM 4 As was pointed out after Theorem 4, statements (a), (b), and
(c) are logically equivalent. So, it sufÔ¨Åces to show (for an arbitrary matrix A) that (a)
and (d) are either both true or both false. This will tie all four statements together.
LetUbe an echelon form of A. Given binRm, we can row reduce the augmented
matrix ¬åAb¬çto an augmented matrix ¬åUd¬çfor some dinRm:
¬åAb¬ç     ¬åUd¬ç
If statement (d) is true, then each row of Ucontains a pivot position and there can be no
pivot in the augmented column. So AxDbhas a solution for any b, and (a) is true. If (d)
is false, the last row of Uis all zeros. Let dbe any vector with a 1 in its last entry. Then
¬åUd¬çrepresents an inconsistent system. Since row operations are reversible, ¬åUd¬ç
can be transformed into the form ¬åAb¬ç. The new system AxDbis also inconsistent,
and (a) is false.
PRACTICE PROBLEMS
1.LetAD2
41 5  2 0
 3 1 9  5
4 8 1 73
5,pD2
6643
 2
0
 43
775, and bD2
4 7
9
03
5. It can be shown that
pis a solution of AxDb. Use this fact to exhibit bas a speciÔ¨Åc linear combination
of the columns of A.
2.LetAD2 5
3 1
,uD4
 1
, and vD 3
5
. Verify Theorem 5(a) in this case
by computing A.uCv/andAuCAv.
3.Construct a 33matrix Aand vectors bandcinR3so that AxDbhas a solution,
butAxDcdoes not.
1.4 EXERCISES
Compute the products in Exercises 1‚Äì4 using (a) the deÔ¨Ånition, as
in Example 1, and (b) the row‚Äìvector rule for computing Ax. If a
product is undeÔ¨Åned, explain why.
1.2
4 4 2
1 6
0 13
52
43
 2
73
5 2.2
42
6
 13
55
 1
3.2
46 5
 4 3
7 63
52
 3
4.8 3  4
5 1 22
41
1
13
5In Exercises 5‚Äì8, use the deÔ¨Ånition of Axto write the matrix
equation as a vector equation, or vice versa.
5.5 1  8 4
 2 7 3  52
6645
 1
3
 23
775D 8
16
6.2
6647 3
2 1
9 6
 3 23
775 2
 5
D2
6641
 9
12
 43
775
SECOND REVISED PAGES


--- Page 58 ---
1.4The Matrix Equation AxDb41
7.x12
6644
 1
7
 43
775Cx22
664 5
3
 5
13
775Cx32
6647
 8
0
23
775D2
6646
 8
0
 73
775
8.¬¥14
 2
C¬¥2 4
5
C¬¥3 5
4
C¬¥43
0
D4
13
In Exercises 9 and 10, write the system Ô¨Årst as a vector equation
and then as a matrix equation.
9.3x1Cx2 5x3D9
x2C4x3D010.8x1 x2D4
5x1C4x2D1
x1 3x2D2
Given Aandbin Exercises 11 and 12, write the augmented matrix
for the linear system that corresponds to the matrix equation
AxDb. Then solve the system and write the solution as a vector.
11.AD2
41 2 4
0 1 5
 2 4 33
5,bD2
4 2
2
93
5
12.AD2
41 2 1
 3 1 2
0 5 33
5,bD2
40
1
 13
5
13. LetuD2
40
4
43
5andAD2
43 5
 2 6
1 13
5. Isuin the plane R3
spanned by the columns of A? (See the Ô¨Ågure.) Why or why
not?
u?u?
Plane spanned by
the columns of A
Where is u?
14. LetuD2
42
 3
23
5andAD2
45 8 7
0 1  1
1 3 03
5. Isuin the subset
ofR3spanned by the columns of A? Why or why not?
15. LetAD2 1
 6 3
andbDb1
b2
. Show that the equation
AxDbdoes not have a solution for all possible b, and
describe the set of all bfor which AxDbdoes have a
solution.
16. Repeat Exercise 15: AD2
41 3 4
 3 2 6
5 1 83
5,bD2
4b1
b2
b33
5.
Exercises 17‚Äì20 refer to the matrices AandBbelow. Make
appropriate calculations that justify your answers and mention an
appropriate theorem.AD2
6641 3 0 3
 1 1 1 1
0 4 2  8
2 0 3  13
775BD2
6641 3  2 2
0 1 1  5
1 2  3 7
 2 8 2  13
775
17. How many rows of Acontain a pivot position? Does the
equation AxDbhave a solution for each binR4?
18. Do the columns of BspanR4? Does the equation BxDy
have a solution for each yinR4?
19. Can each vector in R4be written as a linear combination of
the columns of the matrix Aabove? Do the columns of A
spanR4?
20. Can every vector in R4be written as a linear combination of
the columns of the matrix Babove? Do the columns of B
spanR3?
21. Letv1D2
6641
0
 1
03
775,v2D2
6640
 1
0
13
775,v3D2
6641
0
0
 13
775.
Doesfv1;v2;v3gspanR4? Why or why not?
22. Letv1D2
40
0
 23
5,v2D2
40
 3
83
5,v3D2
44
 1
 53
5.
Doesfv1;v2;v3gspanR3? Why or why not?
In Exercises 23 and 24, mark each statement True or False. Justify
each answer.
23. a.The equation AxDbis referred to as a vector equation .
b.A vector bis a linear combination of the columns of a
matrix Aif and only if the equation AxDbhas at least
one solution.
c.The equation AxDbis consistent if the augmented ma-
trix¬åAb¬çhas a pivot position in every row.
d.The Ô¨Årst entry in the product Axis a sum of products.
e.If the columns of an mnmatrix AspanRm, then the
equation AxDbis consistent for each binRm.
f.IfAis an mnmatrix and if the equation AxDbis
inconsistent for some binRm, then Acannot have a pivot
position in every row.
24. a.Every matrix equation AxDbcorresponds to a vector
equation with the same solution set.
b.Any linear combination of vectors can always be written
in the form Axfor a suitable matrix Aand vector x.
c.The solution set of a linear system whose augmented
matrix is ¬åa1a2a3b¬çis the same as the solution
set of AxDb, ifAD¬åa1a2a3¬ç.
d.If the equation AxDbis inconsistent, then bis not in the
set spanned by the columns of A.
e.If the augmented matrix ¬åAb¬çhas a pivot position in
every row, then the equation AxDbis inconsistent.
SECOND REVISED PAGES


--- Page 59 ---
42 CHAPTER 1 Linear Equations in Linear Algebra
f.IfAis an mnmatrix whose columns do not span Rm,
then the equation AxDbis inconsistent for some bin
Rm.
25. Note that2
44 3 1
5 2 5
 6 2  33
52
4 3
 1
23
5D2
4 7
 3
103
5. Use this fact
(and no row operations) to Ô¨Ånd scalars c1,c2,c3such that2
4 7
 3
103
5Dc12
44
5
 63
5Cc22
4 3
 2
23
5Cc32
41
5
 33
5.
26. LetuD2
47
2
53
5,vD2
43
1
33
5, and wD2
46
1
03
5.
It can be shown that 3u 5v wD0. Use this fact (and
no row operations) to Ô¨Ånd x1andx2that satisfy the equation2
47 3
2 1
5 33
5x1
x2
D2
46
1
03
5.
27. Letq1,q2,q3, and vrepresent vectors in R5, and let x1,x2,
andx3denote scalars. Write the following vector equation as
a matrix equation. Identify any symbols you choose to use.
x1q1Cx2q2Cx3q3Dv
28. Rewrite the (numerical) matrix equation below in symbolic
form as a vector equation, using symbols v1;v2; : : : for the
vectors and c1; c2; : : : for scalars. DeÔ¨Åne what each symbol
represents, using the data given in the matrix equation.
 3 5  4 9 7
5 8 1  2 42
66664 3
2
4
 1
23
77775D8
 1
29. Construct a 33matrix, not in echelon form, whose
columns span R3. Show that the matrix you construct has the
desired property.
30. Construct a 33matrix, not in echelon form, whose
columns do notspanR3. Show that the matrix you construct
has the desired property.
31. LetAbe a32matrix. Explain why the equation AxDb
cannot be consistent for all binR3. Generalize yourargument to the case of an arbitrary Awith more rows than
columns.
32. Could a set of three vectors in R4span all of R4? Explain.
What about nvectors in Rmwhen nis less than m?
33. Suppose Ais a43matrix and bis a vector in R4with the
property that AxDbhas a unique solution. What can you say
about the reduced echelon form of A? Justify your answer.
34. Suppose Ais a33matrix and bis a vector in R3with the
property that AxDbhas a unique solution. Explain why the
columns of Amust span R3.
35. LetAbe a34matrix, let y1andy2be vectors in R3, and
letwDy1Cy2. Suppose y1DAx1andy2DAx2for some
vectors x1andx2inR4. What fact allows you to conclude that
the system AxDwis consistent? ( Note: x1andx2denote
vectors, not scalar entries in vectors.)
36. LetAbe a 53matrix, let ybe a vector in R3, and let z
be a vector in R5. Suppose AyDz. What fact allows you to
conclude that the system AxD4zis consistent?
[M] In Exercises 37‚Äì40, determine if the columns of the matrix
spanR4.
37.2
6647 2  5 8
 5 3 4  9
6 10  2 7
 7 9 2 153
77538.2
6645 7 4 9
6 8 7 5
4 4 9 9
 9 11 16 73
775
39.2
66412 7 11  9 5
 9 4  8 7  3
 6 11  7 3  9
4 6 10  5 123
775
40.2
6648 11  6 7 13
 7 8 5 6  9
11 7  7 9 6
 3 4 1 8 73
775
41. [M] Find a column of the matrix in Exercise 39 that can be
deleted and yet have the remaining matrix columns still span
R4.
42. [M] Find a column of the matrix in Exercise 40 that can be
deleted and yet have the remaining matrix columns still span
R4. Can you delete more than one column?
SG
Mastering Linear Algebra Concepts: Span 1‚Äì18
WEB
SOLUTIONS TO PRACTICE PROBLEMS
1.The matrix equation
2
41 5  2 0
 3 1 9  5
4 8 1 73
52
6643
 2
0
 43
775D2
4 7
9
03
5
SECOND REVISED PAGES


--- Page 60 ---
1.5Solution Sets of Linear Systems 43
is equivalent to the vector equation
32
41
 3
43
5 22
45
1
 83
5C02
4 2
9
 13
5 42
40
 5
73
5D2
4 7
9
03
5
which expresses bas a linear combination of the columns of A.
2. uCvD4
 1
C 3
5
D1
4
A.uCv/D2 5
3 11
4
D2C20
3C4
D22
7
AuCAvD2 5
3 14
 1
C2 5
3 1 3
5
D3
11
C19
 4
D22
7
Remark: There are, in fact, inÔ¨Ånitely many correct solutions to Practice Problem 3.
When creating matrices to satisfy speciÔ¨Åed criteria, it is often useful to create
matrices that are straightforward, such as those already in reduced echelon form.
Here is one possible solution:
3.Let
AD2
41 0 1
0 1 1
0 0 03
5;bD2
43
2
03
5;andcD2
43
2
13
5:
Notice the reduced echelon form of the augmented matrix corresponding to AxDb
is2
41 0 1 3
0 1 1 2
0 0 0 03
5;
which corresponds to a consistent system, and hence AxDbhas solutions. The
reduced echelon form of the augmented matrix corresponding to AxDcis
2
41 0 1 3
0 1 1 2
0 0 0 13
5;
which corresponds to an inconsistent system, and hence AxDcdoes not have any
solutions.
1.5 SOLUTION SETS OF LINEAR SYSTEMS
Solution sets of linear systems are important objects of study in linear algebra. They
will appear later in several different contexts. This section uses vector notation to give
explicit and geometric descriptions of such solution sets.
Homogeneous Linear Systems
A system of linear equations is said to be homogeneous if it can be written in the form
AxD0, where Ais an mnmatrix and 0is the zero vector in Rm. Such a system
AxD0always has at least one solution, namely, xD0(the zero vector in Rn/. This
SECOND REVISED PAGES


--- Page 61 ---
44 CHAPTER 1 Linear Equations in Linear Algebra
zero solution is usually called the trivial solution . For a given equation AxD0;the
important question is whether there exists a nontrivial solution , that is, a nonzero
vector xthat satisÔ¨Åes AxD0:The Existence and Uniqueness Theorem in Section 1.2
(Theorem 2) leads immediately to the following fact.
The homogeneous equation AxD0has a nontrivial solution if and only if the
equation has at least one free variable.
EXAMPLE 1 Determine if the following homogeneous system has a nontrivial
solution. Then describe the solution set.
3x1C5x2 4x3D0
 3x1 2x2C4x3D0
6x1Cx2 8x3D0
SOLUTION LetAbe the matrix of coefÔ¨Åcients of the system and row reduce the
augmented matrix ¬åA0¬çto echelon form:
2
43 5  4 0
 3 2 4 0
6 1  8 03
52
43 5  4 0
0 3 0 0
0 9 0 03
52
43 5  4 0
0 3 0 0
0 0 0 03
5
Since x3is a free variable, AxD0has nontrivial solutions (one for each choice of x3).
To describe the solution set, continue the row reduction of ¬åA0¬çtoreduced echelon
form:
2
41 0  4
30
0 1 0 0
0 0 0 03
5x1 4
3x3D0
x2 D0
0D0
Solve for the basic variables x1andx2and obtain x1D4
3x3,x2D0, with x3free. As a
vector, the general solution of AxD0has the form
xD2
64x1
x2
x33
75D2
644
3x3
0
x33
75Dx32
644
3
0
13
75Dx3v;where vD2
644
3
0
13
75
Here x3is factored out of the expression for the general solution vector. This shows that
every solution of AxD0in this case is a scalar multiple of v. The trivial solution is
obtained by choosing x3D0:Geometrically, the solution set is a line through 0inR3.
See Figure 1.
0vx3
x1Span{ v}
x2 FIGURE 1
Notice that a nontrivial solution xcan have some zero entries so long as not all of
its entries are zero.
EXAMPLE 2 A single linear equation can be treated as a very simple system of
equations. Describe all solutions of the homogeneous ‚Äúsystem‚Äù
10x 1 3x2 2x3D0 (1)
SOLUTION There is no need for matrix notation. Solve for the basic variable x1in
terms of the free variables. The general solution is x1D:3x2C:2x3, with x2andx3
SECOND REVISED PAGES


--- Page 62 ---
1.5Solution Sets of Linear Systems 45
free. As a vector, the general solution is
xD2
4x1
x2
x33
5D2
4:3x2C:2x3
x2
x33
5D2
4:3x2
x2
03
5C2
4:2x3
0
x33
5
Dx22
4:3
1
03
5
6
uCx32
4:2
0
13
5
6v(with x2,x3free) (2)
This calculation shows that every solution of (1) is a linear combination of the vectors
uandv, shown in (2). That is, the solution set is Span fu;vg. Since neither unorvis a
scalar multiple of the other, the solution set is a plane through the origin. See Figure 2.
uvx1x3
x2 FIGURE 2
Examples 1 and 2, along with the exercises, illustrate the fact that the solution
set of a homogeneous equation AxD0can always be expressed explicitly as
Spanfv1; : : : ; vpgfor suitable vectors v1; : : : ; vp. If the only solution is the zero vector,
then the solution set is Span f0g. If the equation AxD0has only one free variable, the
solution set is a line through the origin, as in Figure 1. A plane through the origin, as in
Figure 2, provides a good mental image for the solution set of AxD0when there are
two or more free variables. Note, however, that a similar Ô¨Ågure can be used to visualize
Spanfu;vgeven when uandvdo not arise as solutions of AxD0:See Figure 11 in
Section 1.3.
Parametric Vector Form
The original equation (1) for the plane in Example 2 is an implicit description of the
plane. Solving this equation amounts to Ô¨Ånding an explicit description of the plane as
the set spanned by uandv. Equation (2) is called a parametric vector equation of the
plane. Sometimes such an equation is written as
xDsuCtv.s; t inR/
to emphasize that the parameters vary over all real numbers. In Example 1, the equation
xDx3v(with x3free), or xDtv(with tinR), is a parametric vector equation of a line.
Whenever a solution set is described explicitly with vectors as in Examples 1 and 2, we
say that the solution is in parametric vector form .
Solutions of Nonhomogeneous Systems
When a nonhomogeneous linear system has many solutions, the general solution can be
written in parametric vector form as one vector plus an arbitrary linear combination of
vectors that satisfy the corresponding homogeneous system.
EXAMPLE 3 Describe all solutions of AxDb, where
AD2
43 5  4
 3 2 4
6 1  83
5and bD2
47
 1
 43
5
SECOND REVISED PAGES


--- Page 63 ---
46 CHAPTER 1 Linear Equations in Linear Algebra
SOLUTION Here Ais the matrix of coefÔ¨Åcients from Example 1. Row operations on
¬åAb¬çproduce
2
43 5  4 7
 3 2 4  1
6 1  8 43
52
41 0  4
3 1
0 1 0 2
0 0 0 03
5;x1 4
3x3D  1
x2 D2
0D0
Thus x1D  1C4
3x3,x2D2, andx3is free. As a vector, the general solution of AxDb
has the form
xD2
64x1
x2
x33
75D2
64 1C4
3x3
2
x33
75D2
64 1
2
03
75C2
644
3x3
0
x33
75D2
64 1
2
03
75
6
pCx32
644
3
0
13
75
6v
The equation xDpCx3v, or, writing tas a general parameter,
xDpCtv(tinR) (3)
describes the solution set of AxDbin parametric vector form. Recall from Example 1
that the solution set of AxD0has the parametric vector equation
xDtv(tinR) (4)
[with the same vthat appears in (3)]. Thus the solutions of AxDbare obtained by
adding the vector pto the solutions of AxD0. The vector pitself is just one particular
solution of AxDb[corresponding to tD0in (3)].
p
vv + pFIGURE 3
Adding ptovtranslates vtovCp.
To describe the solution set of AxDbgeometrically, we can think of vector
addition as a translation . Given vandpinR2orR3, the effect of adding ptovis
tomove vin a direction parallel to the line through pand0. We say that vistranslated
L + p
L
FIGURE 4
Translated line.by p tovCp. See Figure 3. If each point on a line LinR2orR3is translated by a
vector p, the result is a line parallel to L. See Figure 4.
Suppose Lis the line through 0andv, described by equation (4). Adding pto each
point on Lproduces the translated line described by equation (3). Note that pis on the
line in equation (3). We call (3) the equation of the line through p parallel to v . Thus
the solution set of AxDbis a line through pparallel to the solution set of AxD0.
Figure 5 illustrates this case.
p
vtvp + tvAx = b
Ax = 0
FIGURE 5 Parallel solution sets of AxDband
AxD0.
The relation between the solution sets of AxDbandAxD0shown in Figure 5
generalizes to any consistent equation AxDb, although the solution set will be larger
than a line when there are several free variables. The following theorem gives the precise
statement. See Exercise 25 for a proof.
SECOND REVISED PAGES


--- Page 64 ---
1.5Solution Sets of Linear Systems 47
T H E O R E M 6 Suppose the equation AxDbis consistent for some given b, and let pbe a
solution. Then the solution set of AxDbis the set of all vectors of the form
wDpCvh, where vhis any solution of the homogeneous equation AxD0.
Theorem 6 says that if AxDbhas a solution, then the solution set is obtained by
translating the solution set of AxD0, using any particular solution pofAxDbfor
the translation. Figure 6 illustrates the case in which there are two free variables. Even
when n > 3 , our mental image of the solution set of a consistent system AxDb(with
b¬§0) is either a single nonzero point or a line or plane not passing through the origin.
px3
x2x1Ax /H11005 b
Ax /H11005 0
FIGURE 6 Parallel solution sets of
AxDbandAxD0.
Warning: Theorem 6 and Figure 6 apply only to an equation AxDbthat has at least
one nonzero solution p. When AxDbhas no solution, the solution set is empty.
The following algorithm outlines the calculations shown in Examples 1, 2, and 3.
WRITING A SOLUTION SET (OF A CONSISTENT SYSTEM) IN PARAMETRIC
VECTOR FORM
1.Row reduce the augmented matrix to reduced echelon form.
2.Express each basic variable in terms of any free variables appearing in an
equation.
3.Write a typical solution xas a vector whose entries depend on the free
variables, if any.
4.Decompose xinto a linear combination of vectors (with numeric entries) using
the free variables as parameters.
PRACTICE PROBLEMS
1.Each of the following equations determines a plane in R3. Do the two planes
intersect? If so, describe their intersection.
x1C4x2 5x3D0
2x1 x2C8x3D9
2.Write the general solution of 10x 1 3x2 2x3D7in parametric vector form, and
relate the solution set to the one found in Example 2.
3.Prove the Ô¨Årst part of Theorem 6: Suppose that pis a solution of AxDb, so that
ApDb. Let vhbe any solution to the homogeneous equation AxD0, and let
wDpCvh. Show that wis a solution to AxDb.
SECOND REVISED PAGES


--- Page 65 ---
48 CHAPTER 1 Linear Equations in Linear Algebra
1.5 EXERCISES
In Exercises 1‚Äì4, determine if the system has a nontrivial solution.
Try to use as few row operations as possible.
1. 2x1 5x2C8x3D0
 2x1 7x2Cx3D0
4x1C2x2C7x3D02. x1 3x2C7x3D0
 2x1Cx2 4x3D0
x1C2x2C9x3D0
3. 3x1C5x2 7x3D0
 6x1C7x2Cx3D04. 5x1C7x2C9x3D0
x1 2x2C6x3D0
In Exercises 5 and 6, follow the method of Examples 1 and 2
to write the solution set of the given homogeneous system in
parametric vector form.
5. x1C3x2Cx3D0
 4x1 9x2C2x3D0
 3x2 6x3D06. x1C3x2 5x3D0
x1C4x2 8x3D0
 3x1 7x2C9x3D0
In Exercises 7‚Äì12, describe all solutions of AxD0in parametric
vector form, where Ais row equivalent to the given matrix.
7.1 3  3 7
0 1  4 5
8.1 2 9 5
0 1 2  6
9.3 9 6
 1 3  2
10.1 3 0  4
2 6 0  8
11.2
6641 4 2 0 3  5
0 0 1 0 0  1
0 0 0 0 1  4
0 0 0 0 0 03
775
12.2
6641 5 2  6 9 0
0 0 1  7 4  8
0 0 0 0 0 1
0 0 0 0 0 03
775
13. Suppose the solution set of a certain system of linear equa-
tions can be described as x1D5C4x3,x2D  2 7x3, with
x3free. Use vectors to describe this set as a line in R3.
14. Suppose the solution set of a certain system of linear
equations can be described as x1D3x4,x2D8Cx4,
x3D2 5x4, with x4free. Use vectors to describe this set
as a ‚Äúline‚Äù in R4.
15. Follow the method of Example 3 to describe the solutions of
the following system in parametric vector form. Also, give
a geometric description of the solution set and compare it to
that in Exercise 5.
x1C3x2Cx3D1
 4x1 9x2C2x3D  1
 3x2 6x3D  3
16. As in Exercise 15, describe the solutions of the following
system in parametric vector form, and provide a geometric
comparison with the solution set in Exercise 6.x1C3x2 5x3D4
x1C4x2 8x3D7
 3x1 7x2C9x3D  6
17. Describe and compare the solution sets of x1C9x2 4x3D0
andx1C9x2 4x3D  2.
18. Describe and compare the solution sets of x1 3x2C5x3D0
andx1 3x2C5x3D4.
In Exercises 19 and 20, Ô¨Ånd the parametric equation of the line
through aparallel to b.
19. aD 2
0
,bD 5
3
20. a D3
 4
,bD 7
8
In Exercises 21 and 22, Ô¨Ånd a parametric equation of the line M
through pandq. [Hint: Mis parallel to the vector q p. See the
Ô¨Ågure below.]
21. pD2
 5
,qD 3
1
22. p D 6
3
,qD0
 4
x1x2
Mq ‚Äì p‚Äìp qp
The line through p and q.
In Exercises 23 and 24, mark each statement True or False. Justify
each answer.
23. a.A homogeneous equation is always consistent.
b.The equation AxD0gives an explicit description of its
solution set.
c.The homogeneous equation AxD0has the trivial so-
lution if and only if the equation has at least one free
variable.
d.The equation xDpCtvdescribes a line through vpar-
allel to p.
e.The solution set of AxDbis the set of all vectors of
the form wDpCvh, where vhis any solution of the
equation AxD0.
24. a.Ifxis a nontrivial solution of AxD0, then every entry in
xis nonzero.
b.The equation xDx2uCx3v, with x2andx3free (and
neither unorva multiple of the other), describes a plane
through the origin.
c.The equation AxDbis homogeneous if the zero vector
is a solution.
d.The effect of adding pto a vector is to move the vector in
a direction parallel to p.
SECOND REVISED PAGES


--- Page 66 ---
1.5Solution Sets of Linear Systems 49
e.The solution set of AxDbis obtained by translating the
solution set of AxD0.
25. Prove the second part of Theorem 6: Let wbe any solution of
AxDb, and deÔ¨Åne vhDw p. Show that vhis a solution
ofAxD0. This shows that every solution of AxDbhas the
form wDpCvh, with pa particular solution of AxDband
vha solution of AxD0.
26. Suppose AxDbhas a solution. Explain why the solution is
unique precisely when AxD0has only the trivial solution.
27. Suppose Ais the 33zero matrix (with all zero entries).
Describe the solution set of the equation AxD0.
28. Ifb¬§0, can the solution set of AxDbbe a plane through
the origin? Explain.
In Exercises 29‚Äì32, (a) does the equation AxD0have a nontriv-
ial solution and (b) does the equation AxDbhave at least one
solution for every possible b?
29.Ais a33matrix with three pivot positions.
30.Ais a33matrix with two pivot positions.
31.Ais a32matrix with two pivot positions.
32.Ais a24matrix with two pivot positions.
33. Given AD2
4 2 6
7 21
 3 93
5, Ô¨Ånd one nontrivial solution of
AxD0by inspection. [ Hint: Think of the equation AxD0
written as a vector equation.]34. Given AD2
44 6
 8 12
6 93
5, Ô¨Ånd one nontrivial solution of
AxD0by inspection.
35. Construct a 33nonzero matrix Asuch that the vector2
41
1
13
5is a solution of AxD0.
36. Construct a 33nonzero matrix Asuch that the vector2
41
 2
13
5is a solution of AxD0.
37. Construct a 22matrix Asuch that the solution set of the
equation AxD0is the line in R2through .4; 1/ and the
origin. Then, Ô¨Ånd a vector binR2such that the solution set
ofAxDbisnota line in R2parallel to the solution set of
AxD0. Why does this notcontradict Theorem 6?
38. Suppose Ais a33matrix and yis a vector in R3such that
the equation AxDydoes nothave a solution. Does there
exist a vector zinR3such that the equation AxDzhas a
unique solution? Discuss.
39. LetAbe an mnmatrix and let ube a vector in Rnthat satis-
Ô¨Åes the equation AxD0. Show that for any scalar c, the vec-
torcualso satisÔ¨Åes AxD0. [That is, show that A.cu/D0.]
40. LetAbe an mnmatrix, and let uandvbe vectors in Rn
with the property that AuD0andAvD0. Explain why
A.uCv/must be the zero vector. Then explain why
A.cuCdv/D0for each pair of scalars candd.
SOLUTIONS TO PRACTICE PROBLEMS
1.Row reduce the augmented matrix:
1 4  5 0
2 1 8 9
1 4  5 0
0 9 18 9
1 0 3 4
0 1  2 1
x1C3x3D4
x2 2x3D  1
Thus x1D4 3x3; x2D  1C2x3, with x3free. The general solution in parametric
vector form is
2
4x1
x2
x33
5D2
44 3x3
 1C2x3
x33
5D2
44
 1
03
5
6
pCx32
4 3
2
13
5
6v
The intersection of the two planes is the line through pin the direction of v.
SECOND REVISED PAGES


--- Page 67 ---
50 CHAPTER 1 Linear Equations in Linear Algebra
2.The augmented matrix10 3 2 7
is row equivalent to1 :3 :2 :7
,
and the general solution is x1D:7C:3x2C:2x3, with x2andx3free. That is,
xD2
4x1
x2
x33
5D2
4:7C:3x2C:2x3
x2
x33
5D2
4:7
0
03
5Cx22
4:3
1
03
5Cx32
4:2
0
13
5
D pC x2uC x3v
The solution set of the nonhomogeneous equation AxDbis the translated plane
pCSpanfu;vg, which passes through pand is parallel to the solution set of the
homogeneous equation in Example 2.
3.Using Theorem 5 from Section 1.4, notice
A.pCvh/DApCAvhDbC0Db;
hence pCvhis a solution to AxDb.
1.6 APPLICATIONS OF LINEAR SYSTEMS
You might expect that a real-life problem involving linear algebra would have only
one solution, or perhaps no solution. The purpose of this section is to show how linear
systems with many solutions can arise naturally. The applications here come from
economics, chemistry, and network Ô¨Çow.
A Homogeneous System in Economics
The system of 500 equations in 500 variables, mentioned in this chapter‚Äôs introduction,
WEB
is now known as a Leontief ‚Äúinput‚Äìoutput‚Äù (or ‚Äúproduction‚Äù) model.1Section 2.6 will
examine this model in more detail, when more theory and better notation are available.
For now, we look at a simpler ‚Äúexchange model,‚Äù also due to Leontief.
Suppose a nation‚Äôs economy is divided into many sectors, such as various manufac-
turing, communication, entertainment, and service industries. Suppose that for each sec-
tor we know its total output for one year and we know exactly how this output is divided
or ‚Äúexchanged‚Äù among the other sectors of the economy. Let the total dollar value of a
sector‚Äôs output be called the price of that output. Leontief proved the following result.
There exist equilibrium prices that can be assigned to the total outputs of the
various sectors in such a way that the income of each sector exactly balances its
expenses.
The following example shows how to Ô¨Ånd the equilibrium prices.
EXAMPLE 1 Suppose an economy consists of the Coal, Electric (power), and Steel
sectors, and the output of each sector is distributed among the various sectors as shown
in Table 1, where the entries in a column represent the fractional parts of a sector‚Äôs total
output.
The second column of Table 1, for instance, says that the total output of the
Electric sector is divided as follows: 40% to Coal, 50% to Steel, and the remaining
10% to Electric. (Electric treats this 10% as an expense it incurs in order to operate its
1See Wassily W. Leontief, ‚ÄúInput‚ÄìOutput Economics,‚Äù ScientiÔ¨Åc American , October 1951, pp. 15‚Äì21.
SECOND REVISED PAGES


--- Page 68 ---
1.6Applications of Linear Systems 51
business.) Since all output must be taken into account, the decimal fractions in each
column must sum to 1.
Denote the prices (i.e., dollar values) of the total annual outputs of the Coal,
Electric, and Steel sectors by pC,pE, and pS, respectively. If possible, Ô¨Ånd equilibrium
prices that make each sector‚Äôs income match its expenditures.
.1
.2.2 .5.4
.4.6
.6SteelCoalElectric
TABLE 1 A Simple Economy
Distribution of Output from:
Coal Electric Steel Purchased by:
.0 .4 .6 Coal
.6 .1 .2 Electric
.4 .5 .2 Steel
SOLUTION A sector looks down a column to see where its output goes, and it looks
across a row to see what it needs as inputs. For instance, the Ô¨Årst row of Table 1 says
that Coal receives (and pays for) 40% of the Electric output and 60% of the Steel
output. Since the respective values of the total outputs are pEandpS, Coal must spend
:4p Edollars for its share of Electric‚Äôs output and :6p Sfor its share of Steel‚Äôs output.
Thus Coal‚Äôs total expenses are :4p EC:6p S. To make Coal‚Äôs income, pC, equal to its
expenses, we want
pCD:4p EC:6p S (1)
The second row of the exchange table shows that the Electric sector spends :6p C
for coal, :1p Efor electricity, and :2p Sfor steel. Hence the income/expense requirement
for Electric is
pED:6p CC:1p EC:2p S (2)
Finally, the third row of the exchange table leads to the Ô¨Ånal requirement:
pSD:4p CC:5p EC:2p S (3)
To solve the system of equations (1), (2), and (3), move all the unknowns to the left
sides of the equations and combine like terms. [For instance, on the left side of (2),
write pE :1p Eas:9p E.]
pC :4p E :6p SD0
 :6p CC:9p E :2p SD0
 :4p C :5p EC:8p SD0
Row reduction is next. For simplicity here, decimals are rounded to two places.
2
41 :4 :6 0
 :6 :9  :2 0
 :4 :5 :8 03
52
41 :4 :6 0
0 :66  :56 0
0 :66 :56 03
52
41 :4 :6 0
0 :66  :56 0
0 0 0 03
5
2
41 :4 :6 0
0 1  :85 0
0 0 0 03
52
41 0  :94 0
0 1  :85 0
0 0 0 03
5
SECOND REVISED PAGES


--- Page 69 ---
52 CHAPTER 1 Linear Equations in Linear Algebra
The general solution is pCD:94p S,pED:85p S, and pSis free. The equilibrium price
vector for the economy has the form
pD2
4pC
pE
pS3
5D2
4:94p S
:85p S
pS3
5DpS2
4:94
:85
13
5
Any (nonnegative) choice for pSresults in a choice of equilibrium prices. For instance,
if we take pSto be 100 (or $100 million), then pCD94andpED85. The incomes and
expenditures of each sector will be equal if the output of Coal is priced at $94 million,
that of Electric at $85 million, and that of Steel at $100 million.
Balancing Chemical Equations
Chemical equations describe the quantities of substances consumed and produced by
chemical reactions. For instance, when propane gas burns, the propane (C 3H8) combines
with oxygen (O 2) to form carbon dioxide (CO 2) and water (H 2O), according to an
equation of the form
.x1/C3H8C.x2/O2!.x3/CO2C.x4/H2O (4)
To ‚Äúbalance‚Äù this equation, a chemist must Ô¨Ånd whole numbers x1; : : : ; x 4such that the
total numbers of carbon (C), hydrogen (H), and oxygen (O) atoms on the left match the
corresponding numbers of atoms on the right (because atoms are neither destroyed nor
created in the reaction).
A systematic method for balancing chemical equations is to set up a vector equation
that describes the numbers of atoms of each type present in a reaction. Since equation
(4) involves three types of atoms (carbon, hydrogen, and oxygen), construct a vector in
R3for each reactant and product in (4) that lists the numbers of ‚Äúatoms per molecule,‚Äù
as follows:
C3H8W2
43
8
03
5;O2W2
40
0
23
5;CO2W2
41
0
23
5;H2OW2
40
2
13
5Carbon
Hydrogen
Oxygen
To balance equation (4), the coefÔ¨Åcients x1; : : : ; x 4must satisfy
x12
43
8
03
5Cx22
40
0
23
5Dx32
41
0
23
5Cx42
40
2
13
5
To solve, move all the terms to the left (changing the signs in the third and fourth
vectors):
x12
43
8
03
5Cx22
40
0
23
5Cx32
4 1
0
 23
5Cx42
40
 2
 13
5D2
40
0
03
5
Row reduction of the augmented matrix for this equation leads to the general solution
x1D1
4x4; x2D5
4x4; x3D3
4x4;withx4free
Since the coefÔ¨Åcients in a chemical equation must be integers, take x4D4, in which
casex1D1,x2D5, and x3D3. The balanced equation is
C3H8C5O2!3CO2C4H2O
The equation would also be balanced if, for example, each coefÔ¨Åcient were doubled. For
most purposes, however, chemists prefer to use a balanced equation whose coefÔ¨Åcients
are the smallest possible whole numbers.
SECOND REVISED PAGES


--- Page 70 ---
1.6Applications of Linear Systems 53
Network Flow
Systems of linear equations arise naturally when scientists, engineers, or economists
WEB
study the Ô¨Çow of some quantity through a network. For instance, urban planners and
trafÔ¨Åc engineers monitor the pattern of trafÔ¨Åc Ô¨Çow in a grid of city streets. Electrical
engineers calculate current Ô¨Çow through electrical circuits. And economists analyze the
distribution of products from manufacturers to consumers through a network of whole-
salers and retailers. For many networks, the systems of equations involve hundreds or
even thousands of variables and equations.
Anetwork consists of a set of points called junctions , ornodes , with lines or arcs
called branches connecting some or all of the junctions. The direction of Ô¨Çow in each
branch is indicated, and the Ô¨Çow amount (or rate) is either shown or is denoted by a
variable.
The basic assumption of network Ô¨Çow is that the total Ô¨Çow into the network equals
the total Ô¨Çow out of the network and that the total Ô¨Çow into a junction equals the total
Ô¨Çow out of the junction. For example, Figure 1 shows 30 units Ô¨Çowing into a junction
through one branch, with x1andx2denoting the Ô¨Çows out of the junction through other
branches. Since the Ô¨Çow is ‚Äúconserved‚Äù at each junction, we must have x1Cx2D30.
In a similar fashion, the Ô¨Çow at each junction is described by a linear equation. The
problem of network analysis is to determine the Ô¨Çow in each branch when partial
information (such as the Ô¨Çow into and out of the network) is known.
30x1
x2FIGURE 1
A junction, or node.
EXAMPLE 2 The network in Figure 2 shows the trafÔ¨Åc Ô¨Çow (in vehicles per hour)
over several one-way streets in downtown Baltimore during a typical early afternoon.
Determine the general Ô¨Çow pattern for the network.
300
300400600
500Inner HarborAB
DC
x1x4
x2 x5x3 100
Calvert St. South St.
Lombard St.
Pratt St.N
FIGURE 2 Baltimore streets.
SOLUTION Write equations that describe the Ô¨Çow, and then Ô¨Ånd the general solution
of the system. Label the street intersections (junctions) and the unknown Ô¨Çows in the
branches, as shown in Figure 2. At each intersection, set the Ô¨Çow in equal to the Ô¨Çow out.
Intersection Flow in Flow out
A 300C500Dx1Cx2
B x2Cx4D300Cx3
C 100C400Dx4Cx5
D x1Cx5D600
SECOND REVISED PAGES


--- Page 71 ---
54 CHAPTER 1 Linear Equations in Linear Algebra
Also, the total Ô¨Çow into the network .500C300C100C400/ equals the total Ô¨Çow
out of the network .300Cx3C600/, which simpliÔ¨Åes to x3D400. Combine this
equation with a rearrangement of the Ô¨Årst four equations to obtain the following system
of equations:
x1Cx2 D800
x2 x3Cx4 D300
x4Cx5D500
x1 Cx5D600
x3 D400
Row reduction of the associated augmented matrix leads to
x1 Cx5D600
x2  x5D200
x3 D400
x4Cx5D500
The general Ô¨Çow pattern for the network is described by
8
¬à¬à¬à¬à¬à¬à<
¬à¬à¬à¬à¬à¬à:x1D600 x5
x2D200Cx5
x3D400
x4D500 x5
x5is free
A negative Ô¨Çow in a network branch corresponds to Ô¨Çow in the direction opposite
to that shown on the model. Since the streets in this problem are one-way, none of the
variables here can be negative. This fact leads to certain limitations on the possible
values of the variables. For instance, x5500because x4cannot be negative. Other
constraints on the variables are considered in Practice Problem 2.
PRACTICE PROBLEMS
1.Suppose an economy has three sectors: Agriculture, Mining, and Manufacturing.
Agriculture sells 5% of its output to Mining and 30% to Manufacturing, and retains
the rest. Mining sells 20% of its output to Agriculture and 70% to Manufacturing,
and retains the rest. Manufacturing sells 20% of its output to Agriculture and 30% to
Mining, and retains the rest. Determine the exchange table for this economy, where
the columns describe how the output of each sector is exchanged among the three
sectors.
2.Consider the network Ô¨Çow studied in Example 2. Determine the possible range of
values of x1andx2. [Hint: The example showed that x5500. What does this imply
about x1andx2? Also, use the fact that x50.]
SECOND REVISED PAGES


--- Page 72 ---
1.6Applications of Linear Systems 55
1.6 EXERCISES
1.Suppose an economy has only two sectors, Goods and Ser-
vices. Each year, Goods sells 80% of its output to Services
and keeps the rest, while Services sells 70% of its output to
Goods and retains the rest. Find equilibrium prices for the
annual outputs of the Goods and Services sectors that make
each sector‚Äôs income match its expenditures.
.7.8
.2 .3Goods Services
2.Find another set of equilibrium prices for the economy in
Example 1. Suppose the same economy used Japanese yen
instead of dollars to measure the value of the various sec-
tors‚Äô outputs. Would this change the problem in any way?
Discuss.
3.Consider an economy with three sectors, Chemicals & Met-
als, Fuels & Power, and Machinery. Chemicals sells 30% of
its output to Fuels and 50% to Machinery and retains the
rest. Fuels sells 80% of its output to Chemicals and 10%
to Machinery and retains the rest. Machinery sells 40% to
Chemicals and 40% to Fuels and retains the rest.
a.Construct the exchange table for this economy.
b.Develop a system of equations that leads to prices at
which each sector‚Äôs income matches its expenses. Then
write the augmented matrix that can be row reduced to
Ô¨Ånd these prices.
c.[M] Find a set of equilibrium prices when the price for
the Machinery output is 100 units.
4.Suppose an economy has four sectors, Agriculture (A), En-
ergy (E), Manufacturing (M), and Transportation (T). Sector
A sells 10% of its output to E and 25% to M and retains the
rest. Sector E sells 30% of its output to A, 35% to M, and 25%
to T and retains the rest. Sector M sells 30% of its output to
A, 15% to E, and 40% to T and retains the rest. Sector T sells
20% of its output to A, 10% to E, and 30% to M and retains
the rest.
a.Construct the exchange table for this economy.
b.[M] Find a set of equilibrium prices for the economy.
Balance the chemical equations in Exercises 5‚Äì10 using the vector
equation approach discussed in this section.
5.Boron sulÔ¨Åde reacts violently with water to form boric acid
and hydrogen sulÔ¨Åde gas (the smell of rotten eggs). Theunbalanced equation is
B2S3CH2O!H3BO 3CH2S
[For each compound, construct a vector that lists the numbers
of atoms of boron, sulfur, hydrogen, and oxygen.]
6.When solutions of sodium phosphate and barium nitrate are
mixed, the result is barium phosphate (as a precipitate) and
sodium nitrate. The unbalanced equation is
Na3PO4CBa(NO3/2!Ba3.PO4/2CNaNO 3
[For each compound, construct a vector that lists the num-
bers of atoms of sodium (Na), phosphorus, oxygen, barium,
and nitrogen. For instance, barium nitrate corresponds to
.0; 0; 6; 1; 2/ .]
7.Alka-Seltzer contains sodium bicarbonate (NaHCO 3) and
citric acid (H 3C6H5O7). When a tablet is dissolved in water,
the following reaction produces sodium citrate, water, and
carbon dioxide (gas):
NaHCO 3CH3C6H5O7!Na3C6H5O7CH2OCCO 2
8.The following reaction between potassium permanganate
(KMnO 4) and manganese sulfate in water produces man-
ganese dioxide, potassium sulfate, and sulfuric acid:
KMnO 4CMnSO 4CH2O!MnO 2CK2SO4CH2SO4
[For each compound, construct a vector that lists the numbers
of atoms of potassium (K), manganese, oxygen, sulfur, and
hydrogen.]
9.[M] If possible, use exact arithmetic or rational format for
calculations in balancing the following chemical reaction:
PbN 6CCrMn 2O8!Pb3O4CCr2O3CMnO 2CNO
10. [M] The chemical reaction below can be used in some indus-
trial processes, such as the production of arsene (AsH 3). Use
exact arithmetic or rational format for calculations to balance
this equation.
MnSCAs2Cr10O35CH2SO4
!HMnO 4CAsH 3CCrS 3O12CH2O
11.Find the general Ô¨Çow pattern of the network shown in the
Ô¨Ågure. Assuming that the Ô¨Çows are all nonnegative, what is
the largest possible value for x3?
20
80x1
x2x3
x4A
CB
12. a.Find the general trafÔ¨Åc pattern in the freeway network
shown in the Ô¨Ågure. (Flow rates are in cars/minute.)
b.Describe the general trafÔ¨Åc pattern when the road whose
Ô¨Çow is x4is closed.
SECOND REVISED PAGES


--- Page 73 ---
56 CHAPTER 1 Linear Equations in Linear Algebra
c.When x4D0, what is the minimum value of x1?
40x1 x2
x3200
100
60x4 x5AB
C
D
13. a.Find the general Ô¨Çow pattern in the network shown in the
Ô¨Ågure.
b.Assuming that the Ô¨Çow must be in the directions indi-
cated, Ô¨Ånd the minimum Ô¨Çows in the branches denoted
byx2,x3,x4, and x5.
6080
90100
x1 x6x2
x3x5
x4
20 4030 40
A
EC
DB14. Intersections in England are often constructed as one-way
‚Äúroundabouts,‚Äù such as the one shown in the Ô¨Ågure. Assume
that trafÔ¨Åc must travel in the directions shown. Find the gen-
eral solution of the network Ô¨Çow. Find the smallest possible
value for x6.
10050x3
80
100120 150
x2
x1x6x5x4
ABE
FCD
SOLUTIONS TO PRACTICE PROBLEMS
1.Write the percentages as decimals. Since all output must be taken into account, each
column must sum to 1. This fact helps to Ô¨Åll in any missing entries.
Distribution of Output from:
Agriculture Mining Manufacturing Purchased by:
.65 .20 .20 Agriculture
.05 .10 .30 Mining
.30 .70 .50 Manufacturing
2.Since x5500, the equations D and A for x1andx2imply that x1100
andx2700. The fact that x50implies that x1600 andx2200. So,
100x1600, and 200x2700.
1.7 LINEAR INDEPENDENCE
The homogeneous equations in Section 1.5 can be studied from a different perspective
by writing them as vector equations. In this way, the focus shifts from the unknown
solutions of AxD0to the vectors that appear in the vector equations.
SECOND REVISED PAGES


--- Page 74 ---
1.7Linear Independence 57
For instance, consider the equation
x12
41
2
33
5Cx22
44
5
63
5Cx32
42
1
03
5D2
40
0
03
5 (1)
This equation has a trivial solution, of course, where x1Dx2Dx3D0. As in Sec-
tion 1.5, the main issue is whether the trivial solution is the only one .
D E F I N I T I O N An indexed set of vectors fv1; : : : ; vpginRnis said to be linearly independent
if the vector equation
x1v1Cx2v2C    C xpvpD0
has only the trivial solution. The set fv1; : : : ; vpgis said to be linearly dependent
if there exist weights c1; : : : ; c p, not all zero, such that
c1v1Cc2v2C    C cpvpD0 (2)
Equation (2) is called a linear dependence relation among v1; : : : ; vpwhen the
weights are not all zero. An indexed set is linearly dependent if and only if it is not lin-
early independent. For brevity, we may say that v1; : : : ; vpare linearly dependent when
we mean that fv1; : : : ; vpgis a linearly dependent set. We use analogous terminology
for linearly independent sets.
EXAMPLE 1 Letv1D2
41
2
33
5,v2D2
44
5
63
5, and v3D2
42
1
03
5.
a.Determine if the set fv1;v2;v3gis linearly independent.
b.If possible, Ô¨Ånd a linear dependence relation among v1,v2, and v3.
SOLUTION
a.We must determine if there is a nontrivial solution of equation (1) above. Row oper-
ations on the associated augmented matrix show that2
41 4 2 0
2 5 1 0
3 6 0 03
52
41 4 2 0
0 3 3 0
0 0 0 03
5
Clearly, x1andx2are basic variables, and x3is free. Each nonzero value of x3
determines a nontrivial solution of (1). Hence v1;v2;v3are linearly dependent (and
not linearly independent).
b.To Ô¨Ånd a linear dependence relation among v1,v2, and v3, completely row reduce
the augmented matrix and write the new system:
2
41 0  2 0
0 1 1 0
0 0 0 03
5x1 2x3D0
x2Cx3D0
0D0
Thus x1D2x3,x2D  x3, and x3is free. Choose any nonzero value for x3‚Äîsay,
x3D5. Then x1D10andx2D  5. Substitute these values into equation (1) and
obtain
10v1 5v2C5v3D0
This is one (out of inÔ¨Ånitely many) possible linear dependence relations among v1,
v2, and v3.
SECOND REVISED PAGES


--- Page 75 ---
58 CHAPTER 1 Linear Equations in Linear Algebra
Linear Independence of Matrix Columns
Suppose that we begin with a matrix AD¬åa1   an¬çinstead of a set of vectors. The
matrix equation AxD0can be written as
x1a1Cx2a2C    C xnanD0
Each linear dependence relation among the columns of Acorresponds to a nontrivial
solution of AxD0. Thus we have the following important fact.
The columns of a matrix Aare linearly independent if and only if the equation
AxD0hasonly the trivial solution. (3)
EXAMPLE 2 Determine if the columns of the matrix AD2
40 1 4
1 2  1
5 8 03
5are
linearly independent.
SOLUTION To study AxD0, row reduce the augmented matrix:
2
40 1 4 0
1 2  1 0
5 8 0 03
52
41 2  1 0
0 1 4 0
0 2 5 03
52
41 2  1 0
0 1 4 0
0 0 13 03
5
At this point, it is clear that there are three basic variables and no free variables. So
the equation AxD0has only the trivial solution, and the columns of Aare linearly
independent.
Sets of One or Two Vectors
A set containing only one vector‚Äîsay, v‚Äîis linearly independent if and only if vis not
the zero vector. This is because the vector equation x1vD0has only the trivial solution
when v¬§0. The zero vector is linearly dependent because x10D0has many nontrivial
solutions.
The next example will explain the nature of a linearly dependent set of two vectors.
EXAMPLE 3 Determine if the following sets of vectors are linearly independent.
a.v1D3
1
,v2D6
2
b.v1D3
2
,v2D6
2
SOLUTION
a.Notice that v2is a multiple of v1, namely, v2D2v1. Hence  2v1Cv2D0, which
shows that fv1;v2gis linearly dependent.
b.The vectors v1andv2are certainly notmultiples of one another. Could they be
linearly dependent? Suppose canddsatisfy
cv1Cdv2D0
Ifc¬§0, then we can solve for v1in terms of v2, namely, v1D. d=c/ v2. This result
is impossible because v1isnota multiple of v2. Socmust be zero. Similarly, dmust
also be zero. Thus fv1;v2gis a linearly independent set.
SECOND REVISED PAGES


--- Page 76 ---
1.7Linear Independence 59
The arguments in Example 3 show that you can always decide by inspection when a
x1x2
Linearly dependent(3, 1)(6, 2)
x1x2
Linearly independent(3, 2) (6, 2)
FIGURE 1set of two vectors is linearly dependent. Row operations are unnecessary. Simply check
whether at least one of the vectors is a scalar times the other. (The test applies only to
sets of twovectors.)
A set of two vectors fv1;v2gis linearly dependent if at least one of the vectors is
a multiple of the other. The set is linearly independent if and only if neither of the
vectors is a multiple of the other.
In geometric terms, two vectors are linearly dependent if and only if they lie on the
same line through the origin. Figure 1 shows the vectors from Example 3.
Sets of Two or More Vectors
The proof of the next theorem is similar to the solution of Example 3. Details are given
at the end of this section.
T H E O R E M 7 Characterization of Linearly Dependent Sets
An indexed set SD fv1; : : : ; vpgof two or more vectors is linearly dependent if
and only if at least one of the vectors in Sis a linear combination of the others. In
fact, if Sis linearly dependent and v1¬§0, then some vj(with j > 1 ) is a linear
combination of the preceding vectors, v1; : : : ; vj 1.
Warning: Theorem 7 does notsay that every vector in a linearly dependent set is a
linear combination of the preceding vectors. A vector in a linearly dependent set may
fail to be a linear combination of the other vectors. See Practice Problem 1(c).
EXAMPLE 4 LetuD2
43
1
03
5andvD2
41
6
03
5. Describe the set spanned by uandv,
and explain why a vector wis in Span fu;vgif and only if fu;v;wgis linearly dependent.
SOLUTION The vectors uandvare linearly independent because neither vector is
a multiple of the other, and so they span a plane in R3. (See Section 1.3.) In fact,
Spanfu;vgis the x1x2-plane (with x3D0/. Ifwis a linear combination of uandv,
thenfu;v;wgis linearly dependent, by Theorem 7. Conversely, suppose that fu;v;wg
is linearly dependent. By Theorem 7, some vector in fu;v;wgis a linear combination
of the preceding vectors (since u¬§0/. That vector must be w, since vis not a multiple
ofu. Sowis in Span fu;vg. See Figure 2.
v
wu
Linearly dependent,
w in Span{ u, v}Linearly independent,
w not in Span{ u, v}wx3
x2
x1vux3
x2
x1
FIGURE 2 Linear dependence in R3.
SECOND REVISED PAGES


--- Page 77 ---
60 CHAPTER 1 Linear Equations in Linear Algebra
Example 4 generalizes to any set fu;v;wginR3with uandvlinearly independent.
The set fu;v;wgwill be linearly dependent if and only if wis in the plane spanned by
uandv.
The next two theorems describe special cases in which the linear dependence of a
set is automatic. Moreover, Theorem 8 will be a key result for work in later chapters.
T H E O R E M 8 If a set contains more vectors than there are entries in each vector, then the set
is linearly dependent. That is, any set fv1; : : : ; vpginRnis linearly dependent if
p > n .
PROOF LetAD¬åv1   vp¬ç. Then Aisnp, and the equation AxD0corre-
sponds to a system of nequations in punknowns. If p > n , there are more variables
than equations, so there must be a free variable. Hence AxD0has a nontrivial solution,
*
***
***p
n *
**
***
**FIGURE 3
Ifp > n , the columns are linearly
dependent.
and the columns of Aare linearly dependent. See Figure 3 for a matrix version of this
theorem.
Warning: Theorem 8 says nothing about the case in which the number of vectors in
the set does notexceed the number of entries in each vector.
EXAMPLE 5 The vectors2
1
,4
 1
, 2
2
are linearly dependent by Theorem
8, because there are three vectors in the set and there are only two entries in each vector.
Notice, however, that none of the vectors is a multiple of one of the other vectors. See
Figure 4.
x1x2
(2, 1)
(4, ‚Äì1)(‚Äì2, 2)FIGURE 4
A linearly dependent set in R2.
T H E O R E M 9 If a set SD fv1; : : : ; vpginRncontains the zero vector, then the set is linearly
dependent.
PROOF By renumbering the vectors, we may suppose v1D0. Then the equation
1v1C0v2C    C 0vpD0shows that Sis linearly dependent.
EXAMPLE 6 Determine by inspection if the given set is linearly dependent.
a.2
41
7
63
5,2
42
0
93
5,2
43
1
53
5,2
44
1
83
5 b.2
42
3
53
5,2
40
0
03
5,2
41
1
83
5 c.2
664 2
4
6
103
775,2
6643
 6
 9
153
775
SOLUTION
a.The set contains four vectors, each of which has only three entries. So the set is
linearly dependent by Theorem 8.
b.Theorem 8 does not apply here because the number of vectors does not exceed the
number of entries in each vector. Since the zero vector is in the set, the set is linearly
dependent by Theorem 9.
c.Compare the corresponding entries of the two vectors. The second vector seems to
be 3=2times the Ô¨Årst vector. This relation holds for the Ô¨Årst three pairs of entries,
but fails for the fourth pair. Thus neither of the vectors is a multiple of the other, and
hence they are linearly independent.
SECOND REVISED PAGES


--- Page 78 ---
1.7Linear Independence 61
In general, you should read a section thoroughly several times to absorb an
SG Mastering: Linear
Independence 1‚Äì31 important concept such as linear independence. The notes in the Study Guide for this
section will help you learn to form mental images of key ideas in linear algebra. For
instance, the following proof is worth reading carefully because it shows how the
deÔ¨Ånition of linear independence can be used.
PROOF OF THEOREM 7 (Characterization of Linearly Dependent Sets)
If some vjinSequals a linear combination of the other vectors, then vjcan be
subtracted from both sides of the equation, producing a linear dependence relation with
a nonzero weight . 1/onvj. [For instance, if v1Dc2v2Cc3v3, then 0D. 1/v1C
c2v2Cc3v3C0v4C    C 0vp.] Thus Sis linearly dependent.
Conversely, suppose Sis linearly dependent. If v1is zero, then it is a (trivial)
linear combination of the other vectors in S. Otherwise, v1¬§0, and there exist weights
c1; : : : ; c p, not all zero, such that
c1v1Cc2v2C    C cpvpD0
Letjbe the largest subscript for which cj¬§0. IfjD1, then c1v1D0, which is
impossible because v1¬§0. Soj > 1 , and
c1v1C    C cjvjC0vjC1C    C 0vpD0
cjvjD  c1v1       cj 1vj 1
vjD
 c1
cj
v1C    C
 cj 1
cj
vj 1
PRACTICE PROBLEMS
1.LetuD2
43
2
 43
5,vD2
4 6
1
73
5,wD2
40
 5
23
5, and zD2
43
7
 53
5.
a.Are the sets fu;vg;fu;wg;fu;zg;fv;wg;fv;zg, andfw;zgeach linearly indepen-
dent? Why or why not?
b.Does the answer to Part (a) imply that fu;v;w;zgis linearly independent?
c.To determine if fu;v;w;zgis linearly dependent, is it wise to check if, say, wis
a linear combination of u,v, and z?
d.Isfu;v;w;zglinearly dependent?
2.Suppose that fv1;v2;v3gis a linearly dependent set of vectors in Rnandv4is vector
inRn. Show that fv1;v2;v3;v4gis also a linearly dependent set.
1.7 EXERCISES
In Exercises 1‚Äì4, determine if the vectors are linearly indepen-
dent. Justify each answer.
1.2
45
0
03
5,2
47
2
 63
5,2
49
4
 83
5 2.2
40
0
23
5,2
40
5
 83
5,2
4 3
4
13
5
3.1
 3
, 3
9
4. 1
4
, 2
 8
In Exercises 5‚Äì8, determine if the columns of the matrix form a
linearly independent set. Justify each answer.5.2
6640 8 5
3 7 4
 1 5  4
1 3 23
7756.2
664 4 3 0
0 1 4
1 0 3
5 4 63
775
7.2
41 4  3 0
 2 7 5 1
 4 5 7 53
5 8.2
41 3 3  2
 3 7  1 2
0 1  4 33
5
In Exercises 9 and 10, (a) for what values of hisv3in
Spanfv1;v2g, and (b) for what values of hisfv1;v2;v3glinearly
dependent ? Justify each answer.
SECOND REVISED PAGES


--- Page 79 ---
62 CHAPTER 1 Linear Equations in Linear Algebra
9.v1D2
41
 3
23
5,v2D2
4 3
9
 63
5,v3D2
45
 7
h3
5
10. v1D2
41
 5
 33
5,v2D2
4 2
10
63
5,v3D2
42
 9
h3
5
In Exercises 11‚Äì14, Ô¨Ånd the value(s) of hfor which the vectors
are linearly dependent . Justify each answer.
11.2
41
 1
43
5,2
43
 5
73
5,2
4 1
5
h3
5 12.2
42
 4
13
5,2
4 6
7
 33
5,2
48
h
43
5
13.2
41
5
 33
5,2
4 2
 9
63
5,2
43
h
 93
5 14.2
41
 1
33
5,2
4 5
7
83
5,2
41
1
h3
5
Determine by inspection whether the vectors in Exercises 15‚Äì20
are linearly independent . Justify each answer.
15.5
1
,2
8
,1
3
, 1
7
16.2
44
 2
63
5,2
46
 3
93
5
17.2
43
5
 13
5,2
40
0
03
5,2
4 6
5
43
5 18.4
4
, 1
3
,2
5
,8
1
19.2
4 8
12
 43
5,2
42
 3
 13
5 20.2
41
4
 73
5,2
4 2
5
33
5,2
40
0
03
5
In Exercises 21 and 22, mark each statement True or False. Justify
each answer on the basis of a careful reading of the text.
21. a.The columns of a matrix Aare linearly independent if the
equation AxD0has the trivial solution.
b.IfSis a linearly dependent set, then each vector is a linear
combination of the other vectors in S.
c.The columns of any 45matrix are linearly dependent.
d.Ifxand yare linearly independent, and if fx;y;zgis
linearly dependent, then zis in Span fx;yg.
22. a.Two vectors are linearly dependent if and only if they lie
on a line through the origin.
b.If a set contains fewer vectors than there are entries in the
vectors, then the set is linearly independent.
c.Ifxand yare linearly independent, and if zis in
Spanfx;yg, then fx;y;zgis linearly dependent.
d.If a set in Rnis linearly dependent, then the set contains
more vectors than there are entries in each vector.
In Exercises 23‚Äì26, describe the possible echelon forms of the
matrix. Use the notation of Example 1 in Section 1.2.
23.Ais a33matrix with linearly independent columns.24.Ais a22matrix with linearly dependent columns.
25.Ais a42matrix, AD¬åa1a2¬ç, and a2is not a multiple of
a1.
26.Ais a43matrix, AD¬åa1a2a3¬ç, such that fa1;a2gis
linearly independent and a3is not in Span fa1;a2g.
27. How many pivot columns must a 75matrix have if its
columns are linearly independent? Why?
28. How many pivot columns must a 57matrix have if its
columns span R5? Why?
29. Construct 32matrices AandBsuch that AxD0has only
the trivial solution and BxD0has a nontrivial solution.
30. a.Fill in the blank in the following statement: ‚ÄúIf Ais
anmnmatrix, then the columns of Aare linearly
independent if and only if Ahas pivot columns.‚Äù
b.Explain why the statement in (a) is true.
Exercises 31 and 32 should be solved without performing row
operations . [Hint: Write AxD0as a vector equation.]
31. Given AD2
6642 3 5
 5 1  4
 3 1 4
1 0 13
775, observe that the third column
is the sum of the Ô¨Årst two columns. Find a nontrivial solution
ofAxD0.
32. Given AD2
44 1 6
 7 5 3
9 3 33
5, observe that the Ô¨Årst column
plus twice the second column equals the third column. Find
a nontrivial solution of AxD0.
Each statement in Exercises 33‚Äì38 is either true (in all cases)
or false (for at least one example). If false, construct a speciÔ¨Åc
example to show that the statement is not always true. Such an
example is called a counterexample to the statement. If a statement
is true, give a justiÔ¨Åcation. (One speciÔ¨Åc example cannot explain
why a statement is always true. You will have to do more work
here than in Exercises 21 and 22.)
33. Ifv1; : : : ; v4are inR4andv3D2v1Cv2, thenfv1;v2;v3;v4g
is linearly dependent.
34. Ifv1; : : : ; v4are inR4andv3D0, then fv1;v2;v3;v4gis
linearly dependent.
35. Ifv1andv2are inR4andv2is not a scalar multiple of v1,
thenfv1;v2gis linearly independent.
36. Ifv1; : : : ; v4are inR4andv3isnota linear combination of
v1;v2;v4, then fv1;v2;v3;v4gis linearly independent.
37. Ifv1; : : : ; v4are inR4andfv1;v2;v3gis linearly dependent,
thenfv1;v2;v3;v4gis also linearly dependent.
38. Ifv1; : : : ; v4are linearly independent vectors in R4, then
fv1;v2;v3gis also linearly independent. [ Hint: Think about
x1v1Cx2v2Cx3v3C0v4D0.]
SECOND REVISED PAGES


--- Page 80 ---
1.8Introduction to Linear Transformations 63
39. Suppose Ais anmnmatrix with the property that for all b
inRmthe equation AxDbhas at most one solution. Use the
deÔ¨Ånition of linear independence to explain why the columns
ofAmust be linearly independent.
40. Suppose an mnmatrix Ahasnpivot columns. Explain
why for each binRmthe equation AxDbhas at most one
solution. [ Hint: Explain why AxDbcannot have inÔ¨Ånitely
many solutions.]
[M] In Exercises 41 and 42, use as many columns of Aas possible
to construct a matrix Bwith the property that the equation BxD0
has only the trivial solution. Solve BxD0to verify your work.
41.AD2
6648 3 0  7 2
 9 4 5 11  7
6 2 2  4 4
5 1 7 0 103
77542.AD2
6666412 10  6 3 7 10
 7 6 4 7  9 5
9 9  9 5 5  1
 4 3 1 6  8 9
8 7  5 9 11  83
77775
43. [M] With AandBas in Exercise 41, select a column vofA
that was not used in the construction of Band determine if
vis in the set spanned by the columns of B. (Describe your
calculations.)
44. [M] Repeat Exercise 43 with the matrices AandBfrom
Exercise 42. Then give an explanation for what you discover,
assuming that Bwas constructed as speciÔ¨Åed.
SOLUTIONS TO PRACTICE PROBLEMS
1.a.Yes. In each case, neither vector is a multiple of the other. Thus each set is linearly
independent.
b.No. The observation in Part (a), by itself, says nothing about the linear indepen-
dence of fu;v;w;zg.
c.No. When testing for linear independence, it is usually a poor idea to check if
one selected vector is a linear combination of the others. It may happen that
the selected vector is not a linear combination of the others and yet the whole
set of vectors is linearly dependent. In this practice problem, wis not a linear
combination of u,v, and z.
Span{ u, v, z}wx3
x2
x1
d.Yes, by Theorem 8. There are more vectors (four) than entries (three) in them.
2.Applying the deÔ¨Ånition of linearly dependent to fv1;v2;v3gimplies that there exist
scalars c1; c2, and c3, not all zero, such that
c1v1Cc2v2Cc3v3D0:
Adding 0v4D0to both sides of this equation results in
c1v1Cc2v2Cc3v3C0v4D0:
Since c1; c2; c3and 0 are not allzero, the set fv1;v2;v3;v4gsatisÔ¨Åes the deÔ¨Ånition of
a linearly dependent set.
1.8 INTRODUCTION TO LINEAR TRANSFORMATIONS
The difference between a matrix equation AxDband the associated vector equation
x1a1C    C xnanDbis merely a matter of notation. However, a matrix equation
AxDbcan arise in linear algebra (and in applications such as computer graphics and
signal processing) in a way that is not directly connected with linear combinations of
vectors. This happens when we think of the matrix Aas an object that ‚Äúacts‚Äù on a vector
xby multiplication to produce a new vector called Ax.
SECOND REVISED PAGES


--- Page 81 ---
64 CHAPTER 1 Linear Equations in Linear Algebra
For instance, the equations
4 3 1 3
2 0 5 12
6641
1
1
13
775D5
8
and4 3 1 3
2 0 5 12
6641
4
 1
33
775D0
0
6 66 6 6 6
A x b A u 0
say that multiplication by Atransforms xintoband transforms uinto the zero vector.
See Figure 1.
0multiplication
by Ax
0
u 0b
4 2multiplication
by A
FIGURE 1 Transforming vectors via matrix
multiplication.
From this new point of view, solving the equation AxDbamounts to Ô¨Ånding
all vectors xinR4that are transformed into the vector binR2under the ‚Äúaction‚Äù of
multiplication by A.
The correspondence from xtoAxis afunction from one set of vectors to another.
This concept generalizes the common notion of a function as a rule that transforms one
real number into another.
Atransformation (orfunction ormapping )TfromRntoRmis a rule that assigns
to each vector xinRna vector T .x/inRm. The set Rnis called the domain ofT, andRm
is called the codomain ofT. The notation TWRn!Rmindicates that the domain of T
isRnand the codomain is Rm. For xinRn, the vector T .x/inRmis called the image ofx
(under the action of T). The set of all images T .x/is called the range ofT. See Figure 2.
mRangeT(x)
Codomain DomainxT
n
FIGURE 2 Domain, codomain, and range of
TWRn!Rm.
The new terminology in this section is important because a dynamic view of matrix‚Äì
vector multiplication is the key to understanding several ideas in linear algebra and to
building mathematical models of physical systems that evolve over time. Such dynam-
ical systems will be discussed in Sections 1.10, 4.8, and 4.9 and throughout Chapter 5.
Matrix Transformations
The rest of this section focuses on mappings associated with matrix multiplication. For
each xinRn,T .x/is computed as Ax, where Ais an mnmatrix. For simplicity, we
sometimes denote such a matrix transformation byx7!Ax. Observe that the domain of
SECOND REVISED PAGES


--- Page 82 ---
1.8Introduction to Linear Transformations 65
TisRnwhen Ahasncolumns and the codomain of TisRmwhen each column of A
hasmentries. The range of Tis the set of all linear combinations of the columns of A,
because each image T .x/is of the form Ax.
EXAMPLE 1 LetAD2
41 3
3 5
 1 73
5,uD2
 1
,bD2
43
2
 53
5,cD2
43
2
53
5, and
deÔ¨Åne a transformation TWR2!R3byT .x/DAx, so that
T .x/DAxD2
41 3
3 5
 1 73
5x1
x2
D2
4x1 3x2
3x1C5x2
 x1C7x23
5
a.Find T .u/, the image of uunder the transformation T.
b.Find an xinR2whose image under Tisb.
c.Is there more than one xwhose image under Tisb?
d.Determine if cis in the range of the transformation T.
SOLUTION
a.Compute
T .u/DAuD2
41 3
3 5
 1 73
52
 1
D2
45
1
 93
5
b.Solve T .x/Dbforx. That is, solve AxDb, or
2
41 3
3 5
 1 73
5x1
x2
D2
43
2
 53
5 (1)
Using the method discussed in Section 1.4, row reduce the augmented matrix:
Tx2
u /H110052
/H110021
5
1 T(u) /H11005
/H110029x1
x3
x1x2
2
41 3 3
3 5 2
 1 7  53
52
41 3 3
0 14  7
0 4  23
52
41 3 3
0 1  :5
0 0 03
52
41 0 1:5
0 1  :5
0 0 03
5(2)
Hence x1D1:5,x2D  :5, and xD1:5
 :5
. The image of this xunder Tis the
given vector b.
c.Any xwhose image under Tisbmust satisfy equation (1). From (2), it is clear that
equation (1) has a unique solution. So there is exactly one xwhose image is b.
d.The vector cis in the range of Tifcis the image of some xinR2, that is, if cDT .x/
for some x. This is just another way of asking if the system AxDcis consistent. To
Ô¨Ånd the answer, row reduce the augmented matrix:
2
41 3 3
3 5 2
 1 7 53
52
41 3 3
0 14  7
0 4 83
52
41 3 3
0 1 2
0 14  73
52
41 3 3
0 1 2
0 0  353
5
The third equation, 0D  35, shows that the system is inconsistent. So cisnotin the
range of T.
The question in Example 1(c) is a uniqueness problem for a system of linear
equations, translated here into the language of matrix transformations: Is bthe image of
aunique xinRn? Similarly, Example 1(d) is an existence problem: Does there exist an
xwhose image is c?
SECOND REVISED PAGES


--- Page 83 ---
66 CHAPTER 1 Linear Equations in Linear Algebra
The next two matrix transformations can be viewed geometrically. They reinforce
the dynamic view of a matrix as something that transforms vectors into other vectors.
Section 2.7 contains other interesting examples connected with computer graphics.
EXAMPLE 2 IfAD2
41 0 0
0 1 0
0 0 03
5, then the transformation x7!Axprojects
points in R3onto the x1x2-plane because
2
4x1
x2
x33
57!2
41 0 0
0 1 0
0 0 03
52
4x1
x2
x33
5D2
4x1
x2
03
5
See Figure 3.
x3
0
x1x2FIGURE 3
A projection transformation.
EXAMPLE 3 LetAD1 3
0 1
. The transformation TWR2!R2deÔ¨Åned by
T .x/DAxis called a shear transformation . It can be shown that if Tacts on each
point in the 22square shown in Figure 4, then the set of images forms the shaded
parallelogram. The key idea is to show that Tmaps line segments onto line segments
(as shown in Exercise 27) and then to check that the corners of the square map onto
the vertices of the parallelogram. For instance, the image of the point uD0
2
is
sheep
sheared shee p
T .u/D1 3
0 10
2
D6
2
, and the image of2
2
is1 3
0 12
2
D8
2
.T
deforms the square as if the top of the square were pushed to the right while the base is
held Ô¨Åxed. Shear transformations appear in physics, geology, and crystallography.
T
2x2
x122x2
8x12
FIGURE 4 A shear transformation.
Linear Transformations
Theorem 5 in Section 1.4 shows that if Aismn, then the transformation x7!Axhas
the properties
A.uCv/DAuCAvand A.cu/DcAu
for all u;vinRnand all scalars c. These properties, written in function notation, identify
the most important class of transformations in linear algebra.
D E F I N I T I O N A transformation (or mapping) Tislinear if:
(i)T .uCv/DT .u/CT .v/for all u;vin the domain of T;
(ii)T .cu/DcT .u/for all scalars cand all uin the domain of T.
Every matrix transformation is a linear transformation. Important examples of
linear transformations that are not matrix transformations will be discussed in Chapters
4 and 5.
SECOND REVISED PAGES


--- Page 84 ---
1.8Introduction to Linear Transformations 67
Linear transformations preserve the operations of vector addition and scalar mul-
tiplication . Property (i) says that the result T .uCv/of Ô¨Årst adding uandvinRnand
then applying Tis the same as Ô¨Årst applying Ttouand to vand then adding T .u/and
T .v/inRm. These two properties lead easily to the following useful facts.
IfTis a linear transformation, then
T .0/D0 (3)
and
T .cuCdv/DcT .u/CdT .v/ (4)
for all vectors u,vin the domain of Tand all scalars c; d.
Property (3) follows from condition (ii) in the deÔ¨Ånition, because T .0/DT .0u/D
0T .u/D0. Property (4) requires both (i) and (ii):
T .cuCdv/DT .cu/CT .dv/DcT .u/CdT .v/
Observe that if a transformation satisÔ¨Åes (4)for all u,vandc; d,it must be linear .
(SetcDdD1for preservation of addition, and set dD0for preservation of scalar
multiplication.) Repeated application of (4) produces a useful generalization:
T .c 1v1C    C cpvp/Dc1T .v1/C    C cpT .vp/ (5)
In engineering and physics, (5) is referred to as a superposition principle . Think
ofv1; : : : ; vpas signals that go into a system and T .v1/; : : : ; T . vp/as the responses of
that system to the signals. The system satisÔ¨Åes the superposition principle if whenever
an input is expressed as a linear combination of such signals, the system‚Äôs response is
the same linear combination of the responses to the individual signals. We will return to
this idea in Chapter 4.
EXAMPLE 4 Given a scalar r, deÔ¨Åne TWR2!R2byT .x/Drx.Tis called a
contraction when 0r1and a dilation when r > 1 . LetrD3, and show that Tis
a linear transformation.
SOLUTION Letu,vbe inR2and let c; dbe scalars. Then
T .cuCdv/D3.cuCdv/ DeÔ¨Ånition of T
D3cuC3dv
Dc.3u/Cd.3v/)
Vector arithmetic
DcT .u/CdT .v/
Thus Tis a linear transformation because it satisÔ¨Åes (4). See Figure 5.
T(u)
x1x2
x1T
ux2
FIGURE 5 A dilation transformation.
SECOND REVISED PAGES


--- Page 85 ---
68 CHAPTER 1 Linear Equations in Linear Algebra
EXAMPLE 5 DeÔ¨Åne a linear transformation TWR2!R2by
T .x/D0 1
1 0x1
x2
D x2
x1
Find the images under TofuD4
1
,vD2
3
, and uCvD6
4
.
SOLUTION
T .u/D0 1
1 04
1
D 1
4
; T . v/D0 1
1 02
3
D 3
2
;
T .uCv/D0 1
1 06
4
D 4
6
Note that T .uCv/is obviously equal to T .u/CT .v/. It appears from Figure 6 that
Trotates u,v, and uCvcounterclockwise about the origin through 90. In fact, T
transforms the entire parallelogram determined by uandvinto the one determined by
T .u/andT .v/. (See Exercise 28.)
T
x1x2
v
uT(u + v)
T(u)
T(v)u + v
FIGURE 6 A rotation transformation.
The Ô¨Ånal example is not geometrical; instead, it shows how a linear mapping can
transform one type of data into another.
EXAMPLE 6 A company manufactures two products, B and C. Using data from
Example 7 in Section 1.3, we construct a ‚Äúunit cost‚Äù matrix, UD¬åb c ¬ç, whose
columns describe the ‚Äúcosts per dollar of output‚Äù for the products:
UDProduct
B C2
4:45 :40
:25 :30
:15 :153
5Materials
Labor
Overhead
LetxD.x1; x2/be a ‚Äúproduction‚Äù vector, corresponding to x1dollars of product B and
x2dollars of product C, and deÔ¨Åne TWR2!R3by
T .x/DUxDx12
4:45
:25
:153
5Cx22
4:40
:30
:153
5D2
4Total cost of materials
Total cost of labor
Total cost of overhead3
5
The mapping Ttransforms a list of production quantities (measured in dollars) into a
list of total costs. The linearity of this mapping is reÔ¨Çected in two ways:
1.If production is increased by a factor of, say, 4, from xto4x, then the costs will
increase by the same factor, from T .x/to4T .x/.
SECOND REVISED PAGES


--- Page 86 ---
1.8Introduction to Linear Transformations 69
2.Ifxandyare production vectors, then the total cost vector associated with the
combined production xCyis precisely the sum of the cost vectors T .x/and
T .y/.
PRACTICE PROBLEMS
1.Suppose TWR5!R2andT .x/DAxfor some matrix Aand for each xinR5. How
many rows and columns does Ahave?
2.LetAD1 0
0 1
. Give a geometric description of the transformation x7!Ax.
3.The line segment from 0to a vector uis the set of points of the form tu, where
0t1. Show that a linear transformation Tmaps this segment into the segment
between 0andT .u/.
1.8 EXERCISES
1.LetAD2 0
0 2
, and deÔ¨Åne TWR2!R2byT .x/DAx.
Find the images under TofuD1
 3
andvDa
b
.
2.LetAD2
4:5 0 0
0 :5 0
0 0 :53
5,uD2
41
0
 43
5, and vD2
4a
b
c3
5.
DeÔ¨Åne TWR3!R3byT .x/DAx. Find T .u/andT .v/.
In Exercises 3‚Äì6, with TdeÔ¨Åned by T .x/DAx, Ô¨Ånd a vector x
whose image under Tisb, and determine whether xis unique.
3.AD2
41 0  2
 2 1 6
3 2 53
5,bD2
4 1
7
 33
5
4.AD2
41 3 2
0 1  4
3 5 93
5,bD2
46
 7
 93
5
5.AD1 5 7
 3 7 5
,bD 2
 2
6.AD2
6641 2 1
3 4 5
0 1 1
 3 5  43
775,bD2
6641
9
3
 63
775
7.LetAbe a65matrix. What must aandbbe in order to
deÔ¨Åne TWRa!RbbyT .x/DAx?
8.How many rows and columns must a matrix Ahave in order
to deÔ¨Åne a mapping from R4intoR5by the rule T .x/DAx?
For Exercises 9 and 10, Ô¨Ånd all xinR4that are mapped into the
zero vector by the transformation x7!Axfor the given matrix A.
9.AD2
41 4 7  5
0 1  4 3
2 6 6  43
510.AD2
6641 3 9 2
1 0 3  4
0 1 2 3
 2 3 0 53
775
11.LetbD2
4 1
1
03
5, and let Abe the matrix in Exercise 9. Is bin
the range of the linear transformation x7!Ax? Why or why
not?
12. LetbD2
664 1
3
 1
43
775, and let Abe the matrix in Exercise 10. Is
bin the range of the linear transformation x7!Ax? Why or
why not?
In Exercises 13‚Äì16, use a rectangular coordinate system to plot
uD5
2
,vD 2
4
, and their images under the given transfor-
mation T. (Make a separate and reasonably large sketch for each
exercise.) Describe geometrically what Tdoes to each vector x
inR2.
13.T .x/D 1 0
0 1 x1
x2
14.T .x/D:5 0
0 :5 x1
x2
15.T .x/D0 0
0 1 x1
x2
16.T .x/D0 1
1 0 x1
x2
17. LetTWR2!R2be a linear transformation that maps
uD5
2
into2
1
and maps vD1
3
into 1
3
. Use the
fact that Tis linear to Ô¨Ånd the images under Tof3u,2v, and
3uC2v.
SECOND REVISED PAGES


--- Page 87 ---
70 CHAPTER 1 Linear Equations in Linear Algebra
18. The Ô¨Ågure shows vectors u,v, and w, along with the images
T .u/andT .v/under the action of a linear transformation
TWR2!R2. Copy this Ô¨Ågure carefully, and draw the image
T .w/as accurately as possible. [ Hint: First, write was a
linear combination of uandv.]
u w
vT(v)
T(u)x2x2
x1x1
19. Lete1D1
0
,e2D0
1
,y1D2
5
, and y2D 1
6
, and
letTWR2!R2be a linear transformation that maps e1
intoy1and maps e2intoy2. Find the images of5
 3
and
x1
x2
.
20. Let xDx1
x2
,v1D 2
5
, and v2D7
 3
, and let
TWR2!R2be a linear transformation that maps xinto
x1v1Cx2v2. Find a matrix Asuch that T .x/isAxfor
each x.
In Exercises 21 and 22, mark each statement True or False. Justify
each answer.
21. a.A linear transformation is a special type of function.
b.IfAis a35matrix and Tis a transformation deÔ¨Åned
byT .x/DAx, then the domain of TisR3.
c.IfAis anmnmatrix, then the range of the transforma-
tionx7!AxisRm.
d.Every linear transformation is a matrix transformation.
e.A transformation Tis linear if and only if T .c 1v1C
c2v2/Dc1T .v1/Cc2T .v2/for all v1and v2in the
domain of Tand for all scalars c1andc2.
22. a.Every matrix transformation is a linear transformation.
b.The codomain of the transformation x7!Axis the set of
all linear combinations of the columns of A.
c.IfTWRn!Rmis a linear transformation and if cis
inRm, then a uniqueness question is ‚ÄúIs cin the range
ofT?‚Äù
d.A linear transformation preserves the operations of vector
addition and scalar multiplication.
e.The superposition principle is a physical description of a
linear transformation.
23. LetTWR2!R2be the linear transformation that reÔ¨Çects
each point through the x1-axis. (See Practice Problem 2.)Make two sketches similar to Figure 6 that illustrate prop-
erties (i) and (ii) of a linear transformation.
24. Suppose vectors v1; : : : ; vpspanRn, and let TWRn!Rnbe
a linear transformation. Suppose T .vi/D0foriD1; : : : ; p .
Show that Tis the zero transformation. That is, show that if
xis any vector in Rn, then T .x/D0.
25. Given v¬§0andpinRn, the line through pin the direction of
vhas the parametric equation xDpCtv. Show that a linear
transformation TWRn!Rnmaps this line onto another line
or onto a single point (a degenerate line ).
26. Letuandvbe linearly independent vectors in R3, and let P
be the plane through u,v, and 0. The parametric equation
ofPisxDsuCtv(with s; tinR). Show that a linear
transformation TWR3!R3maps Ponto a plane through
0, or onto a line through 0, or onto just the origin in R3. What
must be true about T .u/andT .v/in order for the image of
the plane Pto be a plane?
27. a.Show that the line through vectors pandqinRnmay be
written in the parametric form xD.1 t/pCtq. (Refer
to the Ô¨Ågure with Exercises 21 and 22 in Section 1.5.)
b.The line segment from ptoqis the set of points of the
form .1 t/pCtqfor0t1(as shown in the Ô¨Ågure
below). Show that a linear transformation Tmaps this
line segment onto a line segment or onto a single point.
(t = 1) q (1 ‚Äì t)p + tq
x
(t = 0) p
28. Letuandvbe vectors in Rn. It can be shown that the set Pof
all points in the parallelogram determined by uandvhas the
form auCbv, for0a1,0b1. Let TWRn!Rm
be a linear transformation. Explain why the image of a point
inPunder the transformation Tlies in the parallelogram
determined by T .u/andT .v/.
29. DeÔ¨Åne fWR!Rbyf .x/DmxCb.
a.Show that fis a linear transformation when bD0.
b.Find a property of a linear transformation that is violated
when b¬§0.
c.Why is fcalled a linear function?
30. An afÔ¨Åne transformation TWRn!Rmhas the form
T .x/ DAxCb, with Aanmnmatrix and binRm. Show
thatTisnota linear transformation when b¬§0. (AfÔ¨Åne
transformations are important in computer graphics.)
31. LetTWRn!Rmbe a linear transformation, and let
fv1;v2;v3gbe a linearly dependent set in Rn. Explain why
the set fT .v1/; T . v2/; T . v3/gis linearly dependent.
In Exercises 32‚Äì36, column vectors are written as rows, such as
xD.x1; x2/, and T .x/is written as T .x 1; x2/.
32. Show that the transformation TdeÔ¨Åned by T .x 1; x2/D
.4x 1 2x2; 3jx2j/is not linear.
SECOND REVISED PAGES


--- Page 88 ---
1.9The Matrix of a Linear Transformation 71
33. Show that the transformation TdeÔ¨Åned by T .x 1; x2/D
.2x 1 3x2; x1C4; 5x 2/is not linear.
34. LetTWRn!Rmbe a linear transformation. Show that if
Tmaps two linearly independent vectors onto a linearly
dependent set, then the equation T .x/D0has a nontrivial
solution. [ Hint: Suppose uandvinRnare linearly inde-
pendent and yet T .u/andT .v/are linearly dependent. Then
c1T .u/Cc2T .v/D0for some weights c1andc2, not both
zero. Use this equation.]
35. LetTWR3!R3be the transformation that reÔ¨Çects each
vector xD.x1; x2; x3/through the plane x3D0onto
T .x/D.x1; x2; x3/. Show that Tis a linear transformation.
[See Example 4 for ideas.]
36. LetTWR3!R3be the transformation that projects each
vector xD.x1; x2; x3/onto the plane x2D0, soT .x/D
.x1; 0; x 3/. Show that Tis a linear transformation.
[M] In Exercises 37 and 38, the given matrix determines a linear
transformation T. Find all xsuch that T .x/D0.37.2
6644 2 5  5
 9 7  8 0
 6 4 5 3
5 3 8  43
77538.2
664 9 4 9 4
5 8 7 6
7 11 16  9
9 7 4 53
775
39. [M] Let bD2
6647
5
9
73
775and let Abe the matrix in Exercise 37. Is
bin the range of the transformation x7!Ax? If so, Ô¨Ånd an x
whose image under the transformation is b.
40. [M] Let bD2
664 7
 7
13
 53
775and let Abe the matrix in Exercise 38.
Isbin the range of the transformation x7!Ax? If so, Ô¨Ånd an
xwhose image under the transformation is b.
SG
Mastering: Linear Transformations 1‚Äì34
SOLUTIONS TO PRACTICE PROBLEMS
1.Amust have Ô¨Åve columns for Axto be deÔ¨Åned. Amust have two rows for the
Au
ux2
x1
A
The transformation x      Ax.vv
Axx
codomain of Tto beR2.
2.Plot some random points (vectors) on graph paper to see what happens. A point such
as.4; 1/ maps into .4; 1/. The transformation x7!AxreÔ¨Çects points through the
x-axis (or x1-axis).
3.LetxDtufor some tsuch that 0t1. Since Tis linear, T .tu/Dt T .u/, which
is a point on the line segment between 0andT .u/.
1.9 THE MATRIX OF A LINEAR TRANSFORMATION
Whenever a linear transformation Tarises geometrically or is described in words, we
usually want a ‚Äúformula‚Äù for T .x/. The discussion that follows shows that every linear
transformation from RntoRmis actually a matrix transformation x7!Axand that
important properties of Tare intimately related to familiar properties of A. The key to
Ô¨Ånding Ais to observe that Tis completely determined by what it does to the columns
of the nnidentity matrix In.
EXAMPLE 1 The columns of I2D1 0
0 1
aree1D1
0
and e2D0
1
.
Suppose Tis a linear transformation from R2intoR3such that
T .e1/D2
45
 7
23
5and T .e2/D2
4 3
8
03
5
With no additional information, Ô¨Ånd a formula for the image of an arbitrary xinR2.
x1x2
e2 =0
1‚é°‚é£‚é°‚é£
e1 =1
0‚é°‚é£‚é°‚é£
SECOND REVISED PAGES


--- Page 89 ---
72 CHAPTER 1 Linear Equations in Linear Algebra
SOLUTION Write
xDx1
x2
Dx11
0
Cx20
1
Dx1e1Cx2e2 (1)
Since Tis alinear transformation,
T .x/Dx1T .e1/Cx2T .e2/ (2)
Dx12
45
 7
23
5Cx22
4 3
8
03
5D2
45x1 3x2
 7x1C8x2
2x1C03
5
The step from equation (1) to equation (2) explains why knowledge of T .e1/and
T .e2/is sufÔ¨Åcient to determine T .x/for any x. Moreover, since (2) expresses T .x/as
a linear combination of vectors, we can put these vectors into the columns of a matrix
Aand write (2) as
T .x/DT .e1/ T . e2/x1
x2
DAx
T H E O R E M 1 0 LetTWRn!Rmbe a linear transformation. Then there exists a unique matrix
Asuch that
T .x/DAxfor all xinRn
In fact, Ais the mnmatrix whose jth column is the vector T .ej/, where ejis
thejth column of the identity matrix in Rn:
ADT .e1/   T .en/
(3)
PROOF Write xDInxD¬åe1   en¬çxDx1e1C    C xnen, and use the linearity
ofTto compute
T .x/DT .x 1e1C    C xnen/Dx1T .e1/C    C xnT .en/
DT .e1/   T .en/2
64x1
:::
xn3
75DAx
The uniqueness of Ais treated in Exercise 33.
The matrix Ain (3) is called the standard matrix for the linear transforma-
tionT.
We know now that every linear transformation from RntoRmcan be viewed as
a matrix transformation, and vice versa. The term linear transformation focuses on a
property of a mapping, while matrix transformation describes how such a mapping is
implemented, as Examples 2 and 3 illustrate.
EXAMPLE 2 Find the standard matrix Afor the dilation transformation T .x/D3x,
forxinR2.
SECOND REVISED PAGES


--- Page 90 ---
1.9The Matrix of a Linear Transformation 73
SOLUTION Write
T .e1/D3e1D3
0
and T .e2/D3e2D0
3
??
AD3 0
0 3
EXAMPLE 3 LetTWR2!R2be the transformation that rotates each point in R2
about the origin through an angle ', with counterclockwise rotation for a positive angle.
We could show geometrically that such a transformation is linear. (See Figure 6 in
Section 1.8.) Find the standard matrix Aof this transformation.
SOLUTION1
0
rotates intocos'
sin'
, and0
1
rotates into sin'
cos'
. See Figure 1.
By Theorem 10,
ADcos' sin'
sin' cos'
Example 5 in Section 1.8 is a special case of this transformation, with 'D=2.
(‚Äì sin œï, cos œï)
(cos œï, sin œï)
(1, 0)(0, 1)
œï
œïx1x2
FIGURE 1 A rotation transformation.
Geometric Linear Transformations of R2
Examples 2 and 3 illustrate linear transformations that are described geometrically.
Tables 1‚Äì4 illustrate other common geometric linear transformations of the plane.
Because the transformations are linear, they are determined completely by what they
do to the columns of I2. Instead of showing only the images of e1ande2, the tables
show what a transformation does to the unit square (Figure 2).
Other transformations can be constructed from those listed in Tables 1‚Äì4 by
applying one transformation after another. For instance, a horizontal shear could be
followed by a reÔ¨Çection in the x2-axis. Section 2.1 will show that such a composition of
linear transformations is linear. (Also, see Exercise 36.)
x1x2
1
00
1FIGURE 2
The unit square.
Existence and Uniqueness Questions
The concept of a linear transformation provides a new way to understand the existence
and uniqueness questions asked earlier. The two deÔ¨Ånitions following Tables 1‚Äì4 give
the appropriate terminology for transformations.
SECOND REVISED PAGES


--- Page 91 ---
74 CHAPTER 1 Linear Equations in Linear Algebra
TABLE 1 Reflections
Transformation Image of the Unit Square Standard Matrix
Re/f_lection through
the x1-axis01
/H1100210
/H1100210
0 /H1100210 /H110021
10
0 /H110021
/H110021 010
01Re/f_lection through
the x2-axis
Re/f_lection through
the line x2 /H11005 x1
x2 /H11005 /H11002 x1Re/f_lection through
the line x2 /H11005 /H11002 x1
/H110021
0
/H110021
00
/H110021
0
/H110021x1x2
Re/f_lection through
the origin
x1x2
x1x2
1
0
0
/H110021
x1x2
0
1
/H110021
0
x1x2
1
00
1x2 /H11005 x1
SECOND REVISED PAGES


--- Page 92 ---
1.9The Matrix of a Linear Transformation 75
TABLE 2 Contractions and Expansions
Transformation Image of the Unit Square Standard Matrix
Horizontal
contraction
and expansion
Vertical
contraction
and expansion0 k
10
01
k 00 /H11021 k /H11021 1
0 /H11021 k /H11021 1 k /H11022 1k /H11022 10
1
1
01
00
k0
1
0
kk
0k
0
x1x2
x2
x1x2
x1x2
x1
TABLE 3 Shears
Transformation Image of the Unit Square Standard Matrix
Horizontal shear k 1
10
01
1 kk /H11021 0 k /H11022 0
0
10
11
0k
1k
1
1
k
1
kVertical shear
k /H11021 0 k /H11022 0x2
k k 1
0
kkx2
x1x2
x1x2
x1
 x1
SECOND REVISED PAGES


--- Page 93 ---
76 CHAPTER 1 Linear Equations in Linear Algebra
TABLE 4 Projections
Transformation Image of the Unit Square Standard Matrix
0 1
0 0
0 0
1 01
00
0
0
1
Projection onto
the x1-axis
Projection onto
the x2-axisx1x2
0
0
x1x2
D E F I N I T I O N A mapping TWRn!Rmis said to be ontoRmif each binRmis the image of
at least one xinRn.
Equivalently, Tis onto Rmwhen the range of Tis all of the codomain Rm. That is,
TmapsRnontoRmif, for each bin the codomain Rm, there exists at least one solution
ofT .x/Db. ‚ÄúDoes TmapRnontoRm?‚Äù is an existence question. The mapping Tis
notonto when there is some binRmfor which the equation T .x/Dbhas no solution.
See Figure 3.
m
T is ontomnDomainRange
T is not ontomT
nDomainRange
mT
FIGURE 3 Is the range of Tall ofRm?
D E F I N I T I O N A mapping TWRn!Rmis said to be one-to-one if each binRmis the image
ofat most one xinRn.
SECOND REVISED PAGES


--- Page 94 ---
1.9The Matrix of a Linear Transformation 77
Equivalently, Tis one-to-one if, for each binRm, the equation T .x/Dbhas
either a unique solution or none at all. ‚ÄúIs Tone-to-one?‚Äù is a uniqueness question.
The mapping Tisnotone-to-one when some binRmis the image of more than one
vector in Rn. If there is no such b, then Tis one-to-one. See Figure 4.
m
T is not one-to-one T is one-to-oneT T
mn n00 0 0Range Range Domain Domain
FIGURE 4 Is every bthe image of at most one vector?
The projection transformations shown in Table 4 are notone-to-one and do notmap
SG Mastering: Existence
and Uniqueness 1‚Äì39 R2ontoR2. The transformations in Tables 1, 2, and 3 are one-to-one anddo map R2
ontoR2. Other possibilities are shown in the two examples below.
Example 4 and the theorems that follow show how the function properties of being
one-to-one and mapping onto are related to important concepts studied earlier in this
chapter.
EXAMPLE 4 LetTbe the linear transformation whose standard matrix is
AD2
41 4 8 1
0 2  1 3
0 0 0 53
5
Does TmapR4ontoR3? IsTa one-to-one mapping?
SOLUTION Since Ahappens to be in echelon form, we can see at once that Ahas a
pivot position in each row. By Theorem 4 in Section 1.4, for each binR3, the equation
AxDbis consistent. In other words, the linear transformation TmapsR4(its domain)
ontoR3. However, since the equation AxDbhas a free variable (because there are four
variables and only three basic variables), each bis the image of more than one x. That
is,Tisnotone-to-one.
T H E O R E M 1 1 LetTWRn!Rmbe a linear transformation. Then Tis one-to-one if and only if
the equation T .x/D0has only the trivial solution.
Remark: To prove a theorem that says ‚Äústatement Pis true if and only if statement Qis
true,‚Äù one must establish two things: (1) If Pis true, then Qis true and (2) If Qis true,
thenPis true. The second requirement can also be established by showing (2a): If Pis
false, then Qis false. (This is called contrapositive reasoning.) This proof uses (1) and
(2a) to show that PandQare either both true or both false.
PROOF Since Tis linear, T .0/D0. IfTis one-to-one, then the equation T .x/D0
has at most one solution and hence only the trivial solution. If Tis not one-to-one, then
there is a bthat is the image of at least two different vectors in Rn‚Äîsay, uandv. That
is,T .u/DbandT .v/Db. But then, since Tis linear,
T .u v/DT .u/ T .v/Db bD0
The vector u vis not zero, since u¬§v. Hence the equation T .x/D0has more than
one solution. So, either the two conditions in the theorem are both true or they are both
false.
SECOND REVISED PAGES


--- Page 95 ---
78 CHAPTER 1 Linear Equations in Linear Algebra
T H E O R E M 1 2 LetTWRn!Rmbe a linear transformation, and let Abe the standard matrix for
T. Then:
a.TmapsRnontoRmif and only if the columns of AspanRm;
b.Tis one-to-one if and only if the columns of Aare linearly independent.
Remark: ‚ÄúIf and only if‚Äù statements can be linked together. For example if ‚Äú Pif and
only if Q‚Äù is known and ‚Äú Qif and only if R‚Äù is known, then one can conclude ‚Äú Pif
and only if R.‚Äù This strategy is used repeatedly in this proof.
PROOF
a.By Theorem 4 in Section 1.4, the columns of AspanRmif and only if for each b
inRmthe equation AxDbis consistent‚Äîin other words, if and only if for every b,
the equation T .x/Dbhas at least one solution. This is true if and only if Tmaps
RnontoRm.
b.The equations T .x/D0andAxD0are the same except for notation. So, by
Theorem 11, Tis one-to-one if and only if AxD0has only the trivial solution. This
happens if and only if the columns of Aare linearly independent, as was already
noted in the boxed statement (3) in Section 1.7.
Statement (a) in Theorem 12 is equivalent to the statement ‚Äú TmapsRnontoRm
if and only if every vector in Rmis a linear combination of the columns of A.‚Äù See
Theorem 4 in Section 1.4.
In the next example and in some exercises that follow, column vectors are written in
rows, such as xD.x1; x2/, and T .x/is written as T .x 1; x2/instead of the more formal
T ..x 1; x2//.
EXAMPLE 5 LetT .x 1; x2/D.3x1Cx2,5x1C7x2,x1C3x2/. Show that Tis a
one-to-one linear transformation. Does TmapR2ontoR3?
SOLUTION When xandT .x/are written as column vectors, you can determine the
standard matrix of Tby inspection, visualizing the row‚Äìvector computation of each
entry in Ax.
T .x/D2
43x1Cx2
5x1C7x2
x1C3x23
5D2
4? ?
? ?
? ?3
5x1
x2
D2
43 1
5 7
1 33
5x1
x2
(4)
A
SoTis indeed a linear transformation, with its standard matrix Ashown in (4). The
columns of Aare linearly independent because they are not multiples. By Theorem
12(b), Tis one-to-one. To decide if Tis onto R3, examine the span of the columns of
A. Since Ais32, the columns of AspanR3if and only if Ahas 3 pivot positions, by
Theorem 4. This is impossible, since Ahas only 2 columns. So the columns of Ado not
spanR3, and the associated linear transformation is not onto R3.
x2
The transformation T is not
onto     .3e2
e1x1x2
T
Tx3
x1Span{a1, a2}a1a2
PRACTICE PROBLEMS
1.LetTWR2!R2be the transformation that Ô¨Årst performs a horizontal shear that
maps e2intoe2 :5e1(but leaves e1unchanged) and then reÔ¨Çects the result through
thex2-axis. Assuming that Tis linear, Ô¨Ånd its standard matrix. [ Hint: Determine the
Ô¨Ånal location of the images of e1ande2.]
2.Suppose Ais a75matrix with 5 pivots. Let T .x/DAxbe a linear transformation
fromR5intoR7. IsTa one-to-one linear transformation? Is TontoR7?
SECOND REVISED PAGES


--- Page 96 ---
1.9The Matrix of a Linear Transformation 79
1.9 EXERCISES
In Exercises 1‚Äì10, assume that Tis a linear transformation. Find
the standard matrix of T.
1.TWR2!R4,T .e1/D.3; 1; 3; 1/ andT .e2/D. 5; 2; 0; 0/ ,
where e1D.1; 0/ ande2D.0; 1/ .
2.TWR3!R2,T .e1/D.1; 3/ ,T .e2/D.4; 7/, and
T .e3/D. 5; 4/, where e1,e2,e3are the columns of the
33identity matrix.
3.TWR2!R2rotates points (about the origin) through 3=2
radians (counterclockwise).
4.TWR2!R2rotates points (about the origin) through  =4
radians (clockwise). [ Hint: T .e1/D.1=p
2; 1=p
2/.]
5.TWR2!R2is a vertical shear transformation that maps e1
intoe1 2e2but leaves the vector e2unchanged.
6.TWR2!R2is a horizontal shear transformation that leaves
e1unchanged and maps e2intoe2C3e1.
7.TWR2!R2Ô¨Årst rotates points through  3=4 radian
(clockwise) and then reÔ¨Çects points through the horizontal
x1-axis. [ Hint: T .e1/D. 1=p
2; 1=p
2/.]
8.TWR2!R2Ô¨Årst reÔ¨Çects points through the horizontal x1-
axis and then reÔ¨Çects points through the line x2Dx1.
9.TWR2!R2Ô¨Årst performs a horizontal shear that trans-
forms e2into e2 2e1(leaving e1unchanged) and then re-
Ô¨Çects points through the line x2D  x1.
10.TWR2!R2Ô¨Årst reÔ¨Çects points through the vertical x2-axis
and then rotates points =2 radians.
11.A linear transformation TWR2!R2Ô¨Årst reÔ¨Çects points
through the x1-axis and then reÔ¨Çects points through the x2-
axis. Show that Tcan also be described as a linear transfor-
mation that rotates points about the origin. What is the angle
of that rotation?
12. Show that the transformation in Exercise 8 is merely a rota-
tion about the origin. What is the angle of the rotation?
13. LetTWR2!R2be the linear transformation such that T .e1/
andT .e2/are the vectors shown in the Ô¨Ågure. Using the
Ô¨Ågure, sketch the vector T .2; 1/ .
T(e1) T(e2)
x1x2
14. LetTWR2!R2be a linear transformation with standard
matrix AD¬åa1a2¬ç, where a1and a2are shown in the
Ô¨Ågure. Using the Ô¨Ågure, draw the image of 1
3
under thetransformation T.
x1x2
a2
a1
In Exercises 15 and 16, Ô¨Åll in the missing entries of the matrix,
assuming that the equation holds for all values of the variables.
15.2
4¬ã ¬ã ¬ã
¬ã ¬ã ¬ã
¬ã ¬ã ¬ã3
52
4x1
x2
x33
5D2
43x1 2x3
4x1
x1 x2Cx33
5
16.2
4¬ã ¬ã
¬ã ¬ã
¬ã ¬ã3
5x1
x2
D2
4x1 x2
 2x1Cx2
x13
5
In Exercises 17‚Äì20, show that Tis a linear transformation by
Ô¨Ånding a matrix that implements the mapping. Note that x1; x2; : : :
are not vectors but are entries in vectors.
17.T .x 1; x2; x3; x4/D.0; x 1Cx2; x2Cx3; x3Cx4/
18.T .x 1; x2/D.2x 2 3x1; x1 4x2; 0; x 2/
19.T .x 1; x2; x3/D.x1 5x2C4x3; x2 6x3/
20.T .x 1; x2; x3; x4/D2x1C3x3 4x4 .TWR4!R/
21. LetTWR2!R2be a linear transformation such that
T .x 1; x2/D.x1Cx2; 4x 1C5x2/. Find xsuch that T .x/D
.3; 8/ .
22. LetTWR2!R3be a linear transformation such that
T .x 1; x2/D.x1 2x2; x1C3x2; 3x 1 2x2/. Find xsuch
thatT .x/D. 1; 4; 9/ .
In Exercises 23 and 24, mark each statement True or False. Justify
each answer.
23. a.A linear transformation TWRn!Rmis completely de-
termined by its effect on the columns of the nnidentity
matrix.
b.IfTWR2!R2rotates vectors about the origin through
an angle ', then Tis a linear transformation.
c.When two linear transformations are performed one after
another, the combined effect may not always be a linear
transformation.
d.A mapping TWRn!Rmis onto Rmif every vector xin
Rnmaps onto some vector in Rm.
e.IfAis a32matrix, then the transformation x7!Ax
cannot be one-to-one.
24. a.Not every linear transformation from RntoRmis a matrix
transformation.
b.The columns of the standard matrix for a linear transfor-
mation from RntoRmare the images of the columns of
thennidentity matrix.
SECOND REVISED PAGES


--- Page 97 ---
80 CHAPTER 1 Linear Equations in Linear Algebra
c.The standard matrix of a linear transformation from R2
toR2that reÔ¨Çects points through the horizontal axis,
the vertical axis, or the origin has the forma 0
0 d
,
where aanddare1.
d.A mapping TWRn!Rmis one-to-one if each vector in
Rnmaps onto a unique vector in Rm.
e.IfAis a32matrix, then the transformation x7!Ax
cannot map R2ontoR3.
In Exercises 25‚Äì28, determine if the speciÔ¨Åed linear transforma-
tion is (a) one-to-one and (b) onto. Justify each answer.
25. The transformation in Exercise 17
26. The transformation in Exercise 2
27. The transformation in Exercise 19
28. The transformation in Exercise 14
In Exercises 29 and 30, describe the possible echelon forms of the
standard matrix for a linear transformation T. Use the notation of
Example 1 in Section 1.2.
29.TWR3!R4is one-to-one.
30.TWR4!R3is onto.
31. LetTWRn!Rmbe a linear transformation, with Aits
standard matrix. Complete the following statement to make
it true: ‚Äú Tis one-to-one if and only if Ahas pivot
columns.‚Äù Explain why the statement is true. [ Hint: Look in
the exercises for Section 1.7.]
32. LetTWRn!Rmbe a linear transformation, with Aits
standard matrix. Complete the following statement to make
it true: ‚Äú TmapsRnontoRmif and only if Ahas
pivot columns.‚Äù Find some theorems that explain why the
statement is true.
33. Verify the uniqueness of Ain Theorem 10. Let TWRn!Rm
be a linear transformation such that T .x/DBxfor somemnmatrix B. Show that if Ais the standard matrix for
T, then ADB. [Hint: Show that AandBhave the same
columns.]
34. Why is the question ‚ÄúIs the linear transformation Tonto?‚Äù
an existence question?
35. If a linear transformation TWRn!RmmapsRnontoRm,
can you give a relation between mandn? IfTis one-to-one,
what can you say about mandn?
36. LetSWRp!RnandTWRn!Rmbe linear transforma-
tions. Show that the mapping x7!T .S. x//is a linear trans-
formation (from RptoRm). [Hint: Compute T .S.c uCdv//
foru;vinRpand scalars candd. Justify each step of the
computation, and explain why this computation gives the
desired conclusion.]
[M] In Exercises 37‚Äì40, let Tbe the linear transformation whose
standard matrix is given. In Exercises 37 and 38, decide if Tis a
one-to-one mapping. In Exercises 39 and 40, decide if TmapsR5
ontoR5. Justify your answers.
37.2
664 5 10  5 4
8 3  4 7
4 9 5  3
 3 2 5 43
77538.2
6647 5 4  9
10 6 16  4
12 8 12 7
 8 6 2 53
775
39.2
666644 7 3 7 5
6 8 5 12  8
 7 10  8 9 14
3 5 4 2  6
 5 6  6 7 33
77775
40.2
666649 13 5 6  1
14 15  7 6 4
 8 9 12  5 9
 5 6 8 9 8
13 14 15 2 113
77775
SOLUTION TO PRACTICE PROBLEMS
1.Follow what happens to e1ande2. See Figure 5. First, e1is unaffected by the shear
WEB
and then is reÔ¨Çected into  e1. SoT .e1/D  e1. Second, e2goes to e2 :5e1by the
shear transformation. Since reÔ¨Çection through the x2-axis changes e1into e1and
1
01
01
0/H110021
Shear transformation Reflection through the x2-axisx2
0
1/H11002.5
1.5
1
x1x2x2
x1x1
FIGURE 5 The composition of two transformations.
SECOND REVISED PAGES


--- Page 98 ---
1.10 Linear Models in Business, Science, and Engineering 81
leaves e2unchanged, the vector e2 :5e1goes to e2C:5e1. SoT .e2/De2C:5e1.
Thus the standard matrix of Tis
T .e1/ T . e2/
D e1 e2C:5e1
D 1 :5
0 1
2.The standard matrix representation of Tis the matrix A. Since Ahas 5 columns and
5 pivots, there is a pivot in every column so the columns are linearly independent.
By Theorem 12, Tis one-to-one. Since Ahas 7 rows and only 5 pivots, there is not
a pivot in every row and hence the columns of Ado not span R7. By Theorem 12,
andTis not onto.
1.10 LINEAR MODELS IN BUSINESS, SCIENCE, AND ENGINEERING
The mathematical models in this section are all linear ; that is, each describes a
problem by means of a linear equation, usually in vector or matrix form. The Ô¨Årst
model concerns nutrition but actually is representative of a general technique in linear
programming problems. The second model comes from electrical engineering. The third
model introduces the concept of a linear difference equation , a powerful mathematical
tool for studying dynamic processes in a wide variety of Ô¨Åelds such as engineering,
ecology, economics, telecommunications, and the management sciences. Linear models
are important because natural phenomena are often linear or nearly linear when the
variables involved are held within reasonable bounds. Also, linear models are more
easily adapted for computer calculation than are complex nonlinear models.
As you read about each model, pay attention to how its linearity reÔ¨Çects some
property of the system being modeled.
Constructing a Nutritious Weight-Loss Diet
The formula for the Cambridge Diet, a popular diet in the 1980s, was based on years
WEB
of research. A team of scientists headed by Dr. Alan H. Howard developed this diet
at Cambridge University after more than eight years of clinical work with obese
patients.1The very low-calorie powdered formula diet combines a precise balance
of carbohydrate, high-quality protein, and fat, together with vitamins, minerals, trace
elements, and electrolytes. Millions of persons have used the diet to achieve rapid and
substantial weight loss.
To achieve the desired amounts and proportions of nutrients, Dr. Howard had to
incorporate a large variety of foodstuffs in the diet. Each foodstuff supplied several of
the required ingredients, but not in the correct proportions. For instance, nonfat milk was
a major source of protein but contained too much calcium. So soy Ô¨Çour was used for
part of the protein because soy Ô¨Çour contains little calcium. However, soy Ô¨Çour contains
proportionally too much fat, so whey was added since it supplies less fat in relation to
calcium. Unfortunately, whey contains too much carbohydrate : : : :
The following example illustrates the problem on a small scale. Listed in Table 1
are three of the ingredients in the diet, together with the amounts of certain nutrients
supplied by 100 grams (g) of each ingredient.2
1The Ô¨Årst announcement of this rapid weight-loss regimen was given in the International Journal of Obesity
(1978) 2, 321‚Äì332.
2Ingredients in the diet as of 1984; nutrient data for ingredients adapted from USDA Agricultural
Handbooks No. 8-1 and 8-6, 1976.
SECOND REVISED PAGES


--- Page 99 ---
82 CHAPTER 1 Linear Equations in Linear Algebra
TABLE 1
Amounts (g) Supplied per 100 g of Ingredient
Nutrient Nonfat milk Soy Ô¨Çour WheyAmounts (g) Supplied by
Cambridge Diet in One Day
Protein 36 51 13 33
Carbohydrate 52 34 74 45
Fat 0 7 1.1 3
EXAMPLE 1 If possible, Ô¨Ånd some combination of nonfat milk, soy Ô¨Çour, and whey
to provide the exact amounts of protein, carbohydrate, and fat supplied by the diet in
one day (Table 1).
SOLUTION Letx1,x2, and x3, respectively, denote the number of units (100 g) of
these foodstuffs. One approach to the problem is to derive equations for each nutrient
separately. For instance, the product
x1units of
nonfat milk
protein per unit
of nonfat milk
gives the amount of protein supplied by x1units of nonfat milk. To this amount, we
would then add similar products for soy Ô¨Çour and whey and set the resulting sum equal
to the amount of protein we need. Analogous calculations would have to be made for
each nutrient.
A more efÔ¨Åcient method, and one that is conceptually simpler, is to consider a
‚Äúnutrient vector‚Äù for each foodstuff and build just one vector equation. The amount
of nutrients supplied by x1units of nonfat milk is the scalar multiple
Scalar Vectorx1units of
nonfat milk
nutrients per unit
of nonfat milk
Dx1a1(1)
where a1is the Ô¨Årst column in Table 1. Let a2anda3be the corresponding vectors for
soy Ô¨Çour and whey, respectively, and let bbe the vector that lists the total nutrients
required (the last column of the table). Then x2a2andx3a3give the nutrients supplied
byx2units of soy Ô¨Çour and x3units of whey, respectively. So the relevant equation is
x1a1Cx2a2Cx3a3Db (2)
Row reduction of the augmented matrix for the corresponding system of equations
shows that2
436 51 13 33
52 34 74 45
0 7 1 .133
5    2
41 0 0 :277
0 1 0 :392
0 0 1 :2333
5
To three signiÔ¨Åcant digits, the diet requires .277 units of nonfat milk, .392 units of
soy Ô¨Çour, and .233 units of whey in order to provide the desired amounts of protein,
carbohydrate, and fat.
It is important that the values of x1,x2, and x3found above are nonnegative. This is
necessary for the solution to be physically feasible. (How could you use  :233 units of
whey, for instance?) With a large number of nutrient requirements, it may be necessary
to use a larger number of foodstuffs in order to produce a system of equations with
a ‚Äúnonnegative‚Äù solution. Thus many, many different combinations of foodstuffs may
need to be examined in order to Ô¨Ånd a system of equations with such a solution. In
fact, the manufacturer of the Cambridge Diet was able to supply 31 nutrients in precise
amounts using only 33 ingredients.
SECOND REVISED PAGES


--- Page 100 ---
1.10 Linear Models in Business, Science, and Engineering 83
The diet construction problem leads to the linear equation (2) because the amount
of nutrients supplied by each foodstuff can be written as a scalar multiple of a vector,
as in (1). That is, the nutrients supplied by a foodstuff are proportional to the amount of
the foodstuff added to the diet mixture. Also, each nutrient in the mixture is the sumof
the amounts from the various foodstuffs.
Problems of formulating specialized diets for humans and livestock occur fre-
quently. Usually they are treated by linear programming techniques. Our method of
constructing vector equations often simpliÔ¨Åes the task of formulating such problems.
Linear Equations and Electrical Networks
Current Ô¨Çow in a simple electrical network can be described by a system of linear
WEB
equations. A voltage source such as a battery forces a current of electrons to Ô¨Çow through
the network. When the current passes through a resistor (such as a lightbulb or motor),
some of the voltage is ‚Äúused up‚Äù; by Ohm‚Äôs law, this ‚Äúvoltage drop‚Äù across a resistor is
given by
VDRI
where the voltage Vis measured in volts , the resistance Rinohms (denoted by ), and
the current Ô¨Çow Iinamperes (amps , for short).
The network in Figure 1 contains three closed loops. The currents Ô¨Çowing in loops
1, 2, and 3 are denoted by I1; I2, and I3, respectively. The designated directions of such
loop currents are arbitrary. If a current turns out to be negative, then the actual direction
of current Ô¨Çow is opposite to that chosen in the Ô¨Ågure. If the current direction shown is
away from the positive (longer) side of a battery ( ) around to the negative (shorter)
side, the voltage is positive; otherwise, the voltage is negative.
Current Ô¨Çow in a loop is governed by the following rule.
KIRCHHOFF'S VOLTAGE LAW
The algebraic sum of the RIvoltage drops in one direction around a loop equals
the algebraic sum of the voltage sources in the same direction around the loop.
EXAMPLE 2 Determine the loop currents in the network in Figure 1.
1 Œ© 1 Œ©
1 Œ© 1 Œ©4 Œ© 4 Œ©
D CB A
1 Œ©3 Œ©
5 volts
20 volts30 volts
I2I1
I3
FIGURE 1SOLUTION For loop 1, the current I1Ô¨Çows through three resistors, and the sum of the
RIvoltage drops is
4I1C4I1C3I1D.4C4C3/I1D11I 1
Current from loop 2 also Ô¨Çows in part of loop 1, through the short branch between A
andB. The associated RIdrop there is 3I2volts. However, the current direction for the
branch ABin loop 1 is opposite to that chosen for the Ô¨Çow in loop 2, so the algebraic
sum of all RIdrops for loop 1 is 11I 1 3I2. Since the voltage in loop 1 is C30volts,
Kirchhoff‚Äôs voltage law implies that
11I 1 3I2D30
The equation for loop 2 is
 3I1C6I2 I3D5
The term  3I1comes from the Ô¨Çow of the loop 1 current through the branch AB(with
a negative voltage drop because the current Ô¨Çow there is opposite to the Ô¨Çow in loop 2).
The term 6I2is the sum of all resistances in loop 2, multiplied by the loop current. The
SECOND REVISED PAGES


--- Page 101 ---
84 CHAPTER 1 Linear Equations in Linear Algebra
term I3D  1I3comes from the loop 3 current Ô¨Çowing through the 1-ohm resistor
in branch CD, in the direction opposite to the Ô¨Çow in loop 2. The loop 3 equation is
 I2C3I3D  25
Note that the 5-volt battery in branch CDis counted as part of both loop 2 and loop 3,
but it is  5volts for loop 3 because of the direction chosen for the current in loop 3.
The 20-volt battery is negative for the same reason.
The loop currents are found by solving the system
11I 1 3I2 D30
 3I1C6I2 I3D 5
 I2C3I3D  25(3)
Row operations on the augmented matrix lead to the solution: I1D3 amps, I2D1 amp,
andI3D  8amps. The negative value of I3indicates that the actual current in loop 3
Ô¨Çows in the direction opposite to that shown in Figure 1.
It is instructive to look at system (3) as a vector equation:
I12
411
 3
03
5
6
r1CI22
4 3
6
 13
5
6
r2CI32
40
 1
33
5
6
r3D2
430
5
 253
5
6v(4)
The Ô¨Årst entry of each vector concerns the Ô¨Årst loop, and similarly for the second and
third entries. The Ô¨Årst resistor vector r1lists the resistance in the various loops through
which current I1Ô¨Çows. A resistance is written negatively when I1Ô¨Çows against the Ô¨Çow
direction in another loop. Examine Figure 1 and see how to compute the entries in r1;
then do the same for r2andr3. The matrix form of equation (4),
RiDv;where RD¬år1r2r3¬çand iD2
4I1
I2
I33
5
provides a matrix version of Ohm‚Äôs law. If all loop currents are chosen in the same direc-
tion (say, counterclockwise), then all entries off the main diagonal of Rwill be negative.
The matrix equation RiDvmakes the linearity of this model easy to see at a
glance. For instance, if the voltage vector is doubled, then the current vector must
double. Also, a superposition principle holds. That is, the solution of equation (4) is
the sum of the solutions of the equations
RiD2
430
0
03
5; R iD2
40
5
03
5;and RiD2
40
0
 253
5
Each equation here corresponds to the circuit with only one voltage source (the other
sources being replaced by wires that close each loop). The model for current Ô¨Çow is
linear precisely because Ohm‚Äôs law and Kirchhoff‚Äôs law are linear: The voltage drop
across a resistor is proportional to the current Ô¨Çowing through it (Ohm), and the sumof
the voltage drops in a loop equals the sum of the voltage sources in the loop (Kirchhoff).
Loop currents in a network can be used to determine the current in any branch of
the network. If only one loop current passes through a branch, such as from BtoD
in Figure 1, the branch current equals the loop current. If more than one loop current
passes through a branch, such as from AtoB, the branch current is the algebraic sum
of the loop currents in the branch ( Kirchhoff‚Äô s current law ). For instance, the current in
branch ABisI1 I2D3 1D2 amps, in the direction of I1. The current in branch
CDisI2 I3D9 amps.
SECOND REVISED PAGES


--- Page 102 ---
1.10 Linear Models in Business, Science, and Engineering 85
Difference Equations
In many Ô¨Åelds such as ecology, economics, and engineering, a need arises to model
mathematically a dynamic system that changes over time. Several features of the system
are each measured at discrete time intervals, producing a sequence of vectors x0,x1,
x2; : : : : The entries in xkprovide information about the state of the system at the time
of the kth measurement.
If there is a matrix Asuch that x1DAx0,x2DAx1, and, in general,
xkC1DAxkforkD0; 1; 2; : : : (5)
then (5) is called a linear difference equation (orrecurrence relation ). Given such
an equation, one can compute x1,x2, and so on, provided x0is known. Sections 4.8
and 4.9, and several sections in Chapter 5, will develop formulas for xkand describe
what can happen to xkaskincreases indeÔ¨Ånitely. The discussion below illustrates how
a difference equation might arise.
A subject of interest to demographers is the movement of populations or groups of
people from one region to another. The simple model here considers the changes in the
population of a certain city and its surrounding suburbs over a period of years.
Fix an initial year‚Äîsay, 2014‚Äîand denote the populations of the city and suburbs
that year by r0ands0, respectively. Let x0be the population vector
x0Dr0
s0
City population, 2014
Suburban population, 2014
For 2015 and subsequent years, denote the populations of the city and suburbs by the
vectors
x1Dr1
s1
; x2Dr2
s2
; x3Dr3
s3
; : : :
Our goal is to describe mathematically how these vectors might be related.
Suppose demographic studies show that each year about 5% of the city‚Äôs population
moves to the suburbs (and 95% remains in the city), while 3% of the suburban population
moves to the city (and 97% remains in the suburbs). See Figure 2.
.03.05
.95 .97City Suburbs
FIGURE 2 Annual percentage migration between city and suburbs.
After 1 year, the original r0persons in the city are now distributed between city and
suburbs as:95r 0
:05r 0
Dr0:95
:05
Remain in city
Move to suburbs(6)
Thes0persons in the suburbs in 2014 are distributed 1 year later as
s0:03
:97
Move to city
Remain in suburbs(7)
SECOND REVISED PAGES


--- Page 103 ---
86 CHAPTER 1 Linear Equations in Linear Algebra
The vectors in (6) and (7) account for all of the population in 2015.3Thus
r1
s1
Dr0:95
:05
Cs0:03
:97
D:95 :03
:05 :97r0
s0
That is,
x1DMx0 (8)
where Mis the migration matrix determined by the following table:
From:
City Suburbs To::95
:05:03
:97
City
Suburbs
Equation (8) describes how the population changes from 2014 to 2015. If the migration
percentages remain constant, then the change from 2015 to 2016 is given by
x2DMx1
and similarly for 2016 to 2017 and subsequent years. In general,
xkC1DMxkforkD0; 1; 2; : : : (9)
The sequence of vectors fx0;x1;x2; : : :gdescribes the population of the city/suburban
region over a period of years.
EXAMPLE 3 Compute the population of the region just described for the years
2015 and 2016, given that the population in 2014 was 600,000 in the city and 400,000
in the suburbs.
SOLUTION The initial population in 2014 is x0D600;000
400;000
. For 2015,
x1D:95 :03
:05 :97600;000
400;000
D582;000
418;000
For 2016,
x2DMx1D:95 :03
:05 :97582;000
418;000
D565;440
434;560
The model for population movement in (9) is linear because the correspondence
xk7!xkC1is a linear transformation. The linearity depends on two facts: the number
of people who chose to move from one area to another is proportional to the number of
people in that area, as shown in (6) and (7), and the cumulative effect of these choices
is found by adding the movement of people from the different areas.
PRACTICE PROBLEM
Find a matrix Aand vectors xandbsuch that the problem in Example 1 amounts to
solving the equation AxDb.
3For simplicity, we ignore other inÔ¨Çuences on the population such as births, deaths, and migration into and
out of the city/suburban region.
SECOND REVISED PAGES


--- Page 104 ---
1.10 Linear Models in Business, Science, and Engineering 87
1.10 EXERCISES
1.The container of a breakfast cereal usually lists the number
of calories and the amounts of protein, carbohydrate, and
fat contained in one serving of the cereal. The amounts for
two common cereals are given below. Suppose a mixture of
these two cereals is to be prepared that contains exactly 295
calories, 9 g of protein, 48 g of carbohydrate, and 8 g of fat.
a.Set up a vector equation for this problem. Include a state-
ment of what the variables in your equation represent.
b.Write an equivalent matrix equation, and then determine
if the desired mixture of the two cereals can be prepared.
Nutrition Information per Serving
General Mills Quaker¬Æ
Nutrient Cheerios¬Æ100% Natural Cereal
Calories 110 130
Protein (g) 4 3
Carbohydrate (g) 20 18
Fat (g) 2 5
2.One serving of Post Shredded Wheat¬Æsupplies 160 calories,
5 g of protein, 6 g of Ô¨Åber, and 1 g of fat. One serving of
Crispix¬Æsupplies 110 calories, 2 g of protein, .1 g of Ô¨Åber,
and .4 g of fat.
a.Set up a matrix Band a vector usuch that Bugives the
amounts of calories, protein, Ô¨Åber, and fat contained in
a mixture of three servings of Shredded Wheat and two
servings of Crispix.
b.[M] Suppose that you want a cereal with more Ô¨Åber than
Crispix but fewer calories than Shredded Wheat. Is it
possible for a mixture of the two cereals to supply 130
calories, 3.20 g of protein, 2.46 g of Ô¨Åber, and .64 g of
fat? If so, what is the mixture?
3.After taking a nutrition class, a big Annie‚Äôs¬ÆMac and Cheese
fan decides to improve the levels of protein and Ô¨Åber in
her favorite lunch by adding broccoli and canned chicken.
The nutritional information for the foods referred to in this
exercise are given in the table below.
Nutrition Information per Serving
Nutrient Mac and Cheese Broccoli Chicken Shells
Calories 270 51 70 260
Protein (g) 10 5.4 15 9
Fiber (g) 2 5.2 0 5
a.[M] If she wants to limit her lunch to 400 calories but
get 30 g of protein and 10 g of Ô¨Åber, what proportions of
servings of Mac and Cheese, broccoli, and chicken should
she use?
b.[M] She found that there was too much broccoli in the
proportions from part (a), so she decided to switch fromclassical Mac and Cheese to Annie‚Äôs¬ÆWhole Wheat
Shells and White Cheddar. What proportions of servings
of each food should she use to meet the same goals as in
part (a)?
4.The Cambridge Diet supplies .8 g of calcium per day, in
addition to the nutrients listed in Table 1 for Example 1.
The amounts of calcium per unit (100 g) supplied by the
three ingredients in the Cambridge Diet are as follows: 1.26 g
from nonfat milk, .19 g from soy Ô¨Çour, and .8 g from whey.
Another ingredient in the diet mixture is isolated soy protein,
which provides the following nutrients in each unit: 80 g of
protein, 0 g of carbohydrate, 3.4 g of fat, and .18 g of calcium.
a.Set up a matrix equation whose solution determines the
amounts of nonfat milk, soy Ô¨Çour, whey, and isolated
soy protein necessary to supply the precise amounts of
protein, carbohydrate, fat, and calcium in the Cambridge
Diet. State what the variables in the equation represent.
b.[M] Solve the equation in (a) and discuss your answer.
In Exercises 5‚Äì8, write a matrix equation that determines the loop
currents. [M]If MATLAB or another matrix program is available,
solve the system for the loop currents.
5.
I1
I2
I3
I41 Œ©
2 Œ©4 Œ©
3 Œ©3 Œ©
4 Œ©1 Œ©
1 Œ©20 V
30 V
10 V
20 V
10 V1 Œ©
5 Œ©
1 Œ©
2 Œ©
4 Œ© 6.
I1
I2
I3
I44 Œ©
1 Œ©2 Œ©
2 Œ©20 V
40 V
10 V30 V3 Œ©
1 Œ©
4 Œ©
2 Œ©
3 Œ©
7.
10 V I4I1
I3I23 Œ©5 Œ©1 Œ©
7 Œ©
6 Œ©
2 Œ©20 V40 V
30 V4 Œ©4 Œ©
SECOND REVISED PAGES


--- Page 105 ---
88 CHAPTER 1 Linear Equations in Linear Algebra
8.
I1 I4
I2 I3I5
2 Œ©1 Œ©
2 Œ©1 Œ©
1 Œ©3 Œ©50 V 40 V
20 V 30 V1 Œ© 3 Œ©4 Œ©
3 Œ©2 Œ©
3 Œ©
9.In a certain region, about 7% of a city‚Äôs population moves
to the surrounding suburbs each year, and about 5% of the
suburban population moves into the city. In 2015, there were
800,000 residents in the city and 500,000 in the suburbs.
Set up a difference equation that describes this situation,
where x0is the initial population in 2015. Then estimate
the populations in the city and in the suburbs two years
later, in 2017. (Ignore other factors that might inÔ¨Çuence the
population sizes.)
10. In a certain region, about 6% of a city‚Äôs population moves
to the surrounding suburbs each year, and about 4% of the
suburban population moves into the city. In 2015, there were
10,000,000 residents in the city and 800,000 in the suburbs.
Set up a difference equation that describes this situation,
where x0is the initial population in 2015. Then estimate the
populations in the city and in the suburbs two years later, in
2017.
11.In 2012 the population of California was 38,041,430, and the
population living in the United States but outside California
was 275,872,610. During the year, it is estimated that
748,252 persons moved from California to elsewhere in the
United States, while 493,641 persons moved to California
from elsewhere in the United States.4
a.Set up the migration matrix for this situation, using Ô¨Åve
decimal places for the migration rates into and out of
California. Let your work show how you produced the
migration matrix.
b.[M] Compute the projected populations in the year 2022
for California and elsewhere in the United States, assum-
ing that the migration rates did not change during the 10-
year period. (These calculations do not take into account
births, deaths, or the substantial migration of persons into
California and elsewhere in the United States from other
countries.)
4Migration data retrieved from http://www.governing.com/12. [M] Budget¬ÆRent A Car in Wichita, Kansas, has a Ô¨Çeet of
about 500 cars, at three locations. A car rented at one location
may be returned to any of the three locations. The various
fractions of cars returned to the three locations are shown in
the matrix below. Suppose that on Monday there are 295 cars
at the airport (or rented from there), 55 cars at the east side
ofÔ¨Åce, and 150 cars at the west side ofÔ¨Åce. What will be the
approximate distribution of cars on Wednesday?
Cars Rented From:
Airport East West Returned To:2
4:97
:00
:03:05
:90
:05:10
:05
:853
5Airport
East
West
13. [M] Let Mandx0be as in Example 3.
a.Compute the population vectors xkforkD1; : : : ; 20 .
Discuss what you Ô¨Ånd.
b.Repeat part (a) with an initial population of 350,000 in
the city and 650,000 in the suburbs. What do you Ô¨Ånd?
14. [M] Study how changes in boundary temperatures on a steel
plate affect the temperatures at interior points on the plate.
a.Begin by estimating the temperatures T1,T2,T3,T4at
each of the sets of four points on the steel plate shown in
the Ô¨Ågure. In each case, the value of Tkis approximated by
the average of the temperatures at the four closest points.
See Exercises 33 and 34 in Section 1.1, where the values
(in degrees) turn out to be .20; 27:5; 30; 22:5/ . How is this
list of values related to your results for the points in set
(a) and set (b)?
b.Without making any computations, guess the interior
temperatures in (a) when the boundary temperatures are
all multiplied by 3. Check your guess.
c.Finally, make a general conjecture about the correspon-
dence from the list of eight boundary temperatures to the
list of four interior temperatures.
0¬∫
0¬∫0¬∫
0¬∫20¬∫ 20¬∫
20¬∫ 20¬∫12
43
(a)Plate A
10¬∫
10¬∫40¬∫
40¬∫0¬∫ 0¬∫
10¬∫ 10¬∫12
43
(b)Plate B
SECOND REVISED PAGES


--- Page 106 ---
Chapter 1 Supplementary Exercises 89
SOLUTION TO PRACTICE PROBLEM
AD2
436 51 13
52 34 74
0 7 1:13
5;xD2
4x1
x2
x33
5;bD2
433
45
33
5
CHAPTER 1 SUPPLEMENTARY EXERCISES
1.Mark each statement True or False. Justify each answer. (If
true, cite appropriate facts or theorems. If false, explain why
or give a counterexample that shows why the statement is not
true in every case.
a.Every matrix is row equivalent to a unique matrix in
echelon form.
b.Any system of nlinear equations in nvariables has at
most nsolutions.
c.If a system of linear equations has two different solu-
tions, it must have inÔ¨Ånitely many solutions.
d.If a system of linear equations has no free variables, then
it has a unique solution.
e.If an augmented matrix ¬åAb¬çis transformed into
¬åCd¬çby elementary row operations, then the equa-
tions AxDbandCxDdhave exactly the same solu-
tion sets.
f.If a system AxDbhas more than one solution, then so
does the system AxD0.
g.IfAis an mnmatrix and the equation AxDbis
consistent for some b, then the columns of AspanRm.
h.If an augmented matrix ¬åAb¬çcan be transformed by
elementary row operations into reduced echelon form,
then the equation AxDbis consistent.
i.If matrices AandBare row equivalent, they have the
same reduced echelon form.
j.The equation AxD0has the trivial solution if and only
if there are no free variables.
k.IfAis an mnmatrix and the equation AxDbis con-
sistent for every binRm, then Ahasmpivot columns.
l.If an mnmatrix Ahas a pivot position in every row,
then the equation AxDbhas a unique solution for each
binRm.
m.If an nnmatrix Ahasnpivot positions, then the
reduced echelon form of Ais the nnidentity matrix.
n.If33matrices AandBeach have three pivot posi-
tions, then Acan be transformed into Bby elementary
row operations.o.IfAis an mnmatrix, if the equation AxDbhas at
least two different solutions, and if the equation AxDc
is consistent, then the equation AxDchas many solu-
tions.
p.IfAandBare row equivalent mnmatrices and if the
columns of AspanRm, then so do the columns of B.
q.If none of the vectors in the set SD fv1;v2;v3ginR3is
a multiple of one of the other vectors, then Sis linearly
independent.
r.Iffu;v;wgis linearly independent, then u,v, and ware
not inR2.
s.In some cases, it is possible for four vectors to span R5.
t.Ifuandvare inRm, then  uis in Span fu;vg.
u.Ifu,v, and ware nonzero vectors in R2, then wis a linear
combination of uandv.
v.Ifwis a linear combination of uandvinRn, then uis a
linear combination of vandw.
w.Suppose that v1,v2, and v3are inR5,v2is not a multiple
ofv1, and v3is not a linear combination of v1andv2.
Thenfv1;v2;v3gis linearly independent.
x.A linear transformation is a function.
y.IfAis a65matrix, the linear transformation x7!Ax
cannot map R5ontoR6.
z.IfAis an mnmatrix with mpivot columns, then the
linear transformation x7!Axis a one-to-one mapping.
2.Letaandbrepresent real numbers. Describe the possible
solution sets of the (linear) equation axDb. [Hint: The
number of solutions depends upon aandb.]
3.The solutions .x; y; ¬¥/ of a single linear equation
axCbyCc¬¥Dd
form a plane in R3when a,b, andcare not all zero. Construct
sets of three linear equations whose graphs (a) intersect in
a single line, (b) intersect in a single point, and (c) have no
SECOND REVISED PAGES


--- Page 107 ---
90 CHAPTER 1 Linear Equations in Linear Algebra
points in common. Typical graphs are illustrated in the Ô¨Ågure.
Three planes intersecting
in a lineThree planes intersecting
in a point
Three planes with no
intersectionThree planes with no
intersection(a) (b)
(c) (c')
4.Suppose the coefÔ¨Åcient matrix of a linear system of three
equations in three variables has a pivot position in each
column. Explain why the system has a unique solution.
5.Determine handksuch that the solution set of the system
(i) is empty, (ii) contains a unique solution, and (iii) contains
inÔ¨Ånitely many solutions.
a.x1C3x2Dk
4x1Chx2D8b. 2x1Chx2D1
6x1Ckx2D  2
6.Consider the problem of determining whether the following
system of equations is consistent:
4x1 2x2C7x3D  5
8x1 3x2C10x 3D  3
a.DeÔ¨Åne appropriate vectors, and restate the problem in
terms of linear combinations. Then solve that problem.
b.DeÔ¨Åne an appropriate matrix, and restate the problem
using the phrase ‚Äúcolumns of A.‚Äù
c.DeÔ¨Åne an appropriate linear transformation Tusing the
matrix in (b), and restate the problem in terms of T.
7.Consider the problem of determining whether the following
system of equations is consistent for all b1,b2,b3:
2x1 4x2 2x3Db1
 5x1Cx2Cx3Db2
7x1 5x2 3x3Db3
a.DeÔ¨Åne appropriate vectors, and restate the problem in
terms of Span fv1;v2;v3g. Then solve that problem.
b.DeÔ¨Åne an appropriate matrix, and restate the problem
using the phrase ‚Äúcolumns of A.‚Äùc.DeÔ¨Åne an appropriate linear transformation Tusing the
matrix in (b), and restate the problem in terms of T.
8.Describe the possible echelon forms of the matrix A. Use the
notation of Example 1 in Section 1.2.
a.Ais a23matrix whose columns span R2.
b.Ais a33matrix whose columns span R3.
9.Write the vector5
6
as the sum of two vectors,
one on the line f.x; y/ WyD2xgand one on the line
f.x; y/ WyDx=2g.
10. Leta1;a2, and bbe the vectors in R2shown in the Ô¨Ågure, and
letAD¬åa1a2¬ç. Does the equation AxDbhave a solution?
If so, is the solution unique? Explain.
a2a1b
x1x2
11.Construct a 23matrix A, not in echelon form, such that the
solution of AxD0is a line in R3.
12. Construct a 23matrix A, not in echelon form, such that the
solution of AxD0is a plane in R3.
13. Write the reduced echelon form of a 33matrix Asuch
that the Ô¨Årst two columns of Aare pivot columns and
A2
43
 2
13
5D2
40
0
03
5.
14. Determine the value(s) of asuch that1
a
;a
aC2
is
linearly independent.
15. In (a) and (b), suppose the vectors are linearly independent.
What can you say about the numbers a; : : : ; f ? Justify your
answers. [ Hint: Use a theorem for (b).]
a.2
4a
0
03
5,2
4b
c
03
5,2
4d
e
f3
5 b.2
664a
1
0
03
775,2
664b
c
1
03
775,2
664d
e
f
13
775
16. Use Theorem 7 in Section 1.7 to explain why the columns of
the matrix Aare linearly independent.
AD2
6641 0 0 0
2 5 0 0
3 6 8 0
4 7 9 103
775
17. Explain why a set fv1;v2;v3;v4ginR5must be linearly
independent when fv1;v2;v3gis linearly independent and v4
isnotin Span fv1;v2;v3g.
18. Suppose fv1;v2gis a linearly independent set in Rn. Show
thatfv1;v1Cv2gis also linearly independent.
SECOND REVISED PAGES


--- Page 108 ---
Chapter 1 Supplementary Exercises 91
19. Suppose v1;v2;v3are distinct points on one line in R3. The
line need not pass through the origin. Show that fv1;v2;v3g
is linearly dependent.
20. LetTWRn!Rmbe a linear transformation, and suppose
T .u/Dv. Show that T . u/D  v.
21. LetTWR3!R3be the linear transformation that re-
Ô¨Çects each vector through the plane x2D0. That is,
T .x 1; x2; x3/D.x1; x2; x3/. Find the standard matrix of T.
22. LetAbe a 33matrix with the property that the linear
transformation x7!AxmapsR3ontoR3. Explain why the
transformation must be one-to-one.
23. AGivens rotation is a linear transformation from RntoRn
used in computer programs to create a zero entry in a vector
(usually a column of a matrix). The standard matrix of a
Givens rotation in R2has the forma b
b a
; a2Cb2D1
Find aandbsuch that4
3
is rotated into5
0
.
(4, 3)
(5, 0)x1x2
A Givens rotation in R2.24. The following equation describes a Givens rotation in R3.
Find aandb.
2
4a 0  b
0 1 0
b 0 a3
52
42
3
43
5D2
42p
5
3
03
5; a2Cb2D1
25. A large apartment building is to be built using modular
construction techniques. The arrangement of apartments on
any particular Ô¨Çoor is to be chosen from one of three basic
Ô¨Çoor plans. Plan A has 18 apartments on one Ô¨Çoor, in-
cluding 3 three-bedroom units, 7 two-bedroom units, and 8
one-bedroom units. Each Ô¨Çoor of plan B includes 4 three-
bedroom units, 4 two-bedroom units, and 8 one-bedroom
units. Each Ô¨Çoor of plan C includes 5 three-bedroom units,
3 two-bedroom units, and 9 one-bedroom units. Suppose the
building contains a total of x1Ô¨Çoors of plan A, x2Ô¨Çoors of
plan B, and x3Ô¨Çoors of plan C.
a.What interpretation can be given to the vector x12
43
7
83
5?
b.Write a formal linear combination of vectors that ex-
presses the total numbers of three-, two-, and one-
bedroom apartments contained in the building.
c.[M] Is it possible to design the building with exactly 66
three-bedroom units, 74 two-bedroom units, and 136 one-
bedroom units? If so, is there more than one way to do it?
Explain your answer.
WEB
SECOND REVISED PAGES


--- Page 110 ---
2Matrix Algebra
INTRODUCTORY EXAMPLE
Computer Models in Aircraft Design
To design the next generation of commercial and military
aircraft, engineers at Boeing‚Äôs Phantom Works use 3D
modeling and computational Ô¨Çuid dynamics (CFD). They
study the airÔ¨Çow around a virtual airplane to answer
important design questions before physical models are
created. This has drastically reduced design cycle times
and cost‚Äîand linear algebra plays a crucial role in the
process.
The virtual airplane begins as a mathematical ‚Äúwire-
frame‚Äù model that exists only in computer memory and
on graphics display terminals. (Model of a Boeing 777 is
shown.) This mathematical model organizes and inÔ¨Çuences
each step of the design and manufacture of the airplane‚Äî
both the exterior and interior. The CFD analysis concerns
the exterior surface.
Although the Ô¨Ånished skin of a plane may seem
smooth, the geometry of the surface is complicated. In
addition to wings and a fuselage, an aircraft has nacelles,
stabilizers, slats, Ô¨Çaps, and ailerons. The way air Ô¨Çows
around these structures determines how the plane moves
through the sky. Equations that describe the airÔ¨Çow are
complicated, and they must account for engine intake,
engine exhaust, and the wakes left by the wings of the
plane. To study the airÔ¨Çow, engineers need a highly reÔ¨Åned
description of the plane‚Äôs surface.
A computer creates a model of the surface by Ô¨Årst
superimposing a three-dimensional grid of ‚Äúboxes‚Äù on theoriginal wire-frame model. Boxes in this grid lie either
completely inside or completely outside the plane, or they
intersect the surface of the plane. The computer selects
the boxes that intersect the surface and subdivides them,
retaining only the smaller boxes that still intersect the
surface. The subdividing process is repeated until the grid
is extremely Ô¨Åne. A typical grid can include more than
400,000 boxes.
The process for Ô¨Ånding the airÔ¨Çow around the plane
involves repeatedly solving a system of linear equations
AxDbthat may involve up to 2 million equations and
variables. The vector bchanges each time, based on data
from the grid and solutions of previous equations. Using
the fastest computers available commercially, a Phantom
Works team can spend from a few hours to several days
setting up and solving a single airÔ¨Çow problem. After the
team analyzes the solution, they may make small changes
to the airplane surface and begin the whole process again.
Thousands of CFD runs may be required.
This chapter presents two important concepts that
assist in the solution of such massive systems of equations:
Partitioned matrices: A typical CFD system of
equations has a ‚Äúsparse‚Äù coefÔ¨Åcient matrix with
mostly zero entries. Grouping the variables correctly
leads to a partitioned matrix with many zero blocks.
Section 2.4 introduces such matrices and describes
some of their applications.
SECOND REVISED PAGES
93

--- Page 111 ---
94 CHAPTER 2 Matrix Algebra
Matrix factorizations: Even when written with
partitioned matrices, the system of equations is
complicated. To further simplify the computations,
the CFD software at Boeing uses what is called
an LU factorization of the coefÔ¨Åcient matrix.
Section 2.5 discusses LU and other useful matrix
factorizations. Further details about factorizations
appear at several points later in the text.
To analyze a solution of an airÔ¨Çow system, engineers
want to visualize the airÔ¨Çow over the surface of the plane.
They use computer graphics, and linear algebra provides
the engine for the graphics. The wire-frame model of the
plane‚Äôs surface is stored as data in many matrices. Once the
image has been rendered on a computer screen, engineers
can change its scale, zoom in or out of small regions, and
rotate the image to see parts that may be hidden from view.
Each of these operations is accomplished by appropriate
Modern CFD has revolutionized wing design. The Boeing
Blended Wing Body is in design for the year 2020 or sooner.
matrix multiplications. Section 2.7 explains the basic
ideas.
WEB
Our ability to analyze and solve equations will be greatly enhanced when we can perform
algebraic operations with matrices. Furthermore, the deÔ¨Ånitions and theorems in this
chapter provide some basic tools for handling the many applications of linear algebra
that involve two or more matrices. For square matrices, the Invertible Matrix Theorem
in Section 2.3 ties together most of the concepts treated earlier in the text. Sections 2.4
and 2.5 examine partitioned matrices and matrix factorizations, which appear in most
modern uses of linear algebra. Sections 2.6 and 2.7 describe two interesting applications
of matrix algebra, to economics and to computer graphics.
2.1 MATRIX OPERATIONS
IfAis anmnmatrix‚Äîthat is, a matrix with mrows and ncolumns‚Äîthen the scalar
entry in the ith row and jth column of Ais denoted by aijand is called the .i; j / -entry
ofA. See Figure 1. For instance, the .3; 2/ -entry is the number a32in the third row,
second column. Each column of Ais a list of mreal numbers, which identiÔ¨Åes a vector
inRm. Often, these columns are denoted by a1; : : : ; an, and the matrix Ais written as
ADa1a2 an
Observe that the number aijis the ith entry (from the top) of the jth column vector aj.
Thediagonal entries in an mnmatrix AD¬åaij¬çarea11; a22; a33; : : : ; and they
form the main diagonal ofA. Adiagonal matrix is a square nnmatrix whose
nondiagonal entries are zero. An example is the nnidentity matrix, In. Anmn
matrix whose entries are all zero is a zero matrix and is written as 0. The size of a zero
matrix is usually clear from the context.
SECOND REVISED PAGES


--- Page 112 ---
2.1 Matrix Operations 95
a11
am1a1n
amnai1 aina1jColumn
j
amj
a1 an ajaij Row i = A
FIGURE 1 Matrix notation.
Sums and Scalar Multiples
The arithmetic for vectors described earlier has a natural extension to matrices. We say
that two matrices are equal if they have the same size (i.e., the same number of rows
and the same number of columns) and if their corresponding columns are equal, which
amounts to saying that their corresponding entries are equal. If AandBaremn
matrices, then the sum ACBis the mnmatrix whose columns are the sums of
the corresponding columns in AandB. Since vector addition of the columns is done
entrywise, each entry in ACBis the sum of the corresponding entries in AandB. The
sumACBis deÔ¨Åned only when AandBare the same size.
EXAMPLE 1 Let
AD4 0 5
 1 3 2
; B D1 1 1
3 5 7
; C D2 3
0 1
Then
ACBD5 1 6
2 8 9
butACCis not deÔ¨Åned because AandChave different sizes.
Ifris a scalar and Ais a matrix, then the scalar multiple rAis the matrix whose
columns are rtimes the corresponding columns in A. As with vectors,  Astands for
. 1/A, and A Bis the same as AC. 1/B.
EXAMPLE 2 IfAandBare the matrices in Example 1, then
2BD21 1 1
3 5 7
D2 2 2
6 10 14
A 2BD4 0 5
 1 3 2
 2 2 2
6 10 14
D2 2 3
 7 7 12
It was unnecessary in Example 2 to compute A 2BasAC. 1/2B because the
usual rules of algebra apply to sums and scalar multiples of matrices, as the following
theorem shows.
T H E O R E M 1 LetA; B , and Cbe matrices of the same size, and let randsbe scalars.
a.ACBDBCA
b..ACB/CCDAC.BCC /
c.AC0DAd.r.ACB/DrACrB
e..rCs/ADrACsA
f.r.sA/D.rs/A
SECOND REVISED PAGES


--- Page 113 ---
96 CHAPTER 2 Matrix Algebra
Each equality in Theorem 1 is veriÔ¨Åed by showing that the matrix on the left side has
the same size as the matrix on the right and that corresponding columns are equal. Size
is no problem because A,B, and Care equal in size. The equality of columns follows
immediately from analogous properties of vectors. For instance, if the jth columns of
A,B, andCareaj,bj, and cj, respectively, then the jth columns of .ACB/CCand
AC.BCC /are
.ajCbj/Ccjand ajC.bjCcj/
respectively. Since these two vector sums are equal for each j, property (b) is veriÔ¨Åed.
Because of the associative property of addition, we can simply write ACBCC
for the sum, which can be computed either as .ACB/CCor as AC.BCC /. The
same applies to sums of four or more matrices.
Matrix Multiplication
When a matrix Bmultiplies a vector x, it transforms xinto the vector Bx. If this vector
is then multiplied in turn by a matrix A, the resulting vector is A.B x/. See Figure 2.
xMultiplication
by B
BxMultiplication
by A
A(Bx)
FIGURE 2 Multiplication by Band then A.
Thus A.B x/is produced from xby a composition of mappings‚Äîthe linear transfor-
mations studied in Section 1.8. Our goal is to represent this composite mapping as
multiplication by a single matrix, denoted by AB, so that
A.B x/D.AB/ x (1)
See Figure 3.
Multiplication
by ABBxMultiplication
by B
xMultiplication
by A
A(Bx)
FIGURE 3 Multiplication by AB.
IfAismn,Bisnp, and xis inRp, denote the columns of Bbyb1; : : : ; bp
and the entries in xbyx1; : : : ; x p. Then
BxDx1b1C  C xpbp
By the linearity of multiplication by A,
A.B x/DA.x 1b1/C  C A.x pbp/
Dx1Ab1C  C xpAbp
SECOND REVISED PAGES


--- Page 114 ---
2.1 Matrix Operations 97
The vector A.B x/is a linear combination of the vectors Ab1; : : : ; A bp, using the entries
inxas weights. In matrix notation, this linear combination is written as
A.B x/D¬åAb1Ab2Abp¬çx
Thus multiplication by ¬åAb1Ab2Abp¬çtransforms xintoA.B x/. We have found
the matrix we sought!
D E F I N I T I O N IfAis an mnmatrix, and if Bis an npmatrix with columns b1; : : : ; bp,
then the product ABis the mpmatrix whose columns are Ab1; : : : ; A bp. That
is,
ABDAb1b2 bp
DAb1Ab2Abp
This deÔ¨Ånition makes equation (1) true for all xinRp. Equation (1) proves that the
composite mapping in Figure 3 is a linear transformation and that its standard matrix is
AB.Multiplication of matrices corresponds to composition of linear transformations.
EXAMPLE 3 Compute AB, where AD2 3
1 5
andBD4 3 6
1 2 3
.
SOLUTION Write BD¬åb1b2b3¬ç, and compute:
Ab1D2 3
1 54
1
; A b2D2 3
1 53
 2
; A b3D2 3
1 56
3
D11
 1
D0
13
D21
 9
? ?? Then
ABDA¬åb1b2b3¬çD11 0 21
 1 13  9
666
Ab1Ab2Ab3
Notice that since the Ô¨Årst column of ABisAb1;this column is a linear combination
of the columns of Ausing the entries in b1as weights. A similar statement is true for
each column of AB:
Each column of ABis a linear combination of the columns of Ausing weights
from the corresponding column of B.
Obviously, the number of columns of Amust match the number of rows in Bin
order for a linear combination such as Ab1to be deÔ¨Åned. Also, the deÔ¨Ånition of AB
shows that ABhas the same number of rows as A and the same number of columns
as B.
EXAMPLE 4 IfAis a35matrix and Bis a52matrix, what are the sizes of
ABandBA, if they are deÔ¨Åned?
SECOND REVISED PAGES


--- Page 115 ---
98 CHAPTER 2 Matrix Algebra
SOLUTION Since Ahas 5 columns and Bhas 5 rows, the product ABis deÔ¨Åned and
is a32matrix:
A B AB2
4    
    
    3
52
66664 
 
 
 
 3
77775D2
4 
 
 3
5
35 52 32
6 6 6 6 Match
Size of AB
The product BAisnotdeÔ¨Åned because the 2 columns of Bdo not match the 3 rows
ofA.
The deÔ¨Ånition of ABis important for theoretical work and applications, but the
following rule provides a more efÔ¨Åcient method for calculating the individual entries in
ABwhen working small problems by hand.
ROW--COLUMN RULE FOR COMPUTING AB
If the product ABis deÔ¨Åned, then the entry in row iand column jofABis the
sum of the products of corresponding entries from row iofAand column jof
B. If.AB/ ijdenotes the .i; j / -entry in AB, and if Ais anmnmatrix, then
.AB/ ijDai1b1jCai2b2jC  C ainbnj
To verify this rule, let BD¬åb1 bp¬ç. Column jofABisAbj, and we can
compute Abjby the row‚Äìvector rule for computing Axfrom Section 1.4. The ith entry
inAbjis the sum of the products of corresponding entries from row iofAand the
vector bj, which is precisely the computation described in the rule for computing the
.i; j / -entry of AB.
EXAMPLE 5 Use the row‚Äìcolumn rule to compute two of the entries in ABfor the
matrices in Example 3. An inspection of the numbers involved will make it clear how
the two methods for calculating ABproduce the same matrix.
SOLUTION To Ô¨Ånd the entry in row 1 and column 3 of AB, consider row 1 of Aand
column 3 of B. Multiply corresponding entries and add the results, as shown below:
ABD-2 3
1 54 3?
6
1 2 3
D  2.6/C3.3/
  
D  21
  
For the entry in row 2 and column 2 of AB, use row 2 of Aand column 2 of B:
-2 3
1 54?
3 6
1 2 3
D  21
 1.3/C  5. 2/
D  21
 13
SECOND REVISED PAGES


--- Page 116 ---
2.1 Matrix Operations 99
EXAMPLE 6 Find the entries in the second row of AB, where
AD2
6642 5 0
 1 3  4
6 8 7
 3 0 93
775; B D2
44 6
7 1
3 23
5
SOLUTION By the row‚Äìcolumn rule, the entries of the second row of ABcome from
row 2 of A(and the columns of B):
-2
6642 5 0
 1 3  4
6 8 7
 3 0 93
7752
4?
4?
 6
7 1
3 23
5
D2
664 
 4C21 12 6C3 8
 
 3
775D2
664 
5 1
 
 3
775
Notice that since Example 6 requested only the second row of AB, we could have
written just the second row of Ato the left of Band computed
 1 3  42
44 6
7 1
3 23
5D5 1
This observation about rows of ABis true in general and follows from the row‚Äìcolumn
rule. Let row i.A/denote the ith row of a matrix A. Then
row i.AB/Drow i.A/B (2)
Properties of Matrix Multiplication
The following theorem lists the standard properties of matrix multiplication. Recall that
Imrepresents the mmidentity matrix and ImxDxfor all xinRm.
T H E O R E M 2 LetAbe an mnmatrix, and let BandChave sizes for which the indicated
sums and products are deÔ¨Åned.
a.A.BC / D.AB/C (associative law of multiplication)
b.A.BCC /DABCAC (left distributive law)
c..BCC /ADBACCA (right distributive law)
d.r.AB/ D.rA/B DA.rB/
for any scalar r
e.ImADADAIn (identity for matrix multiplication)
PROOF Properties (b)‚Äì(e) are considered in the exercises. Property (a) follows from
the fact that matrix multiplication corresponds to composition of linear transformations
(which are functions), and it is known (or easy to check) that the composition of func-
tions is associative. Here is another proof of (a) that rests on the ‚Äúcolumn deÔ¨Ånition‚Äù of
SECOND REVISED PAGES


--- Page 117 ---
100 CHAPTER 2 Matrix Algebra
the product of two matrices. Let
CD¬åc1 cp¬ç
By the deÔ¨Ånition of matrix multiplication,
BCD¬åBc1Bcp¬ç
A.BC / D¬åA.B c1/A.B cp/¬ç
Recall from equation (1) that the deÔ¨Ånition of ABmakes A.B x/D.AB/ xfor all x, so
A.BC / D¬å.AB/ c1.AB/ cp¬çD.AB/C
The associative and distributive laws in Theorems 1 and 2 say essentially that pairs
of parentheses in matrix expressions can be inserted and deleted in the same way as in
the algebra of real numbers. In particular, we can write ABC for the product, which
can be computed either as A.BC / or as .AB/C .1Similarly, a product ABCD of four
matrices can be computed as A.BCD/ or.ABC /D orA.BC /D , and so on. It does not
matter how we group the matrices when computing the product, so long as the left-to-
right order of the matrices is preserved.
The left-to-right order in products is critical because ABandBAare usually not
the same. This is not surprising, because the columns of ABare linear combinations of
the columns of A, whereas the columns of BAare constructed from the columns of B.
The position of the factors in the product ABis emphasized by saying that Aisright-
multiplied byBor that Bisleft-multiplied byA. IfABDBA, we say that AandB
commute with one another.
EXAMPLE 7 LetAD5 1
3 2
andBD2 0
4 3
. Show that these matrices do
not commute. That is, verify that AB¬§BA.
SOLUTION
ABD5 1
3 22 0
4 3
D14 3
 2 6
BAD2 0
4 35 1
3 2
D10 2
29 2
Example 7 illustrates the Ô¨Årst of the following list of important differences between
matrix algebra and the ordinary algebra of real numbers. See Exercises 9‚Äì12 for exam-
ples of these situations.
WARNINGS:
1.In general, AB¬§BA.
2.The cancellation laws do nothold for matrix multiplication. That is, if
ABDAC, then it is nottrue in general that BDC. (See Exercise 10.)
3.If a product ABis the zero matrix, you cannot conclude in general that either
AD0orBD0. (See Exercise 12.)
1When Bis square and Chas fewer columns than Ahas rows, it is more efÔ¨Åcient to compute A.BC / than
.AB/C .
SECOND REVISED PAGES


--- Page 118 ---
2.1 Matrix Operations 101
Powers of a Matrix
IfAis an nnmatrix and if kis a positive integer, then Akdenotes the product of k
WEB
copies of A:
AkDAA¬Ñ¬É¬Ç¬Ö
k
IfAis nonzero and if xis inRn;thenAkxis the result of left-multiplying xbyA
repeatedly ktimes. If kD0;thenA0xshould be xitself. Thus A0is interpreted as the
identity matrix. Matrix powers are useful in both theory and applications (Sections 2.6,
4.9, and later in the text).
The Transpose of a Matrix
Given an mnmatrix A, the transpose ofAis the nmmatrix, denoted by AT,
whose columns are formed from the corresponding rows of A.
EXAMPLE 8 Let
ADa b
c d
; BD2
4 5 2
1 3
0 43
5; CD1 1 1 1
 3 5  2 7
Then
ATDa c
b d
; BTD 5 1 0
2 3 4
; CTD2
6641 3
1 5
1 2
1 73
775
T H E O R E M 3 LetAandBdenote matrices whose sizes are appropriate for the following sums
and products.
a..AT/TDA
b..ACB/TDATCBT
c.For any scalar r,.rA/TDrAT
d..AB/TDBTAT
Proofs of (a)‚Äì(c) are straightforward and are omitted. For (d), see Exercise 33.
Usually, .AB/Tis not equal to ATBT, even when AandBhave sizes such that the
product ATBTis deÔ¨Åned.
The generalization of Theorem 3(d) to products of more than two factors can be
stated in words as follows:
The transpose of a product of matrices equals the product of their transposes in
thereverse order.
The exercises contain numerical examples that illustrate properties of transposes.
SECOND REVISED PAGES


--- Page 119 ---
102 CHAPTER 2 Matrix Algebra
N U M E R I C A L N O T E S
1.The fastest way to obtain ABon a computer depends on the way in which
the computer stores matrices in its memory. The standard high-performance
algorithms, such as in LAPACK, calculate ABby columns, as in our deÔ¨Ånition
of the product. (A version of LAPACK written in C++ calculates ABby rows.)
2.The deÔ¨Ånition of ABlends itself well to parallel processing on a computer. The
columns of Bare assigned individually or in groups to different processors,
which independently and hence simultaneously compute the corresponding
columns of AB.
PRACTICE PROBLEMS
1.Since vectors in Rnmay be regarded as n1matrices, the properties of transposes
in Theorem 3 apply to vectors, too. Let
AD1 3
 2 4
and xD5
3
Compute .Ax/T,xTAT,xxT, and xTx. IsATxTdeÔ¨Åned?
2.LetAbe a44matrix and let xbe a vector in R4. What is the fastest way to compute
A2x? Count the multiplications.
3.Suppose Ais anmnmatrix, all of whose rows are identical. Suppose Bis annp
matrix, all of whose columns are identical. What can be said about the entries in AB?
2.1 EXERCISES
In Exercises l and 2, compute each matrix sum or product if it is
deÔ¨Åned. If an expression is undeÔ¨Åned, explain why. Let
AD2 0  1
4 5 2
; BD7 5 1
1 4 3
;
CD1 2
 2 1
; DD3 5
 1 4
; ED 5
3
1. 2A,B 2A,AC,CD
2.AC2B,3C E,CB,EB
In the rest of this exercise set and in those to follow, you should
assume that each matrix expression is deÔ¨Åned. That is, the sizes
of the matrices (and vectors) involved ‚Äúmatch‚Äù appropriately.
3.LetAD4 1
5 2
. Compute 3I2 Aand.3I2/A.
4.Compute A 5I3and.5I3/A, when
AD2
49 1 3
 8 7  6
 4 1 83
5:
In Exercises 5 and 6, compute the product ABin two ways: (a) by
the deÔ¨Ånition, where Ab1andAb2are computed separately, and
(b) by the row‚Äìcolumn rule for computing AB.5.AD2
4 1 2
5 4
2 33
5; BD3 2
 2 1
6.AD2
44 2
 3 0
3 53
5; BD1 3
2 1
7.If a matrix Ais53and the product ABis57, what is the
size of B?
8.How many rows does Bhave if BCis a34matrix?
9.LetAD2 5
 3 1
andBD4 5
3 k
:What value(s) of
k, if any, will make ABDBA?
10. LetAD2 3
 4 6
; BD8 4
5 5
;andCD5 2
3 1
:
Verify that ABDACand yet B¬§C:
11.LetAD2
41 1 1
1 2 3
1 4 53
5andDD2
42 0 0
0 3 0
0 0 53
5:Com-
pute ADandDA. Explain how the columns or rows of A
change when Ais multiplied by Don the right or on the
left. Find a 33matrix B, not the identity matrix or the zero
matrix, such that ABDBA:
SECOND REVISED PAGES


--- Page 120 ---
2.1 Matrix Operations 103
12. LetAD3 6
 1 2
:Construct a 22matrix Bsuch that
ABis the zero matrix. Use two different nonzero columns
forB.
13. Letr1; : : : ; rpbe vectors in Rn, and let Qbe an mnmatrix.
Write the matrix ¬åQr1Qrp¬ças aproduct of two matrices
(neither of which is an identity matrix).
14. LetUbe the 32cost matrix described in Example 6 of
Section 1.8. The Ô¨Årst column of Ulists the costs per dollar of
output for manufacturing product B, and the second column
lists the costs per dollar of output for product C. (The costs
are categorized as materials, labor, and overhead.) Let q1be
a vector in R2that lists the output (measured in dollars) of
products B and C manufactured during the Ô¨Årst quarter of
the year, and let q2;q3;and q4be the analogous vectors
that list the amounts of products B and C manufactured in
the second, third, and fourth quarters, respectively. Give an
economic description of the data in the matrix UQ, where
QD¬åq1q2q3q4¬ç:
Exercises 15 and 16 concern arbitrary matrices A,B, and Cfor
which the indicated sums and products are deÔ¨Åned. Mark each
statement True or False. Justify each answer.
15. a.IfAand Bare22with columns a1;a2;and b1;b2;
respectively, then ABD¬åa1b1a2b2¬ç.
b.Each column of ABis a linear combination of the columns
ofBusing weights from the corresponding column of A.
c.ABCACDA .BCC /
d.ATCBTD.ACB/T
e.The transpose of a product of matrices equals the product
of their transposes in the same order.
16. a.IfAand Bare33andBD¬åb1b2b3¬ç;then ABD
¬åAb1CAb2CAb3¬ç:
b.The second row of ABis the second row of Amultiplied
on the right by B.
c..AB/ C D.AC / B
d..AB/TDATBT
e.The transpose of a sum of matrices equals the sum of their
transposes.
17. IfAD1 2
 2 5
andABD 1 2  1
6 9 3
;determine
the Ô¨Årst and second columns of B.
18. Suppose the Ô¨Årst two columns, b1andb2, ofBare equal.
What can you say about the columns of AB(ifABis deÔ¨Åned)?
Why?
19. Suppose the third column of Bis the sum of the Ô¨Årst two
columns. What can you say about the third column of AB?
Why?
20. Suppose the second column of Bis all zeros. What can you
say about the second column of AB?21. Suppose the last column of ABis entirely zero but Bitself
has no column of zeros. What can you say about the columns
ofA?
22. Show that if the columns of Bare linearly dependent, then
so are the columns of AB.
23. Suppose CADIn(thennidentity matrix). Show that the
equation AxD0has only the trivial solution. Explain why
Acannot have more columns than rows.
24. Suppose ADDIm(themmidentity matrix). Show that
for any binRm, the equation AxDbhas a solution. [ Hint:
Think about the equation ADbDb:] Explain why Acannot
have more rows than columns.
25. Suppose Ais anmnmatrix and there exist nmmatrices
CandDsuch that CADInandADDIm:Prove that mDn
andCDD:[Hint: Think about the product CAD .]
26. Suppose Ais a3nmatrix whose columns span R3. Explain
how to construct an n3matrix Dsuch that ADDI3:
In Exercises 27 and 28, view vectors in Rnasn1matrices. For
uandvinRn, the matrix product uTvis a11matrix, called the
scalar product , orinner product , ofuandv. It is usually written
as a single real number without brackets. The matrix product uvT
is an nnmatrix, called the outer product ofuandv. The
products uTvanduvTwill appear later in the text.
27. Letu=2
4 2
3
 43
5andvD2
4a
b
c3
5:Compute uTv;vTu;uvT;and
vuT:
28. Ifuandvare inRn, how are uTvandvTurelated? How are
uvTandvuTrelated?
29. Prove Theorem 2(b) and 2(c). Use the row‚Äìcolumn rule. The
(i,j)-entry in A.BCC /can be written as
ai1.b1jCc1j/C  C ain.bnjCcnj/ornX
kD1aik.bkjCckj/
30. Prove Theorem 2(d). [ Hint: The ( i,j)-entry in ( rA)Bis
.rai1/b1jC  C .rain/bnj:¬ç
31. Show that ImADAwhen Ais an mnmatrix. You can
assume ImxDxfor all xinRm.
32. Show that AInDAwhen Ais an mnmatrix. [ Hint: Use
the (column) deÔ¨Ånition of AIn:¬ç
33. Prove Theorem 3(d). [ Hint: Consider the jth row of .AB/T:¬ç
34. Give a formula for .AB x/T;where xis a vector and AandB
are matrices of appropriate sizes.
35. [M] Read the documentation for your matrix program, and
write the commands that will produce the following matrices
(without keying in each entry of the matrix).
a.A56matrix of zeros
b.A35matrix of ones
SECOND REVISED PAGES


--- Page 121 ---
104 CHAPTER 2 Matrix Algebra
c.The66identity matrix
d.A55diagonal matrix, with diagonal entries 3, 5, 7, 2, 4
A useful way to test new ideas in matrix algebra, or to make
conjectures, is to make calculations with matrices selected at
random. Checking a property for a few matrices does not prove
that the property holds in general, but it makes the property more
believable. Also, if the property is actually false, you may discover
this when you make a few calculations.
36. [M] Write the command(s) that will create a 64matrix
with random entries. In what range of numbers do the entries
lie? Tell how to create a 33matrix with random integer
entries between  9and 9. [ Hint: Ifxis a random number
such that 0 < x < 1; then 9:5 < 19.x  :5/ < 9:5:¬ç
37. [M] Construct a random 44matrix Aand test whether
.ACI/.A I/DA2 I:The best way to do this is to
compute .ACI/.A I/ .A2 I/and verify that this dif-
ference is the zero matrix. Do this for three random matrices.
Then test .ACB/.A B/DA2 B2the same way forthree pairs of random 44matrices. Report your conclu-
sions.
38. [M] Use at least three pairs of random 44matrices Aand
Bto test the equalities .ACB/TDATCBTand.AB/TD
ATBT:(See Exercise 37.) Report your conclusions. [ Note:
Most matrix programs use A0forAT:¬ç
39. [M] Let
SD2
666640 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
0 0 0 0 03
77775
Compute SkforkD2; : : : ; 6:
40. [M] Describe in words what happens when you compute A5,
A10; A20;andA30for
AD2
41=6 1=2 1=3
1=2 1=4 1=4
1=3 1=4 5=123
5
SOLUTIONS TO PRACTICE PROBLEMS
1.AxD1 3
 2 45
3
D 4
2
. So.Ax/TD 4 2
. Also,
xTATD5 31 2
 3 4
D 4 2
:
The quantities .Ax/TandxTATare equal, by Theorem 3(d). Next,
xxTD5
35 3
D25 15
15 9
xTxD5 35
3
D¬å25C9¬çD34
A11matrix such as xTxis usually written without the brackets. Finally, ATxTis
not deÔ¨Åned, because xTdoes not have two rows to match the two columns of AT.
2.The fastest way to compute A2xis to compute A.Ax/. The product Axrequires
16 multiplications, 4 for each entry, and A.Ax/requires 16 more. In contrast, the
product A2requires 64 multiplications, 4 for each of the 16 entries in A2. After that,
A2xtakes 16 more multiplications, for a total of 80.
3.First observe that by the deÔ¨Ånition of matrix multiplication,
ABD¬åAb1Ab2 Abn¬çD¬åAb1Ab1 Ab1¬ç;
so the columns of ABare identical. Next, recall that row i.AB/Drow i.A/B:Since
all the rows of Aare identical, all the rows of ABare identical. Putting this informa-
tion about the rows and columns together, it follows that all the entries in ABare the
same.
2.2 THE INVERSE OF A MATRIX
Matrix algebra provides tools for manipulating matrix equations and creating various
useful formulas in ways similar to doing ordinary algebra with real numbers. This section
SECOND REVISED PAGES


--- Page 122 ---
2.2 The Inverse of a Matrix 105
investigates the matrix analogue of the reciprocal, or multiplicative inverse, of a nonzero
number.
Recall that the multiplicative inverse of a number such as 5 is 1/5 or 5 1. This
inverse satisÔ¨Åes the equations
5 15D1and 55 1D1
The matrix generalization requires both equations and avoids the slanted-line notation
(for division) because matrix multiplication is not commutative. Furthermore, a full
generalization is possible only if the matrices involved are square.1
Annnmatrix Ais said to be invertible if there is an nnmatrix Csuch that
CADIand ACDI
where IDIn;thennidentity matrix. In this case, Cis an inverse ofA. In fact, C
is uniquely determined by A, because if Bwere another inverse of A, then BDBID
B.AC / D.BA/C DICDC:This unique inverse is denoted by A 1, so that
A 1ADIand AA 1DI
A matrix that is notinvertible is sometimes called a singular matrix , and an invertible
matrix is called a nonsingular matrix .
EXAMPLE 1 IfAD2 5
 3 7
andCD 7 5
3 2
, then
ACD2 5
 3 7 7 5
3 2
D1 0
0 1
and
CAD 7 5
3 22 5
 3 7
D1 0
0 1
Thus CDA 1.
Here is a simple formula for the inverse of a 22matrix, along with a test to tell
if the inverse exists.
T H E O R E M 4 LetADa b
c d
. Ifad bc¬§0, then Ais invertible and
A 1D1
ad bcd b
 c a
Ifad bcD0, then Ais not invertible.
The simple proof of Theorem 4 is outlined in Exercises 25 and 26. The quantity
ad bcis called the determinant ofA, and we write
detADad bc
Theorem 4 says that a 22matrix Ais invertible if and only if det A¬§0.
1One could say that an mnmatrix Ais invertible if there exist nmmatrices CandDsuch that
CADInandADDIm:However, these equations imply that Ais square and CDD:Thus Ais invertible
as deÔ¨Åned above. See Exercises 23‚Äì25 in Section 2.1.
SECOND REVISED PAGES


--- Page 123 ---
106 CHAPTER 2 Matrix Algebra
EXAMPLE 2 Find the inverse of AD3 4
5 6
.
SOLUTION Since det AD3.6/ 4.5/D  2¬§0,Ais invertible, and
A 1D1
 26 4
 5 3
D6=. 2/ 4=. 2/
 5=. 2/ 3=.  2/
D 3 2
5=2 3=2
Invertible matrices are indispensable in linear algebra‚Äîmainly for algebraic calcu-
lations and formula derivations, as in the next theorem. There are also occasions when
an inverse matrix provides insight into a mathematical model of a real-life situation, as
in Example 3, below.
T H E O R E M 5 IfAis an invertible nnmatrix, then for each binRn, the equation AxDbhas
the unique solution xDA 1b.
PROOF Take any binRn. A solution exists because if A 1bis substituted for x,
thenAxDA.A 1b/D.AA 1/bDIbDb. SoA 1bis a solution. To prove that the
solution is unique, show that if uis any solution, then u;in fact, must be A 1b. Indeed,
ifAuDb, we can multiply both sides by A 1and obtain
A 1AuDA 1b; I uDA 1b;and uDA 1b
EXAMPLE 3 A horizontal elastic beam is supported at each end and is subjected
to forces at points 1, 2, and 3, as shown in Figure 1. Let finR3list the forces at these
points, and let yinR3list the amounts of deÔ¨Çection (that is, movement) of the beam at
the three points. Using Hooke‚Äôs law from physics, it can be shown that
yDDf
where Dis aÔ¨Çexibility matrix . Its inverse is called the stiffness matrix . Describe the
physical signiÔ¨Åcance of the columns of DandD 1.
‚éß‚é®‚é©‚é´‚é¨‚é≠‚é´‚é¨‚é≠#1 #2 #3
y1 y2y3
f3 f2f1
FIGURE 1 DeÔ¨Çection of an elastic beam.
SOLUTION Write I3D¬åe1e2e3¬çand observe that
DDDI3D¬åDe1De2De3¬ç
Interpret the vector e1D.1; 0; 0/ as a unit force applied downward at point 1 on the
beam (with zero force at the other two points). Then De1;the Ô¨Årst column of D;lists
the beam deÔ¨Çections due to a unit force at point 1. Similar descriptions apply to the
second and third columns of D:
To study the stiffness matrix D 1, observe that the equation fDD 1ycomputes a
force vector fwhen a deÔ¨Çection vector yis given. Write
D 1DD 1I3D¬åD 1e1D 1e2D 1e3¬ç
Now interpret e1as a deÔ¨Çection vector. Then D 1e1lists the forces that create the
deÔ¨Çection. That is, the Ô¨Årst column of D 1lists the forces that must be applied at the
SECOND REVISED PAGES


--- Page 124 ---
2.2 The Inverse of a Matrix 107
three points to produce a unit deÔ¨Çection at point 1 and zero deÔ¨Çections at the other points.
Similarly, columns 2 and 3 of D 1list the forces required to produce unit deÔ¨Çections at
points 2 and 3, respectively. In each column, one or two of the forces must be negative
(point upward) to produce a unit deÔ¨Çection at the desired point and zero deÔ¨Çections at
the other two points. If the Ô¨Çexibility is measured, for example, in inches of deÔ¨Çection
per pound of load, then the stiffness matrix entries are given in pounds of load per inch
of deÔ¨Çection.
The formula in Theorem 5 is seldom used to solve an equation AxDbnumerically
because row reduction of ¬åAb¬çis nearly always faster. (Row reduction is usually
more accurate, too, when computations involve rounding off numbers.) One possible
exception is the 22case. In this case, mental computations to solve AxDbare
sometimes easier using the formula for A 1, as in the next example.
EXAMPLE 4 Use the inverse of the matrix Ain Example 2 to solve the system
3x1C4x2D3
5x1C6x2D7
SOLUTION This system is equivalent to AxDb, so
xDA 1bD 3 2
5=2 3=2 3
7
D5
 3
The next theorem provides three useful facts about invertible matrices.
T H E O R E M 6 a.IfAis an invertible matrix, then A 1is invertible and
.A 1/ 1DA
b.IfAandBarenninvertible matrices, then so is AB, and the inverse of AB
is the product of the inverses of AandBin the reverse order. That is,
.AB/ 1DB 1A 1
c.IfAis an invertible matrix, then so is AT, and the inverse of ATis the transpose
ofA 1. That is,
.AT/ 1D.A 1/T
PROOF To verify statement (a), Ô¨Ånd a matrix Csuch that
A 1CDIand CA 1DI
In fact, these equations are satisÔ¨Åed with Ain place of C. Hence A 1is invertible, and
Ais its inverse. Next, to prove statement (b), compute:
.AB/.B 1A 1/DA.BB 1/A 1DAIA 1DAA 1DI
A similar calculation shows that .B 1A 1/.AB/ DI. For statement (c), use Theorem
3(d), read from right to left, .A 1/TATD.AA 1/TDITDI. Similarly, AT.A 1/TD
ITDI. Hence ATis invertible, and its inverse is .A 1/T.
Remark: Part (b) illustrates the important role that deÔ¨Ånitions play in proofs. The the-
orem claims that B 1A 1is the inverse of AB. The proof establishes this by showing
thatB 1A 1satisÔ¨Åes the deÔ¨Ånition of what it means to be the inverse of AB. Now, the
inverse of ABis a matrix that when multiplied on the left (or right) by AB, the product
is the identity matrix I. So the proof consists of showing that B 1A 1has this property.
SECOND REVISED PAGES


--- Page 125 ---
108 CHAPTER 2 Matrix Algebra
The following generalization of Theorem 6(b) is needed later.
The product of nninvertible matrices is invertible, and the inverse is the
product of their inverses in the reverse order.
There is an important connection between invertible matrices and row operations
that leads to a method for computing inverses. As we shall see, an invertible matrix Ais
row equivalent to an identity matrix, and we can Ô¨Ånd A 1bywatching the row reduction
ofAtoI.
Elementary Matrices
Anelementary matrix is one that is obtained by performing a single elementary row
operation on an identity matrix. The next example illustrates the three kinds of elemen-
tary matrices.
EXAMPLE 5 Let
E1D2
41 0 0
0 1 0
 4 0 13
5; E 2D2
40 1 0
1 0 0
0 0 13
5; E 3D2
41 0 0
0 1 0
0 0 53
5;
AD2
4a b c
d e f
g h i3
5
Compute E1A,E2A, and E3A, and describe how these products can be obtained by
elementary row operations on A.
SOLUTION Verify that
E1AD2
4a b c
d e f
g 4a h  4b i  4c3
5; E 2AD2
4d e f
a b c
g h i3
5;
E3AD2
4a b c
d e f
5g 5h 5i3
5:
Addition of  4times row 1 of Ato row 3 produces E1A. (This is a row replacement
operation.) An interchange of rows 1 and 2 of Aproduces E2A, and multiplication of
row 3 of Aby 5 produces E3A.
Left-multiplication (that is, multiplication on the left) by E1in Example 5 has the
same effect on any 3nmatrix. It adds  4times row 1 to row 3. In particular, since
E1IDE1, we see that E1itself is produced by this same row operation on the identity.
Thus Example 5 illustrates the following general fact about elementary matrices. See
Exercises 27 and 28.
If an elementary row operation is performed on an mnmatrix A, the resulting
matrix can be written as EA, where the mmmatrix Eis created by performing
the same row operation on Im.
SECOND REVISED PAGES


--- Page 126 ---
2.2 The Inverse of a Matrix 109
Since row operations are reversible, as shown in Section 1.1, elementary matrices
are invertible, for if Eis produced by a row operation on I, then there is another row op-
eration of the same type that changes Eback into I. Hence there is an elementary matrix
Fsuch that FEDI. Since EandFcorrespond to reverse operations, EFDI, too.
Each elementary matrix Eis invertible. The inverse of Eis the elementary matrix
of the same type that transforms Eback into I.
EXAMPLE 6 Find the inverse of E1D2
41 0 0
0 1 0
 4 0 13
5.
SOLUTION To transform E1intoI, addC4times row 1 to row 3. The elementary
matrix that does this is
E 1
1D2
41 0 0
0 1 0
C4 0 13
5
The following theorem provides the best way to ‚Äúvisualize‚Äù an invertible matrix,
and the theorem leads immediately to a method for Ô¨Ånding the inverse of a matrix.
T H E O R E M 7 Annnmatrix Ais invertible if and only if Ais row equivalent to In, and in
this case, any sequence of elementary row operations that reduces AtoInalso
transforms InintoA 1.
Remark: The comment on the proof of Theorem 11 in Chapter 1 noted that ‚Äú Pif and
only if Q‚Äù is equivalent to two statements: (1) ‚ÄúIf PthenQ‚Äù and (2) ‚ÄúIf QthenP.‚Äù
The second statement is called the converse of the Ô¨Årst and explains the use of the word
conversely in the second paragraph of this proof.
PROOF Suppose that Ais invertible. Then, since the equation AxDbhas a solution
for each b(Theorem 5), Ahas a pivot position in every row (Theorem 4 in Section 1.4).
Because Ais square, the npivot positions must be on the diagonal, which implies that
the reduced echelon form of AisIn. That is, AIn.
Now suppose, conversely, that AIn. Then, since each step of the row reduction
ofAcorresponds to left-multiplication by an elementary matrix, there exist elementary
matrices E1; : : : ; E psuch that
AE1AE2.E1A/   Ep.Ep 1E1A/DIn
That is,
EpE1ADIn (1)
Since the product EpE1of invertible matrices is invertible, (1) leads to
.EpE1/ 1.EpE1/AD.EpE1/ 1In
AD.EpE1/ 1
Thus Ais invertible, as it is the inverse of an invertible matrix (Theorem 6). Also,
A 1D¬å.EpE1/ 1¬ç 1DEpE1
Then A 1DEpE1In, which says that A 1results from applying E1; : : : ; E psuc-
cessively to In. This is the same sequence in (1) that reduced AtoIn.
SECOND REVISED PAGES


--- Page 127 ---
110 CHAPTER 2 Matrix Algebra
An Algorithm for Finding A‚Äì1
If we place AandIside by side to form an augmented matrix ¬åA I ¬ç, then row
operations on this matrix produce identical operations on Aand on I. By Theorem 7,
either there are row operations that transform AtoInandIntoA 1or else Ais not
invertible.
ALGORITHM FOR FINDING A‚Äì1
Row reduce the augmented matrix ¬åA I ¬ç. IfAis row equivalent to I, then
¬åA I ¬çis row equivalent to ¬åI A 1¬ç. Otherwise, Adoes not have an inverse.
EXAMPLE 7 Find the inverse of the matrix AD2
40 1 2
1 0 3
4 3 83
5, if it exists.
SOLUTION
¬åA I ¬çD2
40 1 2 1 0 0
1 0 3 0 1 0
4 3 8 0 0 13
52
41 0 3 0 1 0
0 1 2 1 0 0
4 3 8 0 0 13
5
2
41 0 3 0 1 0
0 1 2 1 0 0
0 3 4 0  4 13
52
41 0 3 0 1 0
0 1 2 1 0 0
0 0 2 3  4 13
5
2
41 0 3 0 1 0
0 1 2 1 0 0
0 0 1 3=2  2 1=23
5
2
41 0 0  9=2 7  3=2
0 1 0  2 4  1
0 0 1 3=2  2 1=23
5
Theorem 7 shows, since AI, that Ais invertible, and
A 1D2
4 9=2 7  3=2
 2 4  1
3=2 2 1=23
5
It is a good idea to check the Ô¨Ånal answer:
AA 1D2
40 1 2
1 0 3
4 3 83
52
4 9=2 7  3=2
 2 4  1
3=2 2 1=23
5D2
41 0 0
0 1 0
0 0 13
5
It is not necessary to check that A 1ADIsince Ais invertible.
Another View of Matrix Inversion
Denote the columns of Inbye1; : : : ; en. Then row reduction of ¬åA I ¬çto¬åI A 1¬ç
can be viewed as the simultaneous solution of the nsystems
AxDe1; A xDe2; : : : ; A xDen (2)
where the ‚Äúaugmented columns‚Äù of these systems have all been placed next to Ato form
¬åAe1e2 en¬çD¬åA I ¬ç. The equation AA 1DIand the deÔ¨Ånition of matrix
multiplication show that the columns of A 1are precisely the solutions of the systems
SECOND REVISED PAGES


--- Page 128 ---
2.2 The Inverse of a Matrix 111
in (2). This observation is useful because some applied problems may require Ô¨Ånding
only one or two columns of A 1. In this case, only the corresponding systems in (2)
need be solved.
N U M E R I C A L N O T E
In practical work, A 1is seldom computed, unless the entries of A 1are needed.
Computing both A 1andA 1btakes about three times as many arithmetic
operations as solving AxDbby row reduction, and row reduction may be more
accurate.
WEB
PRACTICE PROBLEMS
1.Use determinants to determine which of the following matrices are invertible.
a.3 9
2 6
b.4 9
0 5
c.6 9
 4 6
2.Find the inverse of the matrix AD2
41 2 1
 1 5 6
5 4 53
5, if it exists.
3.IfAis an invertible matrix, prove that 5Ais an invertible matrix.
2.2 EXERCISES
Find the inverses of the matrices in Exercises 1‚Äì4.
1.8 6
5 4
2.3 2
7 4
3.8 5
 7 5
4.3 4
7 8
5.Use the inverse found in Exercise 1 to solve the system
8x1C6x2D2
5x1C4x2D  1
6.Use the inverse found in Exercise 3 to solve the system
8x1C5x2D  9
 7x1 5x2D11
7.LetAD1 2
5 12
;b1D 1
3
;b2D1
 5
;b3D2
6
;
andb4D3
5
:
a.FindA 1;and use it to solve the four equations AxDb1;
AxDb2; AxDb3; AxDb4
b.The four equations in part (a) can be solved by the same
set of row operations, since the coefÔ¨Åcient matrix is the
same in each case. Solve the four equations in part (a) by
row reducing the augmented matrix ¬åAb1b2b3b4¬ç:
8.Use matrix algebra to show that if Ais invertible and D
satisÔ¨Åes ADDI;thenDDA 1:In Exercises 9 and 10, mark each statement True or False. Justify
each answer.
9.a.In order for a matrix Bto be the inverse of A, both
equations ABDIandBADImust be true.
b.IfAandBarennand invertible, then A 1B 1is the
inverse of AB.
c.IfADa b
c d
andab cd¬§0;then Ais invertible.
d.IfAis an invertible nnmatrix, then the equation
AxDbis consistent for each binRn.
e.Each elementary matrix is invertible.
10. a.A product of invertible nnmatrices is invertible, and
the inverse of the product is the product of their inverses
in the same order.
b.IfAis invertible, then the inverse of A 1isAitself.
c.IfADa b
c d
andadDbc, then Ais not invertible.
d.IfAcan be row reduced to the identity matrix, then Amust
be invertible.
e.IfAis invertible, then elementary row operations that
reduce Ato the identity Inalso reduce A 1toIn:
11.LetAbe an invertible nnmatrix, and let Bbe an np
matrix. Show that the equation AXDBhas a unique solu-
tionA 1B:
12. LetAbe an invertible nnmatrix, and let Bbe an npma-
trix. Explain why A 1Bcan be computed by row reduction:
SECOND REVISED PAGES


--- Page 129 ---
112 CHAPTER 2 Matrix Algebra
If¬åA B¬ç    ¬åI X¬ç; thenXDA 1B:
IfAis larger than 22, then row reduction of ¬åA B¬ç is much
faster than computing both A 1andA 1B:
13. Suppose ABDAC; where BandCarenpmatrices and A
is invertible. Show that BDC. Is this true, in general, when
Ais not invertible?
14. Suppose .B C /DD0, where BandCaremnmatrices
andDis invertible. Show that BDC:
15. Suppose A,B, and Care invertible nnmatrices. Show that
ABC is also invertible by producing a matrix Dsuch that
.ABC / D DIandD .ABC / DI:
16. Suppose AandBarenn;Bis invertible, and ABis invert-
ible. Show that Ais invertible. [ Hint: LetCDAB; and solve
this equation for A.]
17. Solve the equation ABDBCforA, assuming that A,B, and
Care square and Bis invertible.
18. Suppose Pis invertible and ADPBP 1:Solve for Bin
terms of A.
19. IfA,B, and Carenninvertible matrices, does the equation
C 1.ACX/B 1DInhave a solution, X? If so, Ô¨Ånd it.
20. Suppose A,B, and Xarennmatrices with A,X, and
A AXinvertible, and suppose
.A AX/ 1DX 1B .3/
a.Explain why Bis invertible.
b.Solve (3) for X.If you need to invert a matrix, explain
why that matrix is invertible.
21. Explain why the columns of an nnmatrix Aare linearly
independent when Ais invertible.
22. Explain why the columns of an nnmatrix AspanRnwhen
Ais invertible. [ Hint: Review Theorem 4 in Section 1.4.]
23. Suppose Aisnnand the equation AxD0has only the
trivial solution. Explain why Ahasnpivot columns and Ais
row equivalent to In:By Theorem 7, this shows that Amust
be invertible. (This exercise and Exercise 24 will be cited in
Section 2.3.)
24. Suppose Aisnnand the equation AxDbhas a solution
for each binRn. Explain why Amust be invertible. [ Hint: Is
Arow equivalent to In?]
Exercises 25 and 26 prove Theorem 4 for ADa b
c d
:
25. Show that if ad bcD0;then the equation AxD0has
more than one solution. Why does this imply that Ais not
invertible? [ Hint: First, consider aDbD0:Then, if aand
bare not both zero, consider the vector xD b
a
:¬ç
26. Show that if ad bc¬§0;the formula for A 1works.
Exercises 27 and 28 prove special cases of the facts about elemen-
tary matrices stated in the box following Example 5. Here Ais a33matrix and IDI3:(A general proof would require slightly
more notation.)
27. a.Use equation (1) from Section 2.1 to show that
row i.A/Drow i.I/A;foriD1; 2; 3:
b.Show that if rows l and 2 of Aare interchanged, then the
result may be written as EA, where Eis an elementary
matrix formed by interchanging rows 1 and 2 of I.
c.Show that if row 3 of Ais multiplied by 5, then the result
may be written as EA, where Eis formed by multiplying
row 3 of Iby 5.
28. Show that if row 3 of Ais replaced by row 3.A/ 4row 1.A/;
the result is EA, where Eis formed from Iby replacing
row 3.I/by row 3.I/ 4row 1.I/:
Find the inverses of the matrices in Exercises 29‚Äì32, if they exist.
Use the algorithm introduced in this section.
29.1 2
4 7
30.5 10
4 7
31.2
41 0  2
 3 1 4
2 3 43
5 32.2
41 2 1
4 7 3
 2 6  43
5
33. Use the algorithm from this section to Ô¨Ånd the inverses of
2
41 0 0
1 1 0
1 1 13
5and2
6641 0 0 0
1 1 0 0
1 1 1 0
1 1 1 13
775:
LetAbe the corresponding nnmatrix, and let Bbe its
inverse. Guess the form of B, and then prove that ABDI
andBADI:
34. Repeat the strategy of Exercise 33 to guess the inverse of
AD2
6666641 0 0  0
1 2 0 0
1 2 3 0
:::::::::
1 2 3  n3
777775:Prove that your guess is
correct.
35. LetAD2
4 2 7 9
2 5 6
1 3 43
5:Find the third column of A 1
without computing the other columns.
36. [M] LetAD2
4 25  9 27
546 180 537
154 50 1493
5:Find the second and
third columns of A 1without computing the Ô¨Årst column.
37. LetAD2
41 2
1 3
1 53
5:Construct a 23matrix C(by trial and
error) using only l,  1, and 0 as entries, such that CADI2:
Compute ACand note that AC¬§I3:
38. LetAD1 1 1 0
0 1 1 1
:Construct a 42matrix D
SECOND REVISED PAGES


--- Page 130 ---
2.3 Characterizations of Invertible Matrices 113
using only 1 and 0 as entries, such that ADDI2:Is it possi-
ble that CADI4for some 42matrix C? Why or why not?
39. LetDD2
4:005 :002 :001
:002 :004 :002
:001 :002 :0053
5be a Ô¨Çexibility matrix,
with Ô¨Çexibility measured in inches per pound. Suppose
that forces of 30, 50, and 20 lb are applied at points 1,
2, and 3, respectively, in Figure 1 of Example 3. Find the
corresponding deÔ¨Çections.
40. [M]Compute the stiffness matrix D 1forDin Exercise 39.
List the forces needed to produce a deÔ¨Çection of .04 in. at
point 3, with zero deÔ¨Çections at the other points.
41. [M] LetDD2
664:0040 :0030 :0010 :0005
:0030 :0050 :0030 :0010
:0010 :0030 :0050 :0030
:0005 :0010 :0030 :00403
775be aÔ¨Çexibility matrix for an elastic beam with four points at which
force is applied. Units are centimeters per newton of force.
Measurements at the four points show deÔ¨Çections of .08, .12,
.16, and .12 cm. Determine the forces at the four points.
f3#1 #2 #3 #4
f1f2f4.08 .12 .16 .12
DeÔ¨Çection of elastic beam in Exercises 41 and 42.
42. [M] With Das in Exercise 41, determine the forces that
produce a deÔ¨Çection of .24 cm at the second point on the
beam, with zero deÔ¨Çections at the other three points. How is
the answer related to the entries in D 1¬ã[Hint: First answer
the question when the deÔ¨Çection is 1 cm at the second point.]
SOLUTIONS TO PRACTICE PROBLEMS
1.a.det3 9
2 6
D36 . 9/2D18C18D36. The determinant is nonzero, so
the matrix is invertible.
b.det4 9
0 5
D45 . 9/0D20¬§0. The matrix is invertible.
c.det6 9
 4 6
D66 . 9/. 4/D36 36D0. The matrix is not invertible.
2.¬åA I ¬ç2
41 2 1 1 0 0
 1 5 6 0 1 0
5 4 5 0 0 13
5
2
41 2 1 1 0 0
0 3 5 1 1 0
0 6 10  5 0 13
5
2
41 2 1 1 0 0
0 3 5 1 1 0
0 0 0  7 2 13
5
So¬åA I ¬çis row equivalent to a matrix of the form ¬åB D ¬ç, where Bis square
and has a row of zeros. Further row operations will not transform BintoI, so we
stop. Adoes not have an inverse.
3.Since Ais an invertible matrix, there exists a matrix Csuch that ACDIDCA. The
goal is to Ô¨Ånd a matrix Dso that ( 5A)DDIDD(5A). Set DD1=5 C . Applying
Theorem 2 from Section 2.1 establishes that ( 5A)(1=5 C )D(5)(1/5)( AC)D1ID
I, and (1/5 C)(5A)D(1/5)(5)( CA) = 1 IDI. Thus 1/5 Cis indeed the inverse of
A, proving that Ais invertible.
2.3 CHARACTERIZATIONS OF INVERTIBLE MATRICES
This section provides a review of most of the concepts introduced in Chapter 1, in
relation to systems of nlinear equations in nunknowns and to square matrices. The
main result is Theorem 8.
SECOND REVISED PAGES


--- Page 131 ---
114 CHAPTER 2 Matrix Algebra
T H E O R E M 8 The Invertible Matrix Theorem
LetAbe a square nnmatrix. Then the following statements are equivalent.
That is, for a given A, the statements are either all true or all false.
a.Ais an invertible matrix.
b.Ais row equivalent to the nnidentity matrix.
c.Ahasnpivot positions.
d.The equation AxD0has only the trivial solution.
e.The columns of Aform a linearly independent set.
f.The linear transformation x7!Axis one-to-one.
g.The equation AxDbhas at least one solution for each binRn.
h.The columns of AspanRn.
i.The linear transformation x7!AxmapsRnontoRn.
j.There is an nnmatrix Csuch that CADI.
k.There is an nnmatrix Dsuch that ADDI.
l.ATis an invertible matrix.
First, we need some notation. If the truth of statement (a) always implies that state-
(c) (d)(j )(a)
(b)
FIGURE 1ment (j) is true, we say that (a) implies (j) and write (a) )(j). The proof will establish
the ‚Äúcircle‚Äù of implications shown in Figure 1. If any one of these Ô¨Åve statements is
true, then so are the others. Finally, the proof will link the remaining statements of the
theorem to the statements in this circle.
PROOF If statement (a) is true, then A 1works for Cin (j), so (a) )(j). Next, (j) )(d)
by Exercise 23 in Section 2.1. (Turn back and read the exercise.) Also, (d) )(c) by
Exercise 23 in Section 2.2. If Ais square and has npivot positions, then the pivots
must lie on the main diagonal, in which case the reduced echelon form of AisIn:Thus
(c))(b). Also, (b) )(a) by Theorem 7 in Section 2.2. This completes the circle in
Figure 1.
Next, (a) )(k) because A 1works for D. Also, (k) )(g) by Exercise 24 in Sec-
tion 2.1, and (g) )(a) by Exercise 24 in Section 2.2. So (k) and (g) are linked to
the circle. Further, (g), (h), and (i) are equivalent for any matrix, by Theorem 4 in
Section 1.4 and Theorem 12(a) in Section 1.9. Thus, (h) and (i) are linked through (g) to
the circle.
Since (d) is linked to the circle, so are (e) and (f), because (d), (e), and (f) are all
equivalent for anymatrix A. (See Section 1.7 and Theorem 12(b) in Section 1.9.) Finally,
(a))(l) by Theorem 6(c) in Section 2.2, and (l) )(a) by the same theorem with Aand
ATinterchanged. This completes the proof.
(g)(k)
(h)(a)
(l) (a)(i) (g)
(e) (f ) (d)
Because of Theorem 5 in Section 2.2, statement (g) in Theorem 8 could also be
written as ‚ÄúThe equation AxDbhas a unique solution for each binRn.‚Äù This statement
certainly implies (b) and hence implies that Ais invertible.
The next fact follows from Theorem 8 and Exercise 8 in Section 2.2.
LetAandBbe square matrices. If ABDI, then AandBare both invertible,
withBDA 1andADB 1.
SECOND REVISED PAGES


--- Page 132 ---
2.3 Characterizations of Invertible Matrices 115
The Invertible Matrix Theorem divides the set of all nnmatrices into two disjoint
classes: the invertible (nonsingular) matrices, and the noninvertible (singular) matrices.
Each statement in the theorem describes a property of every nninvertible matrix.
Thenegation of a statement in the theorem describes a property of every nnsingular
matrix. For instance, an nnsingular matrix is notrow equivalent to In, does nothave
npivot positions, and has linearly dependent columns. Negations of other statements
are considered in the exercises.
EXAMPLE 1 Use the Invertible Matrix Theorem to decide if Ais invertible:
AD2
41 0  2
3 1  2
 5 1 93
5
SOLUTION
A2
41 0  2
0 1 4
0 1 13
52
41 0  2
0 1 4
0 0 33
5
SoAhas three pivot positions and hence is invertible, by the Invertible Matrix Theorem,
statement (c).
The power of the Invertible Matrix Theorem lies in the connections it provides
SG Expanded Table
for the IMT 2‚Äì10 among so many important concepts, such as linear independence of columns of a matrix
Aand the existence of solutions to equations of the form AxDb. It should be empha-
sized, however, that the Invertible Matrix Theorem applies only to square matrices . For
example, if the columns of a 43matrix are linearly independent, we cannot use the
Invertible Matrix Theorem to conclude anything about the existence or nonexistence of
solutions to equations of the form AxDb.
Invertible Linear Transformations
Recall from Section 2.1 that matrix multiplication corresponds to composition of linear
transformations. When a matrix Ais invertible, the equation A 1AxDxcan be viewed
as a statement about linear transformations. See Figure 2.
Multiplication
by A
Multiplication
by A‚Äì1Ax x
FIGURE 2 A 1transforms Axback to x.
A linear transformation TWRn!Rnis said to be invertible if there exists a func-
tionSWRn!Rnsuch that
S.T . x//Dxfor all xinRn(1)
T .S. x//Dxfor all xinRn(2)
The next theorem shows that if such an Sexists, it is unique and must be a linear
transformation. We call Stheinverse ofTand write it as T 1.
SECOND REVISED PAGES


--- Page 133 ---
116 CHAPTER 2 Matrix Algebra
T H E O R E M 9 LetTWRn!Rnbe a linear transformation and let Abe the standard matrix for
T. Then Tis invertible if and only if Ais an invertible matrix. In that case, the
linear transformation Sgiven by S.x/DA 1xis the unique function satisfying
equations .1/and.2/.
Remark: See the comment on the proof of Theorem 7.
PROOF Suppose that Tis invertible. Then (2) shows that Tis onto Rn, for if bis in
RnandxDS.b/, then T .x/DT .S. b//Db, so each bis in the range of T. Thus Ais
invertible, by the Invertible Matrix Theorem, statement (i).
Conversely, suppose that Ais invertible, and let S.x/DA 1x. Then, Sis a linear
transformation, and Sobviously satisÔ¨Åes (1) and (2). For instance,
S.T . x//DS.Ax/DA 1.Ax/Dx
Thus Tis invertible. The proof that Sis unique is outlined in Exercise 39.
EXAMPLE 2 What can you say about a one-to-one linear transformation Tfrom
RnintoRn?
SOLUTION The columns of the standard matrix AofTare linearly independent (by
Theorem 12 in Section 1.9). So Ais invertible, by the Invertible Matrix Theorem, and
TmapsRnontoRn. Also, Tis invertible, by Theorem 9.
N U M E R I C A L N O T E S
In practical work, you might occasionally encounter a ‚Äúnearly singular‚Äù or ill-
conditioned matrix‚Äîan invertible matrix that can become singular if some of
its entries are changed ever so slightly. In this case, row reduction may produce
fewer than npivot positions, as a result of roundoff error. Also, roundoff error
can sometimes make a singular matrix appear to be invertible.
Some matrix programs will compute a condition number for a square
matrix. The larger the condition number, the closer the matrix is to being singular.
The condition number of the identity matrix is 1. A singular matrix has an
inÔ¨Ånite condition number. In extreme cases, a matrix program may not be able to
distinguish between a singular matrix and an ill-conditioned matrix.
Exercises 41‚Äì 45 show that matrix computations can produce substantial
error when a condition number is large.
WEB
PRACTICE PROBLEMS
1.Determine if AD2
42 3 4
2 3 4
2 3 43
5is invertible.
2.Suppose that for a certain nnmatrix A, statement (g) of the Invertible Matrix
Theorem is nottrue. What can you say about equations of the form AxDb?
3.Suppose that AandBarennmatrices and the equation ABxD0has a nontrivial
solution. What can you say about the matrix AB?
SECOND REVISED PAGES


--- Page 134 ---
2.3 Characterizations of Invertible Matrices 117
2.3 EXERCISES
Unless otherwise speciÔ¨Åed, assume that all matrices in these
exercises are nn. Determine which of the matrices in Exercises
1‚Äì10 are invertible. Use as few calculations as possible. Justify
your answers.
1.5 7
 3 6
2. 4 6
6 9
3.2
45 0 0
 3 7 0
8 5  13
5 4.2
4 7 0 4
3 0  1
2 0 93
5
5.2
40 3  5
1 0 2
 4 9 73
5 6.2
41 5 4
0 3 4
 3 6 03
5
7.2
664 1 3 0 1
3 5 8  3
 2 6 3 2
0 1 2 13
7758.2
6641 3 7 4
0 5 9 6
0 0 2 8
0 0 0 103
775
9.[M]2
6644 0  7 7
 6 1 11 9
7 5 10 19
 1 2 3  13
775
10. [M]2
666645 3 1 7 9
6 4 2 8  8
7 5 3 10 9
9 6 4  9 5
8 5 2 11 43
77775
In Exercises 11 and 12, the matrices are all nn. Each part of
the exercises is an implication of the form ‚ÄúIf ‚Äústatement 1‚Äù,
then ‚Äústatement 2‚Äù.‚Äù Mark an implication as True if the truth of
‚Äústatement 2‚Äù always follows whenever ‚Äústatement 1‚Äù happens
to be true. An implication is False if there is an instance in
which ‚Äústatement 2‚Äù is false but ‚Äústatement 1‚Äù is true. Justify each
answer.
11.a.If the equation AxD0has only the trivial solution, then
Ais row equivalent to the nnidentity matrix.
b.If the columns of AspanRn, then the columns are linearly
independent.
c.IfAis an nnmatrix, then the equation AxDbhas at
least one solution for each binRn.
d.If the equation AxD0has a nontrivial solution, then A
has fewer than npivot positions.
e.IfATis not invertible, then Ais not invertible.
12. a.If there is an nnmatrix Dsuch that ADDI, then there
is also an nnmatrix Csuch that CADI:
b.If the columns of Aare linearly independent, then the
columns of AspanRn.
c.If the equation AxDbhas at least one solution for each
binRn, then the solution is unique for each b.d.lf the linear transformation (x)7!AxmapsRnintoRn,
then Ahasnpivot positions.
e.If there is a binRnsuch that the equation AxDbis
inconsistent, then the transformation x7!Axis not one-
to-one.
13. Anmnupper triangular matrix is one whose entries
below the main diagonal are 0‚Äôs (as in Exercise 8). When
is a square upper triangular matrix invertible? Justify your
answer.
14. Anmnlower triangular matrix is one whose entries
above the main diagonal are 0‚Äôs (as in Exercise 3). When
is a square lower triangular matrix invertible? Justify your
answer.
15. Can a square matrix with two identical columns be invert-
ible? Why or why not?
16. Is it possible for a 55matrix to be invertible when its
columns do not span R5? Why or why not?
17. IfAis invertible, then the columns of A 1are linearly
independent. Explain why.
18. IfCis66and the equation CxDvis consistent for every
vinR6, is it possible that for some v, the equation CxDv
has more than one solution? Why or why not?
19. If the columns of a 77matrix Dare linearly independent,
what can you say about solutions of DxDb? Why?
20. Ifnnmatrices EandFhave the property that EFDI,
then EandFcommute. Explain why.
21. If the equation GxDyhas more than one solution for some
yinRn, can the columns of GspanRn? Why or why not?
22. If the equation HxDcis inconsistent for some cinRn, what
can you say about the equation HxD0? Why?
23. If an nnmatrix Kcannot be row reduced to In;what can
you say about the columns of K? Why?
24. IfLisnnand the equation LxD0has the trivial solution,
do the columns of LspanRn? Why?
25. Verify the boxed statement preceding Example 1.
26. Explain why the columns of A2spanRnwhenever the
columns of Aare linearly independent.
27. Show that if ABis invertible, so is A. You cannot use Theorem
6(b), because you cannot assume thatAandBare invertible.
[Hint: There is a matrix Wsuch that ABW DI:Why?]
28. Show that if ABis invertible, so is B.
29. IfAis annnmatrix and the equation AxDbhas more than
one solution for some b, then the transformation x7!Axis
not one-to-one. What else can you say about this transforma-
tion? Justify your answer.
SECOND REVISED PAGES


--- Page 135 ---
118 CHAPTER 2 Matrix Algebra
30. IfAis an nnmatrix and the transformation x7!Axis
one-to-one, what else can you say about this transformation?
Justify your answer.
31. Suppose Ais an nnmatrix with the property that the
equation AxDbhas at least one solution for each binRn.
Without using Theorems 5 or 8, explain why each equation
AxDbhas in fact exactly one solution.
32. Suppose Ais annnmatrix with the property that the equa-
tionAxD0has only the trivial solution. Without using the
Invertible Matrix Theorem, explain directly why the equation
AxDbmust have a solution for each binRn.
In Exercises 33 and 34, Tis a linear transformation from R2into
R2. Show that Tis invertible and Ô¨Ånd a formula for T 1:
33.T .x 1; x2/D. 5x1C9x2; 4x 1 7x2/
34.T .x 1; x2/D.6x1 8x2; 5x1C7x2/
35. LetTWRn!Rnbe an invertible linear transformation. Ex-
plain why Tis both one-to-one and onto Rn. Use equations
(1) and (2). Then give a second explanation using one or more
theorems.
36. LetTbe a linear transformation that maps RnontoRn. Show
thatT 1exists and maps RnontoRn. IsT 1also one-to-
one?
37. Suppose TandUare linear transformations from RntoRn
such that T .U x/Dxfor all xinRn. Is it true that U.T x/Dx
for all xinRn? Why or why not?
38. Suppose a linear transformation TWRn!Rnhas the prop-
erty that T .u/DT .v/for some pair of distinct vectors uand
vinRn. Can TmapRnontoRn? Why or why not?
39. LetTWRn!Rnbe an invertible linear transformation,
and let SandUbe functions from RnintoRnsuch that
S .T . x//DxandU .T . x//Dxfor all xinRn. Show that
U.v/DS.v/for all vinRn. This will show that Thas a
unique inverse, as asserted in Theorem 9. [ Hint: Given any
vinRn, we can write vDT .x/for some x. Why? Compute
S.v/andU.v/.]
40. Suppose TandSsatisfy the invertibility equations (1) and
(2), where Tis a linear transformation. Show directly that
Sis a linear transformation. [ Hint: Given u,vinRn, let
xDS.u/;yDS(v). Then T .x/Du; T .y/Dv:Why? Apply
Sto both sides of the equation T .x/CT .y/DT .xCy/:
Also, consider T .cx/DcT .x/.]41. [M]Suppose an experiment leads to the following system of
equations:
4:5x 1C3:1x 2D19:249 .3/
1:6x 1C1:1x 2D6:843
a.Solve system (3), and then solve system (4), below, in
which the data on the right have been rounded to two
decimal places. In each case, Ô¨Ånd the exact solution.
4:5x 1C3:1x 2D19:25 .4/
1:6x 1C1:1x 2D6:84
b.The entries in (4) differ from those in (3) by less than
:05%. Find the percentage error when using the solution
of (4) as an approximation for the solution of (3).
c.Use your matrix program to produce the condition num-
ber of the coefÔ¨Åcient matrix in (3).
Exercises 42‚Äì44 show how to use the condition number of a ma-
trixAto estimate the accuracy of a computed solution of AxDb:
If the entries of Aandbare accurate to about rsigniÔ¨Åcant digits
and if the condition number of Ais approximately 10k(with ka
positive integer), then the computed solution of AxDbshould
usually be accurate to at least r ksigniÔ¨Åcant digits.
42. [M] Find the condition number of the matrix Ain Exercise 9.
Construct a random vector xinR4and compute bDAx.
Then use your matrix program to compute the solution x1
ofAxDb. To how many digits do xandx1agree? Find out
the number of digits your matrix program stores accurately,
and report how many digits of accuracy are lost when x1is
used in place of the exact solution x.
43. [M] Repeat Exercise 42 for the matrix in Exercise 10.
44. [M] Solve an equation AxDbfor a suitable bto Ô¨Ånd the last
column of the inverse of the Ô¨Åfth-order Hilbert matrix
AD2
666641 1=2 1=3 1=4 1=5
1=2 1=3 1=4 1=5 1=6
1=3 1=4 1=5 1=6 1=7
1=4 1=5 1=6 1=7 1=8
1=5 1=6 1=7 1=8 1=93
77775
How many digits in each entry of xdo you expect to be
correct? Explain. [ Note: The exact solution is .630; 12600;
56700;  88200; 44100/: ]
45. [M] Some matrix programs, such as MATLAB, have a com-
mand to create Hilbert matrices of various sizes. If possible,
use an inverse command to compute the inverse of a twelfth-
order or larger Hilbert matrix, A. Compute AA 1:Report
what you Ô¨Ånd.
SG
Mastering: Reviewing and Reflecting 2‚Äì13
SOLUTIONS TO PRACTICE PROBLEMS
1.The columns of Aare obviously linearly dependent because columns 2 and 3 are mul-
tiples of column 1. Hence Acannot be invertible, by the Invertible Matrix
Theorem.
SECOND REVISED PAGES


--- Page 136 ---
2.4 Partitioned Matrices 119
2.If statement (g) is nottrue, then the equation AxDbis inconsistent for at least one
binRn.
3.Apply the Invertible Matrix Theorem to the matrix ABin place of A. Then statement
(d) becomes: ABxD0has only the trivial solution. This is not true. So ABis not
invertible.
2.4 PARTITIONED MATRICES
A key feature of our work with matrices has been the ability to regard a matrix Aas a list
of column vectors rather than just a rectangular array of numbers. This point of view has
been so useful that we wish to consider other partitions ofA, indicated by horizontal
and vertical dividing rules, as in Example 1 below. Partitioned matrices appear in most
modern applications of linear algebra because the notation highlights essential struc-
tures in matrix analysis, as in the chapter introductory example on aircraft design. This
section provides an opportunity to review matrix algebra and use the Invertible Matrix
Theorem.
EXAMPLE 1 The matrix
AD2
43 0  1 5 9  2
 5 2 4 0  3 1
 8 6 3 1 7  43
5
can also be written as the 23partitioned (orblock )matrix
ADA11 A12 A13
A21 A22 A23
whose entries are the blocks (orsubmatrices )
A11D3 0  1
 5 2 4
; A 12D5 9
0 3
; A 13D 2
1
A21D 8 6 3
; A 22D1 7
; A 23D 4
EXAMPLE 2 When a matrix Aappears in a mathematical model of a physical
system such as an electrical network, a transportation system, or a large corporation,
it may be natural to regard Aas a partitioned matrix. For instance, if a microcomputer
circuit board consists mainly of three VLSI (very large-scale integrated) microchips,
then the matrix for the circuit board might have the general form
AD2
64A11 A12 A13
A21 A22 A23
A31 A32 A333
75
The submatrices on the ‚Äúdiagonal‚Äù of A‚Äînamely, A11,A22, andA33‚Äîconcern the three
VLSI chips, while the other submatrices depend on the interconnections among those
microchips.
Addition and Scalar Multiplication
If matrices AandBare the same size and are partitioned in exactly the same way,
then it is natural to make the same partition of the ordinary matrix sum ACB. In this
SECOND REVISED PAGES


--- Page 137 ---
120 CHAPTER 2 Matrix Algebra
case, each block of ACBis the (matrix) sum of the corresponding blocks of AandB.
Multiplication of a partitioned matrix by a scalar is also computed block by block.
Multiplication of Partitioned Matrices
Partitioned matrices can be multiplied by the usual row‚Äìcolumn rule as if the block
entries were scalars, provided that for a product AB, the column partition of Amatches
the row partition of B.
EXAMPLE 3 Let
AD2
42 3 1 0  4
15 2 3  1
0 4 27 13
5DA11A12
A21A22
; BD2
666646 4
 2 1
 37
 1 3
5 23
77775DB1
B2
The 5 columns of Aare partitioned into a set of 3 columns and then a set of 2
columns. The 5 rows of Bare partitioned in the same way‚Äîinto a set of 3 rows and
then a set of 2 rows. We say that the partitions of AandBareconformable forblock
multiplication . It can be shown that the ordinary product ABcan be written as
ABDA11A12
A21A22B1
B2
DA11B1CA12B2
A21B1CA22B2
D2
4 5 4
 62
2 13
5
It is important for each smaller product in the expression for ABto be written with
the submatrix from Aon the left, since matrix multiplication is not commutative. For
instance,
A11B1D2 3 1
1 5  22
46 4
 2 1
 3 73
5D15 12
2 5
A12B2D0 4
3 1 1 3
5 2
D 20 8
 8 7
Hence the top block in ABis
A11B1CA12B2D15 12
2 5
C 20 8
 8 7
D 5 4
 6 2
The row‚Äìcolumn rule for multiplication of block matrices provides the most general
way to regard the product of two matrices. Each of the following views of a product
has already been described using simple partitions of matrices: (1) the deÔ¨Ånition of Ax
using the columns of A, (2) the column deÔ¨Ånition of AB, (3) the row‚Äìcolumn rule for
computing AB, and (4) the rows of ABas products of the rows of Aand the matrix B.
A Ô¨Åfth view of AB, again using partitions, follows in Theorem 10 below.
The calculations in the next example prepare the way for Theorem 10. Here col k.A/
is the kth column of A, and row k.B/is the kth row of B.
EXAMPLE 4 LetAD 3 1 2
1 4 5
andBD2
4a b
c d
e f3
5. Verify that
ABDcol1.A/row 1.B/Ccol2.A/row 2.B/Ccol3.A/row 3.B/
SECOND REVISED PAGES


--- Page 138 ---
2.4 Partitioned Matrices 121
SOLUTION Each term above is an outer product . (See Exercises 27 and 28 in Sec-
tion 2.1.) By the row‚Äìcolumn rule for computing a matrix product,
col1.A/row 1.B/D 3
1a b
D 3a 3b
a b
col2.A/row 2.B/D1
 4c d
Dc d
 4c 4d
col3.A/row 3.B/D2
5e f
D2e 2f
5e 5f
Thus
3X
kD1colk.A/row k.B/D 3aCcC2e 3bCdC2f
a 4cC5e b  4dC5f
This matrix is obviously AB. Notice that the .1; 1/ -entry in ABis the sum of the .1; 1/ -
entries in the three outer products, the .1; 2/ -entry in ABis the sum of the .1; 2/ -entries
in the three outer products, and so on.
T H E O R E M 1 0 Column--Row Expansion of AB
IfAismnandBisnp, then
ABD¬åcol1.A/ col2.A/ coln.A/¬ç2
6664row 1.B/
row 2.B/
:::
row n.B/3
7775(1)
Dcol1.A/row 1.B/C  C coln.A/row n.B/
PROOF For each row index iand column index j, the.i; j / -entry in col k.A/row k.B/
is the product of aikfrom col k.A/andbkjfrom row k.B/. Hence the .i; j / -entry in the
sum shown in equation (1) is
ai1b1jCai2b2jC  C ainbnj
.kD1/ .kD2/ .kDn/
This sum is also the .i; j / -entry in AB, by the row‚Äìcolumn rule.
Inverses of Partitioned Matrices
The next example illustrates calculations involving inverses and partitioned matrices.
EXAMPLE 5 A matrix of the form
ADA11A12
0 A 22
is said to be block upper triangular . Assume that A11ispp,A22isqq, and Ais
invertible. Find a formula for A 1.
SECOND REVISED PAGES


--- Page 139 ---
122 CHAPTER 2 Matrix Algebra
SOLUTION Denote A 1byBand partition Bso thatA11A12
0 A 22B11B12
B21B22
DIp0
0 I q
(2)
This matrix equation provides four equations that will lead to the unknown blocks
B11; : : : ; B 22. Compute the product on the left side of equation (2), and equate each entry
with the corresponding block in the identity matrix on the right. That is, set
A11B11CA12B21DIp (3)
A11B12CA12B22D0 (4)
A22B21D0 (5)
A22B22DIq (6)
By itself, equation (6) does not show that A22is invertible. However, since A22is
square, the Invertible Matrix Theorem and (6) together show that A22is invertible and
B22DA 1
22. Next, left-multiply both sides of (5) by A 1
22and obtain
B21DA 1
220D0
so that (3) simpliÔ¨Åes to
A11B11C0DIp
Since A11is square, this shows that A11is invertible and B11DA 1
11. Finally, use these
results with (4) to Ô¨Ånd that
A11B12D  A12B22D  A12A 1
22 and B12D  A 1
11A12A 1
22
Thus
A 1D"
A11A12
0 A 22# 1
D"
A 1
11 A 1
11A12A 1
22
0 A 1
22#
Ablock diagonal matrix is a partitioned matrix with zero blocks off the main
diagonal (of blocks). Such a matrix is invertible if and only if each block on the diagonal
is invertible. See Exercises 13 and 14.
N U M E R I C A L N O T E S
1.When matrices are too large to Ô¨Åt in a computer‚Äôs high-speed memory,
partitioning permits the computer to work with only two or three submatrices
at a time. For instance, one linear programming research team simpliÔ¨Åed
a problem by partitioning the matrix into 837 rows and 51 columns. The
problem‚Äôs solution took about 4 minutes on a Cray supercomputer.1
2.Some high-speed computers, particularly those with vector pipeline architec-
ture, perform matrix calculations more efÔ¨Åciently when the algorithms use
partitioned matrices.2
3.Professional software for high-performance numerical linear algebra, such as
LAPACK, makes intensive use of partitioned matrix calculations.
1The solution time doesn‚Äôt sound too impressive until you learn that each of the 51 block columns contained
about 250,000 individual columns. The original problem had 837 equations and more than 12,750,000
variables! Nearly 100 million of the more than 10 billion entries in the matrix were nonzero. See Robert E.
Bixby et al., ‚ÄúVery Large-Scale Linear Programming: A Case Study in Combining Interior Point and
Simplex Methods,‚Äù Operations Research , 40, no. 5 (1992): 885‚Äì897.
2The importance of block matrix algorithms for computer calculations is described in Matrix Computations ,
3rd ed., by Gene H. Golub and Charles F. van Loan (Baltimore: Johns Hopkins University Press, 1996).
SECOND REVISED PAGES


--- Page 140 ---
2.4 Partitioned Matrices 123
The exercises that follow give practice with matrix algebra and illustrate typical
calculations found in applications.
PRACTICE PROBLEMS
1.Show thatI 0
A I
is invertible and Ô¨Ånd its inverse.
2.Compute XTX, where Xis partitioned asX1X2
.
2.4 EXERCISES
In Exercises 1‚Äì9, assume that the matrices are partitioned con-
formably for block multiplication. Compute the products shown
in Exercises 1‚Äì4.
1.I 0
E IA B
C D
2.E 0
0 FA B
C D
3.0 I
I 0W X
Y Z
4.I 0
 X IA B
C D
In Exercises 5‚Äì8, Ô¨Ånd formulas for X,Y, and Zin terms of A,B,
andC, and justify your calculations. In some cases, you may need
to make assumptions about the size of a matrix in order to produce
a formula. [ Hint: Compute the product on the left, and set it equal
to the right side.]
5.A B
C 0I 0
X Y
D0 I
Z 0
6.X 0
Y ZA 0
B C
DI 0
0 I
7.X 0 0
Y 0 I2
4A Z
0 0
B I3
5DI 0
0 I
8.A B
0 IX Y Z
0 0 I
DI 0 0
0 0 I
9.Suppose A11is an invertible matrix. Find matrices XandY
such that the product below has the form indicated. Also,
compute B22. [Hint: Compute the product on the left, and set
it equal to the right side.]
2
4I 0 0
X I 0
Y 0 I3
52
4A11A12
A21A22
A31A323
5D2
4B11B12
0 B 22
0 B 323
5
10. The inverse of2
4I 0 0
C I 0
A B I3
5is2
4I 0 0
Z I 0
X Y I3
5:
Find X,Y, and Z.In Exercises 11 and 12, mark each statement True or False. Justify
each answer.
11.a.IfADA1A2
andBDB1B2
;withA1andA2
the same sizes as B1andB2, respectively, then ACBD
¬åA1CB1A2CB2¬ç:
b.IfADA11A12
A21A22
andBDB1
B2
;then the partitions
ofAandBare conformable for block multiplication.
12. a.The deÔ¨Ånition of the matrix‚Äìvector product Axis a special
case of block multiplication.
b.IfA1; A2; B1;andB2arennmatrices, ADA1
A2
, and
BDB1B2
, then the product BAis deÔ¨Åned, but ABis
not.
13. LetADB 0
0 C
, where BandCare square. Show that A
is invertible if and only if both BandCare invertible.
14. Show that the block upper triangular matrix Ain Example 5 is
invertible if and only if both A11andA22are invertible. [ Hint:
IfA11andA22are invertible, the formula for A 1given in
Example 5 actually works as the inverse of A.] This fact about
Ais an important part of several computer algorithms that
estimate eigenvalues of matrices. Eigenvalues are discussed
in Chapter 5.
15. Suppose A11is invertible. Find XandYsuch that
A11A12
A21A22
DI 0
X IA110
0 SI Y
0 I
(7)
where SDA22 A21A 1
11A12::The matrix Sis called the
Schur complement ofA11:Likewise, if A22is invertible,
the matrix A11 A12A 1
22A21is called the Schur complement
ofA22:Such expressions occur frequently in the theory of
systems engineering, and elsewhere.
16. Suppose the block matrix Aon the left side of (7) is invertible
andA11is invertible. Show that the Schur complement Sof
A11is invertible. [ Hint: The outside factors on the right side
of (7) are always invertible. Verify this.] When AandA11are
both invertible, (7) leads to a formula for A 1, using S 1,
A 1
11, and the other entries in A.
SECOND REVISED PAGES


--- Page 141 ---
124 CHAPTER 2 Matrix Algebra
17. When a deep space probe is launched, corrections may
be necessary to place the probe on a precisely calculated
trajectory. Radio telemetry provides a stream of vectors,
x1; : : : ; xk, giving information at different times about how
the probe‚Äôs position compares with its planned trajectory.
LetXkbe the matrix [ x1xk]. The matrix GkDXkXT
kis
computed as the radar data are analyzed. When xkC1arrives,
a new GkC1must be computed. Since the data vectors arrive
at high speed, the computational burden could be severe.
But partitioned matrix multiplication helps tremendously.
Compute the column‚Äìrow expansions of GkandGkC1;and
describe what must be computed in order to update Gkto
form GkC1.
The probe Galileo was launched October 18,
1989, and arrived near Jupiter in early
December 1995.
18. LetXbe an mndata matrix such that XTXis invertible,
and let MDIm X.XTX/ 1XT:Add a column x0to the
data and form
WD¬åX x0¬ç
Compute WTW:The (1, 1)-entry is XTX. Show that the
Schur complement (Exercise 15) of XTXcan be written in
the form xT
0Mx0:It can be shown that the quantity
.xT
0Mx0/ 1is the (2, 2)-entry in .WTW / 1:This en-
try has a useful statistical interpretation, under appropriate
hypotheses.
In the study of engineering control of physical systems, a standard
set of differential equations is transformed by Laplace transforms
into the following system of linear equations:
A sInB
C I mx
u
D0
y
(8)
where Aisnn;Bisnm;Cismn, and sis a variable. The
vector uinRmis the ‚Äúinput‚Äù to the system, yinRmis the ‚Äúoutput,‚Äù
andxinRnis the ‚Äústate‚Äù vector. (Actually, the vectors x,u, and
yare functions of s, but we suppress this fact because it does not
affect the algebraic calculations in Exercises 19 and 20.)
19. Assume A sInis invertible and view (8) as a system of two
matrix equations. Solve the top equation for xand substituteinto the bottom equation. The result is an equation of the
form W.s/ uDy, where W(s) is a matrix that depends on
s.W.s/ is called the transfer function of the system because
it transforms the input uinto the output y. Find W.s/ and
describe how it is related to the partitioned system matrix on
the left side of (8). See Exercise 15.
20. Suppose the transfer function W.s/ in Exercise 19 is invert-
ible for some s. It can be shown that the inverse transfer
function W .s/ 1, which transforms outputs into inputs, is the
Schur complement of A BC sInfor the matrix below.
Find this Schur complement. See Exercise 15.
A BC sInB
 C Im
21. a.Verify that A2DIwhen AD1 0
3 1
.
b.Use partitioned matrices to show that M2DIwhen
MD2
6641 0 0 0
3 1 0 0
1 0  1 0
0 1  3 13
775
22. Generalize the idea of Exercise 2l(a) [not 2l(b)] by con-
structing a 55matrix MDA 0
C D
such that M2DI:
Make Ca nonzero 23matrix. Show that your construction
works.
23. Use partitioned matrices to prove by induction that the prod-
uct of two lower triangular matrices is also lower triangular.
[Hint: A.kC1/.kC1/matrix A1can be written in the
form below, where ais a scalar, vis inRk, and Ais akk
lower triangular matrix. See the Study Guide for help with
induction.]
A1Da 0T
v A
24. Use partitioned matrices to prove by induction that for
nD2; 3; : : : ; thennmatrix Ashown below is invertible
andBis its inverse.
AD2
6666641 0 0  0
1 1 0 0
1 1 1 0
::::::
1 1 1 : : : 13
777775,
BD2
6666641 0 0  0
 1 1 0 0
0 1 1 0
:::::::::
0 : : :  1 13
777775
For the induction step, assume Aand B are
.kC1/.kC1/matrices, and partition AandBin a form
similar to that displayed in Exercise 23.
SECOND REVISED PAGES


--- Page 142 ---
2.5 Matrix Factorizations 125
25. Without using row reduction, Ô¨Ånd the inverse of
AD2
666641 2 0 0 0
3 5 0 0 0
0 0 2 0 0
0 0 0 7 8
0 0 0 5 63
77775
26. [M] For block operations, it may be necessary to access or
enter submatrices of a large matrix. Describe the functions
or commands of your matrix program that accomplish the
following tasks. Suppose Ais a2030matrix.
a.Display the submatrix of Afrom rows 15 to 20 and
columns 5 to 10.
b.Insert a 510matrix BintoA, beginning at row 10 and
column 20.
c.Create a 5050matrix of the form BDA 0
0 AT
.[Note: It may not be necessary to specify the zero blocks
inB.]
27. [M] Suppose memory or size restrictions prevent your matrix
program from working with matrices having more than 32
rows and 32 columns, and suppose some project involves
5050matrices AandB. Describe the commands or oper-
ations of your matrix program that accomplish the following
tasks.
a.Compute ACB.
b.Compute AB.
c.Solve AxDbfor some vector binR50, assuming that
Acan be partitioned into a 22block matrix
Aij
,
withA11an invertible 2020matrix, A22an invertible
3030matrix, and A12a zero matrix. [ Hint: Describe
appropriate smaller systems to solve, without using any
matrix inverses.]
SOLUTIONS TO PRACTICE PROBLEMS
1.IfI 0
A I
is invertible, its inverse has the formW X
Y Z
. Verify that
I 0
A IW X
Y Z
DW X
AWCY AX CZ
SoW,X,Y, andZmust satisfy WDI,XD0,AWCYD0, andAXCZDI.
It follows that YD  AandZDI. Hence
I 0
A II 0
 A I
DI 0
0 I
The product in the reverse order is also the identity, so the block matrix is invert-
ible, and its inverse isI 0
 A I
. (You could also appeal to the Invertible Matrix
Theorem.)
2.XTXD"
XT
1
XT
2#h
X1X2i
D"
XT
1X1XT
1X2
XT
2X1XT
2X2#
. The partitions of XTandXare
automatically conformable for block multiplication because the columns of XTare
the rows of X. This partition of XTXis used in several computer algorithms for
matrix computations.
2.5 MATRIX FACTORIZATIONS
Afactorization of a matrix Ais an equation that expresses Aas a product of two or more
matrices. Whereas matrix multiplication involves a synthesis of data (combining the
effects of two or more linear transformations into a single matrix), matrix factorization
is an analysis of data. In the language of computer science, the expression of Aas a
product amounts to a preprocessing of the data in A, organizing that data into two or
more parts whose structures are more useful in some way, perhaps more accessible for
computation.
SECOND REVISED PAGES


--- Page 143 ---
126 CHAPTER 2 Matrix Algebra
Matrix factorizations and, later, factorizations of linear transformations will appear
at a number of key points throughout the text. This section focuses on a factorization
that lies at the heart of several important computer programs widely used in applica-
tions, such as the airÔ¨Çow problem described in the chapter introduction. Several other
factorizations, to be studied later, are introduced in the exercises.
The LU Factorization
The LU factorization, described below, is motivated by the fairly common industrial
and business problem of solving a sequence of equations, all with the same coefÔ¨Åcient
matrix:
AxDb1; A xDb2; : : : ; A xDbp (1)
See Exercise 32, for example. Also see Section 5.8, where the inverse power method
is used to estimate eigenvalues of a matrix by solving equations like those in sequence
(1), one at a time.
When Ais invertible, one could compute A 1and then compute A 1b1,A 1b2,
and so on. However, it is more efÔ¨Åcient to solve the Ô¨Årst equation in sequence (1) by
row reduction and obtain an LU factorization of Aat the same time. Thereafter, the
remaining equations in sequence (1) are solved with the LU factorization.
At Ô¨Årst, assume that Ais anmnmatrix that can be row reduced to echelon form,
without row interchanges . (Later, we will treat the general case.) Then Acan be written
in the form ADLU, where Lis an mmlower triangular matrix with 1‚Äôs on the
diagonal and Uis an mnechelon form of A. For instance, see Figure 1. Such a
factorization is called an LU factorization ofA. The matrix Lis invertible and is called
aunitlower triangular matrix.
A = 
LU1
***0
1**0
01*0
001
000*
0
0*
*00*
**0*
*
0
FIGURE 1 An LU factorization.
Before studying how to construct LandU, we should look at why they are so
useful. When ADLU, the equation AxDbcan be written as L.U x/Db. Writing y
forUx, we can Ô¨Ånd xby solving the pair of equations
LyDb
UxDy(2)
First solve LyDbfory;and then solve UxDyforx. See Figure 2. Each equation is
easy to solve because LandUare triangular.
xMultiplication
by A
b
Multiplication
by LMultiplication
by Uy
FIGURE 2 Factorization of the mapping x7!Ax.
SECOND REVISED PAGES


--- Page 144 ---
2.5 Matrix Factorizations 127
EXAMPLE 1 It can be veriÔ¨Åed that
AD2
6643 7 2 2
 3 5 1 0
6 4 0  5
 9 5  5 123
775D2
6641 0 0 0
 1 1 0 0
2 5 1 0
 3 8 3 13
7752
6643 7 2 2
0 2 1 2
0 0  1 1
0 0 0  13
775DLU
Use this LU factorization of Ato solve AxDb, where bD2
664 9
5
7
113
775.
SOLUTION The solution of LyDbneeds only 6 multiplications and 6 additions, be-
cause the arithmetic takes place only in column 5. (The zeros below each pivot in Lare
created automatically by the choice of row operations.)
L b
D2
6641 0 0 0  9
 1 1 0 0 5
2 5 1 0 7
 3 8 3 1 113
7752
6641 0 0 0  9
0 1 0 0  4
0 0 1 0 5
0 0 0 1 13
775DI y
Then, for UxDy, the ‚Äúbackward‚Äù phase of row reduction requires 4 divisions, 6 mul-
tiplications, and 6 additions. (For instance, creating the zeros in column 4 of ¬åUy¬ç
requires 1 division in row 4 and 3 multiplication‚Äìaddition pairs to add multiples of row 4
to the rows above.)
U y
D2
6643 7 2 2  9
0 2 1 2  4
0 0  1 1 5
0 0 0  1 13
7752
6641 0 0 0 3
0 1 0 0 4
0 0 1 0  6
0 0 0 1  13
775;xD2
6643
4
 6
 13
775
To Ô¨Ånd xrequires 28 arithmetic operations, or ‚ÄúÔ¨Çops‚Äù (Ô¨Çoating point operations),
excluding the cost of Ô¨Ånding LandU. In contrast, row reduction of ¬åAb¬çto¬åIx¬ç
takes 62 operations.
The computational efÔ¨Åciency of the LU factorization depends on knowing LandU.
The next algorithm shows that the row reduction of Ato an echelon form Uamounts to
an LU factorization because it produces Lwith essentially no extra work. After the Ô¨Årst
row reduction, LandUare available for solving additional equations whose coefÔ¨Åcient
matrix is A.
An LU Factorization Algorithm
Suppose Acan be reduced to an echelon form Uusing only row replacements that add a
multiple of one row to another row below it . In this case, there exist unit lower triangular
elementary matrices E1; : : : ; E psuch that
EpE1ADU (3)
Then
AD.EpE1/ 1UDLU
where
LD.EpE1/ 1(4)
SECOND REVISED PAGES


--- Page 145 ---
128 CHAPTER 2 Matrix Algebra
It can be shown that products and inverses of unit lower triangular matrices are also unit
lower triangular. (For instance, see Exercise 19.) Thus Lis unit lower triangular.
Note that the row operations in equation (3), which reduce AtoU, also reduce
theLin equation (4) to I, because EpE1LD.EpE1/.EpE1/ 1DI. This
observation is the key to constructing L.
ALGORITHM FOR AN LU FACTORIZATION
1.Reduce Ato an echelon form Uby a sequence of row replacement operations,
if possible.
2.Place entries in Lsuch that the same sequence of row operations reduces L
toI.
Step 1 is not always possible, but when it is, the argument above shows that an LU
factorization exists. Example 2 will show how to implement step 2. By construction, L
will satisfy
.EpE1/LDI
using the same E1; : : : ; E pas in equation (3). Thus Lwill be invertible, by the Invertible
Matrix Theorem, with .EpE1/DL 1. From (3), L 1ADU, and ADLU. So
step 2 will produce an acceptable L.
EXAMPLE 2 Find an LU factorization of
AD2
6642 4  1 5  2
 4 5 3  8 1
2 5 4 1 8
 6 0 7  3 13
775
SOLUTION Since Ahas four rows, Lshould be 44. The Ô¨Årst column of Lis the Ô¨Årst
column of Adivided by the top pivot entry:
LD2
6641 0 0 0
 2 1 0 0
1 1 0
 3 13
775
Compare the Ô¨Årst columns of AandL.The row operations that create zeros in the
Ô¨Årst column of Awill also create zeros in the Ô¨Årst column of L. To make this same
correspondence of row operations on Ahold for the rest of L, watch a row reduction
ofAto an echelon form U. That is, highlight the entries in each matrix that are used to
determine the sequence of row operations that transform AintoU. [See the highlighted
entries in equation (5).]
AD2
6642 4  1 5  2
 4 5 3  8 1
2 5 4 1 8
 6 0 7  3 13
7752
6642 4  1 5  2
03 1 2  3
0 9 3 4 10
0 12 4 12  53
775DA1 (5)
A2D2
6642 4  1 5  2
0 3 1 2  3
0 0 0 2 1
0 0 0 4 73
7752
6642 4  1 5  2
0 3 1 2  3
0 0 0 2 1
0 0 0 0 53
775DU
SECOND REVISED PAGES


--- Page 146 ---
2.5 Matrix Factorizations 129
The highlighted entries on page 128 determine the row reduction of AtoU. At each
pivot column, divide the highlighted entries by the pivot and place the result into L:
2
6642
 4
2
 63
7752
43
 9
123
52
4
5
2325
# ###2
6641
 2 1
1 3 1
 3 4 2 13
775, and LD2
6641 0 0 0
 2 1 0 0
1 3 1 0
 3 4 2 13
775
An easy calculation veriÔ¨Åes that this LandUsatisfy LUDA.
In practical work, row interchanges are nearly always needed, because partial piv-
oting is used for high accuracy. (Recall that this procedure selects, among the possible
choices for a pivot, an entry in the column having the largest absolute value.) To handle
row interchanges, the LU factorization above can be modiÔ¨Åed easily to produce an L
that is permuted lower triangular , in the sense that a rearrangement (called a permu-
tation) of the rows of Lcan make L .unit/lower triangular. The resulting permuted
LU factorization solves AxDbin the same way as before, except that the reduction of
¬åLb¬çto¬åIy¬çfollows the order of the pivots in Lfrom left to right, starting with
the pivot in the Ô¨Årst column. A reference to an ‚ÄúLU factorization‚Äù usually includes the
possibility that Lmight be permuted lower triangular. For details, see the Study Guide .
SGPermuted LU
Factorizations 2‚Äì23
N U M E R I C A L N O T E S
The following operation counts apply to an nndense matrix A(with most
entries nonzero) for nmoderately large, say, n30.1
1.Computing an LU factorization of Atakes about 2n3=3Ô¨Çops (about the same
as row reducing ¬åAb¬ç/, whereas Ô¨Ånding A 1requires about 2n3Ô¨Çops.
2.Solving LyDbandUxDyrequires about 2n2Ô¨Çops, because any nn
triangular system can be solved in about n2Ô¨Çops.
3.Multiplication of bbyA 1also requires about 2n2Ô¨Çops, but the result may
not be as accurate as that obtained from LandU(because of roundoff error
when computing both A 1andA 1b/.
4.IfAis sparse (with mostly zero entries), then LandUmay be sparse, too,
whereas A 1is likely to be dense. In this case, a solution of AxDbwith an
LU factorization is much faster than using A 1. See Exercise 31.
WEB
A Matrix Factorization in Electrical Engineering
Matrix factorization is intimately related to the problem of constructing an electrical
network with speciÔ¨Åed properties. The following discussion gives just a glimpse of the
connection between factorization and circuit design.
1See Section 3.8 in Applied Linear Algebra , 3rd ed., by Ben Noble and James W. Daniel (Englewood Cliffs,
NJ: Prentice-Hall, 1988). Recall that for our purposes, a Ô¨ÇopisC, ,, or.
SECOND REVISED PAGES


--- Page 147 ---
130 CHAPTER 2 Matrix Algebra
Suppose the box in Figure 3 represents some sort of electric circuit, with an input
and output. Record the input voltage and current byv1
i1
(with voltage vin volts and
current iin amps), and record the output voltage and current byv2
i2
. Frequently, the
transformationv1
i1
7!v2
i2
is linear. That is, there is a matrix A, called the transfer
matrix , such that v2
i2
DAv1
i1
i1i2
electric
circuitinput
terminalsoutput
terminalsv1v2
FIGURE 3 A circuit with input and output
terminals.
Figure 4 shows a ladder network , where two circuits (there could be more) are
connected in series, so that the output of one circuit becomes the input of the next circuit.
The left circuit in Figure 4 is called a series circuit , with resistance R1(in ohms).
i1
R1v1i2i2
v2 R2i3
v3
A series circuit A shunt circuit
FIGURE 4 A ladder network.
The right circuit in Figure 4 is a shunt circuit , with resistance R2. Using Ohm‚Äôs law and
Kirchhoff‚Äôs laws, one can show that the transfer matrices of the series and shunt circuits,
respectively, are1 R1
0 1
Transfer matrix
of series circuitand1 0
 1=R 21
Transfer matrix
of shunt circuit
EXAMPLE 3
a.Compute the transfer matrix of the ladder network in Figure 4.
b.Design a ladder network whose transfer matrix is1 8
 :5 5
.
SOLUTION
a.LetA1andA2be the transfer matrices of the series and shunt circuits, respectively.
Then an input vector xis transformed Ô¨Årst into A1xand then into A2.A1x/. The series
connection of the circuits corresponds to composition of linear transformations, and
the transfer matrix of the ladder network is (note the order)
A2A1D1 0
 1=R 21 1 R1
0 1
D1  R1
 1=R 21CR1=R2
(6)
SECOND REVISED PAGES


--- Page 148 ---
2.5 Matrix Factorizations 131
b.To factor the matrix1 8
 :5 5
into the product of transfer matrices, as in equa-
tion (6), look for R1andR2in Figure 4 to satisfy
1  R1
 1=R 21CR1=R2
D1 8
 :5 5
From the .1; 2/ -entries, R1D8ohms, and from the .2; 1/ -entries, 1=R 2D:5ohm
andR2D1=:5D2ohms. With these values, the network in Figure 4 has the desired
transfer matrix.
A network transfer matrix summarizes the input‚Äìoutput behavior (the design spec-
iÔ¨Åcations) of the network without reference to the interior circuits. To physically build
a network with speciÔ¨Åed properties, an engineer Ô¨Årst determines if such a network
can be constructed (or realized ). Then the engineer tries to factor the transfer matrix
into matrices corresponding to smaller circuits that perhaps are already manufactured
and ready for assembly. In the common case of alternating current, the entries in the
transfer matrix are usually rational complex-valued functions. (See Exercises 19 and 20
in Section 2.4 and Example 2 in Section 3.3.) A standard problem is to Ô¨Ånd a minimal
realization that uses the smallest number of electrical components.
PRACTICE PROBLEM
Find an LU factorization of AD2
666642 4 2 3
6 9 5 8
2 7 3 9
4 2 2 1
 6 3 3 43
77775. [Note: It will turn out that A
has only three pivot columns, so the method of Example 2 will produce only the Ô¨Årst
three columns of L. The remaining two columns of Lcome from I5.]
2.5 EXERCISES
In Exercises 1‚Äì6, solve the equation AxDbby using the LU
factorization given for A. In Exercises l and 2, also solve AxDb
by ordinary row reduction.
1.AD2
43 7 2
 3 5 1
6 4 03
5;bD2
4 7
5
23
5
AD2
41 0 0
 1 1 0
2 5 13
52
43 7 2
0 2 1
0 0  13
5
2.AD2
44 3  5
 4 5 7
8 6  83
5,bD2
42
 4
63
5
AD2
41 0 0
 1 1 0
2 0 13
52
44 3  5
0 2 2
0 0 23
5
3.AD2
42 1 2
 6 0  2
8 1 53
5,bD2
41
0
43
5AD2
41 0 0
 3 1 0
4 1 13
52
42 1 2
0 3 4
0 0 13
5
4.AD2
42 2 4
1 3 1
3 7 53
5,bD2
40
 5
73
5
AD2
41 0 0
1=2 1 0
3=2  5 13
52
42 2 4
0 2 1
0 0  63
5
5.AD2
6641 2 4 3
2 7 7 6
 1 2 6 4
 4 1 9 83
775,bD2
6641
7
0
33
775
AD2
6641 0 0 0
2 1 0 0
 1 0 1 0
 4 3  5 13
7752
6641 2 4 3
0 3 1 0
0 0 2 1
0 0 0 13
775
SECOND REVISED PAGES


--- Page 149 ---
132 CHAPTER 2 Matrix Algebra
6.AD2
6641 3 4 0
 3 6 7 2
3 3 0  4
 5 3 2 93
775,bD2
6641
 2
 1
23
775
AD2
6641 0 0 0
 3 1 0 0
3 2 1 0
 5 4  1 13
7752
6641 3 4 0
0 3 5 2
0 0  2 0
0 0 0 13
775
Find an LU factorization of the matrices in Exercises 7‚Äì16 (with
Lunit lower triangular). Note that MATLAB will usually produce
a permuted LU factorization because it uses partial pivoting for
numerical accuracy.
7.2 5
 3 4
8.6 9
4 5
9.2
43 1 2
 3 2 10
9 5 63
5 10.2
4 5 3 4
10 8 9
15 1 23
5
11.2
43 6 3
6 7 2
 1 7 03
5 12.2
42 4 2
1 5  4
 6 2 43
5
13.2
6641 3  5 3
 1 5 8 4
4 2  5 7
 2 4 7 53
77514.2
6641 4  1 5
3 7  2 9
 2 3 1  4
 1 6  1 73
775
15.2
42 4 4  2
6 9 7  3
 1 4 8 03
5 16.2
666642 6 6
 4 5  7
3 5  1
 6 4  8
8 3 93
77775
17. When Ais invertible, MATLAB Ô¨Ånds A 1by factoring AD
LU (where Lmay be permuted lower triangular), inverting
LandU, and then computing U 1L 1:Use this method to
compute the inverse of Ain Exercise 2. (Apply the algorithm
of Section 2.2 to Land to U.)
18. FindA 1as in Exercise 17, using Afrom Exercise 3.
19. LetAbe a lower triangular nnmatrix with nonzero entries
on the diagonal. Show that Ais invertible and A 1is lower
triangular. [ Hint: Explain why Acan be changed into Iusing
only row replacements and scaling. (Where are the pivots?)
Also, explain why the row operations that reduce AtoI
change Iinto a lower triangular matrix.]
20. LetADLUbe an LU factorization. Explain why Acan be
row reduced to Uusing only replacement operations. (This
fact is the converse of what was proved in the text.)
21. Suppose ADBC; where Bis invertible. Show that any
sequence of row operations that reduces BtoIalso reduces
Ato C. The converse is not true, since the zero matrix may
be factored as 0DB0:
Exercises 22‚Äì26 provide a glimpse of some widely used matrix
factorizations, some of which are discussed later in the text.22. (Reduced LU Factorization ) With Aas in the Practice Prob-
lem, Ô¨Ånd a 53matrix Band a 34matrix Csuch that
ADBC: Generalize this idea to the case where Aismn;
ADLU; andUhas only three nonzero rows.
23. (Rank Factorization ) Suppose an mnmatrix Aadmits a
factorization ADCDwhere Cism4andDis4n:
a.Show that Ais the sum of four outer products. (See
Section 2.4.)
b.LetmD400 andnD100: Explain why a computer
programmer might prefer to store the data from Ain the
form of two matrices CandD.
24. (QR Factorization ) Suppose ADQR; where QandRare
nn;Ris invertible and upper triangular, and Qhas the
property that QTQDI:Show that for each binRn, the
equation AxDbhas a unique solution. What computations
with QandRwill produce the solution?
WEB
25. (Singular Value Decomposition ) Suppose ADUDVT;
where UandVarennmatrices with the property that
UTUDIandVTVDI;and where Dis a diagonal matrix
with positive numbers 1; : : : ;  non the diagonal. Show that
Ais invertible, and Ô¨Ånd a formula for A 1.
26. (Spectral Factorization ) Suppose a 33matrix A admits a
factorization as ADPDP 1;where Pis some invertible
33matrix and Dis the diagonal matrix
DD2
41 0 0
0 1=2 0
0 0 1=33
5
Show that this factorization is useful when computing high
powers of A. Find fairly simple formulas for A2; A3;andAk
(ka positive integer), using Pand the entries in D.
27. Design two different ladder networks that each output 9 volts
and 4 amps when the input is 12 volts and 6 amps.
28. Show that if three shunt circuits (with resistances R1; R2; R3)
are connected in series, the resulting network has the same
transfer matrix as a single shunt circuit. Find a formula for
the resistance in that circuit.
29. a.Compute the transfer matrix of the network in the Ô¨Ågure.
b.LetAD4=3  12
 1=4 3
. Design a ladder network
whose transfer matrix is Aby Ô¨Ånding a suitable matrix
factorization of A.
i1i2i2
R1v1v2R2i3i3i4
v4 R3v3
SECOND REVISED PAGES


--- Page 150 ---
2.5 Matrix Factorizations 133
30. Find a different factorization of the Ain Exercise 29, and
thereby design a different ladder network whose transfer
matrix is A.
31. [M] The solution to the steady-state heat Ô¨Çow problem for
the plate in the Ô¨Ågure is approximated by the solution to the
equation AxDb;where bD.5;15;0;10;0;10;20;30/and
AD2
666666666644 1 1
 1 4 0  1
 1 0 4  1 1
 1 1 4 0  1
 1 0 4  1 1
 1 1 4 0  1
 1 0 4  1
 1 1 43
77777777775
WEB
0¬∞
5¬∞
5¬∞20¬∞
20¬∞0¬∞ 0¬∞ 0¬∞
10¬∞10¬∞ 10¬∞ 10¬∞1357
2468
(Refer to Exercise 33 of Section 1.1.) The missing entries in
Aare zeros. The nonzero entries of Alie within a band along
the main diagonal. Such band matrices occur in a variety of
applications and often are extremely large (with thousands of
rows and columns but relatively narrow bands).
a.Use the method of Example 2 to construct an LU factor-
ization of A, and note that both factors are band matrices
(with two nonzero diagonals below or above the main
diagonal). Compute LU Ato check your work.
b.Use the LU factorization to solve AxDb:c.Obtain A 1and note that A 1is a dense matrix with no
band structure. When Ais large, LandUcan be stored in
much less space than A 1:This fact is another reason for
preferring the LU factorization of AtoA 1itself.
32. [M] The band matrix Ashown below can be used to estimate
the unsteady conduction of heat in a rod when the tempera-
tures at points p1; : : : ; p 5on the rod change with time.2
ŒîxŒîx
p1p2p3p4p5
The constant Cin the matrix depends on the physical nature
of the rod, the distance ¬Åxbetween the points on the rod,
and the length of time ¬Åtbetween successive temperature
measurements. Suppose that for kD0;1;2; : : : ; a vector tk
inR5lists the temperatures at time k¬Åt. If the two ends of the
rod are maintained at 0, then the temperature vectors satisfy
the equation AtkC1Dtk.kD0;1; : : : /; where
AD2
66664.1C2C /  C
 C .1 C2C /  C
 C .1 C2C /  C
 C .1 C2C /  C
 C .1 C2C /3
77775
a.Find the LU factorization of Awhen CD1:A matrix
such as Awith three nonzero diagonals is called a tridiag-
onal matrix . The LandUfactors are bidiagonal matrices .
b.Suppose CD1andt0D.10;12;12;12;10/:Use the
LU factorization of Ato Ô¨Ånd the temperature distributions
t1;t2;t3, and t4.
2See Biswa N. Datta, Numerical Linear Algebra and Applications (PaciÔ¨Åc
Grove, CA: Brooks/Cole, 1994), pp. 200‚Äì201.
SOLUTION TO PRACTICE PROBLEM
AD2
666642 4 2 3
6 9 5 8
2 7 3 9
4 2 2 1
 6 3 3 43
777752
666642 4 2 3
03 1  1
0 3 1 6
0 6 2  7
0 9 3 133
77775
2
666642 4 2 3
0 3 1  1
0 0 0 5
0 0 0  5
0 0 0 103
777752
666642 4 2 3
0 3 1  1
0 0 0 5
0 0 0 0
0 0 0 03
77775DU
Divide the entries in each highlighted column by the pivot at the top. The resulting
columns form the Ô¨Årst three columns in the lower half of L. This sufÔ¨Åces to make row
reduction of LtoIcorrespond to reduction of AtoU. Use the last two columns of I5
SECOND REVISED PAGES


--- Page 151 ---
134 CHAPTER 2 Matrix Algebra
to make Lunit lower triangular.
2
666642
6
2
4
 63
777752
6643
 3
6
 93
7752
45
 5
103
5
235
# # #2
666641
3 1
1 1 1 
2 2  1
 3 3 23
77775,LD2
666641 0 0 0 0
3 1 0 0 0
1 1 1 0 0
2 2  1 1 0
 3 3 2 0 13
77775
2.6 THE LEONTIEF INPUT OUTPUT MODEL
Linear algebra played an essential role in the Nobel prize‚Äìwinning work of Wassily
WEB
Leontief, as mentioned at the beginning of Chapter 1. The economic model described
in this section is the basis for more elaborate models used in many parts of the world.
Suppose a nation‚Äôs economy is divided into nsectors that produce goods or services,
and let xbe a production vector inRnthat lists the output of each sector for one
year. Also, suppose another part of the economy (called the open sector ) does not
produce goods or services but only consumes them, and let dbe aÔ¨Ånal demand vector
(orbill of Ô¨Ånal demands) that lists the values of the goods and services demanded
from the various sectors by the nonproductive part of the economy. The vector dcan
represent consumer demand, government consumption, surplus production, exports, or
other external demands.
As the various sectors produce goods to meet consumer demand, the producers
themselves create additional intermediate demand for goods they need as inputs for
their own production. The interrelations between the sectors are very complex, and the
connection between the Ô¨Ånal demand and the production is unclear. Leontief asked if
there is a production level xsuch that the amounts produced (or ‚Äúsupplied‚Äù) will exactly
balance the total demand for that production, so that
8
<
:amount
produced
x9
=
;Dintermediate
demand
C8
<
:Ô¨Ånal
demand
d9
=
;(1)
The basic assumption of Leontief‚Äôs input‚Äìoutput model is that for each sector, there
is aunit consumption vector inRnthat lists the inputs needed per unit of output of
the sector. All input and output units are measured in millions of dollars, rather than in
quantities such as tons or bushels. (Prices of goods and services are held constant.)
As a simple example, suppose the economy consists of three sectors‚Äîmanufac-
turing, agriculture, and services‚Äîwith unit consumption vectors c1,c2, and c3, as shown
in the table that follows.
SECOND REVISED PAGES


--- Page 152 ---
2.6 The Leontief Input Output Model 135
Inputs Consumed per Unit of Output
Purchased from: Manufacturing Agriculture Services
Manufacturing .50 .40 .20
Agriculture .20 .30 .10
Services .10 .10 .30
" " "
c1 c2 c3
EXAMPLE 1 What amounts will be consumed by the manufacturing sector if it
decides to produce 100 units?
SOLUTION Compute
100c1D1002
4:50
:20
:103
5D2
450
20
103
5
To produce 100 units, manufacturing will order (i.e., ‚Äúdemand‚Äù) and consume 50 units
from other parts of the manufacturing sector, 20 units from agriculture, and 10 units
from services.
If manufacturing decides to produce x1units of output, then x1c1represents the
intermediate demands of manufacturing, because the amounts in x1c1will be consumed
in the process of creating the x1units of output. Likewise, if x2andx3denote the planned
outputs of the agriculture and services sectors, x2c2andx3c3list their corresponding
intermediate demands. The total intermediate demand from all three sectors is given by
fintermediate demand g Dx1c1Cx2c2Cx3c3
DCx (2)
where Cis the consumption matrix ¬åc1c2c3¬ç, namely,
CD2
4:50 :40 :20
:20 :30 :10
:10 :10 :303
5 (3)
Equations (1) and (2) yield Leontief‚Äôs model.
THE LEONTIEF INPUT--OUTPUT MODEL, OR PRODUCTION EQUATION
xD CxC d
Amount Intermediate Final
produced demand demand(4)
Equation (4) may also be written as Ix CxDd, or
.I C /xDd (5)
EXAMPLE 2 Consider the economy whose consumption matrix is given by (3).
Suppose the Ô¨Ånal demand is 50 units for manufacturing, 30 units for agriculture, and
20 units for services. Find the production level xthat will satisfy this demand.
SECOND REVISED PAGES


--- Page 153 ---
136 CHAPTER 2 Matrix Algebra
SOLUTION The coefÔ¨Åcient matrix in (5) is
I CD2
41 0 0
0 1 0
0 0 13
5 2
4:5 :4 :2
:2 :3 :1
:1 :1 :33
5D2
4:5 :4 :2
 :2 :7  :1
 :1 :1 :73
5
To solve (5), row reduce the augmented matrix
2
4:5 :4 :2 50
 :2 :7  :1 30
 :1 :1 :7 203
52
45 4 2 500
 2 7  1 300
 1 1 7 2003
5  2
41 0 0 226
0 1 0 119
0 0 1 783
5
The last column is rounded to the nearest whole unit. Manufacturing must produce
approximately 226 units, agriculture 119 units, and services only 78 units.
If the matrix I Cis invertible, then we can apply Theorem 5 in Section 2.2, with
Areplaced by .I C /, and from the equation .I C /xDdobtain xD.I C / 1d.
The theorem below shows that in most practical cases, I Cisinvertible and the
production vector xis economically feasible, in the sense that the entries in xare non-
negative.
In the theorem, the term column sum denotes the sum of the entries in a column
of a matrix. Under ordinary circumstances, the column sums of a consumption matrix
are less than 1 because a sector should require less than one unit‚Äôs worth of inputs to
produce one unit of output.
T H E O R E M 1 1 LetCbe the consumption matrix for an economy, and let dbe the Ô¨Ånal demand.
IfCanddhave nonnegative entries and if each column sum of Cis less than 1,
then.I C / 1exists and the production vector
xD.I C / 1d
has nonnegative entries and is the unique solution of
xDCxCd
The following discussion will suggest why the theorem is true and will lead to a
new way to compute .I C / 1.
A Formula for (I ‚Äì C) ‚Äì1
Imagine that the demand represented by dis presented to the various industries at the
beginning of the year, and the industries respond by setting their production levels at
xDd, which will exactly meet the Ô¨Ånal demand. As the industries prepare to produce d,
they send out orders for their raw materials and other inputs. This creates an intermediate
demand of Cdfor inputs.
To meet the additional demand of Cd, the industries will need as additional inputs
the amounts in C.Cd/DC2d. Of course, this creates a second round of intermediate
demand, and when the industries decide to produce even more to meet this new demand,
they create a third round of demand, namely, C.C2d/DC3d. And so it goes.
Theoretically, this process could continue indeÔ¨Ånitely, although in real life it would
not take place in such a rigid sequence of events. We can diagram this hypothetical
situation as follows:
SECOND REVISED PAGES


--- Page 154 ---
2.6 The Leontief Input Output Model 137
Demand That Inputs Needed to
Must Be Met Meet This Demand
Final demand d Cd
Intermediate demand
1st round Cd C.C d/DC2d
2nd round C2d C.C2d/DC3d
3rd round C3d C.C3d/DC4d
::::::
The production level xthat will meet all of this demand is
xDdCCdCC2dCC3dC 
D.ICCCC2CC3C  /d (6)
To make sense of equation (6), consider the following algebraic identity:
.I C /.ICCCC2C  C Cm/DI CmC1(7)
It can be shown that if the column sums in Care all strictly less than 1, then I Cis in-
vertible, Cmapproaches the zero matrix as mgets arbitrarily large, and I CmC1!I.
(This fact is analogous to the fact that if a positive number tis less than 1, then tm!0
asmincreases.) Using equation (7), write
.I C / 1ICCCC2CC3C  C Cm
when the column sums of Care less than 1.(8)
The approximation in (8) means that the right side can be made as close to .I C / 1
as desired by taking msufÔ¨Åciently large.
In actual input‚Äìoutput models, powers of the consumption matrix approach the zero
matrix rather quickly. So (8) really provides a practical way to compute .I C / 1.
Likewise, for any d, the vectors Cmdapproach the zero vector quickly, and (6) is a
practical way to solve .I C /xDd. If the entries in Canddare nonnegative, then (6)
shows that the entries in xare nonnegative, too.
The Economic Importance of Entries in (I ‚Äì C) ‚Äì1
The entries in .I C / 1are signiÔ¨Åcant because they can be used to predict how the
production xwill have to change when the Ô¨Ånal demand dchanges. In fact, the entries
in column jof.I C / 1are the increased amounts the various sectors will have to
produce in order to satisfy an increase of 1 unit in the Ô¨Ånal demand for output from
sector j. See Exercise 8.
N U M E R I C A L N O T E
In any applied problem (not just in economics), an equation AxDbcan always be
written as .I C /xDb, with CDI A. If the system is large and sparse (with
mostly zero entries), it can happen that the column sums of the absolute values in
Care less than 1. In this case, Cm!0. IfCmapproaches zero quickly enough,
(6) and (8) will provide practical formulas for solving AxDband Ô¨Ånding A 1.
SECOND REVISED PAGES


--- Page 155 ---
138 CHAPTER 2 Matrix Algebra
PRACTICE PROBLEM
Suppose an economy has two sectors: goods and services. One unit of output from goods
requires inputs of .2 unit from goods and .5 unit from services. One unit of output from
services requires inputs of .4 unit from goods and .3 unit from services. There is a Ô¨Ånal
demand of 20 units of goods and 30 units of services. Set up the Leontief input‚Äìoutput
model for this situation.
2.6 EXERCISES
Agriculture Manufacturing Services
Open Sector
Exercises 1‚Äì4 refer to an economy that is divided into three
sectors‚Äîmanufacturing, agriculture, and services. For each unit
of output, manufacturing requires .10 unit from other companies
in that sector, .30 unit from agriculture, and .30 unit from services.
For each unit of output, agriculture uses .20 unit of its own output,
.60 unit from manufacturing, and .10 unit from services. For each
unit of output, the services sector consumes .10 unit from services,
.60 unit from manufacturing, but no agricultural products.
1.Construct the consumption matrix for this economy, and de-
termine what intermediate demands are created if agriculture
plans to produce 100 units.
2.Determine the production levels needed to satisfy a Ô¨Ånal
demand of 18 units for agriculture, with no Ô¨Ånal demand for
the other sectors. (Do not compute an inverse matrix.)
3.Determine the production levels needed to satisfy a Ô¨Ånal
demand of 18 units for manufacturing, with no Ô¨Ånal demand
for the other sectors. (Do not compute an inverse matrix.)4.Determine the production levels needed to satisfy a Ô¨Ånal de-
mand of 18 units for manufacturing, 18 units for agriculture,
and 0 units for services.
5.Consider the production model xDCxCdfor an economy
with two sectors, where
CD:0 :5
:6 :2
; dD50
30
Use an inverse matrix to determine the production level
necessary to satisfy the Ô¨Ånal demand.
6.Repeat Exercise 5 with CD:1 :6
:5 :2
, and dD18
11
.
7.LetCanddbe as in Exercise 5.
a.Determine the production level necessary to satisfy a Ô¨Ånal
demand for 1 unit of output from sector 1.
SECOND REVISED PAGES


--- Page 156 ---
2.6 The Leontief Input Output Model 139
b.Use an inverse matrix to determine the production level
necessary to satisfy a Ô¨Ånal demand of51
30
:
c.Use the fact that51
30
D50
30
C1
0
to explain how
and why the answers to parts (a) and (b) and to Exercise
5 are related.
8.LetCbe an nnconsumption matrix whose column sums
are less than 1. Let xbe the production vector that satisÔ¨Åes
a Ô¨Ånal demand d, and let ¬Åxbe a production vector that
satisÔ¨Åes a different Ô¨Ånal demand ¬Åd:
a.Show that if the Ô¨Ånal demand changes from dtodC¬Åd;
then the new production level must be xC¬Åx:Thus ¬Åx
gives the amounts by which production must change in
order to accommodate the change ¬Ådin demand.
b.Let¬Ådbe the vector in Rnwith 1 as the Ô¨Årst entry and
0‚Äôs elsewhere. Explain why the corresponding production
¬Åxis the Ô¨Årst column of .I C / 1:This shows that the
Ô¨Årst column of .I C / 1gives the amounts the various
sectors must produce to satisfy an increase of 1 unit in the
Ô¨Ånal demand for output from sector 1.
9.Solve the Leontief production equation for an economy with
three sectors, given that
CD2
4:2 :2 :0
:3 :1 :3
:1 :0 :23
5and dD2
440
60
803
5
10. The consumption matrix Cfor the U.S. economy in 1972
has the property that every entry in the matrix .I C / 1is
nonzero (and positive).1What does that say about the effect
of raising the demand for the output of just one sector of the
economy?
11.The Leontief production equation, xDCxCd;is usually
accompanied by a dual price equation ,
pDCTpCv
where pis aprice vector whose entries list the price per unit
for each sector‚Äôs output, and vis avalue added vector whose
entries list the value added per unit of output. (Value added
includes wages, proÔ¨Åt, depreciation, etc.) An important fact
in economics is that the gross domestic product (GDP) can
be expressed in two ways:
fgross domestic product g=pTdDvTx
Verify the second equality. [ Hint: Compute pTxin two
ways.]
1Wassily W. Leontief, ‚ÄúThe World Economy of the Year 2000,‚Äù
ScientiÔ¨Åc American , September 1980, pp. 206‚Äì231.12. Let Cbe a consumption matrix such that Cm!0as
m! 1 ;and for mD1; 2; : : : ; letDmDICCC    C
Cm:Find a difference equation that relates DmandDmC1and
thereby obtain an iterative procedure for computing formula
(8) for .I C / 1:
13. [M] The consumption matrix Cbelow is based on input‚Äì
output data for the U.S. economy in 1958, with data for 81
sectors grouped into 7 larger sectors: (1) nonmetal household
and personal products, (2) Ô¨Ånal metal products (such as motor
vehicles), (3) basic metal products and mining, (4) basic
nonmetal products and agriculture, (5) energy, (6) services,
and (7) entertainment and miscellaneous products.2Find the
production levels needed to satisfy the Ô¨Ånal demand d. (Units
are in millions of dollars.)
2
666666664:1588 :0064 :0025 :0304 :0014 :0083 :1594
:0057 :2645 :0436 :0099 :0083 :0201 :3413
:0264 1506 :3557 :0139 :0142 :0070 :0236
:3299 :0565 :0495 :3636 :0204 :0483 :0649
:0089 :0081 :0333 :0295 :3412 :0237 :0020
:1190 :0901 :0996 :1260 :1722 :2368 :3369
:0063 :0126 :0196 :0098 :0064 :0132 :00123
777777775;
dD2
66666666474;000
56;000
10;500
25;000
17;500
196;000
5;0003
777777775
14. [M] The demand vector in Exercise 13 is reasonable for
1958 data, but Leontief‚Äôs discussion of the economy in the
reference cited there used a demand vector closer to 1964
data:
dD.99640; 75548; 14444; 33501; 23527; 263985; 6526/
Find the production levels needed to satisfy this demand.
15. [M] Use equation (6) to solve the problem in Exer-
cise 13. Set x.0/Dd;and for kD1; 2; : : : ; compute
x.k/DdCCx.k 1/:How many steps are needed to obtain
the answer in Exercise 13 to four signiÔ¨Åcant Ô¨Ågures?
2Wassily W. Leontief, ‚ÄúThe Structure of the U.S. Economy,‚Äù
ScientiÔ¨Åc American , April 1965, pp. 30‚Äì32.
SECOND REVISED PAGES


--- Page 157 ---
140 CHAPTER 2 Matrix Algebra
SOLUTION TO PRACTICE PROBLEM
The following data are given:
Inputs Needed per Unit of Output
Purchased from: Goods Services External Demand
Goods .2 .4 20
Services .5 .3 30
The Leontief input‚Äìoutput model is xDCxCd, where
CD:2 :4
:5 :3
; dD20
30
2.7 APPLICATIONS TO COMPUTER GRAPHICS
Computer graphics are images displayed or animated on a computer screen. Applica-
tions of computer graphics are widespread and growing rapidly. For instance, computer-
aided design (CAD) is an integral part of many engineering processes, such as the
aircraft design process described in the chapter introduction. The entertainment industry
has made the most spectacular use of computer graphics‚Äîfrom the special effects in
Amazing Spider-Man 2 to PlayStation 4 and Xbox One.
Most interactive computer software for business and industry makes use of com-
puter graphics in the screen displays and for other functions, such as graphical display
of data, desktop publishing, and slide production for commercial and educational pre-
sentations. Consequently, anyone studying a computer language invariably spends time
learning how to use at least two-dimensional (2D) graphics.
This section examines some of the basic mathematics used to manipulate and dis-
play graphical images such as a wire-frame model of an airplane. Such an image (or
picture) consists of a number of points, connecting lines or curves, and information
about how to Ô¨Åll in closed regions bounded by the lines and curves. Often, curved lines
are approximated by short straight-line segments, and a Ô¨Ågure is deÔ¨Åned mathematically
by a list of points.
Among the simplest 2D graphics symbols are letters used for labels on the screen.
Some letters are stored as wire-frame objects; others that have curved portions are stored
with additional mathematical formulas for the curves.
EXAMPLE 1 The capital letter N in Figure 1 is determined by eight points, or
vertices . The coordinates of the points can be stored in a data matrix, D.
8
3
12 465
7
FIGURE 1
Regular N:Vertex:
x-coordinate
y-coordinate1
02
:53
:54
65
66
5:57
5:58
0
0 0 6:42 0 8 8 1:58 8
DD
In addition to D, it is necessary to specify which vertices are connected by lines, but we
omit this detail.
The main reason graphical objects are described by collections of straight-line seg-
ments is that the standard transformations in computer graphics map line segments onto
other line segments. (For instance, see Exercise 27 in Section 1.8.) Once the vertices
SECOND REVISED PAGES


--- Page 158 ---
2.7 Applications to Computer Graphics 141
that describe an object have been transformed, their images can be connected with the
appropriate straight lines to produce the complete image of the original object.
EXAMPLE 2 Given AD1 :25
0 1
, describe the effect of the shear transforma-
tionx7!Axon the letter N in Example 1.
SOLUTION By deÔ¨Ånition of matrix multiplication, the columns of the product AD
contain the images of the vertices of the letter N.
ADD1
02
:53
2:1054
65
86
7:57
5:8958
2
0 0 6:420 0 8 8 1:580 8
The transformed vertices are plotted in Figure 2, along with connecting line segments
that correspond to those in the original Ô¨Ågure.
The italic Nin Figure 2 looks a bit too wide. To compensate, shrink the width by a
scale transformation that affects the x-coordinates of the points.
5
12 4736 8FIGURE 2
Slanted N:
EXAMPLE 3 Compute the matrix of the transformation that performs a shear trans-
formation, as in Example 2, and then scales all x-coordinates by a factor of .75.
SOLUTION The matrix that multiplies the x-coordinate of a point by .75 is
FIGURE 3
Composite transformation of N:SD:75 0
0 1
So the matrix of the composite transformation is
SAD:75 0
0 1 1 :25
0 1
D:75 :1875
0 1
The result of this composite transformation is shown in Figure 3.
The mathematics of computer graphics is intimately connected with matrix multi-
plication. Unfortunately, translating an object on a screen does not correspond directly
to matrix multiplication because translation is not a linear transformation. The standard
way to avoid this difÔ¨Åculty is to introduce what are called homogeneous coordinates .
Homogeneous Coordinates
Each point .x; y/ inR2can be identiÔ¨Åed with the point .x; y; 1/ on the plane in R3
that lies one unit above the xy-plane. We say that .x; y/ hashomogeneous coordinates
.x; y; 1/ . For instance, the point .0; 0/ has homogeneous coordinates .0; 0; 1/ . Homo-
geneous coordinates for points are not added or multiplied by scalars, but they can be
transformed via multiplication by 33matrices.
EXAMPLE 4 A translation of the form .x; y/7!.xCh; yCk/is written in ho-
mogeneous coordinates as .x; y; 1/ 7!.xCh; yCk; 1/ . This transformation can be
computed via matrix multiplication:
x2
x1
‚Äì 4 ‚Äì2 224
4Translation by4
3
.
2
41 0 h
0 1 k
0 0 13
52
4x
y
13
5D2
4xCh
yCk
13
5
SECOND REVISED PAGES


--- Page 159 ---
142 CHAPTER 2 Matrix Algebra
EXAMPLE 5 Any linear transformation on R2is represented with respect to homo-
After TranslatingAfter RotatingAfter ScalingOriginal Figure
geneous coordinates by a partitioned matrix of the formA 0
0 1
, where Ais a22
matrix. Typical examples are
2
4cos' sin' 0
sin' cos' 0
0 0 13
5;2
40 1 0
1 0 0
0 0 13
5;2
4s 0 0
0 t 0
0 0 13
5
Counterclockwise ReÔ¨Çection Scale xbys
rotation about the through yDx andybyt
origin, angle '
Composite Transformations
The movement of a Ô¨Ågure on a computer screen often requires two or more basic trans-
formations. The composition of such transformations corresponds to matrix multiplica-
tion when homogeneous coordinates are used.
EXAMPLE 6 Find the 33matrix that corresponds to the composite transforma-
tion of a scaling by .3, a rotation of 90about the origin, and Ô¨Ånally a translation that
adds. :5; 2/ to each point of a Ô¨Ågure.
SOLUTION If'D=2, then sin 'D1and cos 'D0. From Examples 4 and 5, we
have2
4x
y
13
5Scale            !2
4:3 0 0
0 :3 0
0 0 13
52
4x
y
13
5
Rotate            !2
40 1 0
1 0 0
0 0 13
52
4:3 0 0
0 :3 0
0 0 13
52
4x
y
13
5
Translate            !2
41 0  :5
0 1 2
0 0 13
52
40 1 0
1 0 0
0 0 13
52
4:3 0 0
0 :3 0
0 0 13
52
4x
y
13
5
The matrix for the composite transformation is
2
41 0  :5
0 1 2
0 0 13
52
40 1 0
1 0 0
0 0 13
52
4:3 0 0
0 :3 0
0 0 13
5
D2
40 1 :5
1 0 2
0 0 13
52
4:3 0 0
0 :3 0
0 0 13
5D2
40 :3 :5
:3 0 2
0 0 13
5
3D Computer Graphics
Some of the newest and most exciting work in computer graphics is connected with
molecular modeling. With 3D (three-dimensional) graphics, a biologist can examine a
simulated protein molecule and search for active sites that might accept a drug molecule.
The biologist can rotate and translate an experimental drug and attempt to attach it to the
protein. This ability to visualize potential chemical reactions is vital to modern drug and
cancer research. In fact, advances in drug design depend to some extent upon progress
SECOND REVISED PAGES


--- Page 160 ---
2.7 Applications to Computer Graphics 143
in the ability of computer graphics to construct realistic simulations of molecules and
their interactions.1
Current research in molecular modeling is focused on virtual reality , an environ-
ment in which a researcher can see and feelthe drug molecule slide into the protein. In
Figure 4, such tactile feedback is provided by a force-displaying remote manipulator.
FIGURE 4 Molecular modeling in virtual reality.
Another design for virtual reality involves a helmet and glove that detect head, hand, and
Ô¨Ånger movements. The helmet contains two tiny computer screens, one for each eye.
Making this virtual environment more realistic is a challenge to engineers, scientists,
and mathematicians. The mathematics we examine here barely opens the door to this
interesting Ô¨Åeld of research.
Homogeneous 3D Coordinates
By analogy with the 2D case, we say that .x; y; ¬¥; 1/ are homogeneous coordinates for
the point .x; y; ¬¥/ inR3. In general, .X; Y; Z; H/ arehomogeneous coordinates for
.x; y; ¬¥/ ifH¬§0and
xDX
H; y DY
H;and ¬¥DZ
H(1)
Each nonzero scalar multiple of .x; y; ¬¥; 1/ gives a set of homogeneous coordinates
for.x; y; ¬¥/ . For instance, both .10; 6; 14; 2/ and. 15; 9; 21; 3/are homogeneous
coordinates for .5; 3; 7/.
The next example illustrates the transformations used in molecular modeling to
move a drug into a protein molecule.
EXAMPLE 7 Give 44matrices for the following transformations:
a.Rotation about the y-axis through an angle of 30. (By convention, a positive angle
is the counterclockwise direction when looking toward the origin from the positive
half of the axis of rotation‚Äîin this case, the y-axis.)
b.Translation by the vector pD. 6; 4; 5/ .
SOLUTION
a.First, construct the 33matrix for the rotation. The vector e1rotates down toward
the negative ¬¥-axis, stopping at .cos30; 0; sin30/D.p
3=2; 0;  :5/. The vector
1Robert Pool, ‚ÄúComputing in Science,‚Äù Science 256, 3 April 1992, p. 45.
SECOND REVISED PAGES


--- Page 161 ---
144 CHAPTER 2 Matrix Algebra
e2on the y-axis does not move, but e3on the ¬¥-axis rotates down toward the positive
x-axis, stopping at .sin30; 0;cos30/D.:5; 0;p
3=2/ . See Figure 5. From Section
1.9, the standard matrix for this rotation is
z
e3
e1
xe2
y
FIGURE 52
4p
3=2 0 :5
0 1 0
 :5 0p
3=23
5
So the rotation matrix for homogeneous coordinates is
AD2
664p
3=2 0 :5 0
0 1 0 0
 :5 0p
3=2 0
0 0 0 13
775
b.We want .x; y; ¬¥; 1/ to map to .x 6; yC4; ¬¥C5; 1/. The matrix that does this is
2
6641 0 0  6
0 1 0 4
0 0 1 5
0 0 0 13
775
Perspective Projections
A three-dimensional object is represented on the two-dimensional computer screen by
projecting the object onto a viewing plane . (We ignore other important steps, such as
selecting the portion of the viewing plane to display on the screen.) For simplicity, let
thexy-plane represent the computer screen, and imagine that the eye of a viewer is
along the positive ¬¥-axis, at a point .0; 0; d/ . Aperspective projection maps each point
.x; y; ¬¥/ onto an image point .x; y; 0/so that the two points and the eye position,
called the center of projection , are on a line. See Figure 6(a).
(a) (b)(0, 0, d)
zzy
(x*, y*, 0)
x
x00
(x, y, z)
d ‚Äì zx*
FIGURE 6 Perspective projection of .x; y; ¬¥/ onto.x; y; 0/.
SECOND REVISED PAGES


--- Page 162 ---
2.7 Applications to Computer Graphics 145
The triangle in the x¬¥-plane in Figure 6(a) is redrawn in part (b) showing the lengths
of line segments. Similar triangles show that
x
dDx
d ¬¥and xDdx
d ¬¥Dx
1 ¬¥=d
Similarly,
yDy
1 ¬¥=d
Using homogeneous coordinates, we can represent the perspective projection by a ma-
trix, say, P. We want .x; y; ¬¥; 1/ to map intox
1 ¬¥=d;y
1 ¬¥=d; 0; 1
. Scaling these
coordinates by 1 ¬¥=d, we can also use .x; y; 0; 1  ¬¥=d/ as homogeneous coordinates
for the image. Now it is easy to display P. In fact,
P2
664x
y
¬¥
13
775D2
6641 0 0 0
0 1 0 0
0 0 0 0
0 0  1=d 13
7752
664x
y
¬¥
13
775D2
664x
y
0
1 ¬¥=d3
775
EXAMPLE 8 LetSbe the box with vertices .3; 1; 5/ ,.5; 1; 5/ ,.5; 0; 5/ ,.3; 0; 5/ ,
.3; 1; 4/ ,.5; 1; 4/ ,.5; 0; 4/ , and .3; 0; 4/ . Find the image of Sunder the perspective pro-
jection with center of projection at .0; 0; 10/ .
SOLUTION LetPbe the projection matrix, and let Dbe the data matrix for Susing
homogeneous coordinates. The data matrix for the image of Sis
PDD2
6641 0 0 0
0 1 0 0
0 0 0 0
0 0  1=10 13
775Vertex:
2
6641
32
53
54
35
36
57
58
3
1 1 0 0 1 1 0 0
5 5 5 5 4 4 4 4
1 1 1 1 1 1 1 13
775
D2
6643 5 5 3 3 5 5 3
1 1 0 0 1 1 0 0
0 0 0 0 0 0 0 0
:5 :5 :5 :5 :6 :6 :6 :63
775
To obtain R3coordinates, use equation (1) before Example 7, and divide the top three
Sunder the perspective
transformation.entries in each column by the corresponding entry in the fourth row:
Vertex:
2
41
62
103
104
65
56
8:37
8:38
5
2 2 0 0 1:7 1:7 0 0
0 0 0 0 0 0 0 03
5
This text‚Äôs web site has some interesting applications of computer graphics, includ-
WEB
ing a further discussion of perspective projections. One of the computer projects on the
web site involves simple animation.
SECOND REVISED PAGES


--- Page 163 ---
146 CHAPTER 2 Matrix Algebra
N U M E R I C A L N O T E
Continuous movement of graphical 3D objects requires intensive computation
with 44matrices, particularly when the surfaces are rendered to appear
realistic, with texture and appropriate lighting. High-end computer graphics
boards have 44matrix operations and graphics algorithms embedded in their
microchips and circuitry. Such boards can perform the billions of matrix multipli-
cations per second needed for realistic color animation in 3D gaming programs.2
Further Reading
James D. Foley, Andries van Dam, Steven K. Feiner, and John F. Hughes, Computer
Graphics: Principles and Practice , 3rd ed. (Boston, MA: Addison-Wesley, 2002),
Chapters 5 and 6.
PRACTICE PROBLEM
Rotation of a Ô¨Ågure about a point pinR2is accomplished by Ô¨Årst translating the Ô¨Ågure
by p, rotating about the origin, and then translating back by p. See Figure 7. Construct
the33matrix that rotates points  30about the point . 2; 6/, using homogeneous
coordinates.
x2 x2 x2 x2
x1 x1 x1 x1
(a) Original figure. (b) Translated to
      ori gin by ‚Äìp.(c) Rotated about
      the ori gin.(d) Translated
      back b y p.pppp
FIGURE 7 Rotation of Ô¨Ågure about point p.
2.7 EXERCISES
1.What 33matrix will have the same effect on homogeneous
coordinates for R2that the shear matrix Ahas in Example 2?
2.Use matrix multiplication to Ô¨Ånd the image of the triangle
with data matrix DD5 2 4
0 2 3
under the transforma-
tion that reÔ¨Çects points through the y-axis. Sketch both the
original triangle and its image.
In Exercises 3‚Äì8, Ô¨Ånd the 33matrices that produce the de-
scribed composite 2D transformations, using homogeneous coor-
dinates.
3.Translate by (3, 1), and then rotate 45about the origin.
4.Translate by . 2; 3/; and then scale the x-coordinate by
.8 and the y-coordinated by 1.2.5.ReÔ¨Çect points through the x-axis, and then rotate 30about
the origin.
6.Rotate points 30, and then reÔ¨Çect through the x-axis.
7.Rotate points through 60about the point (6, 8).
8.Rotate points through 45about the point (3, 7).
9.A2200 data matrix Dcontains the coordinates of 200
points. Compute the number of multiplications required
to transform these points using two arbitrary 22ma-
trices Aand B. Consider the two possibilities A(BD) and
(AB)D. Discuss the implications of your results for computer
graphics calculations.
10. Consider the following geometric 2D transformations: D, a
dilation (in which x-coordinates and y-coordinates are scaled
2See Jan Ozer, ‚ÄúHigh-Performance Graphics Boards,‚Äù PC Magazine 19, 1 September 2000, pp. 187‚Äì200.
Also, ‚ÄúThe Ultimate Upgrade Guide: Moving On Up,‚Äù PC Magazine 21, 29 January 2002, pp. 82‚Äì91.
SECOND REVISED PAGES


--- Page 164 ---
2.7 Applications to Computer Graphics 147
by the same factor); R, a rotation; and T, a translation. Does
D commute with R? That is, is D .R. x//DR .D. x//for all
xinR2? Does Dcommute with T? Does Rcommute with T?
11.A rotation on a computer screen is sometimes implemented
as the product of two shear-and-scale transformations, which
can speed up calculations that determine how a graphic image
actually appears in terms of screen pixels. (The screen con-
sists of rows and columns of small dots, called pixels .) The
Ô¨Årst transformation A1shears vertically and then compresses
each column of pixels; the second transformation A2shears
horizontally and then stretches each row of pixels. Let
A1D2
41 0 0
sin'cos' 0
0 0 13
5,
A2D2
4sec' tan' 0
0 1 0
0 0 13
5
Show that the composition of the two transformations is a
rotation in R2.
12. A rotation in R2usually requires four multiplications. Com-
pute the product below, and show that the matrix for a rota-
tion can be factored into three shear transformations (each of
which requires only one multiplication).
2
41 tan'=2 0
0 1 0
0 0 13
52
41 0 0
sin' 1 0
0 0 13
5
2
41 tan'=2 0
0 1 0
0 0 13
5
13. The usual transformations on homogeneous coordinates for
2D computer graphics involve 33matrices of the formA p
0T1
where Ais a22matrix and pis inR2. Show
that such a transformation amounts to a linear transformation
onR2followed by a translation. [ Hint: Find an appropriate
matrix factorization involving partitioned matrices.]
14. Show that the transformation in Exercise 7 is equivalent to
a rotation about the origin followed by a translation by p.
Find p.
15. What vector in R3has homogeneous coordinates 1
2; 1
4;1
8;1
24
¬ã
16. Are.1; 2; 3; 4/ and.10; 20; 30; 40/ homogeneous coordi-
nates for the same point in R3? Why or why not?
17. Give the 44matrix that rotates points in R3about the
x-axis through an angle of 60. (See the Ô¨Ågure.)
z
e3
e2
ye1
x18. Give the 44matrix that rotates points in R3about the
z-axis through an angle of  30, and then translates by
pD.5; 2; 1/.
19. LetSbe the triangle with vertices .4:2; 1:2; 4/; .6; 4; 2/;
.2; 2; 6/: Find the image of Sunder the perspective projection
with center of projection at .0; 0; 10/:
20. Let Sbe the triangle with vertices .9; 3; 5/; .12; 8; 2/;
.1:8; 2:7; 1/: Find the image of Sunder the perspective pro-
jection with center of projection at .0; 0; 10/ .
Exercises 21 and 22 concern the way in which color is speciÔ¨Åed
for display in computer graphics. A color on a computer screen
is encoded by three numbers ( R,G,B) that list the amount of
energy an electron gun must transmit to red, green, and blue
phosphor dots on the computer screen. (A fourth number speciÔ¨Åes
the luminance or intensity of the color.)
21. [M] The actual color a viewer sees on a screen is inÔ¨Çuenced
by the speciÔ¨Åc type and amount of phosphors on the screen.
So each computer screen manufacturer must convert between
the (R,G,B) data and an international CIE standard for color,
which uses three primary colors, called X,Y, and Z. A typical
conversion for short-persistence phosphors is
2
4:61 :29 :150
:35 :59 :063
:04 :12 :7873
52
4R
G
B3
5D2
4X
Y
Z3
5
A computer program will send a stream of color information
to the screen, using standard CIE data (X,Y, Z). Find the
equation that converts these data to the ( R,G,B) data needed
for the screen‚Äôs electron gun.
22. [M] The signal broadcast by commercial television describes
each color by a vector ( Y,I,Q). If the screen is black and
white, only the Y-coordinate is used. (This gives a better
monochrome picture than using CIE data for colors.) The
correspondence between YIQ and a ‚Äústandard‚Äù RGB color is
given by
2
4Y
I
Q3
5D2
4:299 :587 :114
:596  :275  :321
:212  :528 :3113
52
4R
G
B3
5
(A screen manufacturer would change the matrix entries to
work for its RGB screens.) Find the equation that converts
theYIQdata transmitted by the television station to the RGB
data needed for the television screen.
SECOND REVISED PAGES


--- Page 165 ---
148 CHAPTER 2 Matrix Algebra
SOLUTION TO PRACTICE PROBLEM
Assemble the matrices right-to-left for the three operations. Using pD. 2; 6/,
cos. 30/Dp
3=2, and sin . 30/D  :5, we have
Translate
back by p2
41 0  2
0 1 6
0 0 13
5Rotate around
the origin2
4p
3=2 1=2 0
 1=2p
3=2 0
0 0 13
5Translate
by p2
41 0 2
0 1  6
0 0 13
5
D2
4p
3=2 1=2p
3 5
 1=2p
3=2 3p
3C5
0 0 13
5
2.8 SUBSPACES OF Rn
This section focuses on important sets of vectors in Rncalled subspaces . Often sub-
spaces arise in connection with some matrix A;and they provide useful information
about the equation AxDb. The concepts and terminology in this section will be used
repeatedly throughout the rest of the book.1
D E F I N I T I O N Asubspace ofRnis any set HinRnthat has three properties:
a.The zero vector is in H.
b.For each uandvinH, the sum uCvis inH.
c.For each uinHand each scalar c, the vector cuis inH.
In words, a subspace is closed under addition and scalar multiplication. As you will
see in the next few examples, most sets of vectors discussed in Chapter 1 are subspaces.
For instance, a plane through the origin is the standard way to visualize the subspace in
Example 1. See Figure 1.
EXAMPLE 1 Ifv1andv2are inRnandHDSpanfv1;v2g, then His a subspace
ofRn. To verify this statement, note that the zero vector is in H(because 0v1C0v2is
a linear combination of v1andv2/. Now take two arbitrary vectors in H, say,
0x3
v2 v1
x2
x1 FIGURE 1
Spanfv1;v2gas a plane through
the origin.uDs1v1Cs2v2and vDt1v1Ct2v2
Then
uCvD.s1Ct1/v1C.s2Ct2/v2
which shows that uCvis a linear combination of v1andv2and hence is in H. Also, for
any scalar c, the vector cuis inH, because cuDc.s1v1Cs2v2/D.cs1/v1C.cs2/v2.
Ifv1is not zero and if v2is a multiple of v1, then v1andv2simply span a line
through the origin. So a line through the origin is another example of a subspace.
1Sections 2.8 and 2.9 are included here to permit readers to postpone the study of most or all of the next two
chapters and to skip directly to Chapter 5, if so desired. Omit these two sections if you plan to work through
Chapter 4 before beginning Chapter 5.
SECOND REVISED PAGES


--- Page 166 ---
2.8 Subspaces of Rn149
EXAMPLE 2 A line Lnotthrough the origin is nota subspace, because it does not
Span{v1, v2}v1v2x2
x1
v1¬§0,v2Dkv1.contain the origin, as required. Also, Figure 2 shows that Lis not closed under addition
or scalar multiplication.
u
vu /H11001 v
u /H11001 v is not on L 2w is not on L2w
w
LL
FIGURE 2
EXAMPLE 3 Forv1; : : : ; vpinRn, the set of all linear combinations of v1; : : : ; vp
is a subspace of Rn. The veriÔ¨Åcation of this statement is similar to the argument given
in Example 1. We shall now refer to Span fv1; : : : ; vpgasthe subspace spanned (or
generated ) byv1; : : : ; vp.
Note that Rnis a subspace of itself because it has the three properties required for
a subspace. Another special subspace is the set consisting of only the zero vector in Rn.
This set, called the zero subspace , also satisÔ¨Åes the conditions for a subspace.
Column Space and Null Space of a Matrix
Subspaces of Rnusually occur in applications and theory in one of two ways. In both
cases, the subspace can be related to a matrix.
D E F I N I T I O N Thecolumn space of a matrix Ais the set Col Aof all linear combinations of the
columns of A.
IfAD¬åa1 an¬ç, with the columns in Rm, then Col Ais the same as
Spanfa1; : : : ; ang. Example 4 shows that the column space of an mnmatrix is a
subspace of Rm. Note that Col Aequals Rmonly when the columns of AspanRm.
Otherwise, Col Ais only part of Rm.
EXAMPLE 4 LetAD2
41 3 4
 4 6  2
 3 7 63
5andbD2
43
3
 43
5. Determine whether bis
in the column space of A.
SOLUTION The vector bis a linear combination of the columns of Aif and only if
bcan be written as Axfor some x, that is, if and only if the equation AxDbhas a
solution. Row reducing the augmented matrix ¬åAb¬ç,
2
41 3 4 3
 4 6  2 3
 3 7 6  43
52
41 3 4 3
0 6 18 15
0 2 6 53
52
41 3 4 3
0 6 18 15
0 0 0 03
5
we conclude that AxDbis consistent and bis in Col A.
0
bx3
x2
x1Col A
SECOND REVISED PAGES


--- Page 167 ---
150 CHAPTER 2 Matrix Algebra
The solution of Example 4 shows that when a system of linear equations is written
in the form AxDb, the column space of Ais the set of all bfor which the system has
a solution.
D E F I N I T I O N Thenull space of a matrix Ais the set Nul Aof all solutions of the homogeneous
equation AxD0.
When Ahasncolumns, the solutions of AxD0belong to Rn, and the null space
ofAis a subset of Rn. In fact, Nul Ahas the properties of a subspace ofRn.
T H E O R E M 1 2 The null space of an mnmatrix Ais a subspace of Rn. Equivalently, the set of all
solutions of a system AxD0ofmhomogeneous linear equations in nunknowns
is a subspace of Rn.
PROOF The zero vector is in Nul A(because A0D0). To show that Nul AsatisÔ¨Åes the
other two properties required for a subspace, take any uandvin Nul A. That is, suppose
AuD0andAvD0. Then, by a property of matrix multiplication,
A.uCv/DAuCAvD0C0D0
Thus uCvsatisÔ¨Åes AxD0, and so uCvis in Nul A. Also, for any scalar c; A.c u/D
c.Au/Dc.0/D0, which shows that cuis in Nul A.
To test whether a given vector vis in Nul A, just compute Avto see whether Avis
the zero vector. Because Nul Ais described by a condition that must be checked for each
vector, we say that the null space is deÔ¨Åned implicitly . In contrast, the column space is
deÔ¨Åned explicitly , because vectors in Col Acan be constructed (by linear combinations)
from the columns of A. To create an explicit description of Nul A, solve the equation
AxD0and write the solution in parametric vector form. (See Example 6, below.)2
Basis for a Subspace
Because a subspace typically contains an inÔ¨Ånite number of vectors, some problems
involving a subspace are handled best by working with a small Ô¨Ånite set of vectors that
span the subspace. The smaller the set, the better. It can be shown that the smallest
possible spanning set must be linearly independent.
D E F I N I T I O N Abasis for a subspace HofRnis a linearly independent set in Hthat spans H.
EXAMPLE 5 The columns of an invertible nnmatrix form a basis for all of Rn
because they are linearly independent and span Rn, by the Invertible Matrix Theorem.
One such matrix is the nnidentity matrix. Its columns are denoted by e1; : : : ; en:
e1D2
6641
0:::
03
775;e2D2
6640
1:::
03
775; : : : ; enD2
6640:::
0
13
775
The set fe1; : : : ; engis called the standard basis forRn. See Figure 3.
x3
e3
e2x2
e1
x1
FIGURE 3
The standard basis for R3.2The contrast between Nul Aand Col Ais discussed further in Section 4.2.
SECOND REVISED PAGES


--- Page 168 ---
2.8 Subspaces of Rn151
The next example shows that the standard procedure for writing the solution set of
AxD0in parametric vector form actually identiÔ¨Åes a basis for Nul A. This fact will be
used throughout Chapter 5.
EXAMPLE 6 Find a basis for the null space of the matrix
AD2
4 3 6  1 1  7
1 2 2 3  1
2 4 5 8  43
5
SOLUTION First, write the solution of AxD0in parametric vector form:
A0
2
41 2 0  1 3 0
0 0 1 2  2 0
0 0 0 0 0 03
5;x1 2x2 x4C3x5D0
x3C2x4 2x5D0
0D0
The general solution is x1D2x2Cx4 3x5,x3D  2x4C2x5, with x2,x4, and x5
free.2
66664x1
x2
x3
x4
x53
77775D2
666642x2Cx4 3x5
x2
 2x4C2x5
x4
x53
77775Dx22
666642
1
0
0
03
77775
6
uCx42
666641
0
 2
1
03
77775
6vCx52
66664 3
0
2
0
13
77775
6w
Dx2uCx4vCx5w (1)
Equation (1) shows that Nul Acoincides with the set of all linear combinations of u,
v;andw:That is, fu;v;wggenerates Nul A. In fact, this construction of u,v, and w
automatically makes them linearly independent, because equation (1) shows that 0D
x2uCx4vCx5wonly if the weights x2,x4, and x5are all zero. (Examine entries 2, 4,
and 5 in the vector x2uCx4vCx5w.) Sofu;v;wgis abasis for Nul A.
Finding a basis for the column space of a matrix is actually less work than Ô¨Ånding
a basis for the null space. However, the method requires some explanation. Let‚Äôs begin
with a simple case.
EXAMPLE 7 Find a basis for the column space of the matrix
BD2
6641 0  3 5 0
0 1 2  1 0
0 0 0 0 1
0 0 0 0 03
775
SOLUTION Denote the columns of Bbyb1; : : : ; b5and note that b3D  3b1C2b2and
b4D5b1 b2. The fact that b3andb4are combinations of the pivot columns means that
any combination of b1; : : : ; b5is actually just a combination of b1,b2, and b5. Indeed,
ifvis any vector in Col B, say,
vDc1b1Cc2b2Cc3b3Cc4b4Cc5b5
then, substituting for b3andb4, we can write vin the form
vDc1b1Cc2b2Cc3. 3b1C2b2/Cc4.5b1 b2/Cc5b5
which is a linear combination of b1,b2, and b5. Sofb1;b2;b5gspans Col B. Also, b1,
b2, and b5are linearly independent, because they are columns from an identity matrix.
So the pivot columns of Bform a basis for Col B.
SECOND REVISED PAGES


--- Page 169 ---
152 CHAPTER 2 Matrix Algebra
The matrix Bin Example 7 is in reduced echelon form. To handle a general matrix
A, recall that linear dependence relations among the columns of Acan be expressed
in the form AxD0for some x. (If some columns are not involved in a particular
dependence relation, then the corresponding entries in xare zero.) When Ais row
reduced to echelon form B, the columns are drastically changed, but the equations
AxD0andBxD0have the same set of solutions. That is, the columns of Ahave
exactly the same linear dependence relationships as the columns of B.
EXAMPLE 8 It can be veriÔ¨Åed that the matrix
AD¬åa1a2 a5¬çD2
6641 3 3 2  9
 2 2 2  8 2
2 3 0 7 1
3 4  1 11  83
775
is row equivalent to the matrix Bin Example 7. Find a basis for Col A.
SOLUTION From Example 7, the pivot columns of Aare columns 1, 2, and 5.
Also, b3D  3b1C2b2andb4D5b1 b2. Since row operations do not affect linear
dependence relations among the columns of the matrix, we should have
a3D  3a1C2a2and a4D5a1 a2
Check that this is true! By the argument in Example 7, a3anda4are not needed to
generate the column space of A. Also, fa1;a2;a5gmust be linearly independent, because
any dependence relation among a1,a2, and a5would imply the same dependence relation
among b1,b2, and b5. Since fb1;b2;b5gis linearly independent, fa1;a2;a5gis also
linearly independent and hence is a basis for Col A.
The argument in Example 8 can be adapted to prove the following theorem.
T H E O R E M 1 3 The pivot columns of a matrix Aform a basis for the column space of A.
Warning: Be careful to use pivot columns of Aitself for the basis of Col A. The
columns of an echelon form Bare often not in the column space of A. (For instance,
in Examples 7 and 8, the columns of Ball have zeros in their last entries and cannot
generate the columns of A.)
PRACTICE PROBLEMS
1.LetAD2
41 1 5
2 0 7
 3 5 33
5anduD2
4 7
3
23
5. Isuin Nul A? Isuin Col A? Justify
each answer.
2.Given AD2
40 1 0
0 0 1
0 0 03
5, Ô¨Ånd a vector in Nul Aand a vector in Col A.
3.Suppose an nnmatrix Ais invertible. What can you say about Col A? About
SGMastering: Subspace,
ColA, NulA, Basis 2‚Äì37
NulA?
SECOND REVISED PAGES


--- Page 170 ---
2.8 Subspaces of Rn153
2.8 EXERCISES
Exercises 1‚Äì4 display sets in R2. Assume the sets include the
bounding lines. In each case, give a speciÔ¨Åc reason why the set
Hisnota subspace of R2. (For instance, Ô¨Ånd two vectors in H
whose sum is notinH, or Ô¨Ånd a vector in Hwith a scalar multiple
that is not in H. Draw a picture.)
1.
2.
3.
4.
5.Letv1D2
42
3
 53
5;v2D2
4 4
 5
83
5;andwD2
48
2
 93
5. Deter-
mine if wis in the subspace of R3generated by v1andv2.
6.Let v1D2
6641
 2
4
33
775,v2D2
6644
 7
9
73
775,v3D2
6645
 8
6
53
775;and uD
2
664 4
10
 7
 53
775. Determine if uis in the subspace of R4generated
byfv1;v2;v3g.7.Letv1D2
42
 8
63
5,v2D2
4 3
8
 73
5,v3D2
4 4
6
 73
5,
pD2
46
 10
113
5, and A=¬åv1v2v3¬ç:
a.How many vectors are in fv1;v2;v3g?
b.How many vectors are in Col A?
c.Ispin Col A? Why or why not?
8.Let v1D2
4 3
0
63
5,v2D2
4 2
2
33
5,v3D2
40
 6
33
5, and pD
2
41
14
 93
5:Determine if pis in Col A, where AD¬åv1v2v3¬ç:
9.With Aandpas in Exercise 7, determine if pis in Nul A.
10. With uD. 2; 3; 1/ andAas in Exercise 8, determine if uis
in Nul A.
In Exercises 11 and 12, give integers pandqsuch that Nul Ais a
subspace of Rpand Col Ais a subspace of Rq.
11.AD2
43 2 1  5
 9 4 1 7
9 2  5 13
5
12. AD2
6641 2 3
4 5 7
 5 1 0
2 7 113
775
13. ForAas in Exercise 11, Ô¨Ånd a nonzero vector in Nul Aand a
nonzero vector in Col A.
14. ForAas in Exercise 12, Ô¨Ånd a nonzero vector in Nul Aand a
nonzero vector in Col A.
Determine which sets in Exercises 15‚Äì20 are bases for R2orR3.
Justify each answer.
15.5
 2
,10
 3
16. 4
6
,2
 3
17.2
40
1
 23
5,2
45
 7
43
5,2
46
3
53
5 18.2
41
1
 23
5;2
4 5
 1
23
5;2
47
0
 53
5
19.2
43
 8
13
5;2
46
2
 53
5
20.2
41
 6
 73
5;2
43
 4
73
5;2
4 2
7
53
5;2
40
8
93
5
SECOND REVISED PAGES


--- Page 171 ---
154 CHAPTER 2 Matrix Algebra
In Exercises 21 and 22, mark each statement True or False. Justify
each answer.
21. a.A subspace of Rnis any set Hsuch that (i) the zero vector
is in H, (ii) u,v, and uCvare in H, and (iii) cis a scalar
andcuis in H.
b.Ifv1; : : : ; vpare inRn, then Span fv1; : : : ; vpgis the same
as the column space of the matrix ¬åv1vp¬ç.
c.The set of all solutions of a system of mhomogeneous
equations in nunknowns is a subspace of Rm.
d.The columns of an invertible nnmatrix form a basis
forRn.
e.Row operations do not affect linear dependence relations
among the columns of a matrix.
22. a.A subset HofRnis a subspace if the zero vector is in H.
b.Given vectors v1; : : : ; vpinRn, the set of all linear com-
binations of these vectors is a subspace of Rn.
c.The null space of an mnmatrix is a subspace of Rn.
d.The column space of a matrix Ais the set of solutions of
AxDb:
e.IfBis an echelon form of a matrix A, then the pivot
columns of Bform a basis for Col A.
Exercises 23‚Äì26 display a matrix Aand an echelon form of A. Find
a basis for Col Aand a basis for Nul A.
23.AD2
44 5 9  2
6 5 1 12
3 4 8  33
52
41 2 6  5
0 1 5  6
0 0 0 03
5
24.AD2
4 3 9  2 7
2 6 4 8
3 9 2 23
52
41 3 6 9
0 0 4 5
0 0 0 03
5
25.AD2
6641 4 8  3 7
 1 2 7 3 4
 2 2 9 5 5
3 6 9  5 23
775
2
6641 4 8 0 5
0 2 5 0  1
0 0 0 1 4
0 0 0 0 03
77526.AD2
6643 1 7 3 9
 2 2  2 7 5
 5 9 3 3 4
 2 6 6 3 73
775
2
6643 1 7 0 6
0 2 4 0 3
0 0 0 1 1
0 0 0 0 03
775
27. Construct a nonzero 33matrix Aand a nonzero vector b
such that bis in Col A, but bis not the same as any one of the
columns of A.
28. Construct a nonzero 33matrix Aand a vector bsuch that
bisnotin Col A.
29. Construct a nonzero 33matrix Aand a nonzero vector b
such that bis in Nul A.
30. Suppose the columns of a matrix AD¬åa1ap¬çare lin-
early independent. Explain why fa1; : : : ; apgis a basis for
ColA.
In Exercises 31‚Äì36, respond as comprehensively as possible, and
justify your answer.
31. Suppose Fis a55matrix whose column space is not equal
toR5. What can you say about Nul F?
32. IfRis a66matrix and Nul Risnotthe zero subspace, what
can you say about Col R?
33. IfQis a44matrix and Col QDR4, what can you say about
solutions of equations of the form QxDbforbinR4?
34. IfPis a55matrix and Nul Pis the zero subspace, what
can you say about solutions of equations of the form PxDb
forbinR5?
35. What can you say about Nul Bwhen Bis a54matrix with
linearly independent columns?
36. What can you say about the shape of an mnmatrix Awhen
the columns of Aform a basis for Rm?
[M] In Exercises 37 and 38, construct bases for the column space
and the null space of the given matrix A. Justify your work.
37.AD2
6643 5 0  1 3
 7 9  4 9  11
 5 7  2 5  7
3 7 3 4 03
775
38.AD2
6645 2 0  8 8
4 1 2  8 9
5 1 3 5 19
 8 5 6 8 53
775
WEB
Column Space and Null Space
WEB
A Basis for Col A
SECOND REVISED PAGES


--- Page 172 ---
2.9 Dimension and Rank 155
SOLUTIONS TO PRACTICE PROBLEMS
1.To determine whether uis in Nul A, simply compute
AuD2
41 1 5
2 0 7
 3 5 33
52
4 7
3
23
5D2
40
0
03
5
The result shows that uis in Nul A:Deciding whether uis in Col Arequires more
work. Reduce the augmented matrix ¬åA u¬çto echelon form to determine whether
the equation AxDuis consistent:
2
41 1 5  7
2 0 7 3
 3 5 3 23
52
41 1 5  7
0 2  3 17
0 8 12  193
52
41 1 5  7
0 2  3 17
0 0 0 493
5
The equation AxDuhas no solution, so uis not in Col A.
2.In contrast to Practice Problem 1, Ô¨Ånding a vector in Nul Arequires more work
than testing whether a speciÔ¨Åed vector is in Nul A. However, since Ais already
in reduced echelon form, the equation AxD0shows that if xD.x1; x2; x3/;then
x2D0; x 3D0;andx1is a free variable. Thus, a basis for Nul AisvD.1; 0; 0/:
Finding just one vector in Col Ais trivial, since each column of Ais in Col A. In
this particular case, the same vector vis in both Nul AandColA. For most nn
matrices, the zero vector of Rnis the only vector in both Nul Aand Col A.
3.IfAis invertible, then the columns of AspanRn;by the Invertible Matrix Theorem.
By deÔ¨Ånition, the columns of any matrix always span the column space, so in this
case Col Ais all of Rn:In symbols, Col ADRn:Also, since Ais invertible, the
equation AxD0has only the trivial solution. This means that Nul Ais the zero
subspace. In symbols, Nul AD f0g:
2.9 DIMENSION AND RANK
This section continues the discussion of subspaces and bases for subspaces, beginning
with the concept of a coordinate system. The deÔ¨Ånition and example below should make
a useful new term, dimension , seem quite natural, at least for subspaces of R3.
Coordinate Systems
The main reason for selecting a basis for a subspace H;instead of merely a spanning
set, is that each vector in Hcan be written in only one way as a linear combination of
the basis vectors. To see why, suppose BD fb1; : : : ; bpgis a basis for H, and suppose
a vector xinHcan be generated in two ways, say,
xDc1b1C  C cpbpand xDd1b1C  C dpbp (1)
Then, subtracting gives
0Dx xD.c1 d1/b1C  C .cp dp/bp (2)
SinceBis linearly independent, the weights in (2) must all be zero. That is, cjDdjfor
1jp;which shows that the two representations in (1) are actually the same.
SECOND REVISED PAGES


--- Page 173 ---
156 CHAPTER 2 Matrix Algebra
D E F I N I T I O N Suppose the set BD fb1; : : : ; bpgis a basis for a subspace H. For each xinH,
thecoordinates of x relative to the basis Bare the weights c1; : : : ; c psuch that
xDc1b1C  C cpbp;and the vector in Rp
¬åx¬çBD2
64c1
:::
cp3
75
is called the coordinate vector of x (relative to B)or the B-coordinate vector
of x.1
EXAMPLE 1 Letv1D2
43
6
23
5,v2D2
4 1
0
13
5,xD2
43
12
73
5, andBD fv1;v2g. Then
Bis a basis for HDSpanfv1;v2gbecause v1andv2are linearly independent. Deter-
mine if xis inH, and if it is, Ô¨Ånd the coordinate vector of xrelative to B.
SOLUTION Ifxis inH, then the following vector equation is consistent:
c12
43
6
23
5Cc22
4 1
0
13
5D2
43
12
73
5
The scalars c1andc2, if they exist, are the B-coordinates of x. Row operations show
that 2
43 1 3
6 0 12
2 1 73
52
41 0 2
0 1 3
0 0 03
5
Thus c1D2,c2D3, and ¬åx¬çBD2
3
. The basis Bdetermines a ‚Äúcoordinate system‚Äù
onH, which can be visualized by the grid shown in Figure 1.
0
x2x3
x13v2
2v2
2v1v2
v1x /H11005 2v1 /H11001 3v2
FIGURE 1 A coordinate system on a plane
HinR3:
1It is important that the elements of Bare numbered because the entries in ¬åx¬çBdepend on the order of the
vectors in B.
SECOND REVISED PAGES


--- Page 174 ---
2.9 Dimension and Rank 157
Notice that although points in Hare also in R3, they are completely determined
by their coordinate vectors, which belong to R2. The grid on the plane in Figure 1
makes H‚Äúlook‚Äù like R2. The correspondence x7!¬åx¬çBis a one-to-one correspondence
between HandR2that preserves linear combinations. We call such a correspondence
anisomorphism , and we say that Hisisomorphic toR2.
In general, if BD fb1; : : : ; bpgis a basis for H, then the mapping x7!¬åx¬çBis a
one-to-one correspondence that makes Hlook and act the same as Rp(even though the
vectors in Hthemselves may have more than pentries). (Section 4.4 has more details.)
The Dimension of a Subspace
It can be shown that if a subspace Hhas a basis of pvectors, then every basis of Hmust
consist of exactly pvectors. (See Exercises 27 and 28.) Thus the following deÔ¨Ånition
makes sense.
D E F I N I T I O N The dimension of a nonzero subspace H, denoted by dim H, is the number of
vectors in any basis for H. The dimension of the zero subspace f0gis deÔ¨Åned to
be zero.2
The space Rnhas dimension n. Every basis for Rnconsists of nvectors. A plane
through 0inR3is two-dimensional, and a line through 0is one-dimensional.
EXAMPLE 2 Recall that the null space of the matrix Ain Example 6 in Section 2.8
had a basis of 3 vectors. So the dimension of Nul Ain this case is 3. Observe how each
basis vector corresponds to a free variable in the equation AxD0. Our construction
always produces a basis in this way. So, to Ô¨Ånd the dimension of Nul A, simply identify
and count the number of free variables in AxD0.
D E F I N I T I O N Therank of a matrix A, denoted by rank A, is the dimension of the column space
ofA.
Since the pivot columns of Aform a basis for Col A, the rank of Ais just the number
of pivot columns in A.
EXAMPLE 3 Determine the rank of the matrix
AD2
6642 5  3 4 8
4 7  4 3 9
6 9  5 2 4
0 9 6 5  63
775
SOLUTION Reduce Ato echelon form:
A2
6642 5  3 4 8
0 3 2 5  7
0 6 4 14  20
0 9 6 5  63
775  2
6642 5  3 4 8
0 3 2 5  7
0 0 0 4  6
0 0 0 0 03
775
Pivot columns
-
-
-
The matrix Ahas 3 pivot columns, so rank AD3.
2The zero subspace has nobasis (because the zero vector by itself forms a linearly dependent set).
SECOND REVISED PAGES


--- Page 175 ---
158 CHAPTER 2 Matrix Algebra
The row reduction in Example 3 reveals that there are two free variables in AxD0,
because two of the Ô¨Åve columns of Aarenotpivot columns. (The nonpivot columns
correspond to the free variables in AxD0.) Since the number of pivot columns plus the
number of nonpivot columns is exactly the number of columns, the dimensions of Col A
and Nul Ahave the following useful connection. (See the Rank Theorem in Section 4.6
for additional details.)
T H E O R E M 1 4 The Rank Theorem
If a matrix Ahasncolumns, then rank ACdim Nul ADn.
The following theorem is important for applications and will be needed in
Chapters 5 and 6. The theorem (proved in Section 4.5) is certainly plausible, if you
think of a p-dimensional subspace as isomorphic to Rp. The Invertible Matrix Theorem
shows that pvectors in Rpare linearly independent if and only if they also span Rp.
T H E O R E M 1 5 The Basis Theorem
LetHbe ap-dimensional subspace of Rn:Any linearly independent set of exactly
pelements in His automatically a basis for H:Also, any set of pelements of H
that spans His automatically a basis for H:
Rank and the Invertible Matrix Theorem
The various vector space concepts associated with a matrix provide several more
statements for the Invertible Matrix Theorem. They are presented below to follow the
statements in the original theorem in Section 2.3.
T H E O R E M The Invertible Matrix Theorem (continued)
LetAbe an nnmatrix. Then the following statements are each equivalent to
the statement that Ais an invertible matrix.
m.The columns of Aform a basis of Rn:
n.ColADRn
o.dim Col ADn
p.rankADn
q.NulAD f0g
r.dim Nul AD0
PROOF Statement (m) is logically equivalent to statements (e) and (h) regarding linear
independence and spanning. The other Ô¨Åve statements are linked to the earlier ones of
the theorem by the following chain of almost trivial implications:
(g))(n))(o))(p))(r))(q))(d)
Statement (g), which says that the equation AxDbhas at least one solution for each
binRn, implies statement (n), because Col Ais precisely the set of all bsuch that
the equation AxDbis consistent. The implications (n) )(o))(p) follow from the
deÔ¨Ånitions of dimension andrank. If the rank of Aisn, the number of columns of A,
then dim Nul AD0, by the Rank Theorem, and so Nul AD f0g. Thus (p) )(r))(q).
SECOND REVISED PAGES


--- Page 176 ---
2.9 Dimension and Rank 159
Also, statement (q) implies that the equation AxD0has only the trivial solution, which
is statement (d). Since statements (d) and (g) are already known to be equivalent to the
statement that Ais invertible, the proof is complete.
SGExpanded Table
for the IMT 2‚Äì39
N U M E R I C A L N O T E S
Many algorithms discussed in this text are useful for understanding concepts
and making simple computations by hand. However, the algorithms are often
unsuitable for large-scale problems in real life.
Rank determination is a good example. It would seem easy to reduce a matrix
to echelon form and count the pivots. But unless exact arithmetic is performed
on a matrix whose entries are speciÔ¨Åed exactly, row operations can change the
apparent rank of a matrix. For instance, if the value of xin the matrix5 7
5 x
is not stored exactly as 7 in a computer, then the rank may be 1 or 2, depending
on whether the computer treats x 7as zero.
In practical applications, the effective rank of a matrix Ais often determined
from the singular value decomposition of A, to be discussed in Section 7.4.
WEB
PRACTICE PROBLEMS
1.Determine the dimension of the subspace HofR3spanned by the vectors v1;v2;
andv3:(First, Ô¨Ånd a basis for H.)
v1D2
42
 8
63
5;v2D2
43
 7
 13
5;v3D2
4 1
6
 73
5
2.Consider the basis
BD1
:2
;:2
1
forR2. If¬åx¬çBD3
2
, what is x?
3.CouldR3possibly contain a four-dimensional subspace? Explain.
2.9 EXERCISES
In Exercises 1 and 2, Ô¨Ånd the vector xdetermined by the given
coordinate vector ¬åx¬çBand the given basis B. Illustrate your
answer with a Ô¨Ågure, as in the solution of Practice Problem 2.
1.BD1
1
;2
 1
; ¬åx¬çBD3
2
2.BD 2
1
;3
1
; ¬åx¬çBD 1
3
In Exercises 3‚Äì6, the vector xis in a subspace Hwith a basis
BD fb1;b2g:Find the B-coordinate vector of x.3.b1D1
 4
;b2D 2
7
;xD 3
7
4.b1D1
 3
;b2D 3
5
;xD 7
5
5.b1D2
41
5
 33
5;b2D2
4 3
 7
53
5;xD2
44
10
 73
5
6.b1D2
4 3
1
 43
5;b2D2
47
5
 63
5;xD2
411
0
73
5
SECOND REVISED PAGES


--- Page 177 ---
160 CHAPTER 2 Matrix Algebra
7.Let b1D3
0
;b2D 1
2
;wD7
 2
;xD4
1
;and
BD fb1;b2g:Use the Ô¨Ågure to estimate ¬åw¬çBand¬åx¬çB:
ConÔ¨Årm your estimate of ¬åx¬çBby using it and fb1;b2gto
compute x.
b2
b1x
w0
8.Letb1D0
2
;b2D2
1
;xD 2
3
;yD2
4
;
zD 1
 2:5
;andBD fb1;b2g:Use the Ô¨Ågure to estimate
¬åx¬çB; ¬åy¬çB, and ¬åz¬çB:ConÔ¨Årm your estimates of ¬åy¬çBand¬åz¬çB
by using them and fb1;b2gto compute yandz.
b2b1x
0
zy
Exercises 9‚Äì12 display a matrix Aand an echelon form of A. Find
bases for Col Aand Nul A, and then state the dimensions of these
subspaces.
9.AD2
6641 3 2  4
 3 9  1 5
2 6 4  3
 4 12 2 73
7752
6641 3 2  4
0 0 5  7
0 0 0 5
0 0 0 03
775
10.AD2
6641 2 9 5 4
1 1 6 5  3
 2 0  6 1  2
4 1 9 1  93
775
2
6641 2 9 5 4
0 1  3 0  7
0 0 0 1  2
0 0 0 0 03
77511.AD2
6641 2  5 0  1
2 5  8 4 3
 3 9 9  7 2
3 10  7 11 73
775
2
6641 2  5 0  1
0 1 2 4 5
0 0 0 1 2
0 0 0 0 03
775
12.AD2
6641 2  4 3 3
5 10  9 7 8
4 8  9 2 7
 2 4 5 0  63
775
2
6641 2  4 3 3
0 0 1  2 0
0 0 0 0  5
0 0 0 0 03
775
In Exercises 13 and 14, Ô¨Ånd a basis for the subspace spanned by
the given vectors. What is the dimension of the subspace?
13.2
6641
 3
2
 43
775;2
664 3
9
 6
123
775;2
6642
 1
4
23
775;2
664 4
5
 3
73
775
14.2
6641
 1
 2
53
775;2
6642
 3
 1
63
775;2
6640
2
 6
83
775;2
664 1
4
 7
73
775;2
6643
 8
9
 53
775
15. Suppose a 35matrix Ahas three pivot columns. Is Col
ADR3? Is Nul ADR2? Explain your answers.
16. Suppose a 47matrix Ahas three pivot columns. Is Col
ADR3? What is the dimension of Nul A? Explain your
answers.
In Exercises 17 and 18, mark each statement True or False. Justify
each answer. Here Ais anmnmatrix.
17. a.IfBD fv1; : : : ; vpgis a basis for a subspace Hand if
xDc1v1C  C cpvp;then c1; : : : ; c pare the coordi-
nates of xrelative to the basis B.
b.Each line in Rnis a one-dimensional subspace of Rn.
c.The dimension of Col Ais the number of pivot columns
ofA.
d.The dimensions of Col Aand Nul Aadd up to the number
of columns of A.
e.If a set of pvectors spans a p-dimensional subspace Hof
Rn, then these vectors form a basis for H.
18. a.IfBis a basis for a subspace H, then each vector in Hcan
be written in only one way as a linear combination of the
vectors in B.
b.IfBD fv1; : : : ; vpgis a basis for a subspace HofRn, then
the correspondence x7!¬åx¬çBmakes Hlook and act the
same as Rp.
SECOND REVISED PAGES


--- Page 178 ---
2.9 Dimension and Rank 161
c.The dimension of Nul Ais the number of variables in the
equation AxD0.
d.The dimension of the column space of Ais rank A.
e.IfHis ap-dimensional subspace of Rn, then a linearly
independent set of pvectors in His a basis for H.
In Exercises 19‚Äì24, justify each answer or construction.
19. If the subspace of all solutions of AxD0has a basis con-
sisting of three vectors and if Ais a57matrix, what is the
rank of A?
20. What is the rank of a 45matrix whose null space is three-
dimensional?
21. If the rank of a 76matrix Ais 4, what is the dimension of
the solution space of AxD0?
22. Show that a set of vectors fv1;v2;‚Ä¶;v5ginRnis linearly
dependent when dim Span fv1;v2;‚Ä¶;v5g D4.
23. If possible, construct a 34matrix Asuch that dim
NulAD2 and dim Col AD2.
24. Construct a 43matrix with rank l.
25. Let Abe an npmatrix whose column space is p-
dimensional. Explain why the columns of Amust be linearly
independent.
26. Suppose columns 1, 3, 5, and 6 of a matrix Aare linearly
independent (but are not necessarily pivot columns) and the
rank of Ais 4. Explain why the four columns mentioned must
be a basis for the column space of A.27. Suppose vectors b1; : : : ; bpspan a subspace W, and let
fa1; : : : ; aqgbe any set in Wcontaining more than p
vectors. Fill in the details of the following argument to
show that fa1; : : : ; aqgmust be linearly dependent. First, let
BD¬åb1bp¬çandAD¬åa1aq¬ç.
a.Explain why for each vector aj, there exists a vector cj
inRpsuch that ajDBcj.
b.LetC=¬åc1cq¬ç. Explain why there is a nonzero
vector usuch that CuD0.
c.Use BandCto show that AuD0. This shows that the
columns of Aare linearly dependent.
28. Use Exercise 27 to show that if AandBare bases for a
subspace WofRn, thenAcannot contain more vectors than
B, and, conversely, Bcannot contain more vectors than A.
29. [M] Let H= Span fv1;v2gandB=fv1;v2g. Show that xis
inH, and Ô¨Ånd the B-coordinate vector of x, when
v1D2
66411
 5
10
73
775;v2D2
66414
 8
13
103
775;xD2
66419
 13
18
153
775
30. [M] Let HDSpanfv1;v2;v3gandBDfv1;v2;v3g. Show that
Bis a basis for Handxis in H, and Ô¨Ånd the B-coordinate
vector of x, when
v1D2
664 6
4
 9
43
775;v2D2
6648
 3
7
 33
775;v3D2
664 9
5
 8
33
775;xD2
6644
7
 8
33
775
SG
Mastering: Dimension and Rank 2‚Äì41
SOLUTIONS TO PRACTICE PROBLEMS
1.Construct AD¬åv1v2v3¬çso that the subspace spanned by v1;v2;v3is the column
space of A. A basis for this space is provided by the pivot columns of A.
AD2
42 3  1
 8 7 6
6 1 73
52
42 3  1
0 5 2
0 10 43
52
42 3  1
0 5 2
0 0 03
5
The Ô¨Årst two columns of Aare pivot columns and form a basis for H. Thus
Col A 
v1
v2 v30
dimHD2:
2.If¬åx¬çBD3
2
, then xis formed from a linear combination of the basis vectors using
weights 3 and 2:
xD3b1C2b2D31
:2
C2:2
1
D3:4
2:6
The basis fb1;b2gdetermines a coordinate system forR2, illustrated by the grid in
the Ô¨Ågure. Note how xis 3 units in the b1-direction and 2 units in the b2-direction.
b1b2x
11
SECOND REVISED PAGES


--- Page 179 ---
162 CHAPTER 2 Matrix Algebra
3.A four-dimensional subspace would contain a basis of four linearly independent
vectors. This is impossible inside R3:Since any linearly independent set in R3has
no more than three vectors, any subspace of R3has dimension no more than 3. The
spaceR3itself is the only three-dimensional subspace of R3. Other subspaces of R3
have dimension 2, 1, or 0.
CHAPTER 2 SUPPLEMENTARY EXERCISES
1.Assume that the matrices mentioned in the statements below
have appropriate sizes. Mark each statement True or False.
Justify each answer.
a.IfAandBaremn, then both ABTandATBare
deÔ¨Åned.
b.IfABDCandChas 2 columns, then Ahas 2 columns.
c.Left-multiplying a matrix Bby a diagonal matrix A, with
nonzero entries on the diagonal, scales the rows of B.
d.IfBCDBD, then CDD.
e.IfACD0, then either AD0orCD0.
f.IfAandBarenn, then .ACB/.A B/DA2 B2.
g.An elementary nnmatrix has either nornC1
nonzero entries.
h.The transpose of an elementary matrix is an elementary
matrix.
i.An elementary matrix must be square.
j.Every square matrix is a product of elementary matrices.
k.IfAis a 33matrix with three pivot positions,
there exist elementary matrices E1; : : : ; E psuch that
EpE1ADI.
l.IfABDI, then Ais invertible.
m.IfAandBare square and invertible, then ABis invert-
ible, and .AB/ 1DA 1B 1.
n.IfABDBAand if Ais invertible, then A 1BDBA 1.
o.IfAis invertible and if r¬§0, then .rA/ 1DrA 1.
p.IfAis a33matrix and the equation AxD2
41
0
03
5has
a unique solution, then Ais invertible.
2.Find the matrix Cwhose inverse is C 1D4 5
6 7
.
3.LetAD2
40 0 0
1 0 0
0 1 03
5. Show that A3D0. Use matrix
algebra to compute the product .I A/.ICACA2/.
4.Suppose AnD0for some n > 1 . Find an inverse for I A.
5.Suppose an nnmatrix AsatisÔ¨Åes the equation
A2 2ACID0. Show that A3D3A 2I and
A4D4A 3I.6.LetAD1 0
0 1
,BD0 1
1 0
. These are Pauli spin
matrices used in the study of electron spin in quantum
mechanics. Show that A2DI,B2DI, and ABD  BA.
Matrices such that ABD  BAare said to anticommute .
7.LetAD2
41 3 8
2 4 11
1 2 53
5andBD2
4 3 5
1 5
3 43
5. Compute
A 1Bwithout computing A 1. [Hint: A 1Bis the solution
of the equation AXDB.]
8.Find a matrix Asuch that the transformation x7!Axmaps1
3
and2
7
into1
1
and3
1
, respectively. [ Hint: Write
a matrix equation involving A, and solve for A.]
9.Suppose ABD5 4
 2 3
andBD7 3
2 1
. Find A.
10. Suppose Ais invertible. Explain why ATAis also invertible.
Then show that A 1D.ATA/ 1AT.
11.Letx1; : : : ; x nbe Ô¨Åxed numbers. The matrix below, called
aVandermonde matrix , occurs in applications such as
signal processing, error-correcting codes, and polynomial
interpolation.
VD2
66641 x 1x2
1 xn 1
1
1 x 2x2
2 xn 1
2::::::::::::
1 x nx2
n xn 1
n3
7775
Given yD.y1; : : : ; y n/inRn, suppose cD.c0; : : : ; c n 1/in
RnsatisÔ¨Åes VcDy, and deÔ¨Åne the polynomial
p.t/Dc0Cc1tCc2t2C  C cn 1tn 1:
a.Show that p.x 1/Dy1; : : : ; p.x n/Dyn. We call
p.t/ aninterpolating polynomial for the points
.x1; y1/; : : : ; .x n; yn/because the graph of p.t/ passes
through the points.
b.Suppose x1; : : : ; x nare distinct numbers. Show that the
columns of Vare linearly independent. [ Hint: How many
zeros can a polynomial of degree n 1have?]
c.Prove: ‚ÄúIf x1; : : : ; x nare distinct numbers, and y1; : : : ; y n
are arbitrary numbers, then there is an interpolating poly-
nomial of degree n 1for.x1; y1/; : : : ; .x n; yn/.‚Äù
12. LetADLU, where Lis an invertible lower triangular ma-
trix and Uis upper triangular. Explain why the Ô¨Årst column
SECOND REVISED PAGES


--- Page 180 ---
Chapter 2 Supplementary Exercises 163
ofAis a multiple of the Ô¨Årst column of L. How is the second
column of Arelated to the columns of L?
13. Given uinRnwith uTuD1, letPDuuT(an outer product)
andQDI 2P. Justify statements (a), (b), and (c).
a.P2DP b.PTDP c.Q2DI
The transformation x7!Pxis called a projection , and
x7!Qxis called a Householder reÔ¨Çection . Such reÔ¨Çections
are used in computer programs to create multiple zeros in a
vector (usually a column of a matrix).
14. LetuD2
40
0
13
5andxD2
41
5
33
5. Determine PandQas in
Exercise 13, and compute PxandQx. The Ô¨Ågure shows that
Qxis the reÔ¨Çection of xthrough the x1x2-plane.
Pxx3
x1x2ux
x /H11002 Px
Qx
A Householder reÔ¨Çection through the plane
x3D0.15. Suppose CDE3E2E1B, where E1,E2, and E3are elemen-
tary matrices. Explain why Cis row equivalent to B.
16. LetAbe an nnsingular matrix. Describe how to construct
annnnonzero matrix Bsuch that ABD0:
17. LetAbe a64matrix and Ba46matrix. Show that the
66matrix ABcannot be invertible.
18. Suppose Ais a53matrix and there exists a 35matrix
Csuch that CADI3. Suppose further that for some given b
inR5, the equation AxDbhas at least one solution. Show
that this solution is unique.
19. [M] Certain dynamical systems can be studied by examining
powers of a matrix, such as those below. Determine what
happens to AkandBkaskincreases (for example, try
kD2; : : : ; 16/ . Try to identify what is special about Aand
B. Investigate large powers of other matrices of this type, and
make a conjecture about such matrices.
AD2
4:4 :2 :3
:3 :6 :3
:3 :2 :43
5; BD2
40 :2 :3
:1 :6 :3
:9 :2 :43
5
20. [M] Let Anbe the nnmatrix with 0‚Äôs on the main diagonal
and 1‚Äôs elsewhere. Compute A 1
nfornD4, 5, and 6, and
make a conjecture about the general form of A 1
nfor larger
values of n.
SECOND REVISED PAGES


--- Page 182 ---
3Determinants
WEB
INTRODUCTORY EXAMPLE
Random Paths and Distortion
In his autobiographical book, Surely You‚Äôre Joking,
Mr. Feynman , the Nobel Prize‚Äìwinning physicist Richard
Feynman tells of observing ants in his Princeton graduate
school apartment. He studied the ants‚Äô behavior by pro-
viding paper ferries to sugar suspended on a string where
the ants would not accidentally Ô¨Ånd it. When an ant would
step onto a paper ferry, Feynman would transport the ant
to the food and then back. After the ants learned to use
the ferry, he relocated the return landing. The colony soon
confused the outbound and return ferry landings, indicating
that their ‚Äúlearning‚Äù consisted of creating and following
trails. Feynman conÔ¨Årmed this conjecture by laying glass
slides on the Ô¨Çoor. Once the ants established trails on the
glass slides, he rearranged the slides and therefore the
trails on them. The ants followed the repositioned trails
and Feynman could direct the ants where he wished.
Suppose Feynman had decided to conduct additional
investigations using a globe built of wire mesh on which
an ant must follow individual wires and choose between
going left and right at each intersection. If several ants and
an equal number of food sources are placed on the globe,
how likely is it that each ant would Ô¨Ånd its own food source
rather than encountering another ant‚Äôs trail and following
it to a shared resource?1
1The solution to the ant-path problem (and two other applications) can
be found in a June 2005 Mathematical Monthly article by Arthur
Benjamin and Naomi Cameron.In order to record the actual routes of the ants and to
communicate the results to others, it is convenient to use
a rectangular map of the globe. There are many ways to
create such maps. One simple way is to use the longitude
and latitude on the globe as xandycoordinates on the map.
As is the case with all maps, the result is not a faithful
representation of the globe. Features near the ‚Äúequator‚Äù
look much the same on the globe and the map, but regions
near the ‚Äúpoles‚Äù of the globe are distorted. Images of polar
regions are much larger than the images of similar sized
regions near the equator. To Ô¨Åt in with its surroundings on
the map, the image of an ant near one of the poles should
be larger than one near the equator. How much larger?
Surprisingly, both the ant-path and the area distortion
problems are best answered through the use of the deter-
minant, the subject of this chapter. Indeed, the determinant
has so many uses that a summary of the applications known
in the early 1900‚Äôs Ô¨Ålled a four-volume treatise by Thomas
Muir. With changes in emphasis and the greatly increased
sizes of the matrices used in modern applications, many
uses that were important then are no longer critical today.
Nevertheless, the determinant still plays an important role.
SECOND REVISED PAGES
165

--- Page 183 ---
166 CHAPTER 3 Determinants
Beyond introducing the determinant in Section 3.1, this chapter presents two important
ideas. Section 3.2 derives an invertibility criterion for a square matrix that plays a pivotal
role in Chapter 5. Section 3.3 shows how the determinant measures the amount by which
a linear transformation changes the area of a Ô¨Ågure. When applied locally, this technique
answers the question of a map‚Äôs expansion rate near the poles. This idea plays a critical
role in multivariable calculus in the form of the Jacobian.
3.1 INTRODUCTION TO DETERMINANTS
Recall from Section 2.2 that a 22matrix is invertible if and only if its determinant
is nonzero. To extend this useful fact to larger matrices, we need a deÔ¨Ånition for the
determinant of an nnmatrix. We can discover the deÔ¨Ånition for the 33case by
watching what happens when an invertible 33matrix Ais row reduced.
Consider AD¬åaij¬çwith a11¬§0. If we multiply the second and third rows of Aby
a11and then subtract appropriate multiples of the Ô¨Årst row from the other two rows, we
Ô¨Ånd that Ais row equivalent to the following two matrices:
2
4a11 a12 a13
a11a21a11a22a11a23
a11a31a11a32a11a333
52
4a11 a12 a13
0 a 11a22 a12a21a11a23 a13a21
0 a 11a32 a12a31a11a33 a13a313
5 (1)
Since Ais invertible, either the .2; 2/ -entry or the .3; 2/ -entry on the right in (1) is
nonzero. Let us suppose that the .2; 2/ -entry is nonzero. (Otherwise, we can make a
row interchange before proceeding.) Multiply row 3 by a11a22 a12a21, and then to the
new row 3 add  .a11a32 a12a31/times row 2. This will show that
A2
4a11 a12 a13
0 a 11a22 a12a21 a11a23 a13a21
0 0 a11¬Å3
5
where
¬ÅDa11a22a33Ca12a23a31Ca13a21a32 a11a23a32 a12a21a33 a13a22a31(2)
Since Ais invertible, ¬Åmust be nonzero. The converse is true, too, as we will see in
Section 3.2. We call ¬Åin (2) the determinant of the 33matrix A.
Recall that the determinant of a 22matrix, AD¬åaij¬ç, is the number
detADa11a22 a12a21
For a 11matrix‚Äîsay, AD¬åa11¬ç‚Äîwe deÔ¨Åne det ADa11. To generalize the deÔ¨Åni-
tion of the determinant to larger matrices, we‚Äôll use 22determinants to rewrite the
33determinant ¬Ådescribed above. Since the terms in ¬Åcan be grouped as
.a11a22a33 a11a23a32/ .a12a21a33 a12a23a31/C.a13a21a32 a13a22a31/,
¬ÅDa11deta22 a23
a32 a33
 a12deta21 a23
a31 a33
Ca13deta21 a22
a31 a32
For brevity, write
¬ÅDa11detA11 a12detA12Ca13detA13 (3)
where A11,A12, and A13are obtained from Aby deleting the Ô¨Årst row and one of the
three columns. For any square matrix A, letAijdenote the submatrix formed by deleting
SECOND REVISED PAGES


--- Page 184 ---
3.1 Introduction to Determinants 167
theith row and jth column of A. For instance, if
AD2
6641 2 5 0
2 0 4  1
3 1 0 7
0 4  2 03
775
thenA32is obtained by crossing out row 3 and column 2,
2
6641 2 5 0
2 0 4  1
3 1 0 7
0 4  2 03
775
so that
A32D2
41 5 0
2 4  1
0 2 03
5
We can now give a recursive deÔ¨Ånition of a determinant. When nD3, detAis deÔ¨Åned
using determinants of the 22submatrices A1j, as in (3) above. When nD4, detA
uses determinants of the 33submatrices A1j. In general, an nndeterminant is
deÔ¨Åned by determinants of .n 1/.n 1/submatrices.
D E F I N I T I O N Forn2, the determinant of an nnmatrix AD¬åaij¬çis the sum of nterms
of the form a1jdetA1j, with plus and minus signs alternating, where the entries
a11; a12; : : : ; a 1nare from the Ô¨Årst row of A. In symbols,
detADa11detA11 a12detA12C    C . 1/1Cna1ndetA1n
DnX
jD1. 1/1Cja1jdetA1j
EXAMPLE 1 Compute the determinant of
AD2
41 5 0
2 4  1
0 2 03
5
SOLUTION Compute det ADa11detA11 a12detA12Ca13detA13:
detAD1det4 1
 2 0
 5det2 1
0 0
C0det2 4
0 2
D1.0 2/ 5.0 0/C0. 4 0/D  2
Another common notation for the determinant of a matrix uses a pair of vertical
lines in place of brackets. Thus the calculation in Example 1 can be written as
detAD14 1
 2 0 52 1
0 0C02 4
0 2D    D   2
To state the next theorem, it is convenient to write the deÔ¨Ånition of det Ain a slightly
different form. Given AD¬åaij¬ç, the .i; j / -cofactor ofAis the number Cijgiven by
CijD. 1/iCjdetAij (4)
Then
detADa11C11Ca12C12C    C a1nC1n
SECOND REVISED PAGES


--- Page 185 ---
168 CHAPTER 3 Determinants
This formula is called a cofactor expansion across the Ô¨Årst row ofA. We omit the
proof of the following fundamental theorem to avoid a lengthy digression.
T H E O R E M 1 The determinant of an nnmatrix Acan be computed by a cofactor expansion
across any row or down any column. The expansion across the ith row using the
cofactors in (4) is
detADai1Ci1Cai2Ci2C    C ainCin
The cofactor expansion down the jth column is
detADa1jC1jCa2jC2jC    C anjCnj
The plus or minus sign in the .i; j / -cofactor depends on the position of aijin the
matrix, regardless of the sign of aijitself. The factor . 1/iCjdetermines the following
checkerboard pattern of signs:2
6664C   C   
  C  
C   C
::::::3
7775
EXAMPLE 2 Use a cofactor expansion across the third row to compute det A, where
AD2
41 5 0
2 4  1
0 2 03
5
SOLUTION Compute
detADa31C31Ca32C32Ca33C33
D. 1/3C1a31detA31C. 1/3C2a32detA32C. 1/3C3a33detA33
D05 0
4 1 . 2/1 0
2 1C01 5
2 4
D0C2. 1/C0D  2
Theorem 1 is helpful for computing the determinant of a matrix that contains many
zeros. For example, if a row is mostly zeros, then the cofactor expansion across that row
has many terms that are zero, and the cofactors in those terms need not be calculated.
The same approach works with a column that contains many zeros.
EXAMPLE 3 Compute det A, where
AD2
666643 7 8 9  6
0 2  5 7 3
0 0 1 5 0
0 0 2 4  1
0 0 0  2 03
77775
SOLUTION The cofactor expansion down the Ô¨Årst column of Ahas all terms equal to
zero except the Ô¨Årst. Thus
detAD32 5 7 3
0 1 5 0
0 2 4  1
0 0  2 0C0C21C0C31C0C41C0C51
SECOND REVISED PAGES


--- Page 186 ---
3.1 Introduction to Determinants 169
Henceforth we will omit the zero terms in the cofactor expansion. Next, expand this
44determinant down the Ô¨Årst column, in order to take advantage of the zeros there.
We have
detAD321 5 0
2 4  1
0 2 0
This 33determinant was computed in Example 1 and found to equal  2. Hence
detAD32. 2/D  12.
The matrix in Example 3 was nearly triangular. The method in that example is easily
adapted to prove the following theorem.
T H E O R E M 2 IfAis a triangular matrix, then det Ais the product of the entries on the main
diagonal of A.
The strategy in Example 3 of looking for zeros works extremely well when an entire
row or column consists of zeros. In such a case, the cofactor expansion along such a row
or column is a sum of zeros! So the determinant is zero. Unfortunately, most cofactor
expansions are not so quickly evaluated.
N U M E R I C A L N O T E
By today‚Äôs standards, a 2525matrix is small. Yet it would be impossible
to calculate a 2525determinant by cofactor expansion. In general, a cofac-
tor expansion requires more than n¬ämultiplications, and 25¬äis approximately
1:51025.
If a computer performs one trillion multiplications per second, it would have
to run for more than 500,000 years to compute a 2525determinant by this
method. Fortunately, there are faster methods, as we‚Äôll soon discover.
Exercises 19‚Äì38 explore important properties of determinants, mostly for the 22
case. The results from Exercises 33‚Äì36 will be used in the next section to derive the
analogous properties for nnmatrices.
PRACTICE PROBLEM
Compute5 7 2 2
0 3 0  4
 5 8 0 3
0 5 0  6.
3.1 EXERCISES
Compute the determinants in Exercises 1‚Äì8 using a cofactor
expansion across the Ô¨Årst row. In Exercises 1‚Äì4, also compute the
determinant by a cofactor expansion down the second column.
1.3 0 4
2 3 2
0 5  12.0 4 1
5 3 0
2 3 13.2 2 3
3 1 2
1 3  14.1 2 4
3 1 1
2 4 2
5.2 3  3
4 0 3
6 1 56.5 2 2
0 3  3
2 4 7
SECOND REVISED PAGES


--- Page 187 ---
170 CHAPTER 3 Determinants
7.4 3 0
6 5 2
9 7 38.4 1 2
4 0 3
3 2 5
Compute the determinants in Exercises 9‚Äì14 by cofactor expan-
sions. At each step, choose a row or column that involves the least
amount of computation.
9.4 0 0 5
1 7 2  5
3 0 0 0
8 3 1 710.1 2 5 2
0 0 3 0
2 4 3 5
2 0 3 5
11.3 5  6 4
0 2 3  3
0 0 1 5
0 0 0 312.3 0 0 0
7 2 0 0
2 6 3 0
3 8 4  3
13.4 0  7 3  5
0 0 2 0 0
7 3  6 4  8
5 0 5 2  3
0 0 9  1 2
14.6 3 2 4 0
9 0  4 1 0
8 5 6 7 1
2 0 0 0 0
4 2 3 2 0
The expansion of a 33determinant can be remembered by the
following device. Write a second copy of the Ô¨Årst two columns to
the right of the matrix, and compute the determinant by multiply-
ing entries on six diagonals:
a11a12a13a11a12
a21a22a23a21a22
a31a32a33a31a32‚Äì‚Äì‚Äì
+++
Add the downward diagonal products and subtract the up-
ward products. Use this method to compute the determinants in
Exercises 15‚Äì18. Warning: This trick does not generalize in any
reasonable way to 44or larger matrices.
15.1 0 4
2 3 2
0 5  216.0 3 1
4 5 0
3 4 1
17.2 3 3
3 2 2
1 3  118.1 3 4
2 3 1
3 3 2
In Exercises 19‚Äì24, explore the effect of an elementary row
operation on the determinant of a matrix. In each case, state the
row operation and describe how it affects the determinant.
19.a b
c d
,c d
a b20.a b
c d
,aCkc b Ckd
c d
21.a b
c d
,a b
kc kd
22.3 2
5 4
,3 2
5C3k 4 C2k
23.2
4a b c
3 2 1
4 5 63
5,2
43 2 1
a b c
4 5 63
5
24.2
41 0 1
 3 4  4
2 3 13
5,2
4k 0 k
 3 4  4
2 3 13
5
Compute the determinants of the elementary matrices given in
Exercises 25‚Äì30. (See Section 2.2.)
25.2
41 0 0
0 1 0
0 k 13
5 26.2
40 0 1
0 1 0
1 0 03
5
27.2
41 0 0
0 1 0
k 0 13
5 28.2
4k 0 0
0 1 0
0 0 13
5
29.2
41 0 0
0 k 0
0 0 13
5 30.2
40 1 0
1 0 0
0 0 13
5
Use Exercises 25‚Äì28 to answer the questions in Exercises 31
and 32. Give reasons for your answers.
31. What is the determinant of an elementary row replacement
matrix?
32. What is the determinant of an elementary scaling matrix with
kon the diagonal?
In Exercises 33‚Äì36, verify that det EAD.detE/.detA/, where
Eis the elementary matrix shown and ADa b
c d
.
33.1 k
0 1
34.1 0
k 1
35.0 1
1 0
36.1 0
0 k
37. LetAD3 1
4 2
. Write 5A. Is det 5AD5detA?
38. LetADa b
c d
and let kbe a scalar. Find a formula that
relates det kAtokand det A.
In Exercises 39 and 40, Ais an nnmatrix. Mark each statement
True or False. Justify each answer.
39. a.Annndeterminant is deÔ¨Åned by determinants of
.n 1/.n 1/submatrices.
b.The.i; j / -cofactor of a matrix Ais the matrix Aijob-
tained by deleting from Aitsith row and jth column.
SECOND REVISED PAGES


--- Page 188 ---
3.2 Properties of Determinants 171
40. a.The cofactor expansion of det Adown a column is equal
to the cofactor expansion along a row.
b.The determinant of a triangular matrix is the sum of the
entries on the main diagonal.
41. LetuD3
0
andvD1
2
. Compute the area of the par-
allelogram determined by u,v,uCv, and 0, and compute
the determinant of ¬åu v ¬ç. How do they compare? Replace
the Ô¨Årst entry of vby an arbitrary number x, and repeat the
problem. Draw a picture and explain what you Ô¨Ånd.
42. LetuDa
b
andvDc
0
, where a,b, and care positive
(for simplicity). Compute the area of the parallelogram deter-
mined by u,v,uCv, and 0, and compute the determinants of
the matrices ¬åu v ¬çand¬åv u ¬ç. Draw a picture and explain
what you Ô¨Ånd.
43. [M] Construct a random 44matrix Awith integer en-
tries between  9and 9. How is det A 1related to det A?
Experiment with random nninteger matrices for nD4,5, and 6, and make a conjecture. Note: In the unlikely event
that you encounter a matrix with a zero determinant, reduce
it to echelon form and discuss what you Ô¨Ånd.
44. [M] Is it true that det ABD.detA/.detB/? To Ô¨Ånd out,
generate random 55matrices AandB, and compute
detAB .detAdetB/. Repeat the calculations for three
other pairs of nnmatrices, for various values of n. Report
your results.
45. [M] Is it true that det .ACB/DdetACdetB? Experiment
with four pairs of random matrices as in Exercise 44, and
make a conjecture.
46. [M] Construct a random 44matrix Awith integer entries
between  9and 9, and compare det Awith det AT, det. A/,
det.2A/ , and det .10A/ . Repeat with two other random 44
integer matrices, and make conjectures about how these de-
terminants are related. (Refer to Exercise 36 in Section 2.1.)
Then check your conjectures with several random 55and
66integer matrices. Modify your conjectures, if necessary,
and report your results.
SOLUTION TO PRACTICE PROBLEM
Take advantage of the zeros. Begin with a cofactor expansion down the third column to
obtain a 33matrix, which may be evaluated by an expansion down its Ô¨Årst column.
5 7 2 2
0 3 0  4
 5 8 0 3
0 5 0  6D. 1/1C320 3  4
 5 8 3
0 5  6
D2. 1/2C1. 5/3 4
5 6D20
The. 1/2C1in the next-to-last calculation came from the .2; 1/ -position of the  5in
the33determinant.
3.2 PROPERTIES OF DETERMINANTS
The secret of determinants lies in how they change when row operations are performed.
The following theorem generalizes the results of Exercises 19‚Äì24 in Section 3.1. The
proof is at the end of this section.
T H E O R E M 3 Row Operations
LetAbe a square matrix.
a.If a multiple of one row of Ais added to another row to produce a matrix B,
then det BDdetA.
b.If two rows of Aare interchanged to produce B, then det BD   detA.
c.If one row of Ais multiplied by kto produce B, then det BDkdetA.
SECOND REVISED PAGES


--- Page 189 ---
172 CHAPTER 3 Determinants
The following examples show how to use Theorem 3 to Ô¨Ånd determinants
efÔ¨Åciently.
EXAMPLE 1 Compute det A, where AD2
41 4 2
 2 8  9
 1 7 03
5.
SOLUTION The strategy is to reduce Ato echelon form and then to use the fact that the
determinant of a triangular matrix is the product of the diagonal entries. The Ô¨Årst two
row replacements in column 1 do not change the determinant:
detAD1 4 2
 2 8  9
 1 7 0D1 4 2
0 0  5
 1 7 0D1 4 2
0 0  5
0 3 2
An interchange of rows 2 and 3 reverses the sign of the determinant, so
detAD  1 4 2
0 3 2
0 0  5D  .1/.3/.  5/D15
A common use of Theorem 3(c) in hand calculations is to factor out a common
multiple of one row of a matrix. For instance,
  
5k 2k 3k
  Dk  
5 2 3
  
where the starred entries are unchanged. We use this step in the next example.
EXAMPLE 2 Compute det A, where AD2
6642 8 6 8
3 9 5 10
 3 0 1  2
1 4 0 63
775.
SOLUTION To simplify the arithmetic, we want a 1 in the upper-left corner. We could
interchange rows 1 and 4. Instead, we factor out 2 from the top row, and then proceed
with row replacements in the Ô¨Årst column:
detAD21 4 3 4
3 9 5 10
 3 0 1  2
1 4 0 6D21 4 3 4
0 3  4 2
0 12 10 10
0 0  3 2
Next, we could factor out another 2 from row 3 or use the 3 in the second column as a
pivot. We choose the latter operation, adding 4 times row 2 to row 3:
detAD21 4 3 4
0 3  4 2
0 0  6 2
0 0  3 2
Finally, adding  1=2times row 3 to row 4, and computing the ‚Äútriangular‚Äù determinant,
we Ô¨Ånd that
detAD21 4 3 4
0 3  4 2
0 0  6 2
0 0 0 1D2.1/.3/.  6/.1/D  36
SECOND REVISED PAGES


--- Page 190 ---
3.2 Properties of Determinants 173
Suppose a square matrix Ahas been reduced to an echelon form Uby row replace-
U = 
det U ‚â† 00
00*
0
0*
*
0*
**
U = 
det U = 00
00*
0
0*
*
0
0*
*
0
FIGURE 1
Typical echelon forms of square
matrices.ments and row interchanges. (This is always possible. See the row reduction algorithm
in Section 1.2.) If there are rinterchanges, then Theorem 3 shows that
detAD. 1/rdetU
Since Uis in echelon form, it is triangular, and so det Uis the product of the
diagonal entries u11; : : : ; u nn. IfAis invertible, the entries uiiare all pivots (because
AInand the uiihave not been scaled to 1‚Äôs). Otherwise, at least unnis zero, and the
product u11  unnis zero. See Figure 1. Thus
detAD8
¬à<
¬à:. 1/r 
product of
pivots in U!
when Ais invertible
0 when Ais not invertible(1)
It is interesting to note that although the echelon form Udescribed above is not unique
(because it is not completely row reduced), and the pivots are not unique, the product
of the pivots isunique, except for a possible minus sign.
Formula (1) not only gives a concrete interpretation of det Abut also proves the
main theorem of this section:
T H E O R E M 4 A square matrix Ais invertible if and only if det A¬§0.
Theorem 4 adds the statement ‚Äúdet A¬§0‚Äù to the Invertible Matrix Theorem. A
useful corollary is that det AD0when the columns of Aare linearly dependent. Also,
detAD0when the rows ofAare linearly dependent. (Rows of Aare columns of AT,
and linearly dependent columns of ATmake ATsingular. When ATis singular, so is A,
by the Invertible Matrix Theorem.) In practice, linear dependence is obvious when two
columns or two rows are the same or a column or a row is zero.
EXAMPLE 3 Compute det A, where AD2
6643 1 2  5
0 5  3 6
 6 7  7 4
 5 8 0 93
775.
SOLUTION Add 2 times row 1 to row 3 to obtain
detADdet2
6643 1 2  5
0 5  3 6
0 5  3 6
 5 8 0 93
775D0
because the second and third rows of the second matrix are equal.
N U M E R I C A L N O T E S
1.Most computer programs that compute det Afor a general matrix Ause the
method of formula (1) above.
2.It can be shown that evaluation of an nndeterminant using row operations
requires about 2n3=3arithmetic operations. Any modern microcomputer can
calculate a 2525determinant in a fraction of a second, since only about
10,000 operations are required.
WEB
SECOND REVISED PAGES


--- Page 191 ---
174 CHAPTER 3 Determinants
Computers can also handle large ‚Äúsparse‚Äù matrices, with special routines that take
advantage of the presence of many zeros. Of course, zero entries can speed hand compu-
tations, too. The calculations in the next example combine the power of row operations
with the strategy from Section 3.1 of using zero entries in cofactor expansions.
EXAMPLE 4 Compute det A, where AD2
6640 1 2  1
2 5  7 3
0 3 6 2
 2 5 4  23
775.
SOLUTION A good way to begin is to use the 2 in column 1 as a pivot, eliminating
the 2below it. Then use a cofactor expansion to reduce the size of the determinant,
followed by another row replacement operation. Thus
detAD0 1 2  1
2 5  7 3
0 3 6 2
0 0  3 1D  21 2  1
3 6 2
0 3 1D  21 2  1
0 0 5
0 3 1
An interchange of rows 2 and 3 would produce a ‚Äútriangular determinant.‚Äù Another
approach is to make a cofactor expansion down the Ô¨Årst column:
detAD. 2/.1/0 5
 3 1D  2.15/D  30
Column Operations
We can perform operations on the columns of a matrix in a way that is analogous to the
row operations we have considered. The next theorem shows that column operations
have the same effects on determinants as row operations.
Remark: The Principle of Mathematical Induction says the following: Let P.n/ be a
statement that is either true or false for each natural number n. Then P.n/ is true for all
n1provided that P.1/ is true, and for each natural number k, ifP.k/ is true, then
P.kC1/is true. The Principle of Mathematical Induction is used to prove the next
theorem.
T H E O R E M 5 IfAis an nnmatrix, then det ATDdetA.
PROOF The theorem is obvious for nD1. Suppose the theorem is true for kk
determinants and let nDkC1. Then the cofactor of a1jinAequals the cofactor
ofaj1inAT, because the cofactors involve kkdeterminants. Hence the cofactor
expansion of det Aalong the Ô¨Årst rowequals the cofactor expansion of det ATdown the
Ô¨Årstcolumn . That is, AandAThave equal determinants. The theorem is true for nD1,
and the truth of the theorem for one value of nimplies its truth for the next value of n.
By the Principle of Mathematical Induction, the theorem is true for all n1.
Because of Theorem 5, each statement in Theorem 3 is true when the word rowis
replaced everywhere by column . To verify this property, one merely applies the original
Theorem 3 to AT. A row operation on ATamounts to a column operation on A.
Column operations are useful for both theoretical purposes and hand computations.
However, for simplicity we‚Äôll perform only row operations in numerical calculations.
SECOND REVISED PAGES


--- Page 192 ---
3.2 Properties of Determinants 175
Determinants and Matrix Products
The proof of the following useful theorem is at the end of the section. Applications are
in the exercises.
T H E O R E M 6 Multiplicative Property
IfAandBarennmatrices, then det ABD.detA/.detB/.
EXAMPLE 5 Verify Theorem 6 for AD6 1
3 2
andBD4 3
1 2
.
SOLUTION
ABD6 1
3 24 3
1 2
D25 20
14 13
and
detABD2513 2014D325 280D45
Since det AD9and det BD5,
.detA/.detB/D95D45DdetAB
Warning: A common misconception is that Theorem 6 has an analogue for sums of
matrices. However, det .ACB/isnotequal to det ACdetB, in general.
A Linearity Property of the Determinant Function
For an nnmatrix A, we can consider det Aas a function of the ncolumn vectors in
A. We will show that if all columns except one are held Ô¨Åxed, then det Ais alinear
function of that one (vector) variable.
Suppose that the jth column of Ais allowed to vary, and write
ADa1   aj 1 x a jC1   an
DeÔ¨Åne a transformation TfromRntoRby
T .x/Ddeta1   aj 1 x a jC1   an
Then,
T .cx/DcT .x/for all scalars cand all xinRn(2)
T .uCv/DT .u/CT .v/for all u,vinRn(3)
Property (2) is Theorem 3(c) applied to the columns of A. A proof of property (3)
follows from a cofactor expansion of det Adown the jth column. (See Exercise 43.)
This (multi-) linearity property of the determinant turns out to have many useful conse-
quences that are studied in more advanced courses.
Proofs of Theorems 3and 6
It is convenient to prove Theorem 3 when it is stated in terms of the elementary matrices
discussed in Section 2.2. We call an elementary matrix Earow replacement (matrix ) if
Eis obtained from the identity Iby adding a multiple of one row to another row; Eis
aninterchange ifEis obtained by interchanging two rows of I; and Eisa scale by r
ifEis obtained by multiplying a row of Iby a nonzero scalar r. With this terminology,
Theorem 3 can be reformulated as follows:
SECOND REVISED PAGES


--- Page 193 ---
176 CHAPTER 3 Determinants
IfAis an nnmatrix and Eis an nnelementary matrix, then
detEAD.detE/.detA/
where
detED8
¬à<
¬à:1ifEis a row replacement
 1ifEis an interchange
rifEis a scale by r
PROOF OF THEOREM 3 The proof is by induction on the size of A. The case of a
22matrix was veriÔ¨Åed in Exercises 33‚Äì36 of Section 3.1. Suppose the theorem has
been veriÔ¨Åed for determinants of kkmatrices with k2, letnDkC1, and let A
benn. The action of EonAinvolves either two rows or only one row. So we can
expand det EAacross a row that is unchanged by the action of E, say, row i. Let
Aij(respectively, Bij/be the matrix obtained by deleting row iand column jfrom
A(respectively, EA). Then the rows of Bijare obtained from the rows of Aijby the
same type of elementary row operation that Eperforms on A. Since these submatrices
are only kk, the induction assumption implies that
detBijDdetAij
where D1, 1, orr, depending on the nature of E. The cofactor expansion across
rowiis
detEADai1. 1/iC1detBi1C    C ain. 1/iCndetBin
Dai1. 1/iC1detAi1C    C ain. 1/iCndetAin
DdetA
In particular, taking ADIn, we see that det ED1, 1, orr, depending on the nature
ofE. Thus the theorem is true for nD2, and the truth of the theorem for one value of n
implies its truth for the next value of n. By the principle of induction, the theorem must
be true for n2. The theorem is trivially true for nD1.
PROOF OF THEOREM 6 IfAis not invertible, then neither is AB, by Exercise 27
in Section 2.3. In this case, det ABD.detA/.detB/, because both sides are zero, by
Theorem 4. If Ais invertible, then Aand the identity matrix Inare row equivalent by
the Invertible Matrix Theorem. So there exist elementary matrices E1; : : : ; E psuch that
ADEpEp 1  E1InDEpEp 1  E1
For brevity, write jAjfor det A. Then repeated application of Theorem 3, as rephrased
above, shows that
jABj D jEp  E1Bj D jEpjjEp 1  E1Bj D   
D jEpj    jE1jjBj D    D j Ep  E1jjBj
D jAjjBj
PRACTICE PROBLEMS
1.Compute1 3 1  2
2 5 1 2
0 4 5 1
 3 10  6 8in as few steps as possible.
SECOND REVISED PAGES


--- Page 194 ---
3.2 Properties of Determinants 177
2.Use a determinant to decide if v1,v2, and v3are linearly independent, when
v1D2
45
 7
93
5; v2D2
4 3
3
 53
5; v3D2
42
 7
53
5
3.LetAbe an nnmatrix such that A2DI. Show that det AD 1.
3.2 EXERCISES
Each equation in Exercises 1‚Äì4 illustrates a property of determi-
nants. State the property.
1.0 5  2
1 3 6
4 1 8D  1 3 6
0 5  2
4 1 8
2.1 2 2
0 3  4
3 7 4D1 2 2
0 3  4
0 1  2
3.3 6 9
3 5  5
1 3 3D31 2 3
3 5  5
1 3 3
4.1 3  4
2 0  3
3 5 2D1 3  4
0 6 5
3 5 2
Find the determinants in Exercises 5‚Äì10 by row reduction to
echelon form.
5.1 5  4
 1 4 5
 2 8 76.3 3  3
3 4  4
2 3 5
7.1 3 0 2
 2 5 7 4
3 5 2 1
1 1 2  38.1 3 2  4
0 1 2  5
2 7 6  3
 3 10 7 2
9.1 1 3 0
0 1 5 4
 1 0 5 3
3 3 2 3
10.1 3  1 0  2
0 2  4 2 6
 2 6 2 3 10
1 5  6 2  3
0 2  4 5 9
Combine the methods of row reduction and cofactor expansion to
compute the determinants in Exercises 11‚Äì14.
11.3 4  3 1
3 0 1  3
 6 0  4 3
6 8  4 112. 1 2 3 0
3 4 3 0
11 4 6 6
4 2 4 313.2 5 4 1
4 7 6 2
6 2 4 0
 6 7 7 014.1 5 4 1
0 2 4 0
3 5 4 1
 6 5 5 0
Find the determinants in Exercises 15‚Äì20, wherea b c
d e f
g h iD7:
15.a b c
d e f
3g 3h 3i16.a b c
5d 5e 5f
g h i
17.2
4aCd b Ce c Cf
d e f
g h i3
518.d e f
a b c
g h i
19.a b c
2dCa 2e Cb 2f Cc
g h i
20.a b c
dC3g e C3h f C3i
g h i
In Exercises 21‚Äì23, use determinants to Ô¨Ånd out if the matrix is
invertible.
21.2
42 6 0
1 3 2
3 9 23
5 22.2
45 1  1
1 3 2
0 5 33
5
23.2
6642 0 0 6
1 7 5 0
3 8 6 0
0 7 5 43
775
In Exercises 24‚Äì26, use determinants to decide if the set of vectors
is linearly independent.
24.2
44
6
23
5,2
4 7
0
73
5,2
4 3
 5
 23
5
25.2
47
 4
 63
5,2
4 8
5
73
5,2
47
0
 53
5
SECOND REVISED PAGES


--- Page 195 ---
178 CHAPTER 3 Determinants
26.2
6643
5
 6
43
775,2
6642
 6
0
73
775,2
664 2
 1
3
03
775,2
6640
0
0
 23
775
In Exercises 27 and 28, AandBarennmatrices. Mark each
statement True or False. Justify each answer.
27. a.A row replacement operation does not affect the determi-
nant of a matrix.
b.The determinant of Ais the product of the pivots in any
echelon form UofA, multiplied by . 1/r, where ris the
number of row interchanges made during row reduction
from AtoU.
c.If the columns of Aare linearly dependent, then
detAD0.
d.det.ACB/DdetACdetB.
28. a.If three row interchanges are made in succession, then the
new determinant equals the old determinant.
b.The determinant of Ais the product of the diagonal entries
inA.
c.If det Ais zero, then two rows or two columns are the
same, or a row or a column is zero.
d.detA 1D. 1/detA.
29. Compute det B4, where BD2
41 0 1
1 1 2
1 2 13
5.
30. Use Theorem 3 (but not Theorem 4) to show that if two rows
of a square matrix Aare equal, then det AD0. The same is
true for two columns. Why?
In Exercises 31‚Äì36, mention an appropriate theorem in your
explanation.
31. Show that if Ais invertible, then det A 1D1
detA.
32. Suppose that Ais a square matrix such that det A3D0.
Explain why Acannot be invertible.
33. LetAandBbe square matrices. Show that even though
ABand BAmay not be equal, it is always true that
detABDdetBA.
34. LetAandPbe square matrices, with Pinvertible. Show that
det.PAP 1/DdetA.
35. LetUbe a square matrix such that UTUDI. Show that
detUD 1.
36. Find a formula for det .rA/ when Ais an nnmatrix.
Verify that det ABD.detA/.detB/for the matrices in Exercises
37 and 38. (Do not use Theorem 6.)
37.AD3 0
6 1
,BD2 0
5 438.AD3 6
 1 2
,BD4 3
 1 3
39. LetAand Bbe33matrices, with det AD  3and
detBD4. Use properties of determinants (in the text and
in the exercises above) to compute:
a.detAB b. det 5A c. det BT
d.detA 1e. det A3
40. LetAand Bbe44matrices, with det AD  3and
detBD  1. Compute:
a.detAB b. det B5c. det 2A
d.detATBA e. det B 1AB
41. Verify that det ADdetBCdetC, where
ADaCe b Cf
c d
; BDa b
c d
; CDe f
c d
42. Let AD1 0
0 1
and BDa b
c d
. Show that
det.ACB/DdetACdetBif and only if aCdD0.
43. Verify that det ADdetBCdetC, where
AD2
4a11 a12 u1Cv1
a21 a22 u2Cv2
a31 a32 u3Cv33
5;
BD2
4a11 a12 u1
a21 a22 u2
a31 a32 u33
5; CD2
4a11 a12 v1
a21 a22 v2
a31 a32 v33
5
Note, however, that Aisnotthe same as BCC.
44. Right-multiplication by an elementary matrix Eaffects the
columns ofAin the same way that left-multiplication affects
therows . Use Theorems 5 and 3 and the obvious fact that ET
is another elementary matrix to show that
detAED.detE/.detA/
Do not use Theorem 6.
45. [M] Compute det ATAand det AATfor several random
45matrices and several random 56matrices. What can
you say about ATAandAATwhen Ahas more columns than
rows?
46. [M] If det Ais close to zero, is the matrix Anearly singular?
Experiment with the nearly singular 44matrix
AD2
6644 0  7 7
 6 1 11 9
7 5 10 19
 1 2 3  13
775
Compute the determinants of A,10A, and 0:1A . In contrast,
compute the condition numbers of these matrices. Repeat
these calculations when Ais the 44identity matrix. Dis-
cuss your results.
SECOND REVISED PAGES


--- Page 196 ---
3.3 Cramer's Rule, Volume, and Linear Transformations 179
SOLUTIONS TO PRACTICE PROBLEMS
1.Perform row replacements to create zeros in the Ô¨Årst column, and then create a row
of zeros.
1 3 1  2
2 5 1 2
0 4 5 1
 3 10  6 8D1 3 1  2
0 1  3 2
0 4 5 1
0 1  3 2D1 3 1  2
0 1  3 2
0 4 5 1
0 0 0 0D0
2.det¬åv1v2v3¬çD5 3 2
 7 3  7
9 5 5D5 3 2
 2 0  5
9 5 5Row 1 added
to row 2
D  . 3/ 2 5
9 5 . 5/5 2
 2 5Cofactors of
column 2
D3.35/C5. 21/D0
By Theorem 4, the matrix ¬åv1v2v3¬çis not invertible. The columns are linearly
dependent, by the Invertible Matrix Theorem.
3.Recall that det ID1. By Theorem 6, det .AA/ = (det A)(det A). Putting these two
observations together results in
1DdetIDdetA2Ddet.AA/ D.detA/.detA/D.detA/2
Taking the square root of both sides establishes that det AD 1.
3.3 CRAMER'S RULE, VOLUME, AND LINEAR TRANSFORMATIONS
This section applies the theory of the preceding sections to obtain important theoretical
formulas and a geometric interpretation of the determinant.
Cramer‚Äôs Rule
Cramer‚Äôs rule is needed in a variety of theoretical calculations. For instance, it can be
used to study how the solution of AxDbis affected by changes in the entries of b.
However, the formula is inefÔ¨Åcient for hand calculations, except for 22or perhaps
33matrices.
For any nnmatrix Aand any binRn, letAi.b/be the matrix obtained from A
by replacing column iby the vector b.
Ai.b/D¬åa1   b   an¬ç-
coli
T H E O R E M 7 Cramer's Rule
LetAbe an invertible nnmatrix. For any binRn, the unique solution xof
AxDbhas entries given by
xiDdetAi.b/
detA; i D1; 2; : : : ; n (1)
SECOND REVISED PAGES


--- Page 197 ---
180 CHAPTER 3 Determinants
PROOF Denote the columns of Abya1; : : : ; anand the columns of the nnidentity
matrix Ibye1; : : : ; en. IfAxDb, the deÔ¨Ånition of matrix multiplication shows that
AIi.x/DAe1   x   en
DAe1   Ax   Aen
Da1   b   an
DAi.b/
By the multiplicative property of determinants,
.detA/.detIi.x//DdetAi.b/
The second determinant on the left is simply xi. (Make a cofactor expansion along the
ith row.) Hence .detA/xiDdetAi.b/. This proves (1) because Ais invertible and
detA¬§0.
EXAMPLE 1 Use Cramer‚Äôs rule to solve the system
3x1 2x2D6
 5x1C4x2D8
SOLUTION View the system as AxDb. Using the notation introduced above,
AD3 2
 5 4
; A 1.b/D6 2
8 4
; A 2.b/D3 6
 5 8
Since det AD2, the system has a unique solution. By Cramer‚Äôs rule,
x1DdetA1.b/
detAD24C16
2D20
x2DdetA2.b/
detAD24C30
2D27
Application to Engineering
A number of important engineering problems, particularly in electrical engineering and
control theory, can be analyzed by Laplace transforms . This approach converts an ap-
propriate system of linear differential equations into a system of linear algebraic equa-
tions whose coefÔ¨Åcients involve a parameter. The next example illustrates the type of
algebraic system that may arise.
EXAMPLE 2 Consider the following system in which sis an unspeciÔ¨Åed parameter.
Determine the values of sfor which the system has a unique solution, and use Cramer‚Äôs
rule to describe the solution.
3sx 1 2x2D4
 6x1Csx2D1
SOLUTION View the system as AxDb. Then
AD3s 2
 6 s
; A 1.b/D4 2
1 s
; A 2.b/D3s 4
 6 1
Since
detAD3s2 12D3.sC2/.s 2/
the system has a unique solution precisely when s¬§ 2. For such an s, the solution is
.x1; x2/, where
x1DdetA1.b/
detAD4sC2
3.sC2/.s 2/
x2DdetA2.b/
detAD3sC24
3.sC2/.s 2/DsC8
.sC2/.s 2/
SECOND REVISED PAGES


--- Page 198 ---
3.3 Cramer's Rule, Volume, and Linear Transformations 181
A Formula for A‚Äì1
Cramer‚Äôs rule leads easily to a general formula for the inverse of an nnmatrix A. The
jth column of A 1is a vector xthat satisÔ¨Åes
AxDej
where ejis the jth column of the identity matrix, and the ith entry of xis the .i; j / -entry
ofA 1. By Cramer‚Äôs rule,

.i; j / -entry of A 1	
DxiDdetAi.ej/
detA(2)
Recall that Ajidenotes the submatrix of Aformed by deleting row jand column i. A
cofactor expansion down column iofAi.ej/shows that
detAi.ej/D. 1/iCjdetAjiDCji (3)
where Cjiis a cofactor of A. By (2), the .i; j / -entry of A 1is the cofactor Cjidivided
by det A. [Note that the subscripts on Cjiare the reverse of .i; j / .] Thus
A 1D1
detA2
6664C11 C21   Cn1
C12 C22   Cn2
:::::::::
C1n C2n   Cnn3
7775(4)
The matrix of cofactors on the right side of (4) is called the adjugate (orclassical
adjoint ) ofA, denoted by adj A. (The term adjoint also has another meaning in advanced
texts on linear transformations.) The next theorem simply restates (4).
T H E O R E M 8 An Inverse Formula
LetAbe an invertible nnmatrix. Then
A 1D1
detAadjA
EXAMPLE 3 Find the inverse of the matrix AD2
42 1 3
1 1 1
1 4  23
5.
SOLUTION The nine cofactors are
C11D C 1 1
4 2D  2; C 12D  1 1
1 2D3; C 13D C1 1
1 4D5
C21D  1 3
4 2D14; C 22D C2 3
1 2D  7; C 23D  2 1
1 4D  7
C31D C1 3
 1 1D4; C 32D  2 3
1 1D1; C 33D C2 1
1 1D  3
The adjugate matrix is the transpose of the matrix of cofactors. [For instance, C12goes
in the .2; 1/ position.] Thus
adjAD2
4 2 14 4
3 7 1
5 7 33
5
SECOND REVISED PAGES


--- Page 199 ---
182 CHAPTER 3 Determinants
We could compute det Adirectly, but the following computation provides a check on
the calculations on page 181 andproduces det A:
.adjA/AD2
4 2 14 4
3 7 1
5 7 33
52
42 1 3
1 1 1
1 4  23
5D2
414 0 0
0 14 0
0 0 143
5D14I
Since .adjA/AD14I, Theorem 8 shows that det AD14and
A 1D1
142
4 2 14 4
3 7 1
5 7 33
5D2
4 1=7 1 2=7
3=14  1=2 1=14
5=14  1=2 3=143
5
N U M E R I C A L N O T E S
Theorem 8 is useful mainly for theoretical calculations. The formula for A 1
permits one to deduce properties of the inverse without actually calculating it.
Except for special cases, the algorithm in Section 2.2 gives a much better way to
compute A 1, if the inverse is really needed.
Cramer‚Äôs rule is also a theoretical tool. It can be used to study how sensitive
the solution of AxDbis to changes in an entry in bor in A(perhaps due
to experimental error when acquiring the entries for borA). When Ais a
33matrix with complex entries, Cramer‚Äôs rule is sometimes selected for hand
computation because row reduction of ¬åAb¬çwith complex arithmetic can be
messy, and the determinants are fairly easy to compute. For a larger nnmatrix
(real or complex), Cramer‚Äôs rule is hopelessly inefÔ¨Åcient. Computing just one
determinant takes about as much work as solving AxDbby row reduction.
Determinants as Area or Volume
In the next application, we verify the geometric interpretation of determinants described
in the chapter introduction. Although a general discussion of length and distance in Rn
will not be given until Chapter 6, we assume here that the usual Euclidean concepts of
length, area, and volume are already understood for R2andR3.
T H E O R E M 9 IfAis a22matrix, the area of the parallelogram determined by the columns of
AisjdetAj. IfAis a33matrix, the volume of the parallelepiped determined
by the columns of AisjdetAj.
PROOF The theorem is obviously true for any 22diagonal matrix:
SG A Geometric Proof
3‚Äì12 deta 0
0 dD jadj Darea of
rectangle
See Figure 1. It will sufÔ¨Åce to show that any 22matrix AD¬åa1a2¬çcan be trans-
‚é°‚é¢‚é£
‚é°‚é¢‚é£‚é°‚é¢‚é£‚é°‚é¢‚é£y
x0
d
a
0
FIGURE 1
AreaD jadj.formed into a diagonal matrix in a way that changes neither the area of the associated
parallelogram nor jdetAj. From Section 3.2, we know that the absolute value of the
determinant is unchanged when two columns are interchanged or a multiple of one
column is added to another. And it is easy to see that such operations sufÔ¨Åce to transform
Ainto a diagonal matrix. Column interchanges do not change the parallelogram at all.
So it sufÔ¨Åces to prove the following simple geometric observation that applies to vectors
inR2orR3:
SECOND REVISED PAGES


--- Page 200 ---
3.3 Cramer's Rule, Volume, and Linear Transformations 183
Leta1anda2be nonzero vectors. Then for any scalar c, the area of the parallelo-
gram determined by a1anda2equals the area of the parallelogram determined by
a1anda2Cca1.
To prove this statement, we may assume that a2is not a multiple of a1, for other-
wise the two parallelograms would be degenerate and have zero area. If Lis the line
through 0anda1, then a2CLis the line through a2parallel to L, and a2Cca1is on
this line. See Figure 2. The points a2anda2Cca1have the same perpendicular distance
toL. Hence the two parallelograms in Figure 2 have the same area, since they share the
base from 0toa1. This completes the proof for R2.
a2 /H11001 ca1a2
ca1a2 /H11001 L
a10L
FIGURE 2 Two parallelograms of equal area.
The proof for R3is similar. The theorem is obviously true for a 33diagonal
matrix. See Figure 3. And any 33matrix Acan be transformed into a diagonal matrix
using column operations that do not change jdetAj. (Think about doing row operations
onAT.) So it sufÔ¨Åces to show that these operations do not affect the volume of the
parallelepiped determined by the columns of A.
0
b
00
0
c
a
0
0z
xy
FIGURE 3
V olume D jabcj.A parallelepiped is shown in Figure 4 as a shaded box with two sloping sides.
Its volume is the area of the base in the plane Span fa1;a3gtimes the altitude of a2
above Span fa1;a3g. Any vector a2Cca1has the same altitude because a2Cca1lies
in the plane a2CSpanfa1;a3g, which is parallel to Span fa1;a3g. Hence the volume of
the parallelepiped is unchanged when ¬åa1a2a3¬çis changed to ¬åa1a2Cca1a3¬ç.
Thus a column replacement operation does not affect the volume of the parallelepiped.
Since column interchanges have no effect on the volume, the proof is complete.
a3
a2
0a1
Span{ a1, a3}a2 /H11001 Span{ a1, a3}
a2
0a1
Span{ a1, a3}a2 /H11001 Span{ a1, a3}
a2 /H11001 ca1a3
FIGURE 4 Two parallelepipeds of equal volume.
EXAMPLE 4 Calculate the area of the parallelogram determined by the points
. 2; 2/,.0; 3/ ,.4; 1/, and .6; 4/ . See Figure 5(a).
SOLUTION First translate the parallelogram to one having the origin as a vertex. For
example, subtract the vertex . 2; 2/from each of the four vertices. The new par-
allelogram has the same area, and its vertices are .0; 0/ ,.2; 5/ ,.6; 1/ , and .8; 6/ . See
SECOND REVISED PAGES


--- Page 201 ---
184 CHAPTER 3 Determinants
x2
x1x1x2
(a) (b)
FIGURE 5 Translating a parallelogram does not change its
area.
Figure 5(b). This parallelogram is determined by the columns of
AD2 6
5 1
Since jdetAj D j  28j, the area of the parallelogram is 28.
Linear Transformations
Determinants can be used to describe an important geometric property of linear trans-
formations in the plane and in R3. IfTis a linear transformation and Sis a set in the
domain of T, letT .S/ denote the set of images of points in S. We are interested in how
the area (or volume) of T .S/ compares with the area (or volume) of the original set S.
For convenience, when Sis a region bounded by a parallelogram, we also refer to Sas
a parallelogram.
T H E O R E M 1 0 LetTWR2!R2be the linear transformation determined by a 22matrix A. If
Sis a parallelogram in R2, then
farea of T .S/g D j detAj  farea of Sg (5)
IfTis determined by a 33matrix A, and if Sis a parallelepiped in R3, then
fvolume of T .S/g D j detAj  fvolume of Sg (6)
PROOF Consider the 22case, with AD¬åa1a2¬ç. A parallelogram at the origin in
R2determined by vectors b1andb2has the form
SD fs1b1Cs2b2W0s11; 0s21g
The image of Sunder Tconsists of points of the form
T .s 1b1Cs2b2/Ds1T .b1/Cs2T .b2/
Ds1Ab1Cs2Ab2
where 0s11,0s21. It follows that T .S/ is the parallelogram determined
by the columns of the matrix ¬åAb1Ab2¬ç. This matrix can be written as AB, where
BD¬åb1b2¬ç. By Theorem 9 and the product theorem for determinants,
farea of T .S/g D j detABj D j detAj  jdetBj
D jdetAj  farea of Sg(7)
SECOND REVISED PAGES


--- Page 202 ---
3.3 Cramer's Rule, Volume, and Linear Transformations 185
An arbitrary parallelogram has the form pCS, where pis a vector and Sis a parallelo-
gram at the origin, as above. It is easy to see that Ttransforms pCSintoT .p/CT .S/ .
(See Exercise 26.) Since translation does not affect the area of a set,
farea of T .pCS/g D f area of T .p/CT .S/g
D farea of T .S/g Translation
D jdetAj  farea of Sg By equation (7)
D jdetAj  farea of pCSg Translation
This shows that (5) holds for all parallelograms in R2. The proof of (6) for the 33
case is analogous.
When we attempt to generalize Theorem 10 to a region in R2orR3that is not
bounded by straight lines or planes, we must face the problem of how to deÔ¨Åne and
compute its area or volume. This is a question studied in calculus, and we shall only
outline the basic idea for R2. IfRis a planar region that has a Ô¨Ånite area, then Rcan
be approximated by a grid of small squares that lie inside R. By making the squares
sufÔ¨Åciently small, the area of Rmay be approximated as closely as desired by the sum
of the areas of the small squares. See Figure 6.
0 0
FIGURE 6 Approximating a planar region by a union of squares.
The approximation improves as the grid becomes Ô¨Åner.
IfTis a linear transformation associated with a 22matrix A, then the image of
a planar region Runder Tis approximated by the images of the small squares inside
R. The proof of Theorem 10 shows that each such image is a parallelogram whose area
isjdetAjtimes the area of the square. If R0is the union of the squares inside R, then
the area of T .R0/isjdetAjtimes the area of R0. See Figure 7. Also, the area of T .R0/
is close to the area of T .R/ . An argument involving a limiting process may be given to
justify the following generalization of Theorem 10.
0 0R' T (R')T
FIGURE 7 Approximating T .R/ by a union of parallelograms.
SECOND REVISED PAGES


--- Page 203 ---
186 CHAPTER 3 Determinants
The conclusions of Theorem 10 hold whenever Sis a region in R2with Ô¨Ånite area
or a region in R3with Ô¨Ånite volume.
EXAMPLE 5 Letaandbbe positive numbers. Find the area of the region Ebounded
by the ellipse whose equation is
x2
1
a2Cx2
2
b2D1
SOLUTION We claim that Eis the image of the unit disk Dunder the linear transfor-
x2
x11
abu1u2
TD
E
mation Tdetermined by the matrix ADa 0
0 b
, because if uDu1
u2
,xDx1
x2
,
andxDAu, then
u1Dx1
aand u2Dx2
b
It follows that uis in the unit disk, with u2
1Cu2
21, if and only if xis in E, with
.x1=a/2C.x2=b/21. By the generalization of Theorem 10,
farea of ellipse g D f area of T .D/g
D jdetAj  farea of Dg
Dab.1/2Dab
PRACTICE PROBLEM
LetSbe the parallelogram determined by the vectors b1D1
3
andb2D5
1
, and
letAD1 :1
0 2
. Compute the area of the image of Sunder the mapping x7!Ax.
3.3 EXERCISES
Use Cramer‚Äôs rule to compute the solutions of the systems in
Exercises 1‚Äì6.
1.5x1C7x2D3
2x1C4x2D12.4x1Cx2D6
3x1C2x2D7
3.3x1 2x2D3
 4x1C6x2D  54. 5x1C2x2D9
3x1 x2D  4
5.x1Cx2 D3
 3x1 C2x3D0
x2 2x3D26.x1C3x2Cx3D4
 x1C 2x3D2
3x1Cx2 D2
In Exercises 7‚Äì10, determine the values of the parameter s
for which the system has a unique solution, and describe the
solution.
7.6sx 1C4x2D5
9x1C2sx 2D  28.3sx 1C5x2D3
12x 1C5sx 2D29.sx1C2sx 2D  1
3x1C6sx 2D410. sx1 2x2D1
4sx 1C4sx 2D2
In Exercises 11‚Äì16, compute the adjugate of the given matrix, and
then use Theorem 8 to give the inverse of the matrix.
11.2
40 2 1
5 0 0
 1 1 13
5 12.2
41 1 3
 2 2 1
0 1 13
5
13.2
43 5 4
1 0 1
2 1 13
5 14.2
41 1 2
0 2 1
2 0 43
5
15.2
45 0 0
 1 1 0
 2 3  13
5 16.2
41 2 4
0 3 1
0 0  23
5
17. Show that if Ais22, then Theorem 8 gives the same
formula for A 1as that given by Theorem 4 in Section 2.2.
18. Suppose that all the entries in Aare integers and det AD1.
Explain why all the entries in A 1are integers.
SECOND REVISED PAGES


--- Page 204 ---
3.3 Cramer's Rule, Volume, and Linear Transformations 187
In Exercises 19‚Äì22, Ô¨Ånd the area of the parallelogram whose
vertices are listed.
19..0; 0/ ,.5; 2/ ,.6; 4/ ,.11; 6/
20..0; 0/ ,. 2; 4/,.4; 5/,.2; 1/
21.. 2; 0/,.0; 3/ ,.1; 3/ ,. 1; 0/
22..0; 2/,.5; 2/,. 3; 1/,.2; 1/
23. Find the volume of the parallelepiped with one vertex at
the origin and adjacent vertices at .1; 0; 3/,.1; 2; 4/ , and
.5; 1; 0/ .
24. Find the volume of the parallelepiped with one vertex at
the origin and adjacent vertices at .1; 3; 0/ ,. 2; 0; 2/ , and
. 1; 3; 1/.
25. Use the concept of volume to explain why the determinant of
a33matrix Ais zero if and only if Ais not invertible. Do
not appeal to Theorem 4 in Section 3.2. [ Hint: Think about
the columns of A.]
26. LetTWRm!Rnbe a linear transformation, and let pbe a
vector and Sa set in Rm. Show that the image of pCSunder
Tis the translated set T .p/CT .S/ inRn.
27. LetSbe the parallelogram determined by the vectors
b1D 2
3
andb2D 2
5
, and let AD6 3
 3 2
.
Compute the area of the image of Sunder the mapping
x7!Ax.
28. Repeat Exercise 27 with b1D4
 7
,b2D0
1
, and
AD5 2
1 1
.
29. Find a formula for the area of the triangle whose vertices are
0,v1, and v2inR2.
30. LetRbe the triangle with vertices at .x1; y1/,.x2; y2/, and
.x3; y3/. Show that
farea of triangle g D1
2det2
4x1 y1 1
x2 y2 1
x3 y3 13
5
[Hint: Translate Rto the origin by subtracting one of the
vertices, and use Exercise 29.]
31. LetTWR3!R3be the linear transformation determined
by the matrix AD2
4a 0 0
0 b 0
0 0 c3
5, where a,b, and carepositive numbers. Let Sbe the unit ball, whose bounding
surface has the equation x2
1Cx2
2Cx2
3D1.
a.Show that T .S/ is bounded by the ellipsoid with the
equationx2
1
a2Cx2
2
b2Cx2
3
c2D1.
b.Use the fact that the volume of the unit ball is 4=3
to determine the volume of the region bounded by the
ellipsoid in part (a).
32. LetSbe the tetrahedron in R3with vertices at the vectors 0,
e1,e2, and e3, and let S0be the tetrahedron with vertices at
vectors 0,v1,v2, and v3. See the Ô¨Ågure.
e3
e2x2 x2
0 0
e1v3 S'
v2
v1Sx3
x1x3
x1
a.Describe a linear transformation that maps Sonto S0.
b.Find a formula for the volume of the tetrahedron S0using
the fact that
fvolume of Sg D.1=3/  farea of base g  fheight g
33. [M] Test the inverse formula of Theorem 8 for a random
44matrix A. Use your matrix program to compute the
cofactors of the 33submatrices, construct the adjugate,
and set BD.adjA/=. detA/. Then compute B inv.A/,
where inv .A/is the inverse of Aas computed by the matrix
program. Use Ô¨Çoating point arithmetic with the maximum
possible number of decimal places. Report your results.
34. [M] Test Cramer‚Äôs rule for a random 44matrix Aand a
random 41vector b. Compute each entry in the solution of
AxDb, and compare these entries with the entries in A 1b.
Write the command (or keystrokes) for your matrix program
that uses Cramer‚Äôs rule to produce the second entry of x.
35. [M] If your version of MATLAB has the flops command,
use it to count the number of Ô¨Çoating point operations to com-
puteA 1for a random 3030matrix. Compare this number
with the number of Ô¨Çops needed to form .adjA/=. detA/.
SOLUTION TO PRACTICE PROBLEM
The area of Sisdet1 5
3 1D14;and det AD2. By Theorem 10, the area of the
image of Sunder the mapping x7!Axis
jdetAj  farea of Sg D214D28
SECOND REVISED PAGES


--- Page 205 ---
188 CHAPTER 3 Determinants
CHAPTER 3 SUPPLEMENTARY EXERCISES
1.Mark each statement True or False. Justify each answer.
Assume that all matrices here are square.
a.IfAis a22matrix with a zero determinant, then one
column of Ais a multiple of the other.
b.If two rows of a 33matrix Aare the same, then
detAD0.
c.IfAis a33matrix, then det 5AD5detA.
d.IfAandBarennmatrices, with det AD2and
detBD3, then det .ACB/D5.
e.IfAisnnand det AD2, then det A3D6.
f.IfBis produced by interchanging two rows of A, then
detBDdetA.
g.IfBis produced by multiplying row 3 of Aby 5, then
detBD5detA.
h.IfBis formed by adding to one row of Aa linear
combination of the other rows, then det BDdetA.
i.detATD   detA.
j.det. A/D   detA.
k.detATA0.
l.Any system of nlinear equations in nvariables can be
solved by Cramer‚Äôs rule.
m.Ifuandvare inR2and det ¬åu v ¬çD10, then the area
of the triangle in the plane with vertices at 0,u, and v
is 10.
n.IfA3D0, then det AD0.
o.IfAis invertible, then det A 1DdetA.
p.IfAis invertible, then .detA/.detA 1/D1.
Use row operations to show that the determinants in Exercises 2‚Äì4
are all zero.
2.12 13 14
15 16 17
18 19 203.1 a b Cc
1 b a Cc
1 c a Cb
4.a b c
aCx b Cx c Cx
aCy b Cy c Cy
Compute the determinants in Exercises 5 and 6.
5.9 1 9 9 9
9 0 9 9 2
4 0 0 5 0
9 0 3 9 0
6 0 0 7 0
6.4 8 8 8 5
0 1 0 0 0
6 8 8 8 7
0 8 8 3 0
0 8 2 0 07.Show that the equation of the line in R2through distinct
points .x1; y1/and.x2; y2/can be written as
det2
41 x y
1 x 1 y1
1 x 2 y23
5D0
8.Find a 33determinant equation similar to that in Exercise 7
that describes the equation of the line through .x1; y1/with
slope m.
Exercises 9 and 10 concern determinants of the following Vander-
monde matrices .
TD2
641 a a2
1 b b2
1 c c23
75; V .t/ D2
666641 t t2t3
1 x 1 x2
1 x3
1
1 x 2 x2
2 x3
2
1 x 3 x2
3 x3
33
77775
9.Use row operations to show that
detTD.b a/.c a/.c b/
10. Letf .t/DdetV, with x1,x2, and x3all distinct. Explain
whyf .t/ is a cubic polynomial, show that the coefÔ¨Åcient of
t3is nonzero, and Ô¨Ånd three points on the graph of f.
11.Find the area of the parallelogram determined by the points
.1; 4/ ,. 1; 5/,.3; 9/ , and .5; 8/ . How can you tell that the
quadrilateral determined by the points is actually a parallel-
ogram?
12. Use the concept of area of a parallelogram to write a state-
ment about a 22matrix Athat is true if and only if Ais
invertible.
13. Show that if Ais invertible, then adj Ais invertible, and
.adjA/ 1D1
detAA
[Hint: Given matrices BandC, what calculation(s) would
show that Cis the inverse of B¬ã¬ç
14. LetA,B,C,D, and Ibennmatrices. Use the deÔ¨Ånition or
properties of a determinant to justify the following formulas.
Part (c) is useful in applications of eigenvalues (Chapter 5).
a.detA 0
0 I
DdetA b. detI 0
C D
DdetD
c.detA 0
C D
D.detA/.detD/DdetA B
0 D
15. LetA,B,C, and Dbennmatrices with Ainvertible.
a.Find matrices XandYto produce the block LU factor-
izationA B
C D
DI 0
X IA B
0 Y
and then show that
detA B
C D
D.detA/det.D CA 1B/
SECOND REVISED PAGES


--- Page 206 ---
Chapter 3 Supplementary Exercises 189
b.Show that if ACDCA, then
detA B
C D
Ddet.AD CB/
16. Let Jbe the nnmatrix of all 1‚Äôs, and consider
AD.a b/ICbJ; that is,
AD2
666664a b b    b
b a b    b
b b a    b
:::::::::::::::
b b b    a3
777775
ConÔ¨Årm that det AD.a b/n 1¬åaC.n 1/b¬ç as follows:
a.Subtract row 2 from row 1, row 3 from row 2, and so on,
and explain why this does not change the determinant of
the matrix.
b.With the resulting matrix from part (a), add column 1 to
column 2, then add this new column 2 to column 3, and so
on, and explain why this does not change the determinant.
c.Find the determinant of the resulting matrix from (b).
17. LetAbe the original matrix given in Exercise 16, and let
BD2
666664a b b b    b
0 a b    b
0 b a    b
:::::::::::::::
0 b b    a3
777775,
CD2
666664b b b    b
b a b    b
b b a    b
:::::::::::::::
b b b    a3
777775
Notice that A,B, and Care nearly the same except that
the Ô¨Årst column of Aequals the sum of the Ô¨Årst columns of
BandC. Alinearity property of the determinant function,
discussed in Section 3.2, says that det ADdetBCdetC.
Use this fact to prove the formula in Exercise 16 by induction
on the size of matrix A.18. [M] Apply the result of Exercise 16 to Ô¨Ånd the determinants
of the following matrices, and conÔ¨Årm your answers using a
matrix program.
2
6643 8 8 8
8 3 8 8
8 8 3 8
8 8 8 33
7752
666648 3 3 3 3
3 8 3 3 3
3 3 8 3 3
3 3 3 8 3
3 3 3 3 83
77775
19. [M] Use a matrix program to compute the determinants of
the following matrices.
2
41 1 1
1 2 2
1 2 33
52
6641 1 1 1
1 2 2 2
1 2 3 3
1 2 3 43
775
2
666641 1 1 1 1
1 2 2 2 2
1 2 3 3 3
1 2 3 4 4
1 2 3 4 53
77775
Use the results to guess the determinant of the matrix below,
and conÔ¨Årm your guess by using row operations to evaluate
that determinant.2
6666641 1 1    1
1 2 2    2
1 2 3    3
:::::::::::::::
1 2 3    n3
777775
20. [M] Use the method of Exercise 19 to guess the determinant
of2
6666641 1 1    1
1 3 3    3
1 3 6    6
:::::::::::::::
1 3 6    3.n 1/3
777775
Justify your conjecture. [ Hint: Use Exercise 14(c) and the
result of Exercise 19.]
SECOND REVISED PAGES


--- Page 207 ---
SECOND REVISED PAGES


--- Page 208 ---
4Vector Spaces
INTRODUCTORY EXAMPLE
Space Flight and Control Systems
Twelve stories high and weighing 75 tons, Columbia rose
majestically off the launching pad on a cool Palm Sunday
morning in April 1981. A product of ten years‚Äô intensive
research and development, the Ô¨Årst U.S. space shuttle was a
triumph of control systems engineering design, involving
many branches of engineering‚Äîaeronautical, chemical,
electrical, hydraulic, and mechanical.
The space shuttle‚Äôs control systems are absolutely
critical for Ô¨Çight. Because the shuttle is an unstable
airframe, it requires constant computer monitoring during
atmospheric Ô¨Çight. The Ô¨Çight control system sends a stream
of commands to aerodynamic control surfaces and 44 small
thruster jets. Figure 1 shows a typical closed-loop feedback
system that controls the pitch of the shuttle during Ô¨Çight.(The pitch is the elevation angle of the nose cone.) The
junction symbols ( 
) show where signals from various
sensors are added to the computer signals Ô¨Çowing along
the top of the Ô¨Ågure.
Mathematically, the input and output signals to an
engineering system are functions. It is important in
applications that these functions can be added, as in
Figure 1, and multiplied by scalars. These two operations
on functions have algebraic properties that are completely
analogous to the operations of adding vectors in Rn
and multiplying a vector by a scalar, as we shall see
in Sections 4.1 and 4.8. For this reason, the set of all
possible inputs (functions) is called a vector space . The
mathematical foundation for systems engineering rests
Commanded 
pitchController
Pitch
ratePitch
rate
errorPitch
acceleration
errorAccelerometer
Rate gyroPitchShuttle
dynamicsCommanded
pitch
accelerationCommanded
pitch
rate
K1K2G1(s) G2(s)
s2+++++
‚Äì‚Äì ‚Äì
s
Inertial measuring unit
1
FIGURE 1 Pitch control system for the space shuttle. ( Source: Adapted from Space Shuttle GN&C Operations
Manual , Rockwell International, ¬©1988.)
SECOND REVISED PAGES
191

--- Page 209 ---
192 CHAPTER 4 Vector Spaces
on vector spaces of functions, and Chapter 4 extends the
theory of vectors in Rnto include such functions. Later on,you will see how other vector spaces arise in engineering,
physics, and statistics.
WEB
The mathematical seeds planted in Chapters 1 and 2 germinate and begin to blossom
in this chapter. The beauty and power of linear algebra will be seen more clearly when
you view Rnas only one of a variety of vector spaces that arise naturally in applied
problems. Actually, a study of vector spaces is not much different from a study of Rn
itself, because you can use your geometric experience with R2andR3to visualize many
general concepts.
Beginning with basic deÔ¨Ånitions in Section 4.1, the general vector space framework
develops gradually throughout the chapter. A goal of Sections 4.3‚Äì4.5 is to demonstrate
how closely other vector spaces resemble Rn. Section 4.6 on rank is one of the high
points of the chapter, using vector space terminology to tie together important facts about
rectangular matrices. Section 4.8 applies the theory of the chapter to discrete signals and
difference equations used in digital control systems such as in the space shuttle. Markov
chains, in Section 4.9, provide a change of pace from the more theoretical sections of
the chapter and make good examples for concepts to be introduced in Chapter 5.
4.1 VECTOR SPACES AND SUBSPACES
Much of the theory in Chapters 1 and 2 rested on certain simple and obvious alge-
braic properties of Rn, listed in Section 1.3. In fact, many other mathematical systems
have the same properties. The speciÔ¨Åc properties of interest are listed in the following
deÔ¨Ånition.
D E F I N I T I O N Avector space is a nonempty set Vof objects, called vectors , on which are de-
Ô¨Åned two operations, called addition andmultiplication by scalars (real numbers),
subject to the ten axioms (or rules) listed below.1The axioms must hold for all
vectors u,v, and winVand for all scalars candd.
1.The sum of uandv, denoted by uCv, is in V.
2.uCvDvCu.
3..uCv/CwDuC.vCw/.
4.There is a zero vector 0inVsuch that uC0Du.
5.For each uinV, there is a vector  uinVsuch that uC. u/D0.
6.The scalar multiple of ubyc, denoted by cu, is in V.
7.c.uCv/DcuCcv.
8..cCd/uDcuCdu.
9.c.du/D.cd/ u.
10.1uDu.
1Technically, Vis areal vector space . All of the theory in this chapter also holds for a complex vector space
in which the scalars are complex numbers. We will look at this brieÔ¨Çy in Chapter 5. Until then, all scalars are
assumed to be real.
SECOND REVISED PAGES


--- Page 210 ---
4.1 Vector Spaces and Subspaces 193
Using only these axioms, one can show that the zero vector in Axiom 4 is unique,
and the vector  u, called the negative ofu, in Axiom 5 is unique for each uinV.
See Exercises 25 and 26. Proofs of the following simple facts are also outlined in the
exercises:
For each uinVand scalar c,
0uD0 (1)
c0D0 (2)
 uD. 1/u (3)
EXAMPLE 1 The spaces Rn, where n1, are the premier examples of vector
spaces. The geometric intuition developed for R3will help you understand and visualize
many concepts throughout the chapter.
EXAMPLE 2 LetVbe the set of all arrows (directed line segments) in three-
dimensional space, with two arrows regarded as equal if they have the same length and
point in the same direction. DeÔ¨Åne addition by the parallelogram rule (from Section 1.3),
and for each vinV, deÔ¨Åne cvto be the arrow whose length is jcjtimes the length of
v, pointing in the same direction as vifc0and otherwise pointing in the opposite
direction. (See Figure 1.) Show that Vis a vector space. This space is a common model
in physical problems for various forces.
v 3v ‚Äìv FIGURE 1
SOLUTION The deÔ¨Ånition of Vis geometric, using concepts of length and direction.
Noxy¬¥-coordinate system is involved. An arrow of zero length is a single point and
represents the zero vector. The negative of vis. 1/v. So Axioms 1, 4, 5, 6, and 10 are
evident. The rest are veriÔ¨Åed by geometry. For instance, see Figures 2 and 3.
vuuv + u
u + v
FIGURE 2 uCvDvCu.
v
w
uv + w
u + v + wu + v FIGURE 3 .uCv/CwDuC.vCw/.
EXAMPLE 3 LetSbe the space of all doubly inÔ¨Ånite sequences of numbers (usually
written in a row rather than a column):
fykg D.: : : ; y 2; y 1; y0; y1; y2; : : :/
Iff¬¥kgis another element of S, then the sum fykg C f¬¥kgis the sequence fykC¬¥kg
formed by adding corresponding terms of fykgandf¬¥kg. The scalar multiple cfykgis
the sequence fcykg. The vector space axioms are veriÔ¨Åed in the same way as for Rn.
Elements of Sarise in engineering, for example, whenever a signal is measured (or
sampled) at discrete times. A signal might be electrical, mechanical, optical, and so on.
The major control systems for the space shuttle, mentioned in the chapter introduction,
use discrete (or digital) signals. For convenience, we will call Sthe space of (discrete-
time) signals . A signal may be visualized by a graph as in Figure 4.
SECOND REVISED PAGES


--- Page 211 ---
194 CHAPTER 4 Vector Spaces
‚Äì505 1 0
FIGURE 4 A discrete-time signal.
EXAMPLE 4 Forn0, the set Pnof polynomials of degree at most nconsists of
all polynomials of the form
p.t/Da0Ca1tCa2t2C  C antn(4)
where the coefÔ¨Åcients a0; : : : ; a nand the variable tare real numbers. The degree of
pis the highest power of tin (4) whose coefÔ¨Åcient is not zero. If p.t/Da0¬§0, the
degree of pis zero. If all the coefÔ¨Åcients are zero, pis called the zero polynomial . The
zero polynomial is included in Pneven though its degree, for technical reasons, is not
deÔ¨Åned.
Ifpis given by (4) and if q.t/Db0Cb1tC  C bntn, then the sum pCqis
deÔ¨Åned by
.pCq/.t/Dp.t/Cq.t/
D.a0Cb0/C.a1Cb1/tC  C .anCbn/tn
The scalar multiple cpis the polynomial deÔ¨Åned by
.cp/.t/Dcp.t/Dca0C.ca1/tC  C .can/tn
These deÔ¨Ånitions satisfy Axioms 1 and 6 because pCqandcpare polynomials
of degree less than or equal to n. Axioms 2, 3, and 7‚Äì10 follow from properties of the
real numbers. Clearly, the zero polynomial acts as the zero vector in Axiom 4. Finally,
. 1/pacts as the negative of p, so Axiom 5 is satisÔ¨Åed. Thus Pnis a vector space.
The vector spaces Pnfor various nare used, for instance, in statistical trend analysis
of data, discussed in Section 6.8.
EXAMPLE 5 LetVbe the set of all real-valued functions deÔ¨Åned on a set D. (Typi-
cally,Dis the set of real numbers or some interval on the real line.) Functions are added
in the usual way: fCgis the function whose value at tin the domain Disf.t/Cg.t/.
Likewise, for a scalar cand an finV, the scalar multiple cfis the function whose value
attiscf.t/. For instance, if DDR,f.t/D1Csin2t, and g.t/D2C:5t, then
.fCg/.t/D3Csin2tC:5t and .2g/.t/D4Ct
Two functions in Vare equal if and only if their values are equal for every tin
D. Hence the zero vector in Vis the function that is identically zero, f.t/D0for all t,
and the negative of fis. 1/f. Axioms 1 and 6 are obviously true, and the other axioms
follow from properties of the real numbers, so Vis a vector space.
It is important to think of each function in the vector space Vof Example 5 as a
single object, as just one ‚Äúpoint‚Äù or vector in the vector space. The sum of two vectors f
andg(functions in V, or elements of anyvector space) can be visualized as in Figure 5,
because this can help you carry over to a general vector space the geometric intuition
you have developed while working with the vector space Rn. See the Study Guide for
help as you learn to adopt this more general point of view.
f + g
gf
0FIGURE 5
The sum of two vectors
(functions).
SECOND REVISED PAGES


--- Page 212 ---
4.1 Vector Spaces and Subspaces 195
Subspaces
In many problems, a vector space consists of an appropriate subset of vectors from some
larger vector space. In this case, only three of the ten vector space axioms need to be
checked; the rest are automatically satisÔ¨Åed.
D E F I N I T I O N Asubspace of a vector space Vis a subset HofVthat has three properties:
a.The zero vector of Vis inH.2
b.His closed under vector addition. That is, for each uandvinH, the sum
uCvis inH.
c.His closed under multiplication by scalars. That is, for each uinHand each
scalar c, the vector cuis inH.
Properties (a), (b), and (c) guarantee that a subspace HofVis itself a vector
space , under the vector space operations already deÔ¨Åned in V. To verify this, note
that properties (a), (b), and (c) are Axioms 1, 4, and 6. Axioms 2, 3, and 7‚Äì10 are
automatically true in Hbecause they apply to all elements of V, including those in H.
Axiom 5 is also true in H, because if uis inH, then . 1/uis inHby property (c), and
we know from equation (3) earlier in this section that . 1/uis the vector  uin Axiom 5.
So every subspace is a vector space. Conversely, every vector space is a subspace
(of itself and possibly of other larger spaces). The term subspace is used when at least
two vector spaces are in mind, with one inside the other, and the phrase subspace of V
identiÔ¨Åes Vas the larger space. (See Figure 6.)
0H
VFIGURE 6
A subspace of V.
EXAMPLE 6 The set consisting of only the zero vector in a vector space Vis a
subspace of V, called the zero subspace and written as f0g.
EXAMPLE 7 LetPbe the set of all polynomials with real coefÔ¨Åcients, with opera-
tions in PdeÔ¨Åned as for functions. Then Pis a subspace of the space of all real-valued
functions deÔ¨Åned on R. Also, for each n0,Pnis a subspace of P, because Pnis a
subset of Pthat contains the zero polynomial, the sum of two polynomials in Pnis also
inPn, and a scalar multiple of a polynomial in Pnis also in Pn.
EXAMPLE 8 The vector space R2isnota subspace of R3because R2is not even a
subset of R3. (The vectors in R3all have three entries, whereas the vectors in R2have
only two.) The set
HD8
<
:2
4s
t
03
5Wsandtare real9
=
;
is a subset of R3that ‚Äúlooks‚Äù and ‚Äúacts‚Äù like R2, although it is logically distinct from
R2. See Figure 7. Show that His a subspace of R3.
x3
x2
x1HFIGURE 7
Thex1x2-plane as a subspace of
R3.
SOLUTION The zero vector is in H, and His closed under vector addition and scalar
multiplication because these operations on vectors in Halways produce vectors whose
third entries are zero (and so belong to H/. Thus His a subspace of R3.
2Some texts replace property (a) in this deÔ¨Ånition by the assumption that His nonempty. Then (a) could be
deduced from (c) and the fact that 0uD0. But the best way to test for a subspace is to look Ô¨Årst for the zero
vector. If 0is inH, then properties (b) and (c) must be checked. If 0isnotinH, then Hcannot be a
subspace and the other properties need not be checked.
SECOND REVISED PAGES


--- Page 213 ---
196 CHAPTER 4 Vector Spaces
EXAMPLE 9 A plane in R3notthrough the origin is not a subspace of R3, because
the plane does not contain the zero vector of R3. Similarly, a line in R2notthrough the
Hx2
x1
FIGURE 8
A line that is not a vector space.origin, such as in Figure 8, is nota subspace of R2.
A Subspace Spanned by a Set
The next example illustrates one of the most common ways of describing a subspace.
As in Chapter 1, the term linear combination refers to any sum of scalar multiples of
vectors, and Span fv1; : : : ; vpgdenotes the set of all vectors that can be written as linear
combinations of v1; : : : ; vp.
EXAMPLE 10 Given v1andv2in a vector space V, letHDSpanfv1;v2g. Show
thatHis a subspace of V.
SOLUTION The zero vector is in H, since 0D0v1C0v2. To show that His closed
under vector addition, take two arbitrary vectors in H, say,
uDs1v1Cs2v2and wDt1v1Ct2v2
By Axioms 2, 3, and 8 for the vector space V,
uCwD.s1v1Cs2v2/C.t1v1Ct2v2/
D.s1Ct1/v1C.s2Ct2/v2
SouCwis inH. Furthermore, if cis any scalar, then by Axioms 7 and 9,
cuDc.s1v1Cs2v2/D.cs1/v1C.cs2/v2
which shows that cuis inHandHis closed under scalar multiplication. Thus His a
subspace of V.
In Section 4.5, you will see that every nonzero subspace of R3, other than R3itself,
is either Span fv1,v2gfor some linearly independent v1andv2or Span fvgforv¬§0. In
the Ô¨Årst case, the subspace is a plane through the origin; in the second case, it is a line
through the origin. (See Figure 9.) It is helpful to keep these geometric pictures in mind,
even for an abstract vector space.
x3
x2
x1v2 v10FIGURE 9
An example of a subspace.
The argument in Example 10 can easily be generalized to prove the following
theorem.
T H E O R E M 1 Ifv1; : : : ; vpare in a vector space V, then Span fv1; : : : ; vpgis a subspace of V.
We call Span fv1; : : : ; vpgthe subspace spanned (orgenerated ) byfv1; : : : ; vpg.
Given any subspace HofV, aspanning (orgenerating )setforHis a set fv1; : : : ; vpg
inHsuch that HDSpanfv1; : : : ; vpg.
The next example shows how to use Theorem 1.
EXAMPLE 11 LetHbe the set of all vectors of the form .a 3b; b a; a; b/ ,
where aandbare arbitrary scalars. That is, let HD f.a 3b; b a; a; b/ Waandbin
Rg. Show that His a subspace of R4.
SOLUTION Write the vectors in Has column vectors. Then an arbitrary vector in H
has the form
SECOND REVISED PAGES


--- Page 214 ---
4.1 Vector Spaces and Subspaces 197
2
664a 3b
b a
a
b3
775Da2
6641
 1
1
03
775
6v1Cb2
664 3
1
0
13
775
6v2
This calculation shows that HDSpanfv1;v2g, where v1andv2are the vectors indicated
above. Thus His a subspace of R4by Theorem 1.
Example 11 illustrates a useful technique of expressing a subspace Has the set
of linear combinations of some small collection of vectors. If HDSpanfv1; : : : ; vpg,
we can think of the vectors v1; : : : ; vpin the spanning set as ‚Äúhandles‚Äù that allow us to
hold on to the subspace H. Calculations with the inÔ¨Ånitely many vectors in Hare often
reduced to operations with the Ô¨Ånite number of vectors in the spanning set.
EXAMPLE 12 For what value(s) of hwillybe in the subspace of R3spanned by
v1;v2;v3, if
v1D2
41
 1
 23
5; v2D2
45
 4
 73
5; v3D2
4 3
1
03
5;and yD2
4 4
3
h3
5
SOLUTION This question is Practice Problem 2 in Section 1.3, written here with
the term subspace rather than Span fv1;v2;v3g. The solution there shows that yis in
Spanfv1;v2;v3gif and only if hD5. That solution is worth reviewing now, along with
Exercises 11‚Äì16 and 19‚Äì21 in Section 1.3.
Although many vector spaces in this chapter will be subspaces of Rn, it is important
to keep in mind that the abstract theory applies to other vector spaces as well. Vector
spaces of functions arise in many applications, and they will receive more attention later.
PRACTICE PROBLEMS
1.Show that the set Hof all points in R2of the form .3s; 2C5s/is not a vector space,
by showing that it is not closed under scalar multiplication. (Find a speciÔ¨Åc vector u
inHand a scalar csuch that cuis not in H.)
2.LetWDSpanfv1; : : : ; vpg, where v1; : : : ; vpare in a vector space V. Show that vk
is inWfor1kp. [Hint: First write an equation that shows that v1is inW.
Then adjust your notation for the general case.]
3.Annnmatrix Ais said to be symmetric if ATDA. LetSbe the set of all 33
symmetric matrices. Show that Sis a subspace of M33, the vector space of 33
matrices.
WEB
4.1 EXERCISES
1.LetVbe the Ô¨Årst quadrant in the xy-plane; that is, let
VDx
y
Wx0; y0
a.Ifuandvare in V, isuCvinV? Why?
b.Find a speciÔ¨Åc vector uinVand a speciÔ¨Åc scalar csuchthatcuisnotinV. (This is enough to show that Visnot
a vector space.)
2.LetWbe the union of the Ô¨Årst and third quadrants in the xy-
plane. That is, let WDx
y
Wxy0
.
a.Ifuis inWandcis any scalar, is cuinW? Why?
SECOND REVISED PAGES


--- Page 215 ---
198 CHAPTER 4 Vector Spaces
b.Find speciÔ¨Åc vectors uandvinWsuch that uCvis
not in W. This is enough to show that Wisnota vector
space.
3.LetHbe the set of points inside and on the unit circle in
thexy-plane. That is, let HDx
y
Wx2Cy21
. Find
a speciÔ¨Åc example‚Äîtwo vectors or a vector and a scalar‚Äîto
show that His not a subspace of R2.
4.Construct a geometric Ô¨Ågure that illustrates why a line in R2
notthrough the origin is not closed under vector addition.
In Exercises 5‚Äì8, determine if the given set is a subspace of Pnfor
an appropriate value of n. Justify your answers.
5.All polynomials of the form p.t/Dat2, where ais inR.
6.All polynomials of the form p.t/DaCt2, where ais inR.
7.All polynomials of degree at most 3, with integers as coefÔ¨Å-
cients.
8.All polynomials in Pnsuch that p.0/D0.
9.LetHbe the set of all vectors of the form2
4s
3s
2s3
5. Find a
vector vinR3such that HDSpanfvg. Why does this show
thatHis a subspace of R3?
10. LetHbe the set of all vectors of the form2
42t
0
 t3
5. Show that
His a subspace of R3. (Use the method of Exercise 9.)
11.LetWbe the set of all vectors of the form2
45bC2c
b
c3
5,
where bandcare arbitrary. Find vectors uandvsuch that
WDSpanfu;vg. Why does this show that Wis a subspace
ofR3?
12. LetWbe the set of all vectors of the form2
664sC3t
s t
2s t
4t3
775.
Show that Wis a subspace of R4. (Use the method of
Exercise 11.)
13. Letv1D2
41
0
 13
5,v2D2
42
1
33
5,v3D2
44
2
63
5, and wD2
43
1
23
5.
a.Iswinfv1;v2;v3g? How many vectors are in fv1;v2;v3g?
b.How many vectors are in Span fv1;v2;v3g?
c.Iswin the subspace spanned by fv1;v2;v3g? Why?
14. Letv1;v2;v3be as in Exercise 13, and let wD2
48
4
73
5. Iswin
the subspace spanned by fv1;v2;v3g? Why?In Exercises 15‚Äì18, let Wbe the set of all vectors of the form
shown, where a,b, andcrepresent arbitrary real numbers. In each
case, either Ô¨Ånd a set Sof vectors that spans Wor give an example
to show that Wisnota vector space.
15.2
43aCb
4
a 5b3
5 16.2
4 aC1
a 6b
2bCa3
5
17.2
664a b
b c
c a
b3
77518.2
6644aC3b
0
aCbCc
c 2a3
775
19. If a mass mis placed at the end of a spring, and if the mass is
pulled downward and released, the mass‚Äìspring system will
begin to oscillate. The displacement yof the mass from its
resting position is given by a function of the form
y.t/Dc1cos!tCc2sin!t (5)
where !is a constant that depends on the spring and the mass.
(See the Ô¨Ågure below.) Show that the set of all functions
described in (5) (with !Ô¨Åxed and c1,c2arbitrary) is a vector
space.
y
20. The set of all continuous real-valued functions deÔ¨Åned on a
closed interval ¬åa; b¬ç inRis denoted by C ¬åa; b¬ç . This set is
a subspace of the vector space of all real-valued functions
deÔ¨Åned on ¬åa; b¬ç .
a.What facts about continuous functions should be proved
in order to demonstrate that C ¬åa; b¬ç is indeed a subspace
as claimed? (These facts are usually discussed in a calcu-
lus class.)
b.Show that ffinC ¬åa; b¬ç Wf.a/Df.b/gis a subspace of
C ¬åa; b¬ç .
For Ô¨Åxed positive integers mandn, the set Mmnof all mn
matrices is a vector space, under the usual operations of addition
of matrices and multiplication by real scalars.
21. Determine if the set Hof all matrices of the forma b
0 d
is a subspace of M22.
22. LetFbe a Ô¨Åxed 32matrix, and let Hbe the set of all
matrices AinM24with the property that FAD0(the zero
matrix in M34/. Determine if His a subspace of M24.
SECOND REVISED PAGES


--- Page 216 ---
4.1 Vector Spaces and Subspaces 199
In Exercises 23 and 24, mark each statement True or False. Justify
each answer.
23. a.Iffis a function in the vector space Vof all real-valued
functions on Rand if f.t/D0for some t, then fis the
zero vector in V.
b.A vector is an arrow in three-dimensional space.
c.A subset Hof a vector space Vis a subspace of Vif the
zero vector is in H.
d.A subspace is also a vector space.
e.Analog signals are used in the major control systems for
the space shuttle, mentioned in the introduction to the
chapter.
24. a.A vector is any element of a vector space.
b.Ifuis a vector in a vector space V, then . 1/uis the same
as the negative of u.
c.A vector space is also a subspace.
d.R2is a subspace of R3.
e.A subset Hof a vector space Vis a subspace of Vif the
following conditions are satisÔ¨Åed: (i) the zero vector of V
is inH, (ii) u,v, and uCvare in H, and (iii) cis a scalar
andcuis inH.
Exercises 25‚Äì29 show how the axioms for a vector space Vcan
be used to prove the elementary properties described after the
deÔ¨Ånition of a vector space. Fill in the blanks with the appropriate
axiom numbers. Because of Axiom 2, Axioms 4 and 5 imply,
respectively, that 0CuDuand uCuD0for all u.
25. Complete the following proof that the zero vector is
unique. Suppose that winVhas the property that
uCwDwCuDufor all uinV. In particular, 0CwD0.
But0CwDw, by Axiom . Hence wD0CwD0.
26. Complete the following proof that  uis the unique vec-
torinVsuch that uC. u/D0. Suppose that wsatisÔ¨Åes
uCwD0. Adding  uto both sides, we have
. u/C¬åuCw¬çD. u/C0
¬å. u/Cu¬çCwD. u/C0 by Axiom (a)
0CwD. u/C0 by Axiom (b)
wD  u by Axiom (c)
27. Fill in the missing axiom numbers in the following proof that
0uD0for every uinV.
0uD.0C0/uD0uC0u by Axiom (a)
Add the negative of 0uto both sides:
0uC. 0u/D¬å0uC0u¬çC. 0u/
0uC. 0u/D0uC¬å0uC. 0u/¬ç by Axiom (b)
0D0uC0 by Axiom (c)
0D0u by Axiom (d)28. Fill in the missing axiom numbers in the following proof that
c0D0for every scalar c.
c0Dc.0C0/ by Axiom (a)
Dc0Cc0 by Axiom (b)
Add the negative of c0to both sides:
c0C. c0/D¬åc0Cc0¬çC. c0/
c0C. c0/Dc0C¬åc0C. c0/¬ç by Axiom (c)
0Dc0C0 by Axiom (d)
0Dc0 by Axiom (e)
29. Prove that . 1/uD  u. [Hint: Show that uC. 1/uD0.
Use some axioms and the results of Exercises 26 and 27.]
30. Suppose cuD0for some nonzero scalar c. Show that uD0.
Mention the axioms or properties you use.
31. Letuandvbe vectors in a vector space V, and let Hbe any
subspace of Vthat contains both uandv. Explain why H
also contains Span fu;vg. This shows that Span fu;vgis the
smallest subspace of Vthat contains both uandv.
32. LetHandKbe subspaces of a vector space V. The intersec-
tion ofHandK, written as H\K, is the set of vinVthat
belong to both HandK. Show that H\Kis a subspace of
V. (See the Ô¨Ågure.) Give an example in R2to show that the
union of two subspaces is not, in general, a subspace.
0
KH
VH /H20669 K
33. Given subspaces HandKof a vector space V, the sum of
HandK, written as HCK, is the set of all vectors in V
that can be written as the sum of two vectors, one in Hand
the other in K; that is,
HCKD fw:w=u+vfor some uinH
and some vinKg
a.Show that HCKis a subspace of V.
b.Show that His a subspace of HCKandKis a subspace
ofHCK.
34. Suppose u1; : : : ; upandv1; : : : ; vqare vectors in a vector
space V, and let
HDSpanfu1; : : : ; upgandKDSpanfv1; : : : ; vqg
Show that HCKDSpanfu1; : : : ; up;v1; : : : ; vqg.
SECOND REVISED PAGES


--- Page 217 ---
200 CHAPTER 4 Vector Spaces
35. [M] Show that wis in the subspace of R4spanned by
v1;v2;v3, where
wD2
6649
 4
 4
73
775;v1D2
6648
 4
 3
93
775;v2D2
664 4
3
 2
 83
775;v3D2
664 7
6
 5
 183
775
36. [M] Determine if yis in the subspace of R4spanned by the
columns of A, where
yD2
664 4
 8
6
 53
775; AD2
6643 5 9
8 7  6
 5 8 3
2 2 93
775
37. [M] The vector space HDSpanf1;cos2t;cos4t;cos6tg
contains at least two interesting functions that will be usedin a later exercise:
f.t/D1 8cos2tC8cos4t
g.t/D  1C18cos2t 48cos4tC32cos6t
Study the graph of ffor0t2, and guess a simple for-
mula for f.t/. Verify your conjecture by graphing the differ-
ence between 1Cf.t/and your formula for f.t/. (Hopefully,
you will see the constant function 1.) Repeat for g.
38. [M] Repeat Exercise 37 for the functions
f.t/D3sint 4sin3t
g.t/D1 8sin2tC8sin4t
h.t/D5sint 20sin3tC16sin5t
in the vector space Span f1;sint;sin2t; : : : ; sin5tg.
SOLUTIONS TO PRACTICE PROBLEMS
1.Take any uinH‚Äîsay, uD3
7
‚Äîand take any c¬§1‚Äîsay, cD2. Then
cuD6
14
. If this is in H, then there is some ssuch that
3s
2C5s
D6
14
That is, sD2andsD12=5 , which is impossible. So 2uis not in HandHis not a
vector space.
2.v1D1v1C0v2C  C 0vp. This expresses v1as a linear combination of
v1; : : : ; vp, sov1is inW. In general, vkis inWbecause
vkD0v1C  C 0vk 1C1vkC0vkC1C  C 0vp
3.The subset Sis a subspace of M33since it satisÔ¨Åes all three of the requirements
listed in the deÔ¨Ånition of a subspace:
a.Observe that the 0inM33is the 33zero matrix and since 0TD0, the matrix
0is symmetric and hence 0is inS.
b.LetAandBinS. Notice that AandBare33symmetric matrices so ATDA
andBTDB. By the properties of transposes of matrices, .ACB/TDATC
BTDACB. Thus ACBis symmetric and hence ACBis inS.
c.LetAbe in Sand let cbe a scalar. Since Ais symmetric, by the properties of
symmetric matrices, .cA/TDc.AT/DcA. Thus cAis also a symmetric matrix
and hence cAis inS.
4.2 NULL SPACES, COLUMN SPACES, AND LINEAR TRANSFORMATIONS
In applications of linear algebra, subspaces of Rnusually arise in one of two ways:
(1) as the set of all solutions to a system of homogeneous linear equations or (2) as the
set of all linear combinations of certain speciÔ¨Åed vectors. In this section, we compare
and contrast these two descriptions of subspaces, allowing us to practice using the
concept of a subspace. Actually, as you will soon discover, we have been working with
SECOND REVISED PAGES


--- Page 218 ---
4.2 Null Spaces, Column Spaces, and Linear Transformations 201
subspaces ever since Section 1.3. The main new feature here is the terminology. The
section concludes with a discussion of the kernel and range of a linear transformation.
The Null Space of a Matrix
Consider the following system of homogeneous equations:
x1 3x2 2x3D0
 5x1C9x2Cx3D0(1)
In matrix form, this system is written as AxD0, where
AD1 3 2
 5 9 1
(2)
Recall that the set of all xthat satisfy (1) is called the solution set of the system (1).
Often it is convenient to relate this set directly to the matrix Aand the equation AxD0.
We call the set of xthat satisfy AxD0thenull space of the matrix A.
D E F I N I T I O N Thenull space of an mnmatrix A, written as Nul A, is the set of all solutions
of the homogeneous equation AxD0. In set notation,
NulAD fxWxis inRnandAxD0g
A more dynamic description of Nul Ais the set of all xinRnthat are mapped into
the zero vector of Rmvia the linear transformation x7!Ax. See Figure 1.
00
/H11938nNul A
/H11938m
FIGURE 1
EXAMPLE 1 LetAbe the matrix in (2) above, and let uD2
45
3
 23
5. Determine if
ubelongs to the null space of A.
SOLUTION To test if usatisÔ¨Åes AuD0, simply compute
AuD1 3 2
 5 9 12
45
3
 23
5D5 9C4
 25C27 2
D0
0
Thus uis in Nul A.
The term space innull space is appropriate because the null space of a matrix is a
vector space, as shown in the next theorem.
T H E O R E M 2 The null space of an mnmatrix Ais a subspace of Rn. Equivalently, the set of all
solutions to a system AxD0ofmhomogeneous linear equations in nunknowns
is a subspace of Rn.
SECOND REVISED PAGES


--- Page 219 ---
202 CHAPTER 4 Vector Spaces
PROOF Certainly Nul Ais a subset of Rnbecause Ahasncolumns. We must show
that Nul AsatisÔ¨Åes the three properties of a subspace. Of course, 0is in Nul A. Next, let
uandvrepresent any two vectors in Nul A. Then
AuD0and AvD0
To show that uCvis in Nul A, we must show that A.uCv/D0. Using a property of
matrix multiplication, compute
A.uCv/DAuCAvD0C0D0
Thus uCvis in Nul A, and Nul Ais closed under vector addition. Finally, if cis any
scalar, then
A.cu/Dc.Au/Dc.0/D0
which shows that cuis in Nul A. Thus Nul Ais a subspace of Rn.
EXAMPLE 2 LetHbe the set of all vectors in R4whose coordinates a,b,c,d
satisfy the equations a 2bC5cDdandc aDb. Show that His a subspace of
R4.
SOLUTION Rearrange the equations that describe the elements of H, and note that H
is the set of all solutions of the following system of homogeneous linear equations:
a 2bC5c dD0
 a bCc D0
By Theorem 2, His a subspace of R4.
It is important that the linear equations deÔ¨Åning the set Hare homogeneous.
Otherwise, the set of solutions will deÔ¨Ånitely notbe a subspace (because the zero vector
is not a solution of a nonhomogeneous system). Also, in some cases, the set of solutions
could be empty.
An Explicit Description of NulA
There is no obvious relation between vectors in Nul Aand the entries in A. We say that
NulAis deÔ¨Åned implicitly , because it is deÔ¨Åned by a condition that must be checked.
No explicit list or description of the elements in Nul Ais given. However, solving
the equation AxD0amounts to producing an explicit description of Nul A. The next
example reviews the procedure from Section 1.5.
EXAMPLE 3 Find a spanning set for the null space of the matrix
AD2
4 3 6  1 1  7
1 2 2 3  1
2 4 5 8  43
5
SOLUTION The Ô¨Årst step is to Ô¨Ånd the general solution of AxD0in terms of free
variables. Row reduce the augmented matrix ¬åA0¬çtoreduced echelon form in order
to write the basic variables in terms of the free variables:
2
41 2 0  1 3 0
0 0 1 2  2 0
0 0 0 0 0 03
5;x1 2x2 x4C3x5D0
x3C2x4 2x5D0
0D0
SECOND REVISED PAGES


--- Page 220 ---
4.2 Null Spaces, Column Spaces, and Linear Transformations 203
The general solution is x1D2x2Cx4 3x5,x3D  2x4C2x5, with x2,x4, and x5
free. Next, decompose the vector giving the general solution into a linear combination
of vectors where the weights are the free variables . That is,
2
66664x1
x2
x3
x4
x53
77775D2
666642x2Cx4 3x5
x2
 2x4C2x5
x4
x53
77775Dx22
666642
1
0
0
03
77775
"
uCx42
666641
0
 2
1
03
77775
"
vCx52
66664 3
0
2
0
13
77775
"
w
Dx2uCx4vCx5w (3)
Every linear combination of u,v, and wis an element of Nul Aand vice versa. Thus
fu;v;wgis a spanning set for Nul A.
Two points should be made about the solution of Example 3 that apply to all
problems of this type where Nul Acontains nonzero vectors. We will use these facts
later.
1.The spanning set produced by the method in Example 3 is automatically linearly
independent because the free variables are the weights on the spanning vectors. For
instance, look at the 2nd, 4th, and 5th entries in the solution vector in (3) and note
thatx2uCx4vCx5wcan be 0only if the weights x2; x4, and x5are all zero.
2.When Nul Acontains nonzero vectors, the number of vectors in the spanning set for
NulAequals the number of free variables in the equation AxD0.
The Column Space of a Matrix
Another important subspace associated with a matrix is its column space. Unlike the
null space, the column space is deÔ¨Åned explicitly via linear combinations.
D E F I N I T I O N Thecolumn space of an mnmatrix A, written as Col A, is the set of all linear
combinations of the columns of A. IfAD¬åa1 an¬ç, then
ColADSpanfa1; : : : ; ang
Since Span fa1; : : : ; angis a subspace, by Theorem 1, the next theorem follows from
the deÔ¨Ånition of Col Aand the fact that the columns of Aare inRm.
T H E O R E M 3 The column space of an mnmatrix Ais a subspace of Rm.
Note that a typical vector in Col Acan be written as Axfor some xbecause the
notation Axstands for a linear combination of the columns of A. That is,
ColAD fbWbDAxfor some xinRng
The notation Axfor vectors in Col Aalso shows that Col Ais the range of the linear
transformation x7!Ax. We will return to this point of view at the end of the section.
SECOND REVISED PAGES


--- Page 221 ---
204 CHAPTER 4 Vector Spaces
EXAMPLE 4 Find a matrix Asuch that WDColA.
Wx3
x2
x10
WD8
<
:2
46a b
aCb
 7a3
5Wa,binR9
=
;
SOLUTION First, write Was a set of linear combinations.
WD8
<
:a2
46
1
 73
5Cb2
4 1
1
03
5Wa,binR9
=
;DSpan8
<
:2
46
1
 73
5;2
4 1
1
03
59
=
;
Second, use the vectors in the spanning set as the columns of A. LetAD2
46 1
1 1
 7 03
5.
Then WDColA, as desired.
Recall from Theorem 4 in Section 1.4 that the columns of AspanRmif and only if
the equation AxDbhas a solution for each b. We can restate this fact as follows:
The column space of an mnmatrix Ais all of Rmif and only if the equation
AxDbhas a solution for each binRm.
The Contrast Between NulAand ColA
It is natural to wonder how the null space and column space of a matrix are related.
In fact, the two spaces are quite dissimilar, as Examples 5‚Äì7 will show. Nevertheless,
a surprising connection between the null space and column space will emerge in
Section 4.6, after more theory is available.
EXAMPLE 5 Let
AD2
42 4  2 1
 2 5 7 3
3 7  8 63
5
a.If the column space of Ais a subspace of Rk, what is k?
b.If the null space of Ais a subspace of Rk, what is k?
SOLUTION
a.The columns of Aeach have three entries, so Col Ais a subspace of Rk, where kD3.
b.A vector xsuch that Axis deÔ¨Åned must have four entries, so Nul Ais a subspace of
Rk, where kD4.
When a matrix is not square, as in Example 5, the vectors in Nul Aand Col Alive
in entirely different ‚Äúuniverses.‚Äù For example, no linear combination of vectors in R3
can produce a vector in R4. When Ais square, Nul Aand Col Ado have the zero vector
in common, and in special cases it is possible that some nonzero vectors belong to both
NulAand Col A.
SECOND REVISED PAGES


--- Page 222 ---
4.2 Null Spaces, Column Spaces, and Linear Transformations 205
EXAMPLE 6 With Aas in Example 5, Ô¨Ånd a nonzero vector in Col Aand a nonzero
vector in Nul A.
SOLUTION It is easy to Ô¨Ånd a vector in Col A. Any column of Awill do, say,2
42
 2
33
5.
To Ô¨Ånd a nonzero vector in Nul A, row reduce the augmented matrix ¬åA0¬çand obtain
¬åA0¬ç2
41 0 9 0 0
0 1  5 0 0
0 0 0 1 03
5
Thus, if xsatisÔ¨Åes AxD0, then x1D  9x3,x2D5x3,x4D0, and x3is free. As-
signing a nonzero value to x3‚Äîsay, x3D1‚Äîwe obtain a vector in Nul A, namely,
xD. 9; 5; 1; 0/ .
EXAMPLE 7 With Aas in Example 5, let uD2
6643
 2
 1
03
775andvD2
43
 1
33
5.
a.Determine if uis in Nul A. Could ube in Col A?
b.Determine if vis in Col A. Could vbe in Nul A?
SOLUTION
a.An explicit description of Nul Ais not needed here. Simply compute the product Au.
AuD2
42 4  2 1
 2 5 7 3
3 7  8 63
52
6643
 2
 1
03
775D2
40
 3
33
5¬§2
40
0
03
5
Obviously, uisnota solution of AxD0, souis not in Nul A. Also, with four entries,
ucould not possibly be in Col A, since Col Ais a subspace of R3.
b.Reduce ¬åAv¬çto an echelon form.
¬åAv¬çD2
42 4  2 1 3
 2 5 7 3  1
3 7  8 6 33
52
42 4  2 1 3
0 1  5 4 2
0 0 0 17 13
5
At this point, it is clear that the equation AxDvis consistent, so vis in Col A. With
only three entries, vcould not possibly be in Nul A, since Nul Ais a subspace of
R4.
The table on page 206 summarizes what we have learned about Nul Aand Col A.
Item 8 is a restatement of Theorems 11 and 12(a) in Section 1.9.
Kernel and Range of a Linear Transformation
Subspaces of vector spaces other than Rnare often described in terms of a linear
transformation instead of a matrix. To make this precise, we generalize the deÔ¨Ånition
given in Section 1.8.
SECOND REVISED PAGES


--- Page 223 ---
206 CHAPTER 4 Vector Spaces
Contrast Between Nul Aand Col Afor an mxnMatrix A
NulA ColA
1. Nul Ais a subspace of Rn. 1. Col Ais a subspace of Rm.
2. Nul Ais implicitly deÔ¨Åned; that is, you are
given only a condition .AxD0/that vec-
tors in Nul Amust satisfy.2. Col Ais explicitly deÔ¨Åned; that is, you are
told how to build vectors in Col A.
3. It takes time to Ô¨Ånd vectors in Nul A. Row
operations on ¬åA0¬çare required.3. It is easy to Ô¨Ånd vectors in Col A. The
columns of Aare displayed; others are
formed from them.
4. There is no obvious relation between Nul A
and the entries in A.4. There is an obvious relation between Col A
and the entries in A, since each column of
Ais in Col A.
5. A typical vector vin Nul Ahas the property
thatAvD0.5. A typical vector vin Col Ahas the property
that the equation AxDvis consistent.
6. Given a speciÔ¨Åc vector v, it is easy to tell if
vis in Nul A. Just compute Av.6. Given a speciÔ¨Åc vector v, it may take time
to tell if vis in Col A. Row operations on
¬åAv¬çare required.
7. Nul AD f0gif and only if the equation
AxD0has only the trivial solution.7. Col ADRmif and only if the equation
AxDbhas a solution for every binRm.
8. Nul AD f0gif and only if the linear trans-
formation x7!Axis one-to-one.8. Col ADRmif and only if the linear trans-
formation x7!AxmapsRnontoRm.
D E F I N I T I O N Alinear transformation Tfrom a vector space Vinto a vector space Wis a rule
that assigns to each vector xinVa unique vector T .x/inW, such that
(i)T .uCv/DT .u/CT .v/ for all u,vinV, and
(ii)T .cu/DcT .u/ for all uinVand all scalars c.
Thekernel (ornull space ) of such a Tis the set of all uinVsuch that T .u/D0
(the zero vector in W /. The range ofTis the set of all vectors in Wof the form T .x/
for some xinV. IfThappens to arise as a matrix transformation‚Äîsay, T .x/DAx
for some matrix A‚Äîthen the kernel and the range of Tare just the null space and the
column space of A, as deÔ¨Åned earlier.
It is not difÔ¨Åcult to show that the kernel of Tis a subspace of V. The proof is
essentially the same as the one for Theorem 2. Also, the range of Tis a subspace of W.
See Figure 2 and Exercise 30.
Kernel is a 
subspace of  VRange is a 
subspace of  WDomainRange
0T
0
VKernel
W
FIGURE 2 Subspaces associated with a
linear transformation.
In applications, a subspace usually arises as either the kernel or the range of an
appropriate linear transformation. For instance, the set of all solutions of a homoge-
neous linear differential equation turns out to be the kernel of a linear transformation.
SECOND REVISED PAGES


--- Page 224 ---
4.2 Null Spaces, Column Spaces, and Linear Transformations 207
Typically, such a linear transformation is described in terms of one or more derivatives
of a function. To explain this in any detail would take us too far aÔ¨Åeld at this point. So
we consider only two examples. The Ô¨Årst explains why the operation of differentiation
is a linear transformation.
EXAMPLE 8 (Calculus required) LetVbe the vector space of all real-valued func-
tions fdeÔ¨Åned on an interval ¬åa; b¬ç with the property that they are differentiable and
their derivatives are continuous functions on ¬åa; b¬ç . LetWbe the vector space C ¬åa; b¬ç
of all continuous functions on ¬åa; b¬ç , and let DWV!Wbe the transformation that
changes finVinto its derivative f0. In calculus, two simple differentiation rules are
D.fCg/DD.f / CD.g/ and D.cf / DcD.f /
That is, Dis a linear transformation. It can be shown that the kernel of Dis the set of
constant functions on ¬åa; b¬ç and the range of Dis the set Wof all continuous functions
on¬åa; b¬ç .
EXAMPLE 9 (Calculus required) The differential equation
y00C!2yD0 (4)
where !is a constant, is used to describe a variety of physical systems, such as the
vibration of a weighted spring, the movement of a pendulum, and the voltage in an
inductance-capacitance electrical circuit. The set of solutions of (4) is precisely the
kernel of the linear transformation that maps a function yDf .t/ into the function
f00.t/C!2f .t/ . Finding an explicit description of this vector space is a problem in
differential equations. The solution set turns out to be the space described in Exercise 19
in Section 4.1.
PRACTICE PROBLEMS
1.LetWD8
<
:2
4a
b
c3
5Wa 3b cD09
=
;. Show in two different ways that Wis a
subspace of R3. (Use two theorems.)
2.LetAD2
47 3 5
 4 1  5
 5 2  43
5,vD2
42
1
 13
5, and wD2
47
6
 33
5. Suppose you know that
the equations AxDvandAxDware both consistent. What can you say about the
equation AxDvCw?
3.LetAbe an nnmatrix. If Col ADNulA, show that Nul A2DRn.
4.2 EXERCISES
1.Determine if wD2
41
3
 43
5is in Nul A, where
AD2
43 5 3
6 2 0
 8 4 13
5:2.Determine if wD2
45
 3
23
5is in Nul A, where
AD2
45 21 19
13 23 2
8 14 13
5:
SECOND REVISED PAGES


--- Page 225 ---
208 CHAPTER 4 Vector Spaces
In Exercises 3‚Äì6, Ô¨Ånd an explicit description of Nul Aby listing
vectors that span the null space.
3.AD1 3 5 0
0 1 4  2
4.AD1 6 4 0
0 0 2 0
5.AD2
41 2 0 4 0
0 0 1  9 0
0 0 0 0 13
5
6.AD2
41 5  4 3 1
0 1  2 1 0
0 0 0 0 03
5
In Exercises 7‚Äì14, either use an appropriate theorem to show that
the given set, W, is a vector space, or Ô¨Ånd a speciÔ¨Åc example to
the contrary.
7.8
<
:2
4a
b
c3
5WaCbCcD29
=
;8.8
<
:2
4r
s
t3
5W5r 1DsC2t9
=
;
9.8
¬à¬à<
¬à¬à:2
664a
b
c
d3
775Wa 2bD4c
2aDcC3d9
>>=
>>;10.8
¬à¬à<
¬à¬à:2
664a
b
c
d3
775WaC3bDc
bCcCaDd9
>>=
>>;
11.8
¬à¬à<
¬à¬à:2
664b 2d
5Cd
bC3d
d3
775Wb; d real9
>>=
>>;12.8
¬à¬à<
¬à¬à:2
664b 5d
2b
2dC1
d3
775Wb; d real9
>>=
>>;
13.8
<
:2
4c 6d
d
c3
5Wc; dreal9
=
;14.8
<
:2
4 aC2b
a 2b
3a 6b3
5Wa; breal9
=
;
In Exercises 15 and 16, Ô¨Ånd Asuch that the given set is Col A.
15.8
¬à¬à<
¬à¬à:2
6642sC3t
rCs 2t
4rCs
3r s t3
775Wr; s; t real9
>>=
>>;
16.8
¬à¬à<
¬à¬à:2
664b c
2bCcCd
5c 4d
d3
775Wb; c; d real9
>>=
>>;
For the matrices in Exercises 17‚Äì20, (a) Ô¨Ånd ksuch that Nul Ais
a subspace of Rk, and (b) Ô¨Ånd ksuch that Col Ais a subspace of
Rk.
17.AD2
6642 6
 1 3
 4 12
3 93
77518.AD2
6647 2 0
 2 0  5
0 5 7
 5 7  23
775
19.AD4 5  2 6 0
1 1 0 1 0
20.AD1 3 9 0  521. With Aas in Exercise 17, Ô¨Ånd a nonzero vector in Nul Aand
a nonzero vector in Col A.
22. With Aas in Exercise 3, Ô¨Ånd a nonzero vector in Nul Aand
a nonzero vector in Col A.
23. LetAD 6 12
 3 6
andwD2
1
. Determine if wis in
ColA. Iswin Nul A?
24. LetAD2
4 8 2 9
6 4 8
4 0 43
5andwD2
42
1
 23
5. Determine if
wis in Col A. Iswin Nul A?
In Exercises 25 and 26, Adenotes an mnmatrix. Mark each
statement True or False. Justify each answer.
25. a.The null space of Ais the solution set of the equation
AxD0.
b.The null space of an mnmatrix is in Rm.
c.The column space of Ais the range of the mapping
x7!Ax.
d.If the equation AxDbis consistent, then Col AisRm.
e.The kernel of a linear transformation is a vector space.
f.ColAis the set of all vectors that can be written as Axfor
some x.
26. a.A null space is a vector space.
b.The column space of an mnmatrix is in Rm.
c.ColAis the set of all solutions of AxDb.
d.NulAis the kernel of the mapping x7!Ax.
e.The range of a linear transformation is a vector space.
f.The set of all solutions of a homogeneous linear differen-
tial equation is the kernel of a linear transformation.
27. It can be shown that a solution of the system below is x1D3,
x2D2, and x3D  1. Use this fact and the theory from this
section to explain why another solution is x1D30,x2D20,
andx3D  10. (Observe how the solutions are related, but
make no other calculations.)
x1 3x2 3x3D0
 2x1C4x2C2x3D0
 x1C5x2C7x3D0
28. Consider the following two systems of equations:
5x1Cx2 3x3D0 5x1Cx2 3x3D0
 9x1C2x2C5x3D1  9x1C2x2C5x3D5
4x1Cx2 6x3D9 4x 1Cx2 6x3D45
It can be shown that the Ô¨Årst system has a solution. Use
this fact and the theory from this section to explain why
the second system must also have a solution. (Make no row
operations.)
SECOND REVISED PAGES


--- Page 226 ---
4.2 Null Spaces, Column Spaces, and Linear Transformations 209
29. Prove Theorem 3 as follows: Given an mnmatrix A, an
element in Col Ahas the form Axfor some xinRn. LetAx
andAwrepresent any two vectors in Col A.
a.Explain why the zero vector is in Col A.
b.Show that the vector AxCAwis in Col A.
c.Given a scalar c, show that c.Ax/is in Col A.
30. LetTWV!Wbe a linear transformation from a vector
space Vinto a vector space W. Prove that the range of Tis
a subspace of W. [Hint: Typical elements of the range have
the form T .x/andT .w/for some x,winV.]
31. DeÔ¨Åne TWP2!R2byT .p/Dp.0/
p.1/
. For instance, if
p.t/D3C5tC7t2, then T .p/D3
15
.
a.Show that Tis a linear transformation. [ Hint: For ar-
bitrary polynomials p,qinP2, compute T .pCq/and
T .cp/.]
b.Find a polynomial pinP2that spans the kernel of T, and
describe the range of T.
32. DeÔ¨Åne a linear transformation TWP2!R2by
T .p/Dp.0/
p.0/
. Find polynomials p1and p2inP2that
span the kernel of T, and describe the range of T.
33. LetM22be the vector space of all 22matrices,
and deÔ¨Åne TWM22!M22byT .A/DACAT, where
ADa b
c d
.
a.Show that Tis a linear transformation.
b.LetBbe any element of M22such that BTDB. Find
anAinM22such that T .A/DB.
c.Show that the range of Tis the set of BinM22with the
property that BTDB.
d.Describe the kernel of T.
34. (Calculus required ) DeÔ¨Åne TWC ¬å0; 1¬ç !C ¬å0; 1¬ç as follows:
ForfinC ¬å0; 1¬ç , letT .f/be the antiderivative Foffsuch
thatF.0/D0. Show that Tis a linear transformation, and
describe the kernel of T. (See the notation in Exercise 20 of
Section 4.1.)35. LetVandWbe vector spaces, and let TWV!Wbe a linear
transformation. Given a subspace UofV, letT .U / denote
the set of all images of the form T .x/, where xis inU. Show
thatT .U / is a subspace of W.
36. Given TWV!Was in Exercise 35, and given a subspace
ZofW, letUbe the set of all xinVsuch that T .x/is inZ.
Show that Uis a subspace of V.
37. [M] Determine whether wis in the column space of A, the
null space of A, or both, where
wD2
6641
1
 1
 33
775; AD2
6647 6  4 1
 5 1 0  2
9 11 7  3
19 9 7 13
775
38. [M] Determine whether wis in the column space of A, the
null space of A, or both, where
wD2
6641
2
1
03
775; AD2
664 8 5  2 0
 5 2 1  2
10 8 6  3
3 2 1 03
775
39. [M] Let a1; : : : ; a5denote the columns of the matrix A, where
AD2
6645 1 2 2 0
3 3 2  1 12
8 4 4  5 12
2 1 1 0  23
775; BD¬åa1a2a4¬ç
a.Explain why a3anda5are in the column space of B.
b.Find a set of vectors that spans Nul A.
c.LetTWR5!R4be deÔ¨Åned by T .x/DAx. Explain why
Tis neither one-to-one nor onto.
40. [M] Let HDSpanfv1;v2gandKDSpanfv3;v4g, where
v1D2
45
3
83
5;v2D2
41
3
43
5;v3D2
42
 1
53
5;v4D2
40
 12
 283
5:
Then HandKare subspaces of R3. In fact, HandK
are planes in R3through the origin, and they intersect
in a line through 0. Find a nonzero vector wthat gen-
erates that line. [ Hint: wcan be written as c1v1Cc2v2
and also as c3v3Cc4v4. To build w, solve the equation
c1v1Cc2v2Dc3v3Cc4v4for the unknown cj‚Äôs.]
SG
Mastering: Vector Space, Subspace,
ColA, and Nul A4‚Äì6
SOLUTIONS TO PRACTICE PROBLEMS
1.First method: Wis a subspace of R3by Theorem 2 because Wis the set of all solu-
tions to a system of homogeneous linear equations (where the system has only one
equation). Equivalently, Wis the null space of the 13matrix AD¬å1 3 1¬ç.
SECOND REVISED PAGES


--- Page 227 ---
210 CHAPTER 4 Vector Spaces
Second method: Solve the equation a 3b cD0for the leading variable ain
terms of the free variables bandc. Any solution has the form2
43bCc
b
c3
5, where b
andcare arbitrary, and
2
43bCc
b
c3
5Db2
43
1
03
5
"
v1Cc2
41
0
13
5
"
v2
This calculation shows that WDSpanfv1;v2g. Thus Wis a subspace of R3by
Theorem 1. We could also solve the equation a 3b cD0forborcand get
alternative descriptions of Was a set of linear combinations of two vectors.
2.Both vandware in Col A. Since Col Ais a vector space, vCwmust be in Col A.
That is, the equation AxDvCwis consistent.
3.Letxbe any vector in Rn. Notice Axis in Col A, since it is a linear combination
of the columns of A. Since Col ADNulA, the vector Axis also in Nul A. Hence
A2xDA.Ax/D0establishing that every vector xfromRnis in Nul A2.
4.3 LINEARLY INDEPENDENT SETS; BASES
In this section we identify and study the subsets that span a vector space Vor a subspace
Has ‚ÄúefÔ¨Åciently‚Äù as possible. The key idea is that of linear independence, deÔ¨Åned as
inRn.
An indexed set of vectors fv1; : : : ; vpginVis said to be linearly independent if
the vector equation
c1v1Cc2v2C  C cpvpD0 (1)
hasonly the trivial solution, c1D0; : : : ; c pD0.1
The set fv1; : : : ; vpgis said to be linearly dependent if (1) has a nontrivial solution,
that is, if there are some weights, c1; : : : ; c p,not all zero , such that (1) holds. In such a
case, (1) is called a linear dependence relation among v1; : : : ; vp.
Just as in Rn, a set containing a single vector vis linearly independent if and only
ifv¬§0. Also, a set of two vectors is linearly dependent if and only if one of the vectors
is a multiple of the other. And any set containing the zero vector is linearly dependent.
The following theorem has the same proof as Theorem 7 in Section 1.7.
T H E O R E M 4 An indexed set fv1; : : : ; vpgof two or more vectors, with v1¬§0, is linearly
dependent if and only if some vj(with j > 1/ is a linear combination of the
preceding vectors, v1; : : : ; vj 1.
The main difference between linear dependence in Rnand in a general vector space
is that when the vectors are not n-tuples, the homogeneous equation (1) usually cannot
be written as a system of nlinear equations. That is, the vectors cannot be made into the
columns of a matrix Ain order to study the equation AxD0. We must rely instead on
the deÔ¨Ånition of linear dependence and on Theorem 4.
1It is convenient to use c1; : : : ; c pin (1) for the scalars instead of x1; : : : ; x p, as we did in Chapter 1.
SECOND REVISED PAGES


--- Page 228 ---
4.3 Linearly Independent Sets; Bases 211
EXAMPLE 1 Letp1.t/D1,p2.t/Dt, and p3.t/D4 t. Then fp1;p2;p3gis
linearly dependent in Pbecause p3D4p1 p2.
EXAMPLE 2 The set fsint;costgis linearly independent in C ¬å0; 1¬ç , the space of
all continuous functions on 0t1, because sin tand cos tare not multiples of one
another as vectors in C ¬å0; 1¬ç . That is, there is no scalar csuch that cos tDcsintfor all
tin¬å0; 1¬ç . (Look at the graphs of sin tand cos t.) However, fsintcost;sin2tgis linearly
dependent because of the identity: sin 2tD2sintcost, for all t.
D E F I N I T I O N LetHbe a subspace of a vector space V. An indexed set of vectors
BD fb1; : : : ; bpginVis abasis forHif
(i)Bis a linearly independent set, and
(ii)the subspace spanned by Bcoincides with H; that is,
HDSpanfb1; : : : ; bpg
The deÔ¨Ånition of a basis applies to the case when HDV, because any vector space
is a subspace of itself. Thus a basis of Vis a linearly independent set that spans V.
Observe that when H¬§V, condition (ii) includes the requirement that each of the
vectors b1; : : : ; bpmust belong to H, because Span fb1; : : : ; bpgcontains b1; : : : ; bp,
as shown in Section 4.1.
EXAMPLE 3 LetAbe an invertible nnmatrix‚Äîsay, AD¬åa1 an¬ç. Then
the columns of Aform a basis for Rnbecause they are linearly independent and they
spanRn, by the Invertible Matrix Theorem.
EXAMPLE 4 Lete1; : : : ; enbe the columns of the nnidentity matrix, In. That
x1x2x3
e3
e2
e1
FIGURE 1
The standard basis for R3.is,
e1D2
66641
0
:::
03
7775;e2D2
66640
1
:::
03
7775; : : : ; enD2
66640
:::
0
13
7775
The set fe1; : : : ; engis called the standard basis forRn(Figure 1).
EXAMPLE 5 Letv1D2
43
0
 63
5,v2D2
4 4
1
73
5, and v3D2
4 2
1
53
5. Determine if
fv1;v2;v3gis a basis for R3.
SOLUTION Since there are exactly three vectors here in R3, we can use any of several
methods to determine if the matrix AD¬åv1v2v3¬çis invertible. For instance, two
row replacements reveal that Ahas three pivot positions. Thus Ais invertible. As in
Example 3, the columns of Aform a basis for R3.
EXAMPLE 6 LetSD f1; t; t2; : : : ; tng. Verify that Sis a basis for Pn. This basis
is called the standard basis forPn.
SOLUTION Certainly SspansPn. To show that Sis linearly independent, suppose that
c0; : : : ; c nsatisfy
c01Cc1tCc2t2C  C cntnD0.t/ (2)
SECOND REVISED PAGES


--- Page 229 ---
212 CHAPTER 4 Vector Spaces
This equality means that the polynomial on the left has the same values as the zero
polynomial on the right. A fundamental theorem in algebra says that the only polynomial
inPnwith more than nzeros is the zero polynomial. That is, equation (2) holds for all
tonly if c0D  D cnD0. This proves that Sis linearly independent and hence is a
y = 1y = ty = t2
tty
FIGURE 2
The standard basis for P2.basis for Pn. See Figure 2.
Problems involving linear independence and spanning in Pnare handled best by a
technique to be discussed in Section 4.4.
The Spanning Set Theorem
As we will see, a basis is an ‚ÄúefÔ¨Åcient‚Äù spanning set that contains no unnecessary vectors.
In fact, a basis can be constructed from a spanning set by discarding unneeded vectors.
EXAMPLE 7 Let
v1D2
40
2
 13
5;v2D2
42
2
03
5;v3D2
46
16
 53
5;and HDSpanfv1;v2;v3g:
Note that v3D5v1C3v2, and show that Span fv1;v2;v3g DSpanfv1;v2g. Then Ô¨Ånd a
basis for the subspace H.
SOLUTION Every vector in Span fv1;v2gbelongs to Hbecause
H
v1v3
v2
c1v1Cc2v2Dc1v1Cc2v2C0v3
Now let xbe any vector in H‚Äîsay, xDc1v1Cc2v2Cc3v3. Since v3D5v1C3v2, we
may substitute
xDc1v1Cc2v2Cc3.5v1C3v2/
D.c1C5c3/v1C.c2C3c3/v2
Thus xis in Span fv1;v2g, so every vector in Halready belongs to Span fv1;v2g. We
conclude that Hand Span fv1;v2gare actually the same set of vectors. It follows that
fv1;v2gis a basis of Hsincefv1;v2gis obviously linearly independent.
The next theorem generalizes Example 7.
T H E O R E M 5 The Spanning Set Theorem
LetSD fv1; : : : ; vpgbe a set in V, and let HDSpanfv1; : : : ; vpg.
a.If one of the vectors in S‚Äîsay, vk‚Äîis a linear combination of the remaining
vectors in S, then the set formed from Sby removing vkstill spans H.
b.IfH¬§ f0g, some subset of Sis a basis for H.
PROOF
a.By rearranging the list of vectors in S, if necessary, we may suppose that vpis a
linear combination of v1; : : : ; vp 1‚Äîsay,
vpDa1v1C  C ap 1vp 1 (3)
Given any xinH, we may write
xDc1v1C  C cp 1vp 1Ccpvp (4)
for suitable scalars c1; : : : ; c p. Substituting the expression for vpfrom (3) into (4),
it is easy to see that xis a linear combination of v1; : : : ; vp 1. Thus fv1; : : : ; vp 1g
spans H, because xwas an arbitrary element of H.
SECOND REVISED PAGES


--- Page 230 ---
4.3 Linearly Independent Sets; Bases 213
b.If the original spanning set Sis linearly independent, then it is already a basis for H.
Otherwise, one of the vectors in Sdepends on the others and can be deleted, by part
(a). So long as there are two or more vectors in the spanning set, we can repeat this
process until the spanning set is linearly independent and hence is a basis for H. If
the spanning set is eventually reduced to one vector, that vector will be nonzero (and
hence linearly independent) because H¬§ f0g.
Bases for NulAand ColA
We already know how to Ô¨Ånd vectors that span the null space of a matrix A. The
discussion in Section 4.2 pointed out that our method always produces a linearly
independent set when Nul Acontains nonzero vectors. So, in this case, that method
produces a basis for Nul A.
The next two examples describe a simple algorithm for Ô¨Ånding a basis for the
column space.
EXAMPLE 8 Find a basis for Col B, where
BDb1 b2 b5
D2
6641 4 0 2 0
0 0 1  1 0
0 0 0 0 1
0 0 0 0 03
775
SOLUTION Each nonpivot column of Bis a linear combination of the pivot columns.
In fact, b2D4b1andb4D2b1 b3. By the Spanning Set Theorem, we may discard
b2andb4, andfb1;b3;b5gwill still span Col B. Let
SD fb1;b3;b5g D8
¬à¬à<
¬à¬à:2
6641
0
0
03
775;2
6640
1
0
03
775;2
6640
0
1
03
7759
>>=
>>;
Since b1¬§0and no vector in Sis a linear combination of the vectors that precede it,
Sis linearly independent (Theorem 4). Thus Sis a basis for Col B.
What about a matrix Athat is notin reduced echelon form? Recall that any
linear dependence relationship among the columns of Acan be expressed in the form
AxD0, where xis a column of weights. (If some columns are not involved in a
particular dependence relation, then their weights are zero.) When Ais row reduced
to a matrix B, the columns of Bare often totally different from the columns of A.
However, the equations AxD0andBxD0have exactly the same set of solutions. If
AD¬åa1 an¬çandBD¬åb1 bn¬ç, then the vector equations
x1a1C  C xnanD0and x1b1C  C xnbnD0
also have the same set of solutions. That is, the columns of Ahave exactly the same
linear dependence relationships as the columns of B.
EXAMPLE 9 It can be shown that the matrix
ADa1 a2 a5
D2
6641 4 0 2  1
3 12 1 5 5
2 8 1 3 2
5 20 2 8 83
775
is row equivalent to the matrix Bin Example 8. Find a basis for Col A.
SECOND REVISED PAGES


--- Page 231 ---
214 CHAPTER 4 Vector Spaces
SOLUTION In Example 8 we saw that
b2D4b1and b4D2b1 b3
so we can expect that
a2D4a1and a4D2a1 a3
Check that this is indeed the case! Thus we may discard a2anda4when selecting
a minimal spanning set for Col A. In fact, fa1;a3;a5gmust be linearly independent
because any linear dependence relationship among a1,a3,a5would imply a linear
dependence relationship among b1,b3,b5. But we know that fb1;b3;b5gis a linearly
independent set. Thus fa1;a3;a5gis a basis for Col A. The columns we have used for
this basis are the pivot columns of A.
Examples 8 and 9 illustrate the following useful fact.
T H E O R E M 6 The pivot columns of a matrix Aform a basis for Col A.
PROOF The general proof uses the arguments discussed above. Let Bbe the reduced
echelon form of A. The set of pivot columns of Bis linearly independent, for no
vector in the set is a linear combination of the vectors that precede it. Since Ais row
equivalent to B, the pivot columns of Aare linearly independent as well, because any
linear dependence relation among the columns of Acorresponds to a linear dependence
relation among the columns of B. For this same reason, every nonpivot column of Ais
a linear combination of the pivot columns of A. Thus the nonpivot columns of Amay
be discarded from the spanning set for Col A, by the Spanning Set Theorem. This leaves
the pivot columns of Aas a basis for Col A.
Warning: The pivot columns of a matrix Aare evident when Ahas been reduced only
toechelon form. But, be careful to use the pivot columns of Aitself for the basis of
ColA. Row operations can change the column space of a matrix. The columns of an
echelon form BofAare often not in the column space of A. For instance, the columns
of matrix Bin Example 8 all have zeros in their last entries, so they cannot span the
column space of matrix Ain Example 9.
Two Views of a Basis
When the Spanning Set Theorem is used, the deletion of vectors from a spanning set
must stop when the set becomes linearly independent. If an additional vector is deleted,
it will not be a linear combination of the remaining vectors, and hence the smaller set
will no longer span V. Thus a basis is a spanning set that is as small as possible.
A basis is also a linearly independent set that is as large as possible. If Sis a basis
forV, and if Sis enlarged by one vector‚Äîsay, w‚Äîfrom V, then the new set cannot be
linearly independent, because Sspans V, and wis therefore a linear combination of the
elements in S.
EXAMPLE 10 The following three sets in R3show how a linearly independent set
can be enlarged to a basis and how further enlargement destroys the linear independence
of the set. Also, a spanning set can be shrunk to a basis, but further shrinking destroys
SECOND REVISED PAGES


--- Page 232 ---
4.3 Linearly Independent Sets; Bases 215
the spanning property.
8
<
:2
41
0
03
5;2
42
3
03
59
=
;8
<
:2
41
0
03
5;2
42
3
03
5;2
44
5
63
59
=
;8
<
:2
41
0
03
5;2
42
3
03
5;2
44
5
63
5;2
47
8
93
59
=
;
Linearly independent A basis Spans R3but is
but does not span R3forR3linearly dependent
PRACTICE PROBLEMS
1.Letv1D2
41
 2
33
5andv2D2
4 2
7
 93
5. Determine if fv1;v2gis a basis for R3. Isfv1;v2g
a basis for R2?
2.Letv1D2
41
 3
43
5,v2D2
46
2
 13
5,v3D2
42
 2
33
5, and v4D2
4 4
 8
93
5. Find a basis for
the subspace Wspanned by fv1;v2;v3;v4g.
3.Letv1D2
41
0
03
5,v2D2
40
1
03
5, andHD8
<
:2
4s
s
03
5WsinR9
=
;. Then every vector in His
a linear combination of v1andv2because
2
4s
s
03
5Ds2
41
0
03
5Cs2
40
1
03
5
Isfv1;v2ga basis for H?
SG Mastering: Basis 4‚Äì9
4.LetVandWbe vector spaces, let TWV!WandUWV!Wbe linear transfor-
mations, and let fv1;‚Ä¶;vpgbe a basis for V. IfT .vj/DU.vj/for every value of
jbetween 1 and p, show that T .x/DU.x/for every vector xinV.
4.3 EXERCISES
Determine which sets in Exercises 1‚Äì8 are bases for R3. Of the sets
that are notbases, determine which ones are linearly independent
and which ones span R3. Justify your answers.
1.2
41
0
03
5,2
41
1
03
5,2
41
1
13
5 2.2
41
0
13
5,2
40
0
03
5,2
40
1
03
5
3.2
41
0
 23
5,2
43
2
 43
5,2
4 3
 5
13
5 4.2
42
 2
13
5,2
41
 3
23
5,2
4 7
5
43
5
5.2
41
 3
03
5,2
4 2
9
03
5,2
40
0
03
5,2
40
 3
53
56.2
41
2
 33
5,2
4 4
 5
63
5
7.2
4 2
3
03
5,2
46
 1
53
5 8.2
41
 4
33
5,2
40
3
 13
5,2
43
 5
43
5,2
40
2
 23
5Find bases for the null spaces of the matrices given in Exercises 9
and 10. Refer to the remarks that follow Example 3 in Section 4.2.
9.2
41 0  3 2
0 1  5 4
3 2 1  23
5 10.2
41 0  5 1 4
 2 1 6  2 2
0 2  8 1 93
5
11.Find a basis for the set of vectors in R3in the plane
xC2yC¬¥D0. [Hint: Think of the equation as a ‚Äúsystem‚Äù
of homogeneous equations.]
12. Find a basis for the set of vectors in R2on the line yD5x.
In Exercises 13 and 14, assume that Ais row equivalent to B. Find
bases for Nul Aand Col A.
13.AD2
4 2 4  2 4
2 6 3 1
 3 8 2  33
5,BD2
41 0 6 5
0 2 5 3
0 0 0 03
5
SECOND REVISED PAGES


--- Page 233 ---
216 CHAPTER 4 Vector Spaces
14.AD2
6641 2  5 11  3
2 4  5 15 2
1 2 0 4 5
3 6  5 19  23
775,
BD2
6641 2 0 4 5
0 0 5  7 8
0 0 0 0  9
0 0 0 0 03
775
In Exercises 15‚Äì18, Ô¨Ånd a basis for the space spanned by the given
vectors, v1; : : : ; v5.
15.2
6641
0
 3
23
775,2
6640
1
2
 33
775,2
664 3
 4
1
63
775,2
6641
 3
 8
73
775,2
6642
1
 6
93
775
16.2
6641
0
0
13
775,2
664 2
1
 1
13
775,2
6646
 1
2
 13
775,2
6645
 3
3
 43
775,2
6640
3
 1
13
775
17. [M]2
666648
9
 3
 6
03
77775,2
666644
5
1
 4
43
77775,2
66664 1
 4
 9
6
 73
77775,2
666646
8
4
 7
103
77775,2
66664 1
4
11
 8
 73
77775
18. [M]2
66664 8
7
6
5
 73
77775,2
666648
 7
 9
 5
73
77775,2
66664 8
7
4
5
 73
77775,2
666641
4
9
6
 73
77775,2
66664 9
3
 4
 1
03
77775
19. Let v1D2
44
 3
73
5,v2D2
41
9
 23
5,v3D2
47
11
63
5, and HD
Spanfv1;v2;v3g. It can be veriÔ¨Åed that 4v1C5v2 3v3D0.
Use this information to Ô¨Ånd a basis for H. There is more than
one answer.
20. Let v1D2
6647
4
 9
 53
775,v2D2
6644
 7
2
53
775,v3D2
6641
 5
3
43
775. It can be
veriÔ¨Åed that v1 3v2C5v3D0. Use this information to Ô¨Ånd
a basis for HDSpanfv1;v2;v3g.
In Exercises 21 and 22, mark each statement True or False. Justify
each answer.
21. a.A single vector by itself is linearly dependent.
b.IfHDSpanfb1; : : : ; bpg, thenfb1; : : : ; bpgis a basis for
H.
c.The columns of an invertible nnmatrix form a basis
forRn.
d.A basis is a spanning set that is as large as possible.e.In some cases, the linear dependence relations among the
columns of a matrix can be affected by certain elementary
row operations on the matrix.
22. a.A linearly independent set in a subspace His a basis for
H.
b.If a Ô¨Ånite set Sof nonzero vectors spans a vector space
V, then some subset of Sis a basis for V.
c.A basis is a linearly independent set that is as large as
possible.
d.The standard method for producing a spanning set for
NulA, described in Section 4.2, sometimes fails to pro-
duce a basis for Nul A.
e.IfBis an echelon form of a matrix A, then the pivot
columns of Bform a basis for Col A.
23. Suppose R4DSpanfv1; : : : ; v4g. Explain why fv1; : : : ; v4g
is a basis for R4.
24. LetBD fv1; : : : ; vngbe a linearly independent set in Rn.
Explain why Bmust be a basis for Rn.
25. Letv1D2
41
0
13
5,v2D2
40
1
13
5,v3D2
40
1
03
5, and let Hbe the
set of vectors in R3whose second and third entries are equal.
Then every vector in Hhas a unique expansion as a linear
combination of v1;v2;v3, because2
4s
t
t3
5Ds2
41
0
13
5C.t s/2
40
1
13
5Cs2
40
1
03
5
for any sandt. Isfv1;v2;v3ga basis for H? Why or why
not?
26. In the vector space of all real-valued functions, Ô¨Ånd a basis
for the subspace spanned by fsint;sin2t;sintcostg.
27. LetVbe the vector space of functions that describe the
vibration of a mass‚Äìspring system. (Refer to Exercise 19 in
Section 4.1.) Find a basis for V.
28. (RLC circuit ) The circuit in the Ô¨Ågure consists of a resistor
(Rohms), an inductor ( Lhenrys), a capacitor ( Cfarads),
and an initial voltage source. Let bDR=.2L/ , and sup-
pose R,L, and Chave been selected so that balso equals
1=p
LC. (This is done, for instance, when the circuit is used
in a voltmeter.) Let v.t/ be the voltage (in volts) at time t,
measured across the capacitor. It can be shown that vis in
the null space Hof the linear transformation that maps v.t/
intoLv00.t/CRv0.t/C.1=C /v.t/ , and Hconsists of all
functions of the form v.t/De bt.c1Cc2t/. Find a basis
forH.
V oltage
source
LR
C
SECOND REVISED PAGES


--- Page 234 ---
4.3 Linearly Independent Sets; Bases 217
Exercises 29 and 30 show that every basis for Rnmust contain
exactly nvectors.
29. LetSD fv1; : : : ; vkgbe a set of kvectors in Rn, with k < n .
Use a theorem from Section 1.4 to explain why Scannot be
a basis for Rn.
30. LetSD fv1; : : : ; vkgbe a set of kvectors in Rn, with k > n .
Use a theorem from Chapter 1 to explain why Scannot be a
basis for Rn.
Exercises 31 and 32 reveal an important connection between
linear independence and linear transformations and provide prac-
tice using the deÔ¨Ånition of linear dependence. Let VandWbe
vector spaces, let TWV!Wbe a linear transformation, and let
fv1; : : : ; vpgbe a subset of V.
31. Show that if fv1; : : : ; vpgis linearly dependent in V, then
the set of images, fT .v1/; : : : ; T . vp/g, is linearly depen-
dent in W. This fact shows that if a linear transforma-
tion maps a set fv1; : : : ; vpgonto a linearly independent set
fT .v1/; : : : ; T . vp/g, then the original set is linearly indepen-
dent, too (because it cannot be linearly dependent).
32. Suppose that Tis a one-to-one transformation, so that an
equation T .u/DT .v/always implies uDv. Show that if
the set of images fT .v1/; : : : ; T . vp/gis linearly dependent,
thenfv1; : : : ; vpgis linearly dependent. This fact shows that
a one-to-one linear transformation maps a linearly indepen-
dent set onto a linearly independent set (because in this case
the set of images cannot be linearly dependent).
33. Consider the polynomials p1.t/D1Ct2andp2.t/D1 
t2. Isfp1;p2ga linearly independent set in P3? Why or why
not?
34. Consider the polynomials p1.t/D1Ct,p2.t/D1 t, and
p3.t/D2(for all t). By inspection, write a linear depen-dence relation among p1,p2, and p3. Then Ô¨Ånd a basis for
Spanfp1;p2;p3g.
35. LetVbe a vector space that contains a linearly indepen-
dent set fu1;u2;u3;u4g. Describe how to construct a set of
vectors fv1;v2;v3;v4ginVsuch that fv1;v3gis a basis for
Spanfv1;v2;v3;v4g.
36. [M] Let HDSpanfu1;u2;u3gandKDSpanfv1;v2;v3g,
where
u1D2
6641
2
0
 13
775;u2D2
6640
2
 1
13
775;u3D2
6643
4
1
 43
775;
v1D2
664 2
 2
 1
33
775;v2D2
6642
3
2
 63
775;v3D2
664 1
4
6
 23
775
Find bases for H,K, and HCK. (See Exercises 33 and 34
in Section 4.1.)
37. [M] Show that ft;sint;cos2t;sintcostgis a linearly inde-
pendent set of functions deÔ¨Åned on R. Start by assuming that
c1tCc2sintCc3cos2tCc4sintcostD0 (5)
Equation (5) must hold for all real t, so choose several
speciÔ¨Åc values of t(say,tD0; :1; :2/ until you get a system
of enough equations to determine that all the cjmust be zero.
38. [M] Show that f1;cost;cos2t; : : : ; cos6tgis a linearly inde-
pendent set of functions deÔ¨Åned on R. Use the method of
Exercise 37. (This result will be needed in Exercise 34 in
Section 4.5.)
WEB
SOLUTIONS TO PRACTICE PROBLEMS
1.LetAD¬åv1v2¬ç. Row operations show that
AD2
41 2
 2 7
3 93
52
41 2
0 3
0 03
5
Not every row of Acontains a pivot position. So the columns of Ado not span R3, by
Theorem 4 in Section 1.4. Hence fv1;v2gis not a basis for R3. Since v1andv2are not
inR2, they cannot possibly be a basis for R2. However, since v1andv2are obviously
linearly independent, they are a basis for a subspace of R3, namely, Span fv1;v2g.
2.Set up a matrix Awhose column space is the space spanned by fv1;v2;v3;v4g, and
then row reduce Ato Ô¨Ånd its pivot columns.
AD2
41 6 2  4
 3 2  2 8
4 1 3 93
52
41 6 2  4
0 20 4  20
0 25 5 253
52
41 6 2  4
0 5 1  5
0 0 0 03
5
SECOND REVISED PAGES


--- Page 235 ---
218 CHAPTER 4 Vector Spaces
The Ô¨Årst two columns of Aare the pivot columns and hence form a basis of
ColADW. Hence fv1;v2gis a basis for W. Note that the reduced echelon form
ofAis not needed in order to locate the pivot columns.
3.Neither v1norv2is inH, sofv1;v2gcannot be a basis for H. In fact, fv1;v2gis a
basis for the plane of all vectors of the form .c1; c2; 0/, butHis only a line.
4.Sincefv1; : : : ; vpgis a basis for V, for any vector xinV, there exist scalars c1; : : : ; c p
such that xDc1v1C  C cpvp. Then since TandUare linear transformations
T .x/DT .c 1v1C  C cpvp/Dc1T .v1/C  C cpT .vp/
Dc1U.v1/C  C cpU.vp/DU.c 1v1C  C cpvp/
DU.x/
4.4 COORDINATE SYSTEMS
An important reason for specifying a basis Bfor a vector space Vis to impose a
‚Äúcoordinate system‚Äù on V. This section will show that if Bcontains nvectors, then
the coordinate system will make Vact like Rn. IfVis already Rnitself, then Bwill
determine a coordinate system that gives a new ‚Äúview‚Äù of V.
The existence of coordinate systems rests on the following fundamental result.
T H E O R E M 7 The Unique Representation Theorem
LetBD fb1; : : : ; bngbe a basis for a vector space V. Then for each xinV, there
exists a unique set of scalars c1; : : : ; c nsuch that
xDc1b1C  C cnbn (1)
PROOF Since Bspans V, there exist scalars such that (1) holds. Suppose xalso has the
representation
xDd1b1C  C dnbn
for scalars d1; : : : ; d n. Then, subtracting, we have
0Dx xD.c1 d1/b1C  C .cn dn/bn (2)
Since Bis linearly independent, the weights in (2) must all be zero. That is, cjDdjfor
1jn.
D E F I N I T I O N Suppose BD fb1; : : : ; bngis a basis for Vandxis inV. The coordinates of x
relative to the basis B(or the B-coordinates of x) are the weights c1; : : : ; c nsuch
thatxDc1b1C  C cnbn.
Ifc1; : : : ; c nare the B-coordinates of x, then the vector in Rn
x
BD2
64c1
:::
cn3
75
is the coordinate vector of x (relative to B/, or the B-coordinate vector of x . The
mapping x7!x
Bis the coordinate mapping (determined by B/.1
1The concept of a coordinate mapping assumes that the basis Bis an indexed set whose vectors are listed in
some Ô¨Åxed preassigned order. This property makes the deÔ¨Ånition of ¬åx¬çBunambiguous.
SECOND REVISED PAGES


--- Page 236 ---
4.4 Coordinate Systems 219
EXAMPLE 1 Consider a basis BD fb1;b2gforR2, where b1D1
0
and
b2D1
2
. Suppose an xinR2has the coordinate vector ¬åx¬çBD 2
3
. Find x.
SOLUTION TheB-coordinates of xtell how to build xfrom the vectors in B. That is,
xD. 2/b1C3b2D. 2/1
0
C31
2
D1
6
EXAMPLE 2 The entries in the vector xD1
6
are the coordinates of xrelative to
thestandard basis ED fe1;e2g, since
1
6
D11
0
C60
1
D1e1C6e2
IfED fe1;e2g, then ¬åx¬çEDx.
A Graphical Interpretation of Coordinates
A coordinate system on a set consists of a one-to-one mapping of the points in the set
intoRn. For example, ordinary graph paper provides a coordinate system for the plane
when one selects perpendicular axes and a unit of measurement on each axis. Figure 1
shows the standard basis fe1;e2g, the vectors b1.De1/andb2from Example 1, and the
vector xD1
6
. The coordinates 1 and 6 give the location of xrelative to the standard
basis: 1 unit in the e1direction and 6 units in the e2direction.
Figure 2 shows the vectors b1,b2, and xfrom Figure 1. (Geometrically, the three
vectors lie on a vertical line in both Ô¨Ågures.) However, the standard coordinate grid
was erased and replaced by a grid especially adapted to the basis Bin Example 1. The
coordinate vector ¬åx¬çBD 2
3
gives the location of xon this new coordinate system:
 2units in the b1direction and 3 units in the b2direction.
b2x
b1 = e1e2
0
FIGURE 1 Standard graph
paper.
b2
b1x
0FIGURE 2 B-graph paper.
EXAMPLE 3 In crystallography, the description of a crystal lattice is aided by
choosing a basis fu;v;wgforR3that corresponds to three adjacent edges of one ‚Äúunit
cell‚Äù of the crystal. An entire lattice is constructed by stacking together many copies of
one cell. There are fourteen basic types of unit cells; three are displayed in Figure 3.2
2Adapted from The Science and Engineering of Materials , 4th Ed., by Donald R. Askeland (Boston:
Prindle, Weber & Schmidt, ¬©2002), p. 36.
SECOND REVISED PAGES


--- Page 237 ---
220 CHAPTER 4 Vector Spaces
(b)
Body-centered
cubicuvw
0
(c)
Face-centered
orthorhombic0
uw
v
(a)
Simple
monoclinic0
uw
v
FIGURE 3 Examples of unit cells.
The coordinates of atoms within the crystal are given relative to the basis for the
lattice. For instance, 2
41=2
1=2
13
5
identiÔ¨Åes the top face-centered atom in the cell in Figure 3(c).
Coordinates in Rn
When a basis BforRnis Ô¨Åxed, the B-coordinate vector of a speciÔ¨Åed xis easily found,
as in the next example.
EXAMPLE 4 Letb1D2
1
,b2D 1
1
,xD4
5
, and BD fb1;b2g. Find the
coordinate vector ¬åx¬çBofxrelative to B.
SOLUTION TheB-coordinates c1,c2ofxsatisfy
c12
1
b1Cc2 1
1
b2D4
5
x
or 2 1
1 1
b1b2c1
c2
D4
5
x(3)
This equation can be solved by row operations on an augmented matrix or by using
the inverse of the matrix on the left. In any case, the solution is c1D3,c2D2. Thus
xD3b1C2b2, and
¬åx¬çBDc1
c2
D3
2
See Figure 4.
b2b1xFIGURE 4
TheB-coordinate vector of xis
.3; 2/ .
The matrix in (3) changes the B-coordinates of a vector xinto the standard
coordinates for x. An analogous change of coordinates can be carried out in Rnfor
a basis BD fb1; : : : ; bng. Let
PBD¬åb1b2 bn¬ç
SECOND REVISED PAGES


--- Page 238 ---
4.4 Coordinate Systems 221
Then the vector equation
xDc1b1Cc2b2C  C cnbn
is equivalent to
xDPB¬åx¬çB(4)
We call PBthechange-of-coordinates matrix from Bto the standard basis in Rn.
Left-multiplication by PBtransforms the coordinate vector ¬åx¬çBintox. The change-of-
coordinates equation (4) is important and will be needed at several points in Chapters 5
and 7.
Since the columns of PBform a basis for Rn,PBis invertible (by the Invertible
Matrix Theorem). Left-multiplication by P 1
Bconverts xinto its B-coordinate vector:
P 1
BxD¬åx¬çB
The correspondence x7!¬åx¬çB, produced here by P 1
B, is the coordinate mapping
mentioned earlier. Since P 1
Bis an invertible matrix, the coordinate mapping is a one-
to-one linear transformation from RnontoRn, by the Invertible Matrix Theorem. (See
also Theorem 12 in Section 1.9.) This property of the coordinate mapping is also true
in a general vector space that has a basis, as we shall see.
The Coordinate Mapping
Choosing a basis BD fb1; : : : ; bngfor a vector space Vintroduces a coordinate system
inV. The coordinate mapping x7!¬åx¬çBconnects the possibly unfamiliar space Vto
the familiar space Rn. See Figure 5. Points in Vcan now be identiÔ¨Åed by their new
‚Äúnames.‚Äù
/H11938n
V[ ]B
[x]B x
FIGURE 5 The coordinate mapping from VontoRn.
T H E O R E M 8 LetBD fb1; : : : ; bngbe a basis for a vector space V. Then the coordinate mapping
x7!¬åx¬çBis a one-to-one linear transformation from VontoRn.
PROOF Take two typical vectors in V, say,
uDc1b1C  C cnbn
wDd1b1C  C dnbn
Then, using vector operations,
uCwD.c1Cd1/b1C  C .cnCdn/bn
SECOND REVISED PAGES


--- Page 239 ---
222 CHAPTER 4 Vector Spaces
It follows that
¬åuCw¬çBD2
64c1Cd1
:::
cnCdn3
75D2
64c1
:::
cn3
75C2
64d1
:::
dn3
75D¬åu¬çBC¬åw¬çB
So the coordinate mapping preserves addition. If ris any scalar, then
ruDr.c1b1C  C cnbn/D.rc1/b1C  C .rcn/bn
So
¬åru¬çBD2
64rc1
:::
rcn3
75Dr2
64c1
:::
cn3
75Dr¬åu¬çB
Thus the coordinate mapping also preserves scalar multiplication and hence is a linear
transformation. See Exercises 23 and 24 for veriÔ¨Åcation that the coordinate mapping is
one-to-one and maps VontoRn.
The linearity of the coordinate mapping extends to linear combinations, just as in
Section 1.8. If u1; : : : ; upare in Vand if c1; : : : ; c pare scalars, then
¬åc1u1C  C cpup¬çBDc1¬åu1¬çBC  C cp¬åup¬çB(5)
In words, (5) says that the B-coordinate vector of a linear combination of u1; : : : ; upis
thesame linear combination of their coordinate vectors.
The coordinate mapping in Theorem 8 is an important example of an isomorphism
from VontoRn. In general, a one-to-one linear transformation from a vector space V
onto a vector space Wis called an isomorphism from VontoW(isofrom the Greek
for ‚Äúthe same,‚Äù and morph from the Greek for ‚Äúform‚Äù or ‚Äústructure‚Äù). The notation and
terminology for VandWmay differ, but the two spaces are indistinguishable as vector
spaces. Every vector space calculation in Vis accurately reproduced in W, and vice
versa . In particular, any real vector space with a basis of nvectors is indistinguishable
fromRn. See Exercises 25 and 26.
SGIsomorphic Vector
Spaces 4‚Äì11
EXAMPLE 5 LetBbe the standard basis of the space P3of polynomials; that is, let
BD f1; t; t2; t3g. A typical element pofP3has the form
p.t/Da0Ca1tCa2t2Ca3t3
Since pis already displayed as a linear combination of the standard basis vectors, we
conclude that
¬åp¬çBD2
664a0
a1
a2
a33
775
Thus the coordinate mapping p7!¬åp¬çBis an isomorphism from P3ontoR4. All vector
space operations in P3correspond to operations in R4.
If we think of P3andR4as displays on two computer screens that are connected
via the coordinate mapping, then every vector space operation in P3on one screen is
exactly duplicated by a corresponding vector operation in R4on the other screen. The
vectors on the P3screen look different from those on the R4screen, but they ‚Äúact‚Äù as
vectors in exactly the same way. See Figure 6.
SECOND REVISED PAGES


--- Page 240 ---
4.4 Coordinate Systems 223
FIGURE 6 The space P3is isomorphic to R4.
EXAMPLE 6 Use coordinate vectors to verify that the polynomials 1C2t2,
4CtC5t2, and 3C2tare linearly dependent in P2.
SOLUTION The coordinate mapping from Example 5 produces the coordinate vectors
.1; 0; 2/ ,.4; 1; 5/ , and .3; 2; 0/ , respectively. Writing these vectors as the columns of a
matrix A, we can determine their independence by row reducing the augmented matrix
forAxD0: 2
41 4 3 0
0 1 2 0
2 5 0 03
52
41 4 3 0
0 1 2 0
0 0 0 03
5
The columns of Aare linearly dependent, so the corresponding polynomials are linearly
dependent. In fact, it is easy to check that column 3 of Ais 2 times column 2 minus 5
times column 1. The corresponding relation for the polynomials is
3C2tD2.4CtC5t2/ 5.1C2t2/
The Ô¨Ånal example concerns a plane in R3that is isomorphic to R2.
EXAMPLE 7 Let
v1D2
43
6
23
5;v2D2
4 1
0
13
5;xD2
43
12
73
5;
andBD fv1;v2g. Then Bis a basis for HDSpanfv1;v2g. Determine if xis inH, and
if it is, Ô¨Ånd the coordinate vector of xrelative to B.
SOLUTION Ifxis inH, then the following vector equation is consistent:
c12
43
6
23
5Cc22
4 1
0
13
5D2
43
12
73
5
The scalars c1andc2, if they exist, are the B-coordinates of x. Using row operations,
we obtain 2
43 1 3
6 0 12
2 1 73
52
41 0 2
0 1 3
0 0 03
5
SECOND REVISED PAGES


--- Page 241 ---
224 CHAPTER 4 Vector Spaces
Thus c1D2,c2D3, and ¬åx¬çBD2
3
. The coordinate system on Hdetermined by B
is shown in Figure 7.
x /H11005 2v1 /H11001 3v2/H11001/H11001
v1
2v13v1
2v1
v2
0
x2
x1x3
FIGURE 7 A coordinate system on a plane HinR3.
If a different basis for Hwere chosen, would the associated coordinate system also
make Hisomorphic to R2? Surely, this must be true. We shall prove it in the next section.
PRACTICE PROBLEMS
1.Letb1D2
41
0
03
5,b2D2
4 3
4
03
5,b3D2
43
 6
33
5, and xD2
4 8
2
33
5.
a.Show that the set BD fb1;b2;b3gis a basis of R3.
b.Find the change-of-coordinates matrix from Bto the standard basis.
c.Write the equation that relates xinR3to¬åx¬çB.
d.Find¬åx¬çB, for the xgiven above.
2.The set BD f1Ct; 1Ct2; tCt2gis a basis for P2. Find the coordinate vector of
p.t/D6C3t t2relative to B.
4.4 EXERCISES
In Exercises 1‚Äì4, Ô¨Ånd the vector xdetermined by the given
coordinate vector ¬åx¬çBand the given basis B.
1.BD3
 5
; 4
6
,¬åx¬çBD5
3
2.BD4
5
;6
7
,¬åx¬çBD8
 5
3.BD8
<
:2
41
 4
33
5;2
45
2
 23
5;2
44
 7
03
59
=
;,¬åx¬çBD2
43
0
 13
5
4.BD8
<
:2
4 1
2
03
5;2
43
 5
23
5;2
44
 7
33
59
=
;,¬åx¬çBD2
4 4
8
 73
5In Exercises 5‚Äì8, Ô¨Ånd the coordinate vector ¬åx¬çBofxrelative to
the given basis BD fb1; : : : ; bng.
5.b1D1
 3
,b2D2
 5
,xD 2
1
6.b1D1
 2
,b2D5
 6
,xD4
0
7.b1D2
41
 1
 33
5,b2D2
4 3
4
93
5,b3D2
42
 2
43
5,xD2
48
 9
63
5
8.b1D2
41
0
33
5,b2D2
42
1
83
5,b3D2
41
 1
23
5,xD2
43
 5
43
5
SECOND REVISED PAGES


--- Page 242 ---
4.4 Coordinate Systems 225
In Exercises 9 and 10, Ô¨Ånd the change-of-coordinates matrix from
Bto the standard basis in Rn.
9.BD2
 9
,1
8
10.BD8
<
:2
43
 1
43
5,2
42
0
 53
5,2
48
 2
73
59
=
;
In Exercises 11 and 12, use an inverse matrix to Ô¨Ånd ¬åx¬çBfor the
given xandB.
11.BD3
 5
; 4
6
;xD2
 6
12.BD4
5
;6
7
;xD2
0
13. The set BD f1Ct2; tCt2; 1C2tCt2gis a basis for P2.
Find the coordinate vector of p.t/D1C4tC7t2relative
toB.
14. The set BD f1 t2; t t2; 2 2tCt2gis a basis for P2.
Find the coordinate vector of p.t/D3Ct 6t2relative
toB.
In Exercises 15 and 16, mark each statement True or False. Justify
each answer. Unless stated otherwise, Bis a basis for a vector
space V.
15. a.Ifxis in Vand if Bcontains nvectors, then the B-
coordinate vector of xis inRn.
b.IfPBis the change-of-coordinates matrix, then ¬åx¬çBD
PBx, for xinV.
c.The vector spaces P3andR3are isomorphic.
16. a.IfBis the standard basis for Rn, then the B-coordinate
vector of an xinRnisxitself.
b.The correspondence ¬åx¬çB7!xis called the coordinate
mapping.
c.In some cases, a plane in R3can be isomorphic to R2.
17. The vectors v1D1
 3
,v2D2
 8
,v3D 3
7
spanR2
but do not form a basis. Find two different ways to express1
1
as a linear combination of v1,v2,v3.
18. LetBD fb1; : : : ; bngbe a basis for a vector space V. Explain
why the B-coordinate vectors of b1; : : : ; bnare the columns
e1; : : : ; enof the nnidentity matrix.
19. LetSbe a Ô¨Ånite set in a vector space Vwith the property
that every xinVhas a unique representation as a linear
combination of elements of S. Show that Sis a basis of V.
20. Suppose fv1; : : : ; v4gis a linearly dependent spanning set for
a vector space V. Show that each winVcan be expressed
in more than one way as a linear combination of v1; : : : ; v4.
[Hint: LetwDk1v1C  C k4v4be an arbitrary vector in V.Use the linear dependence of fv1; : : : ; v4gto produce another
representation of was a linear combination of v1; : : : ; v4.]
21. LetBD1
 4
; 2
9
. Since the coordinate mapping
determined by Bis a linear transformation from R2intoR2,
this mapping must be implemented by some 22matrix A.
Find it. [ Hint: Multiplication by Ashould transform a vector
xinto its coordinate vector ¬åx¬çB.]
22. LetBD fb1; : : : ; bngbe a basis for Rn. Produce a description
of an nnmatrix Athat implements the coordinate mapping
x7!¬åx¬çB. (See Exercise 21.)
Exercises 23‚Äì26 concern a vector space V, a basis BD
fb1; : : : ; bng, and the coordinate mapping x7!¬åx¬çB.
23. Show that the coordinate mapping is one-to-one. [ Hint: Sup-
pose ¬åu¬çBD¬åw¬çBfor some uandwinV, and show that
uDw.]
24. Show that the coordinate mapping is ontoRn. That is, given
anyyinRn, with entries y1; : : : ; y n, produce uinVsuch that
¬åu¬çBDy.
25. Show that a subset fu1; : : : ; upginVis linearly
independent if and only if the set of coordinate vectors
f¬åu1¬çB; : : : ; ¬å up¬çBgis linearly independent in Rn. [Hint:
Since the coordinate mapping is one-to-one, the following
equations have the same solutions, c1; : : : ; c p.]
c1u1C  C cpupD0 The zero vector in V
¬åc1u1C  C cpup¬çBD¬å0¬çBThe zero vector in Rn
26. Given vectors u1; : : : ; up, and winV, show that wis a linear
combination of u1; : : : ; upif and only if ¬åw¬çBis a linear
combination of the coordinate vectors ¬åu1¬çB; : : : ; ¬å up¬çB.
In Exercises 27‚Äì30, use coordinate vectors to test the linear inde-
pendence of the sets of polynomials. Explain your work.
27.1C2t3,2Ct 3t2, tC2t2 t3
28.1 2t2 t3,tC2t3,1Ct 2t2
29..1 t/2,t 2t2Ct3,.1 t/3
30..2 t/3,.3 t/2,1C6t 5t2Ct3
31. Use coordinate vectors to test whether the following sets of
polynomials span P2. Justify your conclusions.
a.1 3tC5t2, 3C5t 7t2, 4C5t 6t2,1 t2
b.5tCt2,1 8t 2t2, 3C4tC2t2,2 3t
32. Letp1.t/D1Ct2,p2.t/Dt 3t2,p3.t/D1Ct 3t2.
a.Use coordinate vectors to show that these polynomials
form a basis for P2.
b.Consider the basis BD fp1;p2;p3gforP2. Find qinP2,
given that ¬åq¬çBD2
4 1
1
23
5.
SECOND REVISED PAGES


--- Page 243 ---
226 CHAPTER 4 Vector Spaces
In Exercises 33 and 34, determine whether the sets of polynomials
form a basis for P3. Justify your conclusions.
33. [M]3C7t; 5Ct 2t3; t 2t2; 1C16t 6t2C2t3
34. [M]5 3tC4t2C2t3; 9CtC8t2 6t3; 6 2tC5t2; t3
35. [M] Let HDSpanfv1;v2gandBD fv1;v2g. Show that xis
inHand Ô¨Ånd the B-coordinate vector of x, for
v1D2
66411
 5
10
73
775;v2D2
66414
 8
13
103
775;xD2
66419
 13
18
153
775
36. [M] Let HDSpanfv1;v2;v3gandBD fv1;v2;v3g. Show
thatBis a basis for Handxis inH, and Ô¨Ånd the B-coordinate
vector of x, for
v1D2
664 6
4
 9
43
775;v2D2
6648
 3
7
 33
775;v3D2
664 9
5
 8
33
775;xD2
6644
7
 8
33
775
[M] Exercises 37 and 38 concern the crystal lattice for titanium,
which has the hexagonal structure shown on the left in the ac-
companying Ô¨Ågure. The vectors2
42:6
 1:5
03
5,2
40
3
03
5,2
40
0
4:83
5inR3
form a basis for the unit cell shown on the right. The numbers
here are √Öngstrom units (1 √Ö D10 8cm). In alloys of titanium,some additional atoms may be in the unit cell at the octahedral
andtetrahedral sites (so named because of the geometric objects
formed by atoms at these locations).
uw
v0
The hexagonal close-packed lattice and its unit cell.
37. One of the octahedral sites is2
41=2
1=4
1=63
5, relative to the lattice
basis. Determine the coordinates of this site relative to the
standard basis of R3.
38. One of the tetrahedral sites is2
41=2
1=2
1=33
5. Determine the coor-
dinates of this site relative to the standard basis of R3.
SOLUTIONS TO PRACTICE PROBLEMS
1.a.It is evident that the matrix PBD¬åb1b2b3¬çis row-equivalent to the identity
matrix. By the Invertible Matrix Theorem, PBis invertible and its columns form
a basis for R3.
b.From part (a), the change-of-coordinates matrix is PBD2
41 3 3
0 4  6
0 0 33
5.
c.xDPB¬åx¬çB
d.To solve the equation in (c), it is probably easier to row reduce an augmented
matrix than to compute P 1
B:
2
41 3 3  8
0 4  6 2
0 0 3 33
5
PB x2
41 0 0  5
0 1 0 2
0 0 1 13
5
I ¬åx¬çB
Hence
¬åx¬çBD2
4 5
2
13
5
2.The coordinates of p.t/D6C3t t2with respect to Bsatisfy
c1.1Ct/Cc2.1Ct2/Cc3.tCt2/D6C3t t2
SECOND REVISED PAGES


--- Page 244 ---
4.5 The Dimension of a Vector Space 227
Equating coefÔ¨Åcients of like powers of t, we have
c1Cc2 D6
c1 Cc3D3
c2Cc3D  1
Solving, we Ô¨Ånd that c1D5,c2D1,c3D  2, and ¬åp¬çBD2
45
1
 23
5.
4.5 THE DIMENSION OF A VECTOR SPACE
Theorem 8 in Section 4.4 implies that a vector space Vwith a basis Bcontaining
nvectors is isomorphic to Rn. This section shows that this number nis an intrinsic
property (called the dimension) of the space Vthat does not depend on the particular
choice of basis. The discussion of dimension will give additional insight into properties
of bases.
The Ô¨Årst theorem generalizes a well-known result about the vector space Rn.
T H E O R E M 9 If a vector space Vhas a basis BD fb1; : : : ; bng, then any set in Vcontaining
more than nvectors must be linearly dependent.
PROOF Letfu1; : : : ; upgbe a set in Vwith more than nvectors. The coordinate vectors
¬åu1¬çB; : : : ; ¬å up¬çBform a linearly dependent set in Rn, because there are more vectors
(p) than entries ( n) in each vector. So there exist scalars c1; : : : ; c p, not all zero, such
that
c1¬åu1¬çBC  C cp¬åup¬çBD2
640
:::
03
75 The zero vector in Rn
Since the coordinate mapping is a linear transformation,
c1u1C  C cpup
BD2
640
:::
03
75
The zero vector on the right displays the nweights needed to build the vector
c1u1C  C cpupfrom the basis vectors in B. That is, c1u1C  C cpupD
0b1C  C 0bnD0. Since the ciare not all zero, fu1; : : : ; upgis linearly
dependent.1
Theorem 9 implies that if a vector space Vhas a basis BD fb1; : : : ; bng, then each
linearly independent set in Vhas no more than nvectors.
1Theorem 9 also applies to inÔ¨Ånite sets in V. An inÔ¨Ånite set is said to be linearly dependent if some Ô¨Ånite
subset is linearly dependent; otherwise, the set is linearly independent. If Sis an inÔ¨Ånite set in V, take any
subsetfu1; : : : ; upgofS, with p > n . The proof above shows that this subset is linearly dependent, and
hence so is S.
SECOND REVISED PAGES


--- Page 245 ---
228 CHAPTER 4 Vector Spaces
T H E O R E M 1 0 If a vector space Vhas a basis of nvectors, then every basis of Vmust consist of
exactly nvectors.
PROOF LetB1be a basis of nvectors and B2be any other basis (of V). Since B1is
a basis and B2is linearly independent, B2has no more than nvectors, by Theorem 9.
Also, since B2is a basis and B1is linearly independent, B2has at least nvectors. Thus
B2consists of exactly nvectors.
If a nonzero vector space Vis spanned by a Ô¨Ånite set S, then a subset of Sis a
basis for V, by the Spanning Set Theorem. In this case, Theorem 10 ensures that the
following deÔ¨Ånition makes sense.
D E F I N I T I O N IfVis spanned by a Ô¨Ånite set, then Vis said to be Ô¨Ånite-dimensional , and the
dimension ofV, written as dim V, is the number of vectors in a basis for V. The
dimension of the zero vector space f0gis deÔ¨Åned to be zero. If Vis not spanned
by a Ô¨Ånite set, then Vis said to be inÔ¨Ånite-dimensional .
EXAMPLE 1 The standard basis for Rncontains nvectors, so dim RnDn. The
standard polynomial basis f1; t; t2gshows that dim P2D3. In general, dim PnDnC1.
The space Pof all polynomials is inÔ¨Ånite-dimensional (Exercise 27).
EXAMPLE 2 LetHDSpanfv1;v2g, where v1D2
43
6
23
5andv2D2
4 1
0
13
5. Then
x20x3
x12v1v13v2
2v2
v2
His the plane studied in Example 7 in Section 4.4. A basis for Hisfv1;v2g, since v1
andv2are not multiples and hence are linearly independent. Thus dim HD2.
EXAMPLE 3 Find the dimension of the subspace
HD8
¬à¬à<
¬à¬à:2
664a 3bC6c
5aC4d
b 2c d
5d3
775Wa,b,c,dinR9
>>=
>>;
SOLUTION It is easy to see that His the set of all linear combinations of the vectors
v1D2
6641
5
0
03
775;v2D2
664 3
0
1
03
775;v3D2
6646
0
 2
03
775;v4D2
6640
4
 1
53
775
Clearly, v1¬§0,v2is not a multiple of v1, but v3is a multiple of v2. By the Spanning
Set Theorem, we may discard v3and still have a set that spans H. Finally, v4is not a
linear combination of v1andv2. Sofv1;v2;v4gis linearly independent (by Theorem 4
in Section 4.3) and hence is a basis for H. Thus dim HD3.
EXAMPLE 4 The subspaces of R3can be classiÔ¨Åed by dimension. See Figure 1.
0-dimensional subspaces . Only the zero subspace.
1-dimensional subspaces . Any subspace spanned by a single nonzero vector. Such
subspaces are lines through the origin.
SECOND REVISED PAGES


--- Page 246 ---
4.5 The Dimension of a Vector Space 229
2-dimensional subspaces . Any subspace spanned by two linearly independent
vectors. Such subspaces are planes through the origin.
3-dimensional subspaces . Only R3itself. Any three linearly independent vectors
inR3span all of R3, by the Invertible Matrix Theorem.
x2
x1
(a)0-dimx2
x1
(b) (c)1-dim3-dim
2-dim 2-dim
x1x2x3x3x3
FIGURE 1 Sample subspaces of R3.
Subspaces of a Finite-Dimensional Space
The next theorem is a natural counterpart to the Spanning Set Theorem.
T H E O R E M 1 1 LetHbe a subspace of a Ô¨Ånite-dimensional vector space V. Any linearly
independent set in Hcan be expanded, if necessary, to a basis for H. Also, His
Ô¨Ånite-dimensional and
dimHdimV
PROOF IfHD f0g, then certainly dim HD0dimV. Otherwise, let SD fu1; : : : ;
ukgbe any linearly independent set in H. IfSspans H, then Sis a basis for H.
Otherwise, there is some ukC1inHthat is not in Span S. But then fu1; : : : ; uk;ukC1g
will be linearly independent, because no vector in the set can be a linear combination of
vectors that precede it (by Theorem 4).
So long as the new set does not span H, we can continue this process of expanding
Sto a larger linearly independent set in H. But the number of vectors in a linearly
independent expansion of Scan never exceed the dimension of V, by Theorem 9.
So eventually the expansion of Swill span Hand hence will be a basis for H, and
dimHdimV.
When the dimension of a vector space or subspace is known, the search for a basis
is simpliÔ¨Åed by the next theorem. It says that if a set has the right number of elements,
then one has only to show either that the set is linearly independent or that it spans the
space. The theorem is of critical importance in numerous applied problems (involving
differential equations or difference equations, for example) where linear independence
is much easier to verify than spanning.
T H E O R E M 1 2 The Basis Theorem
LetVbe ap-dimensional vector space, p1. Any linearly independent set of
exactly pelements in Vis automatically a basis for V. Any set of exactly p
elements that spans Vis automatically a basis for V.
SECOND REVISED PAGES


--- Page 247 ---
230 CHAPTER 4 Vector Spaces
PROOF By Theorem 11, a linearly independent set Sofpelements can be extended to
a basis for V. But that basis must contain exactly pelements, since dim VDp. SoS
must already be a basis for V. Now suppose that Shaspelements and spans V. Since
Vis nonzero, the Spanning Set Theorem implies that a subset S0ofSis a basis of V.
Since dim VDp,S0must contain pvectors. Hence SDS0.
The Dimensions of NulAand ColA
Since the pivot columns of a matrix Aform a basis for Col A, we know the dimension
of Col Aas soon as we know the pivot columns. The dimension of Nul Amight seem to
require more work, since Ô¨Ånding a basis for Nul Ausually takes more time than a basis
for Col A. But there is a shortcut!
LetAbe an mnmatrix, and suppose the equation AxD0haskfree variables.
From Section 4.2, we know that the standard method of Ô¨Ånding a spanning set for Nul A
will produce exactly klinearly independent vectors‚Äîsay, u1; : : : ; uk‚Äîone for each
free variable. So fu1; : : : ; ukgis a basis for Nul A, and the number of free variables
determines the size of the basis. Let us summarize these facts for future reference.
The dimension of Nul Ais the number of free variables in the equation AxD0,
and the dimension of Col Ais the number of pivot columns in A.
EXAMPLE 5 Find the dimensions of the null space and the column space of
AD2
4 3 6  1 1  7
1 2 2 3  1
2 4 5 8  43
5
SOLUTION Row reduce the augmented matrix ¬åA0¬çto echelon form:
2
41 2 2 3  1 0
0 0 1 2  2 0
0 0 0 0 0 03
5
There are three free variables‚Äî x2,x4, andx5. Hence the dimension of Nul Ais 3. Also,
dim Col AD2because Ahas two pivot columns.
PRACTICE PROBLEMS
1.Decide whether each statement is True or False, and give a reason for each answer.
Here Vis a nonzero Ô¨Ånite-dimensional vector space.
a.If dim VDpand if Sis a linearly dependent subset of V, then Scontains more
thanpvectors.
b.IfSspans Vand if Tis a subset of Vthat contains more vectors than S, then T
is linearly dependent.
2.LetHandKbe subspaces of a vector space V. In Section 4.1 Exercise 32 it is
established that H\Kis also a subspace of V. Prove dim ( H\K) ‚â§ dim H.
SECOND REVISED PAGES


--- Page 248 ---
4.5 The Dimension of a Vector Space 231
4.5 EXERCISES
For each subspace in Exercises 1‚Äì8, (a) Ô¨Ånd a basis, and (b) state
the dimension.
1.8
<
:2
4s 2t
sCt
3t3
5Ws; tinR9
=
;2.8
<
:2
44s
 3s
 t3
5Ws; tinR9
=
;
3.8
¬à¬à<
¬à¬à:2
6642c
a b
b 3c
aC2b3
775Wa; b; c inR9
>>=
>>;4.8
¬à¬à<
¬à¬à:2
664aCb
2a
3a b
 b3
775Wa; binR9
>>=
>>;
5.8
¬à¬à<
¬à¬à:2
664a 4b 2c
2aC5b 4c
 aC2c
 3aC7bC6c3
775Wa; b; c inR9
>>=
>>;
6.8
¬à¬à<
¬à¬à:2
6643aC6b c
6a 2b 2c
 9aC5bC3c
 3aCbCc3
775Wa; b; c inR9
>>=
>>;
7.f.a; b; c/ Wa 3bCcD0; b 2cD0; 2b cD0g
8.f.a; b; c; d/ Wa 3bCcD0g
9.Find the dimension of the subspace of all vectors in R3whose
Ô¨Årst and third entries are equal.
10. Find the dimension of the subspace HofR2spanned by2
 5
, 4
10
, 3
6
.
In Exercises 11 and 12, Ô¨Ånd the dimension of the subspace
spanned by the given vectors.
11.2
41
0
23
5,2
43
1
13
5,2
49
4
 23
5,2
4 7
 3
13
5
12.2
41
 2
03
5,2
4 3
4
13
5,2
4 8
6
53
5,2
4 3
0
73
5
Determine the dimensions of Nul Aand Col Afor the matrices
shown in Exercises 13‚Äì18.
13.AD2
6641 6 9 0  2
0 1 2  4 5
0 0 0 5 1
0 0 0 0 03
775
14.AD2
6641 3  4 2  1 6
0 0 1  3 7 0
0 0 0 1 4  3
0 0 0 0 0 03
775
15.AD1 0 9 5
0 0 1  4
16.AD3 4
 6 1017.AD2
41 1 0
0 4 7
0 0 53
5 18.AD2
41 4  1
0 7 0
0 0 03
5
In Exercises 19 and 20, Vis a vector space. Mark each statement
True or False. Justify each answer.
19. a.The number of pivot columns of a matrix equals the
dimension of its column space.
b.A plane in R3is a two-dimensional subspace of R3.
c.The dimension of the vector space P4is 4.
d.If dim VDnandSis a linearly independent set in V,
thenSis a basis for V.
e.If a set fv1; : : : ; vpgspans a Ô¨Ånite-dimensional vector
space Vand if Tis a set of more than pvectors in V,
thenTis linearly dependent.
20. a.R2is a two-dimensional subspace of R3.
b.The number of variables in the equation AxD0equals
the dimension of Nul A.
c.A vector space is inÔ¨Ånite-dimensional if it is spanned by
an inÔ¨Ånite set.
d.If dim VDnand if Sspans V, then Sis a basis of V.
e.The only three-dimensional subspace of R3isR3itself.
21. The Ô¨Årst four Hermite polynomials are 1, 2t, 2C4t2, and
 12tC8t3. These polynomials arise naturally in the study
of certain important differential equations in mathematical
physics.2Show that the Ô¨Årst four Hermite polynomials form
a basis of P3.
22. The Ô¨Årst four Laguerre polynomials are 1, 1 t,2 4tCt2,
and6 18tC9t2 t3. Show that these polynomials form a
basis of P3.
23. LetBbe the basis of P3consisting of the Hermite polyno-
mials in Exercise 21, and let p.t/D7 12t 8t2C12t3.
Find the coordinate vector of prelative to B.
24. LetBbe the basis of P2consisting of the Ô¨Årst three
Laguerre polynomials listed in Exercise 22, and let
p.t/D7 8tC3t2. Find the coordinate vector of prelative
toB.
25. LetSbe a subset of an n-dimensional vector space V, and
suppose Scontains fewer than nvectors. Explain why S
cannot span V.
26. LetHbe an n-dimensional subspace of an n-dimensional
vector space V. Show that HDV.
27. Explain why the space Pof all polynomials is an inÔ¨Ånite-
dimensional space.
2SeeIntroduction to Functional Analysis , 2nd ed., by A. E. Taylor and
David C. Lay (New York: John Wiley & Sons, 1980), pp. 92‚Äì93. Other
sets of polynomials are discussed there, too.
SECOND REVISED PAGES


--- Page 249 ---
232 CHAPTER 4 Vector Spaces
28. Show that the space C.R/of all continuous functions deÔ¨Åned
on the real line is an inÔ¨Ånite-dimensional space.
In Exercises 29 and 30, Vis a nonzero Ô¨Ånite-dimensional vector
space, and the vectors listed belong to V. Mark each statement
True or False. Justify each answer. (These questions are more
difÔ¨Åcult than those in Exercises 19 and 20.)
29. a.If there exists a set fv1; : : : ; vpgthat spans V, then
dimVp.
b.If there exists a linearly independent set fv1; : : : ; vpgin
V, then dim Vp.
c.If dim VDp, then there exists a spanning set of pC1
vectors in V.
30. a.If there exists a linearly dependent set fv1; : : : ; vpginV,
then dim Vp.
b.If every set of pelements in Vfails to span V, then
dimV > p .
c.Ifp2and dim VDp, then every set of p 1nonzero
vectors is linearly independent.
Exercises 31 and 32 concern Ô¨Ånite-dimensional vector spaces V
andWand a linear transformation TWV!W.
31. LetHbe a nonzero subspace of V, and let T .H/ be the set of
images of vectors in H. Then T .H/ is a subspace of W, by
Exercise 35 in Section 4.2. Prove that dim T .H/ dimH.
32. LetHbe a nonzero subspace of V, and suppose Tis
a one-to-one (linear) mapping of VintoW. Prove that
dimT .H/ DdimH. IfThappens to be a one-to-one map-
ping of VontoW, then dim VDdimW. Isomorphic Ô¨Ånite-
dimensional vector spaces have the same dimension.33. [M] According to Theorem 11, a linearly independent set
fv1; : : : ; vkginRncan be expanded to a basis for Rn. One
way to do this is to create AD¬åv1 vke1 en¬ç,
with e1; : : : ; enthe columns of the identity matrix; the pivot
columns of Aform a basis for Rn.
a.Use the method described to extend the following vectors
to a basis for R5:
v1D2
66664 9
 7
8
 5
73
77775;v2D2
666649
4
1
6
 73
77775;v3D2
666646
7
 8
5
 73
77775
b.Explain why the method works in general: Why are the
original vectors v1; : : : ; vkincluded in the basis found for
ColA? Why is Col ADRn?
34. [M] LetBD f1;cost;cos2t; : : : ; cos6tgandCD f1;cost;
cos2t; : : : ; cos6tg. Assume the following trigonometric
identities (see Exercise 37 in Section 4.1).
cos2tD  1C2cos2t
cos3tD  3costC4cos3t
cos4tD1 8cos2tC8cos4t
cos5tD5cost 20cos3tC16cos5t
cos6tD  1C18cos2t 48cos4tC32cos6t
LetHbe the subspace of functions spanned by the functions
inB. Then Bis a basis for H, by Exercise 38 in Section 4.3.
a.Write the B-coordinate vectors of the vectors in C, and
use them to show that Cis a linearly independent set in
H.
b.Explain why Cis a basis for H.
SOLUTIONS TO PRACTICE PROBLEMS
1.a.False. Consider the set f0g.
b.True. By the Spanning Set Theorem, Scontains a basis for V; call that basis S0.
Then Twill contain more vectors than S0. By Theorem 9, Tis linearly dependent.
2.Letfv1;‚Ä¶;vpgbe a basis for H\K. Notice fv1;‚Ä¶;vpgis a linearly independent
subset of H, hence by Theorem 11, fv1;‚Ä¶;vpgcan be expanded, if necessary, to
a basis for H. Since the dimension of a subspace is just the number of vectors in a
basis, it follows that dim .H\K/DpdimH.
4.6 RANK
With the aid of vector space concepts, this section takes a look inside a matrix and
reveals several interesting and useful relationships hidden in its rows and columns.
For instance, imagine placing 2000 random numbers into a 4050matrix Aand
then determining both the maximum number of linearly independent columns in Aand
the maximum number of linearly independent columns in AT(rows in A). Remarkably,
SECOND REVISED PAGES


--- Page 250 ---
4.6 Rank 233
the two numbers are the same. As we‚Äôll soon see, their common value is the rank of the
matrix. To explain why, we need to examine the subspace spanned by the rows of A.
The Row Space
IfAis an mnmatrix, each row of Ahasnentries and thus can be identiÔ¨Åed with
a vector in Rn. The set of all linear combinations of the row vectors is called the row
space ofAand is denoted by Row A. Each row has nentries, so Row Ais a subspace
ofRn. Since the rows of Aare identiÔ¨Åed with the columns of AT, we could also write
ColATin place of Row A.
EXAMPLE 1 Let
AD2
664 2 5 8 0  17
1 3  5 1 5
3 11  19 7 1
1 7  13 5  33
775andr1D. 2; 5; 8; 0;  17/
r2D.1; 3; 5; 1; 5/
r3D.3; 11;  19; 7; 1/
r4D.1; 7; 13; 5; 3/
The row space of Ais the subspace of R5spanned by fr1;r2;r3;r4g. That is, Row AD
Spanfr1;r2;r3;r4g. It is natural to write row vectors horizontally; however, they may
also be written as column vectors if that is more convenient.
If we knew some linear dependence relations among the rows of matrix Ain
Example 1, we could use the Spanning Set Theorem to shrink the spanning set to a
basis. Unfortunately, row operations on Awill not give us that information, because
row operations change the row-dependence relations. But row reducing Ais certainly
worthwhile, as the next theorem shows!
T H E O R E M 1 3 If two matrices AandBare row equivalent, then their row spaces are the same.
IfBis in echelon form, the nonzero rows of Bform a basis for the row space of
Aas well as for that of B.
PROOF IfBis obtained from Aby row operations, the rows of Bare linear com-
binations of the rows of A. It follows that any linear combination of the rows of B
is automatically a linear combination of the rows of A. Thus the row space of Bis
contained in the row space of A. Since row operations are reversible, the same argument
shows that the row space of Ais a subset of the row space of B. So the two row spaces
are the same. If Bis in echelon form, its nonzero rows are linearly independent because
no nonzero row is a linear combination of the nonzero rows below it. (Apply Theorem
4 to the nonzero rows of Bin reverse order, with the Ô¨Årst row last.) Thus the nonzero
rows of Bform a basis of the (common) row space of BandA.
The main result of this section involves the three spaces: Row A, ColA, and Nul A.
The following example prepares the way for this result and shows how onesequence of
row operations on Aleads to bases for all three spaces.
EXAMPLE 2 Find bases for the row space, the column space, and the null space of
the matrix
AD2
664 2 5 8 0  17
1 3  5 1 5
3 11  19 7 1
1 7  13 5  33
775
SECOND REVISED PAGES


--- Page 251 ---
234 CHAPTER 4 Vector Spaces
SOLUTION To Ô¨Ånd bases for the row space and the column space, row reduce Ato an
echelon form:
ABD2
6641 3  5 1 5
0 1  2 2  7
0 0 0  4 20
0 0 0 0 03
775
By Theorem 13, the Ô¨Årst three rows of Bform a basis for the row space of A(as well
as for the row space of B). Thus
Basis for Row A:f.1; 3; 5; 1; 5/; .0; 1;  2; 2; 7/; .0; 0; 0;  4; 20/g
For the column space, observe from Bthat the pivots are in columns 1, 2, and 4. Hence
columns 1, 2, and 4 of A(notB) form a basis for Col A:
Basis for Col A:8
¬à¬à<
¬à¬à:2
664 2
1
3
13
775;2
664 5
3
11
73
775;2
6640
1
7
53
7759
>>=
>>;
Notice that any echelon form of Aprovides (in its nonzero rows) a basis for Row A
and also identiÔ¨Åes the pivot columns of Afor Col A. However, for Nul A, we need the
reduced echelon form . Further row operations on Byield
ABCD2
6641 0 1 0 1
0 1  2 0 3
0 0 0 1  5
0 0 0 0 03
775
The equation AxD0is equivalent to CxD0, that is,
x1C x3 Cx5D0
x2 2x3 C3x5D0
x4 5x5D0
Sox1D  x3 x5; x2D2x3 3x5; x4D5x5, with x3andx5free variables. The usual
calculations (discussed in Section 4.2) show that
Basis for Nul A:8
¬à¬à¬à¬à<
¬à¬à¬à¬à:2
66664 1
2
1
0
03
77775;2
66664 1
 3
0
5
13
777759
>>>>=
>>>>;
Observe that, unlike the basis for Col A, the bases for Row Aand Nul Ahave no simple
connection with the entries in Aitself.1
1It is possible to Ô¨Ånd a basis for the row space Row Athat uses rows of A. First form AT, and then row
reduce until the pivot columns of ATare found. These pivot columns of ATare rows of A, and they form
a basis for the row space of A.
SECOND REVISED PAGES


--- Page 252 ---
4.6 Rank 235
Warning: Although the Ô¨Årst three rows of Bin Example 2 are linearly independent,
it is wrong to conclude that the Ô¨Årst three rows of Aare linearly independent. (In fact,
the third row of Ais 2 times the Ô¨Årst row plus 7 times the second row.) Row operations
may change the linear dependence relations among the rows of a matrix.
The Rank Theorem
The next theorem describes fundamental relations among the dimensions of Col A,
WEB
RowA, and Nul A.
D E F I N I T I O N Therank ofAis the dimension of the column space of A.
Since Row Ais the same as Col AT, the dimension of the row space of Ais the rank
ofAT. The dimension of the null space is sometimes called the nullity ofA, though we
will not use this term.
An alert reader may have already discovered part or all of the next theorem while
working the exercises in Section 4.5 or reading Example 2 above.
T H E O R E M 1 4 The Rank Theorem
The dimensions of the column space and the row space of an mnmatrix Aare
equal. This common dimension, the rank of A, also equals the number of pivot
positions in Aand satisÔ¨Åes the equation
rankACdim Nul ADn
PROOF By Theorem 6 in Section 4.3, rank Ais the number of pivot columns in A.
Equivalently, rank Ais the number of pivot positions in an echelon form BofA.
Furthermore, since Bhas a nonzero row for each pivot, and since these rows form a
basis for the row space of A, the rank of Ais also the dimension of the row space.
From Section 4.5, the dimension of Nul Aequals the number of free variables in
the equation AxD0. Expressed another way, the dimension of Nul Ais the number of
columns of Athat are notpivot columns. (It is the number of these columns, not the
columns themselves, that is related to Nul A.) Obviously,
number of
pivot columns
Cnumber of
nonpivot columns
Dnumber of
columns
This proves the theorem.
The ideas behind Theorem 14 are visible in the calculations in Example 2. The
three pivot positions in the echelon form Bdetermine the basic variables and identify
the basis vectors for Col Aand those for Row A.
EXAMPLE 3
a.IfAis a79matrix with a two-dimensional null space, what is the rank of A?
b.Could a 69matrix have a two-dimensional null space?
SECOND REVISED PAGES


--- Page 253 ---
236 CHAPTER 4 Vector Spaces
SOLUTION
a.Since Ahas 9 columns, .rankA/C2D9, and hence rank AD7.
b.No. If a 69matrix, call it B, had a two-dimensional null space, it would have to
have rank 7, by the Rank Theorem. But the columns of Bare vectors in R6, and so
the dimension of Col Bcannot exceed 6; that is, rank Bcannot exceed 6.
The next example provides a nice way to visualize the subspaces we have been
studying. In Chapter 6, we will learn that Row Aand Nul Ahave only the zero vector
in common and are actually ‚Äúperpendicular‚Äù to each other. The same fact will apply to
RowAT.DColA/and Nul AT. So Figure 1, which accompanies Example 4, creates
a good mental image for the general case. (The value of studying ATalong with Ais
demonstrated in Exercise 29.)
EXAMPLE 4 LetAD2
43 0  1
3 0  1
4 0 53
5. It is readily checked that Nul Ais the x2-
axis, Row Ais the x1x3-plane, Col Ais the plane whose equation is x1 x2D0, and
NulATis the set of all multiples of .1; 1; 0/. Figure 1 shows Nul Aand Row Ain the
domain of the linear transformation x7!Ax; the range of this mapping, Col A, is shown
in a separate copy of R3, along with Nul AT.
A
00x3
x1x2x1x2x3
/H119383/H119383Nul 
ANul AT
Row A Col 
A
FIGURE 1 Subspaces determined by a matrix A.
Applications to Systems of Equations
The Rank Theorem is a powerful tool for processing information about systems of
linear equations. The next example simulates the way a real-life problem using linear
equations might be stated, without explicit mention of linear algebra terms such as
matrix, subspace, and dimension.
EXAMPLE 5 A scientist has found two solutions to a homogeneous system of
40 equations in 42 variables. The two solutions are not multiples, and all other solutions
can be constructed by adding together appropriate multiples of these two solutions.
Can the scientist be certain that an associated nonhomogeneous system (with the same
coefÔ¨Åcients) has a solution?
SOLUTION Yes. Let Abe the 4042coefÔ¨Åcient matrix of the system. The given
information implies that the two solutions are linearly independent and span Nul A.
So dim Nul AD2. By the Rank Theorem, dim Col AD42 2D40. Since R40is the
only subspace of R40whose dimension is 40, Col Amust be all of R40. This means that
every nonhomogeneous equation AxDbhas a solution.
SECOND REVISED PAGES


--- Page 254 ---
4.6 Rank 237
Rank and the Invertible Matrix Theorem
The various vector space concepts associated with a matrix provide several more
statements for the Invertible Matrix Theorem. The new statements listed here follow
those in the original Invertible Matrix Theorem in Section 2.3.
T H E O R E M The Invertible Matrix Theorem (continued)
LetAbe an nnmatrix. Then the following statements are each equivalent to
the statement that Ais an invertible matrix.
m.The columns of Aform a basis of Rn.
n.ColADRn
o.dim Col ADn
p.rankADn
q.NulAD f0g
r.dim Nul AD0
PROOF Statement (m) is logically equivalent to statements (e) and (h) regarding linear
independence and spanning. The other Ô¨Åve statements are linked to the earlier ones of
the theorem by the following chain of almost trivial implications:
.g/).n/).o/).p/).r/).q/).d/
Statement (g), which says that the equation AxDbhas at least one solution for each bin
Rn, implies (n), because Col Ais precisely the set of all bsuch that the equation AxDb
is consistent. The implications (n) )(o))(p) follow from the deÔ¨Ånitions of dimension
and rank. If the rank of Aisn, the number of columns of A, then dim Nul AD0, by the
Rank Theorem, and so Nul AD f0g. Thus (p) )(r))(q). Also, (q) implies that the
equation AxD0has only the trivial solution, which is statement (d). Since statements
(d) and (g) are already known to be equivalent to the statement that Ais invertible, the
proof is complete.
SGExpanded Table for the
IMT 4‚Äì19
We have refrained from adding to the Invertible Matrix Theorem obvious state-
ments about the row space of A, because the row space is the column space of AT.
Recall from statement (l) of the Invertible Matrix Theorem that Ais invertible if and
only if ATis invertible. Hence every statement in the Invertible Matrix Theorem can
also be stated for AT. To do so would double the length of the theorem and produce a
list of more than 30 statements!
SECOND REVISED PAGES


--- Page 255 ---
238 CHAPTER 4 Vector Spaces
N U M E R I C A L N O T E
Many algorithms discussed in this text are useful for understanding concepts
and making simple computations by hand. However, the algorithms are often
unsuitable for large-scale problems in real life.
Rank determination is a good example. It would seem easy to reduce a matrix
to echelon form and count the pivots. But unless exact arithmetic is performed
on a matrix whose entries are speciÔ¨Åed exactly, row operations can change the
apparent rank of a matrix. For instance, if the value of xin the matrix5 7
5 x
is not stored exactly as 7 in a computer, then the rank may be 1 or 2, depending
on whether the computer treats x 7as zero.
In practical applications, the effective rank of a matrix Ais often determined
from the singular value decomposition of A, to be discussed in Section 7.4. This
decomposition is also a reliable source of bases for Col A, Row A, Nul A, and
NulAT.
WEB
PRACTICE PROBLEMS
The matrices below are row equivalent.
AD2
6642 1 1  6 8
1 2 4 3  2
 7 8 10 3  10
4 5 7 0 43
775; B D2
6641 2 4 3  2
0 3 9  12 12
0 0 0 0 0
0 0 0 0 03
775
1.Find rank Aand dim Nul A.
2.Find bases for Col Aand Row A.
3.What is the next step to perform to Ô¨Ånd a basis for Nul A?
4.How many pivot columns are in a row echelon form of AT?
4.6 EXERCISES
In Exercises 1‚Äì4, assume that the matrix Ais row equivalent to B.
Without calculations, list rank Aand dim Nul A. Then Ô¨Ånd bases
for Col A, Row A, and Nul A.
1.AD2
41 4 9  7
 1 2  4 1
5 6 10 73
5,
BD2
41 0  1 5
0 2 5  6
0 0 0 03
5
2.AD2
6641 3 4  1 9
 2 6  6 1 10
 3 9  6 6 3
3 9 4 9 03
775,
BD2
6641 3 0 5  7
0 0 2  3 8
0 0 0 0 5
0 0 0 0 03
7753.AD2
6642 3 6 2 5
 2 3  3 3 4
4 6 9 5 9
 2 3 3  4 13
775,
BD2
6642 3 6 2 5
0 0 3  1 1
0 0 0 1 3
0 0 0 0 03
775
4.AD2
666641 1  3 7 9  9
1 2  4 10 13  12
1 1 1 1 1  3
1 3 1  5 7 3
1 2 0 0  5 43
77775,
BD2
666641 1  3 7 9  9
0 1  1 3 4  3
0 0 0 1  1 2
0 0 0 0 0 0
0 0 0 0 0 03
77775
SECOND REVISED PAGES


--- Page 256 ---
4.6 Rank 239
5.If a38matrix Ahas rank 3, Ô¨Ånd dim Nul A, dim Row A,
and rank AT.
6.If a63matrix Ahas rank 3, Ô¨Ånd dim Nul A, dim Row A,
and rank AT.
7.Suppose a 47matrix Ahas four pivot columns. Is
ColADR4? Is Nul ADR3? Explain your answers.
8.Suppose a 56matrix Ahas four pivot columns. What is
dim Nul A? Is Col ADR4? Why or why not?
9.If the null space of a 56matrix Ais 4-dimensional, what
is the dimension of the column space of A?
10. If the null space of a 76matrix Ais 5-dimensional, what
is the dimension of the column space of A?
11.If the null space of an 85matrix Ais 2-dimensional, what
is the dimension of the row space of A?
12. If the null space of a 56matrix Ais 4-dimensional, what
is the dimension of the row space of A?
13. IfAis a75matrix, what is the largest possible rank of A?
IfAis a57matrix, what is the largest possible rank of A?
Explain your answers.
14. IfAis a43matrix, what is the largest possible dimension
of the row space of A? IfAis a34matrix, what is the
largest possible dimension of the row space of A? Explain.
15. IfAis a68matrix, what is the smallest possible dimension
of Nul A?
16. IfAis a64matrix, what is the smallest possible dimension
of Nul A?
In Exercises 17 and 18, Ais anmnmatrix. Mark each statement
True or False. Justify each answer.
17. a.The row space of Ais the same as the column space of
AT.
b.IfBis any echelon form of A, and if Bhas three nonzero
rows, then the Ô¨Årst three rows of Aform a basis for
RowA.
c.The dimensions of the row space and the column space
ofAare the same, even if Ais not square.
d.The sum of the dimensions of the row space and the null
space of Aequals the number of rows in A.
e.On a computer, row operations can change the apparent
rank of a matrix.
18. a.IfBis any echelon form of A, then the pivot columns of
Bform a basis for the column space of A.
b.Row operations preserve the linear dependence relations
among the rows of A.
c.The dimension of the null space of Ais the number of
columns of Athat are notpivot columns.
d.The row space of ATis the same as the column space of
A.e.IfAandBare row equivalent, then their row spaces are
the same.
19. Suppose the solutions of a homogeneous system of Ô¨Åve linear
equations in six unknowns are all multiples of one nonzero
solution. Will the system necessarily have a solution for
every possible choice of constants on the right sides of the
equations? Explain.
20. Suppose a nonhomogeneous system of six linear equations
in eight unknowns has a solution, with two free variables. Is
it possible to change some constants on the equations‚Äô right
sides to make the new system inconsistent? Explain.
21. Suppose a nonhomogeneous system of nine linear equations
in ten unknowns has a solution for all possible constants on
the right sides of the equations. Is it possible to Ô¨Ånd two
nonzero solutions of the associated homogeneous system that
arenotmultiples of each other? Discuss.
22. Is it possible that all solutions of a homogeneous system of
ten linear equations in twelve variables are multiples of one
Ô¨Åxed nonzero solution? Discuss.
23. A homogeneous system of twelve linear equations in eight
unknowns has two Ô¨Åxed solutions that are not multiples of
each other, and all other solutions are linear combinations of
these two solutions. Can the set of all solutions be described
with fewer than twelve homogeneous linear equations? If so,
how many? Discuss.
24. Is it possible for a nonhomogeneous system of seven equa-
tions in six unknowns to have a unique solution for some
right-hand side of constants? Is it possible for such a system
to have a unique solution for every right-hand side? Explain.
25. A scientist solves a nonhomogeneous system of ten linear
equations in twelve unknowns and Ô¨Ånds that three of the
unknowns are free variables. Can the scientist be certain
that, if the right sides of the equations are changed, the new
nonhomogeneous system will have a solution? Discuss.
26. In statistical theory, a common requirement is that a matrix
be of full rank . That is, the rank should be as large as
possible. Explain why an mnmatrix with more rows than
columns has full rank if and only if its columns are linearly
independent.
Exercises 27‚Äì29 concern an mnmatrix Aand what are often
called the fundamental subspaces determined by A.
27. Which of the subspaces Row A, Col A, Nul A, Row AT,
ColAT, and Nul ATare in Rmand which are in Rn? How
many distinct subspaces are in this list?
28. Justify the following equalities:
a.dim Row ACdim Nul ADnNumber of columns of A
b.dim Col ACdim Nul ATDmNumber of rows of A
29. Use Exercise 28 to explain why the equation AxDbhas a
solution for all binRmif and only if the equation ATxD0
has only the trivial solution.
SECOND REVISED PAGES


--- Page 257 ---
240 CHAPTER 4 Vector Spaces
30. Suppose Aismnandbis inRm. What has to be true about
the two numbers rank ¬åAb¬çand rank Ain order for the
equation AxDbto be consistent?
Rank 1 matrices are important in some computer algorithms and
several theoretical contexts, including the singular value decom-
position in Chapter 7. It can be shown that an mnmatrix A
has rank 1 if and only if it is an outer product; that is, ADuvT
for some uinRmandvinRn. Exercises 31‚Äì33 suggest why this
property is true.
31. Verify that rank uvT1ifuD2
42
 3
53
5andvD2
4a
b
c3
5.
32. LetuD1
2
. Find vinR3such that1 3 4
2 6 8
DuvT.
33. LetAbe any 23matrix such that rank AD1, letube the
Ô¨Årst column of A, and suppose u¬§0. Explain why there
is a vector vinR3such that ADuvT. How could this
construction be modiÔ¨Åed if the Ô¨Årst column of Awere zero?
34. LetAbe an mnmatrix of rank r > 0 and let Ube an ech-
elon form of A. Explain why there exists an invertible matrix
Esuch that ADEU, and use this factorization to write A
as the sum of rrank1matrices. [ Hint: See Theorem 10 in
Section 2.4.]35. [M] Let AD2
666647 9 4 5 3  3 7
 4 6 7  2 6 5 5
5 7 6 5  6 2 8
 3 5 8  1 7 4 8
6 8 5 4 4 9 33
77775.
a.Construct matrices CandNwhose columns are bases for
ColAand Nul A, respectively, and construct a matrix R
whose rows form a basis for Row A.
b.Construct a matrix Mwhose columns form a ba-
sis for Nul AT, form the matrices SD¬åRTN¬çand
TD¬åC M ¬ç, and explain why SandTshould be
square. Verify that both SandTare invertible.
36. [M] Repeat Exercise 35 for a random integer-valued 67
matrix Awhose rank is at most 4. One way to make A
is to create a random integer-valued 64matrix Jand a
random integer-valued 47matrix K, and set ADJK.
(See Supplementary Exercise 12 at the end of the chapter;
and see the Study Guide for matrix-generating programs.)
37. [M] Let Abe the matrix in Exercise 35. Construct a matrix
Cwhose columns are the pivot columns of A, and construct
a matrix Rwhose rows are the nonzero rows of the reduced
echelon form of A. Compute CR, and discuss what you see.
38. [M] Repeat Exercise 37 for three random integer-valued
57matrices Awhose ranks are 5, 4, and 3. Make a con-
jecture about how CRis related to Afor any matrix A. Prove
your conjecture.
SOLUTIONS TO PRACTICE PROBLEMS
1.Ahas two pivot columns, so rank AD2. Since Ahas 5 columns altogether,
dim Nul AD5 2D3.
2.The pivot columns of Aare the Ô¨Årst two columns. So a basis for Col Ais
fa1;a2g D8
¬à¬à<
¬à¬à:2
6642
1
 7
43
775;2
664 1
 2
8
 53
7759
>>=
>>;
The nonzero rows of Bform a basis for Row A, namely, f.1; 2; 4; 3; 2/,
.0; 3; 9;  12; 12/ g. In this particular example, it happens that any two rows of A
form a basis for the row space, because the row space is two-dimensional and none
of the rows of Ais a multiple of another row. In general, the nonzero rows of an
echelon form of Ashould be used as a basis for Row A, not the rows of Aitself.
3.For Nul A, the next step is to perform row operations on Bto obtain the reduced
echelon form of A.
4.Rank ATDrankA, by the Rank Theorem, because Col ATDRowA. SoAThas
two pivot positions.
SGMajor Review of Key
Concepts 4‚Äì22
SECOND REVISED PAGES


--- Page 258 ---
4.7 Change of Basis 241
4.7 CHANGE OF BASIS
When a basis Bis chosen for an n-dimensional vector space V, the associated coordinate
mapping onto Rnprovides a coordinate system for V. Each xinVis identiÔ¨Åed uniquely
by its B-coordinate vector ¬åx¬çB.1
In some applications, a problem is described initially using a basis B, but the
problem‚Äôs solution is aided by changing Bto a new basis C. (Examples will be given in
Chapters 5 and 7.) Each vector is assigned a new C-coordinate vector. In this section,
we study how ¬åx¬çCand¬åx¬çBare related for each xinV.
To visualize the problem, consider the two coordinate systems in Figure 1. In
Figure 1(a), xD3b1Cb2, while in Figure 1(b), the same xis shown as xD6c1C4c2.
That is,
¬åx¬çBD3
1
and ¬åx¬çCD6
4
Our problem is to Ô¨Ånd the connection between the two coordinate vectors. Example 1
shows how to do this, provided we know how b1andb2are formed from c1andc2.
b2
b1
3b1x0
(a) (b)c24c2
6c1c1x 0
FIGURE 1 Two coordinate systems for the same vector space.
EXAMPLE 1 Consider two bases BD fb1;b2gandCD fc1;c2gfor a vector space
V, such that
b1D4c1Cc2and b2D  6c1Cc2 (1)
Suppose
xD3b1Cb2 (2)
That is, suppose ¬åx¬çBD3
1
. Find ¬åx¬çC.
SOLUTION Apply the coordinate mapping determined by Ctoxin (2). Since the
coordinate mapping is a linear transformation,
¬åx¬çCD¬å3b1Cb2¬çC
D3¬åb1¬çCC¬åb2¬çC
We can write this vector equation as a matrix equation, using the vectors in the linear
combination as the columns of a matrix:
¬åx¬çCD¬åb1¬çC¬åb2¬çC3
1
(3)
1Think of ¬åx¬çBas a ‚Äúname‚Äù for xthat lists the weights used to build xas a linear combination of the basis
vectors in B.
SECOND REVISED PAGES


--- Page 259 ---
242 CHAPTER 4 Vector Spaces
This formula gives ¬åx¬çC, once we know the columns of the matrix. From (1),
¬åb1¬çCD4
1
and ¬åb2¬çCD 6
1
Thus (3) provides the solution:
¬åx¬çCD4 6
1 13
1
D6
4
TheC-coordinates of xmatch those of the xin Figure 1.
The argument used to derive formula (3) can be generalized to yield the following
result. (See Exercises 15 and 16.)
T H E O R E M 1 5 LetBD fb1; : : : ; bngandCD fc1; : : : ; cngbe bases of a vector space V. Then
there is a unique nnmatrix PC Bsuch that
¬åx¬çCDPC B¬åx¬çB(4)
The columns of PC Bare the C-coordinate vectors of the vectors in the basis B.
That is,
PC BD¬åb1¬çC¬åb2¬çC ¬åbn¬çC
(5)
The matrix PC Bin Theorem 15 is called the change-of-coordinates matrix from
BtoC. Multiplication by PC Bconverts B-coordinates into C-coordinates.2Figure 2
illustrates the change-of-coordinates equation (4).
/H11938n/H11938n[  ]C
[x]Cx
[  ]B
C‚ÜêBmultiplication
by     P[x]BV
FIGURE 2 Two coordinate systems for V.
The columns of PC Bare linearly independent because they are the coordinate
vectors of the linearly independent set B. (See Exercise 25 in Section 4.4.) Since PC B
is square, it must be invertible, by the Invertible Matrix Theorem. Left-multiplying both
sides of equation (4) by .PC B/ 1yields
.PC B/ 1¬åx¬çCD¬åx¬çB
2To remember how to construct the matrix, think ofPC B¬åx¬çBas a linear combination of the columns of
PC B. The matrix-vector product is a C-coordinate vector, so the columns ofPC Bshould be C-coordinate
vectors, too.
SECOND REVISED PAGES


--- Page 260 ---
4.7 Change of Basis 243
Thus .PC B/ 1is the matrix that converts C-coordinates into B-coordinates. That is,
.PC B/ 1DPB C(6)
Change of Basis in Rn
IfBD fb1; : : : ; bngandEis the standard basis fe1; : : : ; enginRn, then ¬åb1¬çEDb1,
and likewise for the other vectors in B. In this case, PE Bis the same as the change-of-
coordinates matrix PBintroduced in Section 4.4, namely,
PBD¬åb1b2 bn¬ç
To change coordinates between two nonstandard bases in Rn, we need Theorem 15.
The theorem shows that to solve the change-of-basis problem, we need the coordinate
vectors of the old basis relative to the new basis.
EXAMPLE 2 Letb1D 9
1
,b2D 5
 1
,c1D1
 4
,c2D3
 5
, and con-
sider the bases for R2given by BD fb1;b2gandCD fc1;c2g. Find the change-of-
coordinates matrix from BtoC.
SOLUTION The matrix PC Binvolves the C-coordinate vectors of b1and b2. Let
¬åb1¬çCDx1
x2
and¬åb2¬çCDy1
y2
. Then, by deÔ¨Ånition,
c1 c2x1
x2
Db1andc1 c2y1
y2
Db2
To solve both systems simultaneously, augment the coefÔ¨Åcient matrix with b1andb2,
and row reduce:
c1 c2 b1 b2
D1 3  9 5
 4 5 1 1
1 0 6 4
0 1  5 3
(7)
Thus
¬åb1¬çCD6
 5
and ¬åb2¬çCD4
 3
The desired change-of-coordinates matrix is therefore
PC BD¬åb1¬çC¬åb2¬çC
D6 4
 5 3
Observe that the matrix PC Bin Example 2 already appeared in (7). This is not
surprising because the Ô¨Årst column of PC Bresults from row reducing ¬åc1c2b1¬çto
¬åI¬åb1¬çC¬ç, and similarly for the second column of PC B. Thus
¬åc1c2b1b2¬ç¬å I PC B¬ç
An analogous procedure works for Ô¨Ånding the change-of-coordinates matrix between
any two bases in Rn.
SECOND REVISED PAGES


--- Page 261 ---
244 CHAPTER 4 Vector Spaces
EXAMPLE 3 Letb1D1
 3
,b2D 2
4
,c1D 7
9
,c2D 5
7
, and con-
sider the bases for R2given by BD fb1;b2gandCD fc1;c2g.
a.Find the change-of-coordinates matrix from CtoB.
b.Find the change-of-coordinates matrix from BtoC.
SOLUTION
a.Notice that PB Cis needed rather than PC B, and compute
b1 b2 c1 c2
D1 2 7 5
 3 4 9 7
1 0 5 3
0 1 6 4
So
PB CD5 3
6 4
b.By part (a) and property (6) above (with BandCinterchanged),
PC BD.PB C/ 1D1
24 3
 6 5
D2 3=2
 3 5=2
Another description of the change-of-coordinates matrix PC Buses the change-of-
coordinate matrices PBandPCthat convert B-coordinates and C-coordinates, respec-
tively, into standard coordinates. Recall that for each xinRn,
PB¬åx¬çBDx; P C¬åx¬çCDx;and ¬åx¬çCDP 1
Cx
Thus
¬åx¬çCDP 1
CxDP 1
CPB¬åx¬çB
InRn, the change-of-coordinates matrix PC Bmay be computed as P 1
CPB. Actually,
for matrices larger than 22, an algorithm analogous to the one in Example 3 is faster
than computing P 1
Cand then P 1
CPB. See Exercise 12 in Section 2.2.
PRACTICE PROBLEMS
1.LetFD ff1;f2gandGD fg1;g2gbe bases for a vector space V, and let Pbe a matrix
whose columns are ¬åf1¬çGand¬åf2¬çG. Which of the following equations is satisÔ¨Åed
byPfor all vinV?
(i)¬åv¬çFDP ¬åv¬çG(ii)¬åv¬çGDP¬åv¬çF
2.LetBandCbe as in Example 1. Use the results of that example to Ô¨Ånd the change-
of-coordinates matrix from CtoB.
4.7 EXERCISES
1.LetBD fb1;b2gandCD fc1;c2gbe bases for a vector space
V, and suppose b1D6c1 2c2andb2D9c1 4c2.
a.Find the change-of-coordinates matrix from BtoC.
b.Find¬åx¬çCforxD  3b1C2b2. Use part (a).2.LetBD fb1;b2gandCD fc1;c2gbe bases for a vector space
V, and suppose b1D  c1C4c2andb2D5c1 3c2.
a.Find the change-of-coordinates matrix from BtoC.
b.Find¬åx¬çCforxD5b1C3b2.
SECOND REVISED PAGES


--- Page 262 ---
4.7 Change of Basis 245
3.LetUD fu1;u2gandWD fw1;w2gbe bases for V, and let
Pbe a matrix whose columns are ¬åu1¬çWand¬åu2¬çW. Which
of the following equations is satisÔ¨Åed by Pfor all xinV?
(i)¬åx¬çUDP ¬åx¬çW(ii)¬åx¬çWDP ¬åx¬çU
4.LetAD fa1;a2;a3gandDD fd1;d2;d3gbe bases for V,
and let PD¬å¬åd1¬çA¬åd2¬çA¬åd3¬çA¬ç. Which of the follow-
ing equations is satisÔ¨Åed by Pfor all xinV?
(i)¬åx¬çADP ¬åx¬çD(ii)¬åx¬çDDP ¬åx¬çA
5.LetAD fa1;a2;a3gandBD fb1;b2;b3gbe bases
for a vector space V, and suppose a1D4b1 b2,
a2D  b1Cb2Cb3, and a3Db2 2b3.
a.Find the change-of-coordinates matrix from AtoB.
b.Find¬åx¬çBforxD3a1C4a2Ca3.
6.LetDD fd1;d2;d3gandFD ff1;f2;f3gbe bases for
a vector space V, and suppose f1D2d1 d2Cd3,
f2D3d2Cd3, and f3D  3d1C2d3.
a.Find the change-of-coordinates matrix from FtoD.
b.Find¬åx¬çDforxDf1 2f2C2f3.
In Exercises 7‚Äì10, let BD fb1;b2gandCD fc1;c2gbe bases for
R2. In each exercise, Ô¨Ånd the change-of-coordinates matrix from
BtoCand the change-of-coordinates matrix from CtoB.
7.b1D7
5
,b2D 3
 1
,c1D1
 5
,c2D 2
2
8.b1D 1
8
,b2D1
 5
,c1D1
4
,c2D1
1
9.b1D 6
 1
,b2D2
0
,c1D2
 1
,c2D6
 2
10. b1D7
 2
,b2D2
 1
,c1D4
1
,c2D5
2
In Exercises 11 and 12, BandCare bases for a vector space V.
Mark each statement True or False. Justify each answer.
11.a.The columns of the change-of-coordinates matrix P
C B
areB-coordinate vectors of the vectors in C.
b.IfVDRnandCis the standard basis for V, then P
C B
is the same as the change-of-coordinates matrix PBintro-
duced in Section 4.4.
12. a.The columns of P
C Bare linearly independent.
b.IfVDR2,BD fb1;b2g, andCD fc1;c2g, then row
reduction of ¬åc1c2b1b2¬çto¬åI P ¬çproduces a
matrix Pthat satisÔ¨Åes ¬åx¬çBDP ¬åx¬çCfor all xinV.
13. InP2, Ô¨Ånd the change-of-coordinates matrix from the basis
BD f1 2tCt2; 3 5tC4t2; 2tC3t2gto the standard
basisCD f1; t; t2g. Then Ô¨Ånd the B-coordinate vector for
 1C2t.
14. InP2, Ô¨Ånd the change-of-coordinates matrix from the ba-
sisBD f1 3t2; 2Ct 5t2; 1C2tgto the standard basis.
Then write t2as a linear combination of the polynomials in B.Exercises 15 and 16 provide a proof of Theorem 15. Fill in a
justiÔ¨Åcation for each step.
15. Given vinV, there exist scalars x1; : : : ; x n, such that
vDx1b1Cx2b2C  C xnbn
because (a) . Apply the coordinate mapping deter-
mined by the basis C, and obtain
¬åv¬çCDx1¬åb1¬çCCx2¬åb2¬çCC  C xn¬åbn¬çC
because (b) . This equation may be written in the form
¬åv¬çCD¬åb1¬çC¬åb2¬çC¬åbn¬çC2
64x1:::
xn3
75 .8/
by the deÔ¨Ånition of (c) . This shows that the matrix
P
C Bshown in (5) satisÔ¨Åes ¬åv¬çCDP
C B¬åv¬çBfor each vinV,
because the vector on the right side of (8) is (d) .
16. Suppose Qis any matrix such that
¬åv¬çCDQ¬åv¬çBfor each vinV .9/
SetvDb1in (9). Then (9) shows that ¬åb1¬çCis the Ô¨Årst column
ofQbecause (a) . Similarly, for kD2; : : : ; n , thekth
column of Qis (b) because (c) . This shows
that the matrix P
C BdeÔ¨Åned by (5) in Theorem 15 is the only
matrix that satisÔ¨Åes condition (4).
17. [M] LetBD fx0; : : : ; x6gandCD fy0; : : : ; y6g, where xkis
the function cosktandykis the function cos kt. Exercise 34
in Section 4.5 showed that both BandCare bases for the
vector space HDSpanfx0; : : : ; x6g.
a.SetPD¬åy0¬çB ¬åy6¬çB
, and calculate P 1.
b.Explain why the columns of P 1are the C-coordinate
vectors of x0; : : : ; x6. Then use these coordinate vectors
to write trigonometric identities that express powers of
costin terms of the functions in C.
See the Study Guide .
18. [M] (Calculus required )3Recall from calculus that integrals
such as
Z
.5cos3t 6cos4tC5cos5t 12cos6t/ dt .10/
are tedious to compute. (The usual method is to apply inte-
gration by parts repeatedly and use the half-angle formula.)
Use the matrix PorP 1from Exercise 17 to transform (10);
then compute the integral.
3The idea for Exercises 17 and 18 and Ô¨Åve related exercises in earlier
sections came from a paper by Jack W. Rogers, Jr., of Auburn University,
presented at a meeting of the International Linear Algebra Society,
August 1995. See ‚ÄúApplications of Linear Algebra in Calculus,‚Äù American
Mathematical Monthly 104(1), 1997.
SECOND REVISED PAGES


--- Page 263 ---
246 CHAPTER 4 Vector Spaces
19. [M] Let
PD2
41 2  1
 3 5 0
4 6 13
5,
v1D2
4 2
2
33
5,v2D2
4 8
5
23
5,v3D2
4 7
2
63
5
a.Find a basis fu1;u2;u3gforR3such that Pis the
change-of-coordinates matrix from fu1;u2;u3gto the
basisfv1;v2;v3g. [Hint: What do the columns of P
C B
represent?]b.Find a basis fw1;w2;w3gforR3such that Pis the change-
of-coordinates matrix from fv1;v2;v3gtofw1;w2;w3g.
20. LetBD fb1;b2g,CD fc1;c2g, andDD fd1;d2gbe bases
for a two-dimensional vector space.
a.Write an equation that relates the matrices P
C B,P
D C,
and P
D B. Justify your result.
b.[M] Use a matrix program either to help you Ô¨Ånd the
equation or to check the equation you write. Work with
three bases for R2. (See Exercises 7‚Äì10.)
SOLUTIONS TO PRACTICE PROBLEMS
1.Since the columns of PareG-coordinate vectors, a vector of the form Pxmust be
aG-coordinate vector. Thus PsatisÔ¨Åes equation (ii).
2.The coordinate vectors found in Example 1 show that
PC BD¬åb1¬çC¬åb2¬çC
D4 6
1 1
Hence
PB CD.PC B/ 1D1
101 6
 1 4
D:1 :6
 :1 :4
4.8 APPLICATIONS TO DIFFERENCE EQUATIONS
Now that powerful computers are widely available, more and more scientiÔ¨Åc and
engineering problems are being treated in a way that uses discrete, or digital, data rather
than continuous data. Difference equations are often the appropriate tool to analyze
such data. Even when a differential equation is used to model a continuous process, a
numerical solution is often produced from a related difference equation.
This section highlights some fundamental properties of linear difference equations
that are best explained using linear algebra.
Discrete-Time Signals
The vector space Sof discrete-time signals was introduced in Section 4.1. A signal in
Sis a function deÔ¨Åned only on the integers and is visualized as a sequence of numbers,
say,fykg. Figure 1 shows three typical signals whose general terms are .:7/k,1k, and
. 1/k, respectively.
yk = .7k
‚Äì2‚Äì1012 ‚Äì2‚Äì1012 ‚Äì202yk = 1kyk = (‚Äì1)k
FIGURE 1 Three signals in S.
SECOND REVISED PAGES


--- Page 264 ---
4.8 Applications to Difference Equations 247
Digital signals obviously arise in electrical and control systems engineering, but
discrete-data sequences are also generated in biology, physics, economics, demography,
and many other areas, wherever a process is measured, or sampled , at discrete time
intervals. When a process begins at a speciÔ¨Åc time, it is sometimes convenient to write
a signal as a sequence of the form .y0; y1; y2; : : :/ . The terms ykfork < 0 either are
assumed to be zero or are simply omitted.
EXAMPLE 1 The crystal-clear sounds from a compact disc player are produced
from music that has been sampled at the rate of 44,100 times per second. See Figure 2.
At each measurement, the amplitude of the music signal is recorded as a number, say,
yk. The original music is composed of many different sounds of varying frequencies,
yet the sequence fykgcontains enough information to reproduce all the frequencies
in the sound up to about 20,000 cycles per second, higher than the human ear can
sense.
ty
FIGURE 2 Sampled data from a music signal.
Linear Independence in the Space Sof Signals
To simplify notation, we consider a set of only three signals in S, say,fukg,fvkg, and
fwkg. They are linearly independent precisely when the equation
c1ukCc2vkCc3wkD0for all k (1)
implies that c1Dc2Dc3D0. The phrase ‚Äúfor all k‚Äù means for all integers‚Äîpositive,
negative, and zero. One could also consider signals that start with kD0, for example,
in which case, ‚Äúfor all k‚Äù would mean for all integers k0.
Suppose c1,c2,c3satisfy (1). Then equation (1) holds for any three consecutive
values of k, say, k,kC1, and kC2. Thus (1) implies that
c1ukC1Cc2vkC1Cc3wkC1D0for all k
and
c1ukC2Cc2vkC2Cc3wkC2D0for all k
Hence c1,c2,c3satisfy
2
4uk vk wk
ukC1vkC1wkC1
ukC2vkC2wkC23
52
4c1
c2
c33
5D2
40
0
03
5for all k (2)
The coefÔ¨Åcient matrix in this system is called the Casorati matrix of the signals, and
SG The Casorati Test 4‚Äì30
the determinant of the matrix is called the Casoratian offukg,fvkg, andfwkg. If
the Casorati matrix is invertible for at least one value of k, then (2) will imply that
c1Dc2Dc3D0, which will prove that the three signals are linearly independent.
SECOND REVISED PAGES


--- Page 265 ---
248 CHAPTER 4 Vector Spaces
EXAMPLE 2 Verify that 1k,. 2/k, and 3kare linearly independent signals.
SOLUTION The Casorati matrix is
k2
‚Äì2‚Äì2 ‚Äì4
The signals 1k,. 2/k, and 3k.2
41k. 2/k3k
1kC1. 2/kC13kC1
1kC2. 2/kC23kC23
5
Row operations can show fairly easily that this matrix is always invertible. However, it
is faster to substitute a value for k‚Äîsay, kD0‚Äîand row reduce the numerical matrix:
2
41 1 1
1 2 3
1 4 93
52
41 1 1
0 3 2
0 3 83
52
41 1 1
0 3 2
0 0 103
5
The Casorati matrix is invertible for kD0. So 1k,. 2/k, and 3kare linearly
independent.
If a Casorati matrix is not invertible, the associated signals being tested may or
may not be linearly dependent. (See Exercise 33.) However, it can be shown that if
the signals are all solutions of the same homogeneous difference equation (described
below), then either the Casorati matrix is invertible for all kand the signals are linearly
independent, or else the Casorati matrix is not invertible for all kand the signals are
linearly dependent. A nice proof using linear transformations is in the Study Guide .
Linear Difference Equations
Given scalars a0; : : : ; a n, with a0andannonzero, and given a signal f¬¥kg, the equation
a0ykCnCa1ykCn 1C  C an 1ykC1CanykD¬¥kfor all k (3)
is called a linear difference equation (orlinear recurrence relation )of order n. For
simplicity, a0is often taken equal to 1. If f¬¥kgis the zero sequence, the equation is
homogeneous ; otherwise, the equation is nonhomogeneous .
EXAMPLE 3 In digital signal processing, a difference equation such as (3) de-
scribes a linear Ô¨Ålter , and a0; : : : ; a nare called the Ô¨Ålter coefÔ¨Åcients . Iffykgis treated
as the input and f¬¥kgas the output, then the solutions of the associated homogeneous
equation are the signals that are Ô¨Åltered outand transformed into the zero signal. Let us
feed two different signals into the Ô¨Ålter
:35y kC2C:5ykC1C:35y kD¬¥k
Here .35 is an abbreviation forp
2=4. The Ô¨Årst signal is created by sampling the
continuous signal yDcos.t=4/ at integer values of t, as in Figure 3(a). The discrete
signal is
fykg D f : : : ;cos.0/;cos.=4/; cos.2=4/; cos.3=4/; : : : g
For simplicity, write :7in place of p
2=2, so that
fykg D f : : : ; 1; :7; 0;  :7; 1; :7; 0; :7; 1; :7; 0; : : : g-kD0
Table 1 shows a calculation of the output sequence f¬¥kg, where :35.:7/ is an abbreviation
for.p
2=4/.p
2=2/D:25. The output is fykg, shifted by one term.
SECOND REVISED PAGES


--- Page 266 ---
4.8 Applications to Difference Equations 249
y
1
‚Äì112y
1
‚Äì11
2ty = cos‚éõ
‚éù‚éõ‚éùœÄt‚Äì‚Äì4y = cos‚éõ
‚éù‚éõ‚éù3œÄt‚Äì‚Äì‚Äì4
t
(a) (b)
FIGURE 3 Discrete signals with different frequencies.
TABLE 1 Computing the Output of a Filter
k ykykC1ykC2 :35y kC:5ykC1C:35y kC2D¬¥k
0 1 .7 0 .35(1) + .5(.7) + .35(0) = .7
1 .7 0  :7 .35(.7) + .5(0) + :35. :7/= 0
2 0 :7 1 .35(0) + :5. :7/+:35. 1/= :7
3  :7 1 :7 :35. :7/+:5. 1/+:35. :7/= 1
4  1 :7 0 :35. 1/+:5. :7/+ .35(0) =  :7
5  :7 0 .7 :35. :7/+ .5(0) + .35(.7) = 0
:::::::::
A different input signal is produced from the higher frequency signal
yDcos.3t=4/ , shown in Figure 3(b). Sampling at the same rate as before produces a
new input sequence:
fwkg D f : : : ; 1;  :7; 0; :7;  1; :7; 0;  :7; 1;  :7; 0; : : : g-kD0
When fwkgis fed into the Ô¨Ålter, the output is the zero sequence. The Ô¨Ålter, called a
low-pass Ô¨Ålter , letsfykgpass through, but stops the higher frequency fwkg.
In many applications, a sequence f¬¥kgis speciÔ¨Åed for the right side of a difference
equation (3), and a fykgthat satisÔ¨Åes (3) is called a solution of the equation. The next
example shows how to Ô¨Ånd solutions for a homogeneous equation.
EXAMPLE 4 Solutions of a homogeneous difference equation often have the form
ykDrkfor some r. Find some solutions of the equation
ykC3 2ykC2 5ykC1C6ykD0for all k (4)
SOLUTION Substitute rkforykin the equation and factor the left side:
rkC3 2rkC2 5rkC1C6rkD0 (5)
rk.r3 2r2 5rC6/D0
rk.r 1/.rC2/.r 3/D0 (6)
Since (5) is equivalent to (6), rksatisÔ¨Åes the difference equation (4) if and only if r
satisÔ¨Åes (6). Thus 1k,. 2/k, and 3kare all solutions of (4). For instance, to verify that
3kis a solution of (4), compute
3kC3 23kC2 53kC1C63k
D3k.27 18 15C6/D0for all k
SECOND REVISED PAGES


--- Page 267 ---
250 CHAPTER 4 Vector Spaces
In general, a nonzero signal rksatisÔ¨Åes the homogeneous difference equation
ykCnCa1ykCn 1C  C an 1ykC1CanykD0for all k
if and only if ris a root of the auxiliary equation
rnCa1rn 1C  C an 1rCanD0
We will not consider the case in which ris a repeated root of the auxiliary equation.
When the auxiliary equation has a complex root , the difference equation has solutions
of the form skcosk!andsksink!, for constants sand!. This happened in Example 3.
Solution Sets of Linear Difference Equations
Given a1; : : : ; a n, consider the mapping TWS!Sthat transforms a signal fykginto a
signal fwkggiven by
wkDykCnCa1ykCn 1C  C an 1ykC1Canyk
It is readily checked that Tis alinear transformation. This implies that the solution set
of the homogeneous equation
ykCnCa1ykCn 1C  C an 1ykC1CanykD0for all k
is the kernel of T(the set of signals that Tmaps into the zero signal), and hence the
solution set is a subspace ofS. Any linear combination of solutions is again a solution.
The next theorem, a simple but basic result, will lead to more information about the
solution sets of difference equations.
T H E O R E M 1 6 Ifan¬§0and iff¬¥kgis given, the equation
ykCnCa1ykCn 1C  C an 1ykC1CanykD¬¥kfor all k (7)
has a unique solution whenever y0; : : : ; y n 1are speciÔ¨Åed.
PROOF Ify0; : : : ; y n 1are speciÔ¨Åed, use (7) to deÔ¨Åne
ynD¬¥0 ¬åa1yn 1C  C an 1y1Cany0¬ç
And now that y1; : : : ; y nare speciÔ¨Åed, use (7) to deÔ¨Åne ynC1. In general, use the
recurrence relation
ynCkD¬¥k ¬åa1ykCn 1C  C anyk¬ç (8)
to deÔ¨Åne ynCkfork0. To deÔ¨Åne ykfork < 0 , use the recurrence relation
ykD1
an¬¥k 1
an¬åykCnCa1ykCn 1C  C an 1ykC1¬ç (9)
This produces a signal that satisÔ¨Åes (7). Conversely, any signal that satisÔ¨Åes (7) for all
kcertainly satisÔ¨Åes (8) and (9), so the solution of (7) is unique.
T H E O R E M 1 7 The set Hof all solutions of the nth-order homogeneous linear difference equation
ykCnCa1ykCn 1C  C an 1ykC1CanykD0for all k (10)
is ann-dimensional vector space.
SECOND REVISED PAGES


--- Page 268 ---
4.8 Applications to Difference Equations 251
PROOF As was pointed out earlier, His a subspace of Sbecause His the kernel
of a linear transformation. For fykginH, letFfykgbe the vector in Rngiven by
.y0; y1; : : : ; y n 1/. It is readily veriÔ¨Åed that FWH!Rnis a linear transformation.
Given any vector .y0; y1; : : : ; y n 1/inRn, Theorem 16 says that there is a unique
signal fykginHsuch that Ffykg D.y0; y1; : : : ; y n 1/. This means that Fis a one-
to-one linear transformation of HontoRn; that is, Fis an isomorphism. Thus
dimHDdimRnDn. (See Exercise 32 in Section 4.5.)
EXAMPLE 5 Find a basis for the set of all solutions to the difference equation
ykC3 2ykC2 5ykC1C6ykD0for all k
SOLUTION Our work in linear algebra really pays off now! We know from Examples 2
and 4 that 1k,. 2/k, and 3kare linearly independent solutions. In general, it can be
difÔ¨Åcult to verify directly that a set of signals spans the solution space. But that is no
problem here because of two key theorems‚ÄîTheorem 17, which shows that the solution
space is exactly three-dimensional, and the Basis Theorem in Section 4.5, which says
that a linearly independent set of nvectors in an n-dimensional space is automatically
a basis. So 1k,. 2/k, and 3kform a basis for the solution space.
The standard way to describe the ‚Äúgeneral solution‚Äù of the difference equation (10)
is to exhibit a basis for the subspace of all solutions. Such a basis is usually called a
fundamental set of solutions of (10). In practice, if you can Ô¨Ånd nlinearly independent
signals that satisfy (10), they will automatically span the n-dimensional solution space,
as explained in Example 5.
Nonhomogeneous Equations
The general solution of the nonhomogeneous difference equation
ykCnCa1ykCn 1C  C an 1ykC1CanykD¬¥kfor all k (11)
can be written as one particular solution of (11) plus an arbitrary linear combination of
a fundamental set of solutions of the corresponding homogeneous equation (10). This
fact is analogous to the result in Section 1.5 showing that the solution sets of AxDb
andAxD0are parallel. Both results have the same explanation: The mapping x7!Ax
is linear, and the mapping that transforms the signal fykginto the signal f¬¥kgin (11) is
linear. See Exercise 35.
EXAMPLE 6 Verify that the signal ykDk2satisÔ¨Åes the difference equation
ykC2 4ykC1C3ykD  4k for all k (12)
Then Ô¨Ånd a description of all solutions of this equation.
SOLUTION Substitute k2forykon the left side of (12):
.kC2/2 4.kC1/2C3k2
D.k2C4kC4/ 4.k2C2kC1/C3k2
D  4k
Sok2is indeed a solution of (12). The next step is to solve the homogeneous equation
ykC2 4ykC1C3ykD0 (13)
The auxiliary equation is
r2 4rC3D.r 1/.r 3/D0
SECOND REVISED PAGES


--- Page 269 ---
252 CHAPTER 4 Vector Spaces
The roots are rD1; 3. So two solutions of the homogeneous difference equation are 1k
and3k. They are obviously not multiples of each other, so they are linearly independent
signals. By Theorem 17, the solution space is two-dimensional, so 3kand1kform a basis
for the set of solutions of equation (13). Translating that set by a particular solution of
the nonhomogeneous equation (12), we obtain the general solution of (12):
k2Cc11kCc23k;ork2Cc1Cc23k
Figure 4 gives a geometric visualization of the two solution sets. Each point in the Ô¨Ågure
corresponds to one signal in S.
x3
1k3k
k2
x1
x2Span{1k, 3k}k2 /H11001 Span{1k, 3k}
FIGURE 4
Solution sets of difference
equations (12) and (13).Reduction to Systems of First-Order Equations
A modern way to study a homogeneous nth-order linear difference equation is to replace
it by an equivalent system of Ô¨Årst-order difference equations, written in the form
xkC1DAxkfor all k
where the vectors xkare inRnandAis annnmatrix.
A simple example of such a (vector-valued) difference equation was already studied
in Section 1.10. Further examples will be covered in Sections 4.9 and 5.6.
EXAMPLE 7 Write the following difference equation as a Ô¨Årst-order system:
ykC3 2ykC2 5ykC1C6ykD0for all k
SOLUTION For each k, set
xkD2
4yk
ykC1
ykC23
5
The difference equation says that ykC3D  6ykC5ykC1C2ykC2, so
xkC1D2
4ykC1
ykC2
ykC33
5D2
640CykC1C0
0C0 CykC2
 6ykC5ykC1C2ykC23
75D2
40 1 0
0 0 1
 6 5 23
52
4yk
ykC1
ykC23
5
That is,
xkC1DAxkfor all k; where AD2
40 1 0
0 0 1
 6 5 23
5
In general, the equation
ykCnCa1ykCn 1C  C an 1ykC1CanykD0for all k
can be rewritten as xkC1DAxkfor all k, where
xkD2
6664yk
ykC1
:::
ykCn 13
7775; AD2
6666640 1 0 : : : 0
0 0 1 0
:::::::::
0 0 0 1
 an an 1 an 2   a13
777775
SECOND REVISED PAGES


--- Page 270 ---
4.8 Applications to Difference Equations 253
Further Reading
Hamming, R. W., Digital Filters , 3rd ed. (Englewood Cliffs, NJ: Prentice-Hall, 1989),
pp. 1‚Äì37.
Kelly, W. G., and A. C. Peterson, Difference Equations , 2nd ed. (San Diego: Harcourt-
Academic Press, 2001).
Mickens, R. E., Difference Equations , 2nd ed. (New York: Van Nostrand Reinhold,
1990), pp. 88‚Äì141.
Oppenheim, A. V ., and A. S. Willsky, Signals and Systems , 2nd ed. (Upper Saddle River,
NJ: Prentice-Hall, 1997), pp. 1‚Äì14, 21‚Äì30, 38‚Äì43.
PRACTICE PROBLEM
It can be shown that the signals 2k,3ksink
2, and 3kcosk
2are solutions of
ykC3 2ykC2C9ykC1 18y kD0
Show that these signals form a basis for the set of all solutions of the difference equation.
4.8 EXERCISES
Verify that the signals in Exercises 1 and 2 are solutions of the
accompanying difference equation.
1.2k; . 4/kIykC2C2ykC1 8ykD0
2.3k; . 3/kIykC2 9ykD0
Show that the signals in Exercises 3‚Äì6 form a basis for the solution
set of the accompanying difference equation.
3.The signals and equation in Exercise 1
4.The signals and equation in Exercise 2
5.. 3/k; k. 3/kIykC2C6ykC1C9ykD0
6.5kcosk
2; 5ksink
2IykC2C25y kD0
In Exercises 7‚Äì12, assume the signals listed are solutions of the
given difference equation. Determine if the signals form a basis
for the solution space of the equation. Justify your answers using
appropriate theorems.
7.1k; 2k; . 2/kIykC3 ykC2 4ykC1C4ykD0
8.2k; 4k; . 5/kIykC3 ykC2 22y kC1C40y kD0
9.1k; 3kcosk
2; 3ksink
2IykC3 ykC2C9ykC1 9ykD0
10.. 1/k; k. 1/k; 5kIykC3 3ykC2 9ykC1 5ykD0
11.. 1/k; 3kIykC3CykC2 9ykC1 9ykD0
12.1k; . 1/kIykC4 2ykC2CykD0In Exercises 13‚Äì16, Ô¨Ånd a basis for the solution space of the
difference equation. Prove that the solutions you Ô¨Ånd span the
solution set.
13.ykC2 ykC1C2
9ykD014.ykC2 7ykC1C12y kD0
15.ykC2 25y kD0 16.16y kC2C8ykC1 3ykD0
Exercises 17 and 18 concern a simple model of the national
economy described by the difference equation
YkC2 a.1Cb/Y kC1CabY kD1 (14)
Here Ykis the total national income during year k,ais a constant
less than 1, called the marginal propensity to consume , and bis
a positive constant of adjustment that describes how changes in
consumer spending affect the annual rate of private investment.1
17. Find the general solution of equation (14) when aD:9and
bD4
9. What happens to Ykaskincreases? [ Hint: First Ô¨Ånd a
particular solution of the form YkDT, where Tis a constant,
called the equilibrium level of national income.]
18. Find the general solution of equation (14) when aD:9and
bD:5.
1For example, see Discrete Dynamical Systems , by James T. Sandefur
(Oxford: Clarendon Press, 1990), pp. 267‚Äì276. The original
accelerator-multiplier model is attributed to the economist P. A.
Samuelson.
SECOND REVISED PAGES


--- Page 271 ---
254 CHAPTER 4 Vector Spaces
A lightweight cantilevered beam is supported at Npoints spaced
10 ft apart, and a weight of 500 lb is placed at the end of the
beam, 10 ft from the Ô¨Årst support, as in the Ô¨Ågure. Let ykbe
the bending moment at the kth support. Then y1D5000 ft-lb.
Suppose the beam is rigidly attached at the Nth support and the
bending moment there is zero. In between, the moments satisfy
thethree-moment equation
ykC2C4ykC1CykD0forkD1; 2; : : : ; N  2 .15/
123
y1y2y3yNN 500
lb10' 10' 10'
Bending moments on a cantilevered beam.
19. Find the general solution of difference equation (15). Justify
your answer.
20. Find the particular solution of (15) that satisÔ¨Åes the boundary
conditions y1D5000 andyND0. (The answer involves N.)
21. When a signal is produced from a sequence of measurements
made on a process (a chemical reaction, a Ô¨Çow of heat
through a tube, a moving robot arm, etc.), the signal usually
contains random noise produced by measurement errors. A
standard method of preprocessing the data to reduce the noise
is to smooth or Ô¨Ålter the data. One simple Ô¨Ålter is a moving
average that replaces each ykby its average with the two
adjacent values:
1
3ykC1C1
3ykC1
3yk 1D¬¥kforkD1; 2; : : :
Suppose a signal yk, forkD0; : : : ; 14 , is
9; 5; 7; 3; 2; 4; 6; 5; 7; 6; 8; 10; 9; 5; 7
Use the Ô¨Ålter to compute ¬¥1; : : : ; ¬¥ 13. Make a broken-line
graph that superimposes the original signal and the smoothed
signal.
22. Letfykgbe the sequence produced by sampling the continu-
ous signal 2cost
4Ccos3t
4attD0; 1; 2; : : : ; as shown in
the Ô¨Ågure. The values of yk, beginning with kD0, are
3; :7; 0;  :7; 3; :7; 0; :7; 3; :7; 0; : : :
where .7 is an abbreviation forp
2=2.
a.Compute the output signal f¬¥kgwhenfykgis fed into the
Ô¨Ålter in Example 3.
b.Explain how and why the output in part (a) is related to
the calculations in Example 3.
y
1
‚Äì112t‚éõ
‚éù‚éõ‚éù3œÄt‚Äì‚Äì‚Äì4 ‚éõ
‚éù‚éõ‚éùœÄt‚Äì‚Äì4y = 2 cos        + cos          Sampled data from 2cost
4Ccos3t
4.
Exercises 23 and 24 refer to a difference equation of the form
ykC1 aykDb, for suitable constants aandb.
23. A loan of $10,000 has an interest rate of 1% per month and a
monthly payment of $450. The loan is made at month kD0,
and the Ô¨Årst payment is made one month later, at kD1. For
kD0; 1; 2; : : : ; letykbe the unpaid balance of the loan just
after the kth monthly payment. Thus
y1D10;000 C.:01/10;000  450
New Balance Interest Payment
balance due added
a.Write a difference equation satisÔ¨Åed by fykg.
b.[M] Create a table showing kand the balance ykat month
k. List the program or the keystrokes you used to create
the table.
c.[M] What will kbe when the last payment is made? How
much will the last payment be? How much money did the
borrower pay in total?
24. At time kD0, an initial investment of $1000 is made into a
savings account that pays 6% interest per year compounded
monthly. (The interest rate per month is .005.) Each month
after the initial investment, an additional $200 is added to
the account. For kD0; 1; 2; : : : ; letykbe the amount in the
account at time k, just after a deposit has been made.
a.Write a difference equation satisÔ¨Åed by fykg.
b.[M] Create a table showing kand the total amount in the
savings account at month k, forkD0through 60. List
your program or the keystrokes you used to create the
table.
c.[M] How much will be in the account after two years (that
is, 24 months), four years, and Ô¨Åve years? How much of
the Ô¨Åve-year total is interest?
In Exercises 25‚Äì28, show that the given signal is a solution of
the difference equation. Then Ô¨Ånd the general solution of that
difference equation.
25.ykDk2IykC2C3ykC1 4ykD7C10k
26.ykD1CkIykC2 8ykC1C15y kD2C8k
27.ykD2 2kIykC2 9
2ykC1C2ykD2C3k
28.ykD2k 4IykC2C3
2ykC1 ykD1C3k
SECOND REVISED PAGES


--- Page 272 ---
4.9 Applications to Markov Chains 255
Write the difference equations in Exercises 29 and 30 as Ô¨Årst-order
systems, xkC1DAxk, for all k.
29.ykC4 6ykC3C8ykC2C6ykC1 9ykD0
30.ykC3 3
4ykC2C1
16ykD0
31. Is the following difference equation of order 3? Explain.
ykC3C5ykC2C6ykC1D0
32. What is the order of the following difference equation? Ex-
plain your answer.
ykC3Ca1ykC2Ca2ykC1Ca3ykD0
33. LetykDk2and¬¥kD2kjkj. Are the signals fykgandf¬¥kg
linearly independent? Evaluate the associated Casorati ma-
trixC.k/ forkD0,kD  1, and kD  2, and discuss your
results.
34. Letf,g, andhbe linearly independent functions deÔ¨Åned for
all real numbers, and construct three signals by sampling the
values of the functions at the integers:
ukDf .k/; v kDg.k/; w kDh.k/Must the signals be linearly independent in S? Discuss.
35. Letaandbbe nonzero numbers. Show that the mapping T
deÔ¨Åned by Tfykg D f wkg, where
wkDykC2CaykC1Cbyk
is a linear transformation from SintoS.
36. LetVbe a vector space, and let TWV!Vbe a linear trans-
formation. Given zinV, suppose xpinVsatisÔ¨Åes T .xp/Dz,
and let ube any vector in the kernel of T. Show that uCxp
satisÔ¨Åes the nonhomogeneous equation T .x/Dz.
37. LetS0be the vector space of all sequences of the form
.y0; y1; y2; : : :/ , and deÔ¨Åne linear transformations TandD
fromS0intoS0by
T .y 0; y1; y2; : : :/D.y1; y2; y3; : : :/
D.y 0; y1; y2; : : :/D.0; y 0; y1; y2; : : :/
Show that TDDI(the identity transformation on S0) and
yetDT¬§I.
SOLUTION TO PRACTICE PROBLEM
Examine the Casorati matrix:
C.k/D2
66642k3ksink
23kcosk
2
2kC13kC1sin.kC1/
23kC1cos.kC1/
2
2kC23kC2sin.kC2/
23kC2cos.kC2/
23
7775
SetkD0and row reduce the matrix to verify that it has three pivot positions and hence
is invertible:
C.0/D2
41 0 1
2 3 0
4 0  93
52
41 0 1
0 3  2
0 0  133
5
The Casorati matrix is invertible at kD0, so the signals are linearly independent.
Since there are three signals, and the solution space Hof the difference equation has
dimension 3 (Theorem 17), the signals form a basis for H, by the Basis Theorem.
4.9 APPLICATIONS TO MARKOV CHAINS
The Markov chains described in this section are used as mathematical models of a
wide variety of situations in biology, business, chemistry, engineering, physics, and
elsewhere. In each case, the model is used to describe an experiment or measurement
that is performed many times in the same way, where the outcome of each trial of the
experiment will be one of several speciÔ¨Åed possible outcomes, and where the outcome
of one trial depends only on the immediately preceding trial.
For example, if the population of a city and its suburbs were measured each year,
then a vector such as
x0D:60
:40
(1)
SECOND REVISED PAGES


--- Page 273 ---
256 CHAPTER 4 Vector Spaces
could indicate that 60% of the population lives in the city and 40% in the suburbs. The
decimals in x0add up to 1 because they account for the entire population of the region.
Percentages are more convenient for our purposes here than population totals.
A vector with nonnegative entries that add up to 1 is called a probability vector . A
stochastic matrix is a square matrix whose columns are probability vectors. A Markov
chain is a sequence of probability vectors x0;x1;x2; : : :, together with a stochastic
matrix P, such that
x1DPx0;x2DPx1;x3DPx2; : : :
Thus the Markov chain is described by the Ô¨Årst-order difference equation
xkC1DPxkforkD0; 1; 2; : : :
When a Markov chain of vectors in Rndescribes a system or a sequence of
experiments, the entries in xklist, respectively, the probabilities that the system is in
each of npossible states, or the probabilities that the outcome of the experiment is one
ofnpossible outcomes. For this reason, xkis often called a state vector .
EXAMPLE 1 Section 1.10 examined a model for population movement between a
city and its suburbs. See Figure 1. The annual migration between these two parts of the
metropolitan region was governed by the migration matrix M:
From:
City Suburbs To:
MD:95
:05:03
:97
City
Suburbs
That is, each year 5% of the city population moves to the suburbs, and 3% of the
suburban population moves to the city. The columns of Mare probability vectors, so
Mis a stochastic matrix. Suppose the 2014 population of the region is 600,000 in the
city and 400,000 in the suburbs. Then the initial distribution of the population in the
region is given by x0in (1) above. What is the distribution of the population in 2015?
In 2016?
.03.05
.95 .97
City Suburbs
FIGURE 1 Annual percentage migration between city and suburbs.
SOLUTION In Example 3 of Section 1.10, we saw that after one year, the population
vector600;000
400;000
changed to
:95 :03
:05 :97600;000
400;000
D582;000
418;000
SECOND REVISED PAGES


--- Page 274 ---
4.9 Applications to Markov Chains 257
If we divide both sides of this equation by the total population of 1 million, and use the
fact that kMxDM.k x/, we Ô¨Ånd that
:95 :03
:05 :97:600
:400
D:582
:418
The vector x1D:582
:418
gives the population distribution in 2015. That is, 58.2% of
the region lived in the city and 41.8% lived in the suburbs. Similarly, the population
distribution in 2016 is described by a vector x2, where
x2DMx1D:95 :03
:05 :97:582
:418
D:565
:435
EXAMPLE 2 Suppose the voting results of a congressional election at a certain
voting precinct are represented by a vector xinR3:
xD2
4% voting Democratic (D)
% voting Republican (R)
% voting Libertarian (L)3
5
Suppose we record the outcome of the congressional election every two years by a vector
of this type and the outcome of one election depends only on the results of the preceding
election. Then the sequence of vectors that describe the votes every two years may be a
Markov chain. As an example of a stochastic matrix Pfor this chain, we take
From:
D R L To:
PD2
4:70
:20
:10:10
:80
:10:30
:30
:403
5D
R
L
The entries in the Ô¨Årst column, labeled D, describe what the persons voting Democratic
in one election will do in the next election. Here we have supposed that 70% will vote D
again in the next election, 20% will vote R, and 10% will vote L. Similar interpretations
hold for the other columns of P. A diagram for this matrix is shown in Figure 2.
Democratic
voteRepublican
vote
Libertarian
vote.30 .30.20.80 .70
.40.10 .10.10
FIGURE 2 V oting changes from one election to the
next.
SECOND REVISED PAGES


--- Page 275 ---
258 CHAPTER 4 Vector Spaces
If the ‚Äútransition‚Äù percentages remain constant over many years from one election
to the next, then the sequence of vectors that give the voting outcomes forms a Markov
chain. Suppose the outcome of one election is given by
x0D2
4:55
:40
:053
5
Determine the likely outcome of the next election and the likely outcome of the election
after that.
SOLUTION The outcome of the next election is described by the state vector x1and
that of the election after that by x2, where
x1DPx0D2
4:70 :10 :30
:20 :80 :30
:10 :10 :403
52
4:55
:40
:053
5D2
4:440
:445
:1153
544% will vote D.
44.5% will vote R.
11.5% will vote L.
x2DPx1D2
4:70 :10 :30
:20 :80 :30
:10 :10 :403
52
4:440
:445
:1153
5D2
4:3870
:4785
:13453
538.7% will vote D.
47.8% will vote R.
13.5% will vote L.
To understand why x1does indeed give the outcome of the next election, suppose 1000
persons voted in the ‚ÄúÔ¨Årst‚Äù election, with 550 voting D, 400 voting R, and 50 voting L.
(See the percentages in x0.) In the next election, 70% of the 550 will vote D again, 10%
of the 400 will switch from R to D, and 30% of the 50 will switch from L to D. Thus
the total D vote will be
:70.550/ C:10.400/ C:30.50/ D385C40C15D440 (2)
Thus 44% of the vote next time will be for the D candidate. The calculation in (2) is
essentially the same as that used to compute the Ô¨Årst entry in x1. Analogous calculations
could be made for the other entries in x1, for the entries in x2, and so on.
Predicting the Distant Future
The most interesting aspect of Markov chains is the study of a chain‚Äôs long-term
behavior. For instance, what can be said in Example 2 about the voting after many
elections have passed (assuming that the given stochastic matrix continues to describe
the transition percentages from one election to the next)? Or, what happens to the pop-
ulation distribution in Example 1 ‚Äúin the long run‚Äù? Before answering these questions,
we turn to a numerical example.
EXAMPLE 3 LetPD2
4:5 :2 :3
:3 :8 :3
:2 0 :43
5andx0D2
41
0
03
5. Consider a system whose
state is described by the Markov chain xkC1DPxk, forkD0; 1; : : : What happens to
the system as time passes? Compute the state vectors x1; : : : ; x15to Ô¨Ånd out.
SECOND REVISED PAGES


--- Page 276 ---
4.9 Applications to Markov Chains 259
SOLUTION
x1DPx0D2
4:5 :2 :3
:3 :8 :3
:2 0 :43
52
41
0
03
5D2
4:5
:3
:23
5
x2DPx1D2
4:5 :2 :3
:3 :8 :3
:2 0 :43
52
4:5
:3
:23
5D2
4:37
:45
:183
5
x3DPx2D2
4:5 :2 :3
:3 :8 :3
:2 0 :43
52
4:37
:45
:183
5D2
4:329
:525
:1463
5
The results of further calculations are shown below, with entries rounded to four or Ô¨Åve
signiÔ¨Åcant Ô¨Ågures.
x4D2
4:3133
:5625
:12423
5; x5D2
4:3064
:5813
:11233
5; x6D2
4:3032
:5906
:10623
5; x7D2
4:3016
:5953
:10313
5
x8D2
4:3008
:5977
:10163
5; x9D2
4:3004
:5988
:10083
5;x10D2
4:3002
:5994
:10043
5;x11D2
4:3001
:5997
:10023
5
x12D2
4:30005
:59985
:100103
5;x13D2
4:30002
:59993
:100053
5;x14D2
4:30001
:59996
:100023
5;x15D2
4:30001
:59998
:100013
5
These vectors seem to be approaching qD2
4:3
:6
:13
5. The probabilities are hardly changing
from one value of kto the next. Observe that the following calculation is exact (with no
rounding error):
PqD2
4:5 :2 :3
:3 :8 :3
:2 0 :43
52
4:3
:6
:13
5D2
64:15C:12C:03
:09C:48C:03
:06C0C:043
75D2
4:30
:60
:103
5Dq
When the system is in state q, there is no change in the system from one measurement
to the next.
Steady-State Vectors
IfPis a stochastic matrix, then a steady-state vector (orequilibrium vector ) forPis
a probability vector qsuch that
PqDq
It can be shown that every stochastic matrix has a steady-state vector. In Example 3, q
is a steady-state vector for P.
EXAMPLE 4 The probability vector qD:375
:625
is a steady-state vector for the
population migration matrix Min Example 1, because
MqD:95 :03
:05 :97:375
:625
D:35625 C:01875
:01875 C:60625
D:375
:625
Dq
SECOND REVISED PAGES


--- Page 277 ---
260 CHAPTER 4 Vector Spaces
If the total population of the metropolitan region in Example 1 is 1 million, then
qfrom Example 4 would correspond to having 375,000 persons in the city and
625,000 in the suburbs. At the end of one year, the migration out of the city would
be.:05/.375;000/ D18;750 persons, and the migration intothe city from the suburbs
would be .:03/.625;000/ D18;750 persons. As a result, the population in the city would
remain the same. Similarly, the suburban population would be stable.
The next example shows how to Ô¨Ånda steady-state vector.
EXAMPLE 5 LetPD:6 :3
:4 :7
. Find a steady-state vector for P.
SOLUTION First, solve the equation PxDx.
Px xD0
Px IxD0 Recall from Section 1.4 that IxDx.
.P I/xD0
ForPas above,
P ID:6 :3
:4 :7
 1 0
0 1
D :4 :3
:4 :3
To Ô¨Ånd all solutions of .P I/xD0, row reduce the augmented matrix:
 :4 :3 0
:4 :3 0
 :4 :3 0
0 0 0
1 3=4 0
0 0 0
Then x1D3
4x2andx2is free. The general solution is x23=4
1
.
Next, choose a simple basis for the solution space. One obvious choice is3=4
1
but a better choice with no fractions is wD3
4
(corresponding to x2D4/.
Finally, Ô¨Ånd a probability vector in the set of all solutions of PxDx. This process
is easy, since every solution is a multiple of the solution wabove. Divide wby the sum
of its entries and obtain
qD3=7
4=7
As a check, compute
PqD6=10 3=10
4=10 7=103=7
4=7
D18=70 C12=70
12=70 C28=70
D30=70
40=70
Dq
The next theorem shows that what happened in Example 3 is typical of many
stochastic matrices. We say that a stochastic matrix is regular if some matrix power
Pkcontains only strictly positive entries. For Pin Example 3,
P2D2
4:37 :26 :33
:45 :70 :45
:18 :04 :223
5
Since every entry in P2is strictly positive, Pis a regular stochastic matrix.
Also, we say that a sequence of vectors fxkWkD1; 2; : : : gconverges to a vector
qask! 1 if the entries in xkcan be made as close as desired to the corresponding
entries in qby taking ksufÔ¨Åciently large.
SECOND REVISED PAGES


--- Page 278 ---
4.9 Applications to Markov Chains 261
T H E O R E M 1 8 IfPis annnregular stochastic matrix, then Phas a unique steady-state vector
q. Further, if x0is any initial state and xkC1DPxkforkD0; 1; 2; : : : ; then the
Markov chain fxkgconverges to qask! 1 .
This theorem is proved in standard texts on Markov chains. The amazing part of the
theorem is that the initial state has no effect on the long-term behavior of the Markov
chain. You will see later (in Section 5.2) why this is true for several stochastic matrices
studied here.
EXAMPLE 6 In Example 2, what percentage of the voters are likely to vote for the
Republican candidate in some election many years from now, assuming that the election
outcomes form a Markov chain?
SOLUTION For computations by hand, the wrong approach is to pick some initial
vector x0and compute x1; : : : ; xkfor some large value of k. You have no way of knowing
how many vectors to compute, and you cannot be sure of the limiting values of the
entries in xk.
The correct approach is to compute the steady-state vector and then appeal to
Theorem 18. Given Pas in Example 2, form P Iby subtracting 1 from each diagonal
entry in P. Then row reduce the augmented matrix:
¬å.P I/ 0¬çD2
4 :3 :1 :3 0
:2 :2 :3 0
:1 :1  :6 03
5
Recall from earlier work with decimals that the arithmetic is simpliÔ¨Åed by multiplying
each row by 10.1
2
4 3 1 3 0
2 2 3 0
1 1  6 03
52
41 0  9=4 0
0 1  15=4 0
0 0 0 03
5
The general solution of .P I/xD0isx1D9
4x3; x2D15
4x3, andx3is free. Choosing
x3D4, we obtain a basis for the solution space whose entries are integers, and from this
we easily Ô¨Ånd the steady-state vector whose entries sum to 1:
wD2
49
15
43
5;and qD2
49=28
15=28
4=283
52
4:32
:54
:143
5
The entries in qdescribe the distribution of votes at an election to be held many years
from now (assuming the stochastic matrix continues to describe the changes from one
election to the next). Thus, eventually, about 54% of the vote will be for the Republican
candidate.
1Warning: Don‚Äôt multiply only Pby 10. Instead, multiply the augmented matrix for equation
.P I/xD0by 10.
SECOND REVISED PAGES


--- Page 279 ---
262 CHAPTER 4 Vector Spaces
N U M E R I C A L N O T E
You may have noticed that if xkC1DPxkforkD0; 1; : : : ; then
x2DPx1DP.P x0/DP2x0;
and, in general,xkDPkx0forkD0; 1; : : :
To compute a speciÔ¨Åc vector such as x3, fewer arithmetic operations are needed
to compute x1,x2, and x3, rather than P3andP3x0. However, if Pis small‚Äîsay,
3030‚Äîthe machine computation time is insigniÔ¨Åcant for both methods, and a
command to compute P3x0might be preferred because it requires fewer human
keystrokes.
PRACTICE PROBLEMS
1.Suppose the residents of a metropolitan region move according to the probabilities
in the migration matrix Min Example 1 and a resident is chosen ‚Äúat random.‚Äù Then
a state vector for a certain year may be interpreted as giving the probabilities that the
person is a city resident or a suburban resident at that time.
a.Suppose the person chosen is a city resident now, so that x0D1
0
. What is the
likelihood that the person will live in the suburbs next year?
b.What is the likelihood that the person will be living in the suburbs in two years?
2.LetPD:6 :2
:4 :8
andqD:3
:7
. Isqa steady-state vector for P?
3.What percentage of the population in Example 1 will live in the suburbs after many
years?
4.9 EXERCISES
1.A small remote village receives radio broadcasts from two
radio stations, a news station and a music station. Of the
listeners who are tuned to the news station, 70% will remain
listening to the news after the station break that occurs each
half hour, while 30% will switch to the music station at the
station break. Of the listeners who are tuned to the music
station, 60% will switch to the news station at the station
break, while 40% will remain listening to the music. Suppose
everyone is listening to the news at 8:15 A.M.
a.Give the stochastic matrix that describes how the radio
listeners tend to change stations at each station break.
Label the rows and columns.
b.Give the initial state vector.
c.What percentage of the listeners will be listening to the
music station at 9:25 A.M. (after the station breaks at 8:30
and 9:00 A.M. )?
2.A laboratory animal may eat any one of three foods each day.
Laboratory records show that if the animal chooses one foodon one trial, it will choose the same food on the next trial with
a probability of 50%, and it will choose the other foods on the
next trial with equal probabilities of 25%.
a.What is the stochastic matrix for this situation?
b.If the animal chooses food #1 on an initial trial, what is
the probability that it will choose food #2 on the second
trial after the initial trial?
3.On any given day, a student is either healthy or ill. Of
the students who are healthy today, 95% will be healthy
SECOND REVISED PAGES


--- Page 280 ---
4.9 Applications to Markov Chains 263
tomorrow. Of the students who are ill today, 55% will still
be ill tomorrow.
a.What is the stochastic matrix for this situation?
b.Suppose 20% of the students are ill on Monday. What
fraction or percentage of the students are likely to be ill
on Tuesday? On Wednesday?
c.If a student is well today, what is the probability that he
or she will be well two days from now?
4.The weather in Columbus is either good, indifferent, or bad
on any given day. If the weather is good today, there is
a 60% chance the weather will be good tomorrow, a 30%
chance the weather will be indifferent, and a 10% chance the
weather will be bad. If the weather is indifferent today, it will
be good tomorrow with probability .40 and indifferent with
probability .30. Finally, if the weather is bad today, it will
be good tomorrow with probability .40 and indifferent with
probability .50.
a.What is the stochastic matrix for this situation?
b.Suppose there is a 50% chance of good weather today
and a 50% chance of indifferent weather. What are the
chances of bad weather tomorrow?
c.Suppose the predicted weather for Monday is 40% in-
different weather and 60% bad weather. What are the
chances for good weather on Wednesday?
In Exercises 5‚Äì8, Ô¨Ånd the steady-state vector.
5.:1 :6
:9 :4
6.:8 :5
:2 :5
7.2
4:7 :1 :1
:2 :8 :2
:1 :1 :73
5 8.2
4:7 :2 :2
0 :2 :4
:3 :6 :43
5
9.Determine if PD:2 1
:8 0
is a regular stochastic matrix.
10. Determine if PD1 :2
0 :8
is a regular stochastic matrix.
11.a.Find the steady-state vector for the Markov chain in
Exercise 1.
b.At some time late in the day, what fraction of the listeners
will be listening to the news?
12. Refer to Exercise 2. Which food will the animal prefer after
many trials?
13. a.Find the steady-state vector for the Markov chain in
Exercise 3.
b.What is the probability that after many days a speciÔ¨Åc
student is ill? Does it matter if that person is ill today?
14. Refer to Exercise 4. In the long run, how likely is it for the
weather in Columbus to be good on a given day?
15. [M] The Demographic Research Unit of the California State
Department of Finance supplied data for the following mi-
gration matrix, which describes the movement of the UnitedStates population during 2012. In 2012, about 12.5% of the
total population lived in California. What percentage of the
total population would eventually live in California if the
listed migration probabilities were to remain constant over
many years?
From:
CA Rest of U.S. To::9871
:0129:0027
:9973California
Rest of U.S.
16. [M] In Detroit, Hertz Rent A Car has a Ô¨Çeet of about
2000 cars. The pattern of rental and return locations is given
by the fractions in the table below. On a typical day, about
how many cars will be rented or ready to rent from the
downtown location?
Cars Rented from:
City Down- Metro
Airport town Airport Returned to:2
4:90
:01
:09:01
:90
:09:09
:01
:903
5City Airport
Downtown
Metro Airport
17. LetPbe an nnstochastic matrix. The following argument
shows that the equation PxDxhas a nontrivial solution. (In
fact, a steady-state solution exists with nonnegative entries. A
proof is given in some advanced texts.) Justify each assertion
below. (Mention a theorem when appropriate.)
a.If all the other rows of P Iare added to the bottom
row, the result is a row of zeros.
b.The rows of P Iare linearly dependent.
c.The dimension of the row space of P Iis less than n.
d.P Ihas a nontrivial null space.
18. Show that every 22stochastic matrix has at least one
steady-state vector. Any such matrix can be written in the
form PD1  
 1  
, where andare constants
between 0 and 1. (There are two linearly independent steady-
state vectors if DD0. Otherwise, there is only one.)
19. LetSbe the 1nrow matrix with a 1 in each column,
SD¬å1 1 1¬ç
a.Explain why a vector xinRnis a probability vector if and
only if its entries are nonnegative and SxD1. (A11
matrix such as the product Sxis usually written without
the matrix bracket symbols.)
b.LetPbe an nnstochastic matrix. Explain why
SPDS.
c.LetPbe an nnstochastic matrix, and let xbe a
probability vector. Show that Pxis also a probability
vector.
20. Use Exercise 19 to show that if Pis an nnstochastic
matrix, then so is P2.
SECOND REVISED PAGES


--- Page 281 ---
264 CHAPTER 4 Vector Spaces
21. [M] Examine powers of a regular stochastic matrix.
a.Compute PkforkD2; 3; 4; 5 , when
PD2
664:3355 :3682 :3067 :0389
:2663 :2723 :3277 :5451
:1935 :1502 :1589 :2395
:2047 :2093 :2067 :17653
775
Display calculations to four decimal places. What hap-
pens to the columns of Pkaskincreases? Compute the
steady-state vector for P.
b.Compute QkforkD10; 20; : : : ; 80 , when
QD2
4:97 :05 :10
0 :90 :05
:03 :05 :853
5
(Stability for Qkto four decimal places may require
kD116or more.) Compute the steady-state vector for Q.Conjecture what might be true for any regular stochastic
matrix.
c.Use Theorem 18 to explain what you found in parts (a)
and (b).
22. [M] Compare two methods for Ô¨Ånding the steady-state vector
qof a regular stochastic matrix P: (1) computing qas in
Example 5, or (2) computing Pkfor some large value of k
and using one of the columns of Pkas an approximation for
q. [The Study Guide describes a program nulbasis that almost
automates method (1).]
Experiment with the largest random stochastic matrices
your matrix program will allow, and use kD100or some
other large value. For each method, describe the time you
need to enter the keystrokes and run your program. (Some
versions of MATLAB have commands flops andtic
: : :toc that record the number of Ô¨Çoating point operations
and the total elapsed time MATLAB uses.) Contrast the
advantages of each method, and state which you prefer.
SOLUTIONS TO PRACTICE PROBLEMS
1.a.Since 5% of the city residents will move to the suburbs within one year, there
is a 5% chance of choosing such a person. Without further knowledge about the
person, we say that there is a 5% chance the person will move to the suburbs.
This fact is contained in the second entry of the state vector x1, where
x1DMx0D:95 :03
:05 :971
0
D:95
:05
b.The likelihood that the person will be living in the suburbs after two years is
9.6%, because
x2DMx1D:95 :03
:05 :97:95
:05
D:904
:096
2.The steady-state vector satisÔ¨Åes PxDx. Since
PqD:6 :2
:4 :8:3
:7
D:32
:68
¬§q
we conclude that qisnotthe steady-state vector for P.
3.Min Example 1 is a regular stochastic matrix because its entries are all strictly
positive. So we may use Theorem 18. We already know the steady-state vector from
Example 4. Thus the population distribution vectors xkconverge to
qD:375
:625
Eventually 62.5% of the population will live in the suburbs.
WEB
CHAPTER 4 SUPPLEMENTARY EXERCISES
1.Mark each statement True or False. Justify each answer.
(If true, cite appropriate facts or theorems. If false, explain
why or give a counterexample that shows why the statement
is not true in every case.) In parts (a)‚Äì(f), v1; : : : ; vparevectors in a nonzero Ô¨Ånite-dimensional vector space V, and
SD fv1; : : : ; vpg.
a.The set of all linear combinations of v1; : : : ; vpis a vector
space.
SECOND REVISED PAGES


--- Page 282 ---
Chapter 4 Supplementary Exercises 265
b.Iffv1; : : : ; vp 1gspans V, then Sspans V.
c.Iffv1; : : : ; vp 1gis linearly independent, then so is S.
d.IfSis linearly independent, then Sis a basis for V.
e.If Span SDV, then some subset of Sis a basis for V.
f.If dim VDpand Span SDV, then Scannot be linearly
dependent.
g.A plane in R3is a two-dimensional subspace.
h.The nonpivot columns of a matrix are always linearly
dependent.
i.Row operations on a matrix Acan change the linear
dependence relations among the rows of A.
j.Row operations on a matrix can change the null space.
k.The rank of a matrix equals the number of nonzero rows.
l.If anmnmatrix Ais row equivalent to an echelon ma-
trixUand if Uhasknonzero rows, then the dimension
of the solution space of AxD0ism k.
m.IfBis obtained from a matrix Aby several elementary
row operations, then rank BDrankA.
n.The nonzero rows of a matrix Aform a basis for Row A.
o.If matrices AandBhave the same reduced echelon form,
then Row ADRowB.
p.IfHis a subspace of R3, then there is a 33matrix A
such that HDColA.
q.IfAismnand rank ADm, then the linear transfor-
mation x7!Axis one-to-one.
r.IfAismnand the linear transformation x7!Axis
onto, then rank ADm.
s.A change-of-coordinates matrix is always invertible.
t.IfBD fb1; : : : ; bngandCD fc1; : : : ; cngare bases for a
vector space V, then the jth column of the change-of-
coordinates matrix P
C Bis the coordinate vector ¬åcj¬çB.
2.Find a basis for the set of all vectors of the form2
664a 2bC5c
2aC5b 8c
 a 4bC7c
3aCbCc3
775:(Be careful.)
3.Let u1D2
4 2
4
 63
5,u2D2
41
2
 53
5,bD2
4b1
b2
b33
5, and
WDSpanfu1;u2g. Find an implicit description of W; that
is, Ô¨Ånd a set of one or more homogeneous equations that
characterize the points of W. [Hint: When is binW ¬ã¬ç
4.Explain what is wrong with the following discussion: Let
f.t/D3Ctandg.t/D3tCt2, and note that g.t/Dtf.t/.
Thenff;ggis linearly dependent because gis a multiple of f.
5.Consider the polynomials p1.t/D1Ct,p2.t/D1 t,
p3.t/D4,p4.t/DtCt2, and p5.t/D1C2tCt2, and
letHbe the subspace of P5spanned by the set
SD fp1;p2;p3;p4;p5g. Use the method described in theproof of the Spanning Set Theorem (Section 4.3) to produce
a basis for H. (Explain how to select appropriate members
ofS.)
6.Suppose p1,p2,p3, and p4are speciÔ¨Åc polynomials that span
a two-dimensional subspace HofP5. Describe how one can
Ô¨Ånd a basis for Hby examining the four polynomials and
making almost no computations.
7.What would you have to know about the solution set of a
homogeneous system of 18 linear equations in 20 variables
in order to know that every associated nonhomogeneous
equation has a solution? Discuss.
8.LetHbe an n-dimensional subspace of an n-dimensional
vector space V. Explain why HDV.
9.LetTWRn!Rmbe a linear transformation.
a.What is the dimension of the range of TifTis a one-to-
one mapping? Explain.
b.What is the dimension of the kernel of T(see Section 4.2)
ifTmapsRnontoRm? Explain.
10. LetSbe a maximal linearly independent subset of a vector
space V. That is, Shas the property that if a vector not in S
is adjoined to S, then the new set will no longer be linearly
independent. Prove that Smust be a basis for V. [Hint: What
ifSwere linearly independent but not a basis of V ¬ã¬ç
11.LetSbe a Ô¨Ånite minimal spanning set of a vector space V.
That is, Shas the property that if a vector is removed from
S, then the new set will no longer span V. Prove that Smust
be a basis for V.
Exercises 12‚Äì17 develop properties of rank that are sometimes
needed in applications. Assume the matrix Aismn.
12. Show from parts (a) and (b) that rank ABcannot exceed the
rank of Aor the rank of B. (In general, the rank of a product of
matrices cannot exceed the rank of any factor in the product.)
a.Show that if Bisnp, then rank ABrankA. [Hint:
Explain why every vector in the column space of ABis in
the column space of A.]
b.Show that if Bisnp, then rank ABrankB. [Hint:
Use part (a) to study rank .AB/T.]
13. Show that if Pis an invertible mmmatrix, then
rankPADrankA. [Hint: Apply Exercise 12 to PA and
P 1.PA/ .]
14. Show that if Qis invertible, then rank AQDrankA. [Hint:
Use Exercise 13 to study rank .AQ/T.]
15. LetAbe an mnmatrix, and let Bbe an npmatrix
such that ABD0. Show that rank ACrankBn. [Hint:
One of the four subspaces Nul A, Col A, Nul B, and Col B
is contained in one of the other three subspaces.]
16. IfAis an mnmatrix of rank r, then a rank factorization
ofAis an equation of the form ADCR, where Cis an
mrmatrix of rank randRis an rnmatrix of rank r.
Such a factorization always exists (Exercise 38 in Section
SECOND REVISED PAGES


--- Page 283 ---
266 CHAPTER 4 Vector Spaces
4.6). Given any two mnmatrices AandB, use rank
factorizations of AandBto prove that
rank.ACB/rankACrankB
[Hint: Write ACBas the product of two partitioned matri-
ces.]
17. Asubmatrix of a matrix Ais any matrix that results from
deleting some (or no) rows and/or columns of A. It can be
shown that Ahas rank rif and only if Acontains an invertible
rrsubmatrix and no larger square submatrix is invertible.
Demonstrate part of this statement by explaining (a) why
anmnmatrix Aof rank rhas an mrsubmatrix A1of
rankr, and (b) why A1has an invertible rrsubmatrix A2.
The concept of rank plays an important role in the design of
engineering control systems, such as the space shuttle system
mentioned in this chapter‚Äôs introductory example. A state-space
model of a control system includes a difference equation of the
form
xkC1DAxkCBukforkD0; 1; : : : (1)
where Aisnn,Bisnm,fxkgis a sequence of ‚Äústate vectors‚Äù
inRnthat describe the state of the system at discrete times, and
fukgis acontrol , orinput , sequence. The pair .A; B/ is said to be
controllable if
rank¬åBAB A2BAn 1B¬çDn (2)
The matrix that appears in (2) is called the controllability matrix
for the system. If .A; B/ is controllable, then the system can be
controlled, or driven from the state 0to any speciÔ¨Åed state v(in
Rn) in at most nsteps, simply by choosing an appropriate control
sequence in Rm. This fact is illustrated in Exercise 18 for nD4andmD2. For a further discussion of controllability, see this
text‚Äôs web site (Case Study for Chapter 4).
WEB
18. Suppose Ais a44matrix and Bis a42matrix, and let
u0; : : : ; u3represent a sequence of input vectors in R2.
a.Setx0D0, compute x1; : : : ; x4from equation (1), and
write a formula for x4involving the controllability matrix
Mappearing in equation (2). ( Note: The matrix Mis
constructed as a partitioned matrix. Its overall size here
is48.)
b.Suppose .A; B/ is controllable and vis any vector in R4.
Explain why there exists a control sequence u0; : : : ; u3in
R2such that x4Dv.
Determine if the matrix pairs in Exercises 19‚Äì22 are controllable.
19.AD2
4:9 1 0
0 :9 0
0 0 :53
5,BD2
40
1
13
5
20.AD2
4:8 :3 0
:2 :5 1
0 0  :53
5,BD2
41
1
03
5
21. [M]AD2
6640 1 0 0
0 0 1 0
0 0 0 1
 2 4:2 4:8 3:63
775,BD2
6641
0
0
 13
775
22. [M]AD2
6640 1 0 0
0 0 1 0
0 0 0 1
 1 13 12:2  1:53
775,BD2
6641
0
0
 13
775
SECOND REVISED PAGES


--- Page 284 ---
5Eigenvalues and
Eigenvectors
INTRODUCTORY EXAMPLE
Dynamical Systems and Spotted Owls
In 1990, the northern spotted owl became the center of
a nationwide controversy over the use and misuse of the
majestic forests in the PaciÔ¨Åc Northwest. Environmental-
ists convinced the federal government that the owl was
threatened with extinction if logging continued in the old-
growth forests (with trees more than 200 years old), where
the owls prefer to live. The timber industry, anticipating
the loss of 30,000 to 100,000 jobs as a result of new
government restrictions on logging, argued that the owl
should not be classiÔ¨Åed as a ‚Äúthreatened species‚Äù and cited
a number of published scientiÔ¨Åc reports to support its case.1
Caught in the crossÔ¨Åre of the two lobbying groups,
mathematical ecologists intensiÔ¨Åed their drive to under-
stand the population dynamics of the spotted owl. The life
cycle of a spotted owl divides naturally into three stages:
juvenile (up to 1 year old), subadult (1 to 2 years), and
adult (older than 2 years). The owls mate for life during
the subadult and adult stages, begin to breed as adults,
and live for up to 20 years. Each owl pair requires about
1000 hectares (4 square miles) for its own home territory.
A critical time in the life cycle is when the juveniles leave
the nest. To survive and become a subadult, a juvenile must
successfully Ô¨Ånd a new home range (and usually a mate).
1‚ÄúThe Great Spotted Owl War,‚Äù Reader‚Äô s Digest , November 1992,
pp. 91‚Äì95.A Ô¨Årst step in studying the population dynamics is to
model the population at yearly intervals, at times denoted
bykD0; 1; 2; : : : : Usually, one assumes that there is a 1:1
ratio of males to females in each life stage and counts only
the females. The population at year kcan be described
by a vector xkD.jk; sk; ak/, where jk,sk, and akare the
numbers of females in the juvenile, subadult, and adult
stages, respectively.
Using actual Ô¨Åeld data from demographic studies,
R. Lamberson and co-workers considered the following
stage-matrix model :2
2
4jkC1
skC1
akC13
5D2
40 0 :33
:18 0 0
0 :71 :943
52
4jk
sk
ak3
5
Here the number of new juvenile females in year kC1
is .33 times the number of adult females in year k(based
on the average birth rate per owl pair). Also, 18% of the
juveniles survive to become subadults, and 71% of the
subadults and 94% of the adults survive to be counted as
adults.
The stage-matrix model is a difference equation of the
form xkC1DAxk. Such an equation is often called a
2R. H. Lamberson, R. McKelvey, B. R. Noon, and C. V oss, ‚ÄúA Dynamic
Analysis of the Viability of the Northern Spotted Owl in a Fragmented
Forest Environment,‚Äù Conservation Biology 6(1992), 505‚Äì512. Also, a
private communication from Professor Lamberson, 1993.
SECOND REVISED PAGES
267

--- Page 285 ---
268 CHAPTER 5 Eigenvalues and Eigenvectors
dynamical system (or a discrete linear dynamical
system ) because it describes the changes in a system as
time passes.
The 18% juvenile survival rate in the Lamberson stage
matrix is the entry affected most by the amount of old-
growth forest available. Actually, 60% of the juveniles
normally survive to leave the nest, but in the Willow
Creek region of California studied by Lamberson and his
colleagues, only 30% of the juveniles that left the nest were
able to Ô¨Ånd new home ranges. The rest perished during the
search process.A signiÔ¨Åcant reason for the failure of owls to Ô¨Ånd new
home ranges is the increasing fragmentation of old-growth
timber stands due to clear-cutting of scattered areas on
the old-growth land. When an owl leaves the protective
canopy of the forest and crosses a clear-cut area, the risk of
attack by predators increases dramatically. Section 5.6 will
show that the model described above predicts the eventual
demise of the spotted owl, but that if 50% of the juveniles
who survive to leave the nest also Ô¨Ånd new home ranges,
then the owl population will thrive.
WEB
The goal of this chapter is to dissect the action of a linear transformation x7!Axinto el-
ements that are easily visualized. Except for a brief digression in Section 5.4, all matrices
in the chapter are square. The main applications described here are to discrete dynamical
systems, including the spotted owls discussed above. However, the basic concepts‚Äî
eigenvectors and eigenvalues‚Äîare useful throughout pure and applied mathematics,
and they appear in settings far more general than we consider here. Eigenvalues are also
used to study differential equations and continuous dynamical systems, they provide
critical information in engineering design, and they arise naturally in Ô¨Åelds such as
physics and chemistry.
5.1 EIGENVECTORS AND EIGENVALUES
Although a transformation x7!Axmay move vectors in a variety of directions, it often
happens that there are special vectors on which the action of Ais quite simple.
EXAMPLE 1 LetAD3 2
1 0
,uD 1
1
, and vD2
1
. The images of uand
vunder multiplication by Aare shown in Figure 1. In fact, Avis just 2v. SoAonly
‚Äústretches,‚Äù or dilates, v.
v
x1x2
Av
Auv
u1
1
FIGURE 1 Effects of multiplication by A.
As another example, readers of Section 4.9 will recall that if Ais a stochastic matrix,
then the steady-state vector qforAsatisÔ¨Åes the equation AxDx. That is, AqD1q.
SECOND REVISED PAGES


--- Page 286 ---
5.1 Eigenvectors and Eigenvalues 269
This section studies equations such as
AxD2xorAxD  4x
where special vectors are transformed by Ainto scalar multiples of themselves.
D E F I N I T I O N Aneigenvector of an nnmatrix Ais a nonzero vector xsuch that AxDx
for some scalar . A scalar is called an eigenvalue ofAif there is a nontrivial
solution xofAxDx; such an xis called an eigenvector corresponding to .1
It is easy to determine if a given vector is an eigenvector of a matrix. It is also easy
to decide if a speciÔ¨Åed scalar is an eigenvalue.
EXAMPLE 2 LetAD1 6
5 2
,uD6
 5
, and vD3
 2
. Are uandveigen-
Au
Av
v
u20
‚Äì30 30
‚Äì1 0
‚Äì2 0x1x2
AuD  4u, butAv¬§v.vectors of A?
SOLUTION
AuD1 6
5 26
 5
D 24
20
D  46
 5
D  4u
AvD1 6
5 23
 2
D 9
11
¬§3
 2
Thus uis an eigenvector corresponding to an eigenvalue . 4/, butvis not an eigenvector
ofA, because Avis not a multiple of v.
EXAMPLE 3 Show that 7 is an eigenvalue of matrix Ain Example 2, and Ô¨Ånd the
corresponding eigenvectors.
SOLUTION The scalar 7 is an eigenvalue of Aif and only if the equation
AxD7x (1)
has a nontrivial solution. But (1) is equivalent to Ax 7xD0, or
.A 7I/xD0 (2)
To solve this homogeneous equation, form the matrix
A 7ID1 6
5 2
 7 0
0 7
D 6 6
5 5
The columns of A 7Iare obviously linearly dependent, so (2) has nontrivial solu-
tions. Thus 7 isan eigenvalue of A. To Ô¨Ånd the corresponding eigenvectors, use row
operations: 6 6 0
5 5 0
1 1 0
0 0 0
The general solution has the form x21
1
. Each vector of this form with x2¬§0is an
eigenvector corresponding to D7.
1Note that an eigenvector must be nonzero , by deÔ¨Ånition, but an eigenvalue may be zero. The case in which
the number 0 is an eigenvalue is discussed after Example 5.
SECOND REVISED PAGES


--- Page 287 ---
270 CHAPTER 5 Eigenvalues and Eigenvectors
Warning: Although row reduction was used in Example 3 to Ô¨Ånd eigen vectors , it
cannot be used to Ô¨Ånd eigen values . An echelon form of a matrix Ausually does not
display the eigenvalues of A.
The equivalence of equations (1) and (2) obviously holds for any in place of
D7. Thus is an eigenvalue of an nnmatrix Aif and only if the equation
.A I/xD0 (3)
has a nontrivial solution. The set of allsolutions of (3) is just the null space of the matrix
A I. So this set is a subspace ofRnand is called the eigenspace ofAcorresponding
to. The eigenspace consists of the zero vector and all the eigenvectors corresponding
to.
Example 3 shows that for matrix Ain Example 2, the eigenspace corresponding to
D7consists of allmultiples of .1; 1/ , which is the line through .1; 1/ and the origin.
From Example 2, you can check that the eigenspace corresponding to D  4is the
line through .6; 5/. These eigenspaces are shown in Figure 2, along with eigenvectors
.1; 1/ and.3=2; 5=4/ and the geometric action of the transformation x7!Axon each
eigenspace.
x1x2
Eigenspace
for Œª = 7Multiplication
by 7
Eigenspace
for Œª = ‚Äì4Multiplication
by ‚Äì42
2
(6, ‚Äì5)
FIGURE 2 Eigenspaces for D  4andD7.
EXAMPLE 4 LetAD2
44 1 6
2 1 6
2 1 83
5. An eigenvalue of Ais 2. Find a basis for
the corresponding eigenspace.
SOLUTION Form
A 2ID2
44 1 6
2 1 6
2 1 83
5 2
42 0 0
0 2 0
0 0 23
5D2
42 1 6
2 1 6
2 1 63
5
and row reduce the augmented matrix for .A 2I/xD0:
2
42 1 6 0
2 1 6 0
2 1 6 03
52
42 1 6 0
0 0 0 0
0 0 0 03
5
SECOND REVISED PAGES


--- Page 288 ---
5.1 Eigenvectors and Eigenvalues 271
At this point, it is clear that 2 is indeed an eigenvalue of Abecause the equation
.A 2I/xD0has free variables. The general solution is
2
4x1
x2
x33
5Dx22
41=2
1
03
5Cx32
4 3
0
13
5; x 2andx3free
The eigenspace, shown in Figure 3, is a two-dimensional subspace of R3. A basis is8
<
:2
41
2
03
5;2
4 3
0
13
59
=
;
Eigenspace for /H9261 /H11005 2
Multiplication by A
x2
x1x3
Eigenspace for /H9261 /H11005 2x2
x1x3
FIGURE 3 Aacts as a dilation on the eigenspace.
N U M E R I C A L N O T E
Example 4 shows a good method for manual computation of eigenvectors in
simple cases when an eigenvalue is known. Using a matrix program and row
reduction to Ô¨Ånd an eigenspace (for a speciÔ¨Åed eigenvalue) usually works, too,
but this is not entirely reliable. Roundoff error can lead occasionally to a reduced
echelon form with the wrong number of pivots. The best computer programs
compute approximations for eigenvalues and eigenvectors simultaneously, to
any desired degree of accuracy, for matrices that are not too large. The size
of matrices that can be analyzed increases each year as computing power and
software improve.
The following theorem describes one of the few special cases in which eigenvalues
can be found precisely. Calculation of eigenvalues will also be discussed in Section 5.2.
T H E O R E M 1 The eigenvalues of a triangular matrix are the entries on its main diagonal.
PROOF For simplicity, consider the 33case. If Ais upper triangular, then A I
has the form
A ID2
4a11 a12 a13
0 a 22 a23
0 0 a 333
5 2
4 0 0
0  0
0 0 3
5
D2
4a11  a 12 a13
0 a 22  a 23
0 0 a 33 3
5
SECOND REVISED PAGES


--- Page 289 ---
272 CHAPTER 5 Eigenvalues and Eigenvectors
The scalar is an eigenvalue of Aif and only if the equation .A I/xD0has a
nontrivial solution, that is, if and only if the equation has a free variable. Because of the
zero entries in A I, it is easy to see that .A I/xD0has a free variable if and
only if at least one of the entries on the diagonal of A Iis zero. This happens if and
only if equals one of the entries a11,a22,a33inA. For the case in which Ais lower
triangular, see Exercise 28.
EXAMPLE 5 LetAD2
43 6  8
0 0 6
0 0 23
5andBD2
44 0 0
 2 1 0
5 3 43
5. The eigenval-
ues of Aare 3, 0, and 2. The eigenvalues of Bare 4 and 1.
What does it mean for a matrix Ato have an eigenvalue of 0, such as in Example 5?
This happens if and only if the equation
AxD0x (4)
has a nontrivial solution. But (4) is equivalent to AxD0, which has a nontrivial solution
if and only if Ais not invertible. Thus 0 is an eigenvalue of Aif and only if Ais not
invertible . This fact will be added to the Invertible Matrix Theorem in Section 5.2.
The following important theorem will be needed later. Its proof illustrates a typical
calculation with eigenvectors. One way to prove the statement ‚ÄúIf PthenQ‚Äù is to show
thatPand the negation of Qleads to a contradiction. This strategy is used in the proof
of the theorem.
T H E O R E M 2 Ifv1; : : : ; vrare eigenvectors that correspond to distinct eigenvalues 1; : : : ;  r
of an nnmatrix A, then the set fv1; : : : ; vrgis linearly independent.
PROOF Suppose fv1; : : : ; vrgis linearly dependent. Since v1is nonzero, Theorem 7 in
Section 1.7 says that one of the vectors in the set is a linear combination of the preceding
vectors. Let pbe the least index such that vpC1is a linear combination of the preceding
(linearly independent) vectors. Then there exist scalars c1; : : : ; c psuch that
c1v1C  C cpvpDvpC1 (5)
Multiplying both sides of (5) by Aand using the fact that AvkDkvkfor each k, we
obtain
c1Av1C  C cpAvpDAvpC1
c11v1C  C cppvpDpC1vpC1 (6)
Multiplying both sides of (5) by pC1and subtracting the result from (6), we have
c1.1 pC1/v1C  C cp.p pC1/vpD0 (7)
Sincefv1; : : : ; vpgis linearly independent, the weights in (7) are all zero. But none of
the factors i pC1are zero, because the eigenvalues are distinct. Hence ciD0for
iD1; : : : ; p . But then (5) says that vpC1D0, which is impossible. Hence fv1; : : : ; vrg
cannot be linearly dependent and therefore must be linearly independent.
SECOND REVISED PAGES


--- Page 290 ---
5.1 Eigenvectors and Eigenvalues 273
Eigenvectors and Difference Equations
This section concludes by showing how to construct solutions of the Ô¨Årst-order differ-
ence equation discussed in the chapter introductory example:
xkC1DAxk.kD0; 1; 2; : : :/ (8)
IfAis an nnmatrix, then (8) is a recursive description of a sequence fxkginRn.
Asolution of (8) is an explicit description of fxkgwhose formula for each xkdoes not
depend directly on Aor on the preceding terms in the sequence other than the initial
term x0.
The simplest way to build a solution of (8) is to take an eigenvector x0and its
corresponding eigenvalue and let
xkDkx0.kD1; 2; : : :/ (9)
This sequence is a solution because
AxkDA.kx0/Dk.Ax0/Dk.x0/DkC1x0DxkC1
Linear combinations of solutions in the form of equation (9) are solutions, too! See
Exercise 33.
PRACTICE PROBLEMS
1.Is 5 an eigenvalue of AD2
46 3 1
3 0 5
2 2 63
5?
2.Ifxis an eigenvector of Acorresponding to , what is A3x?
3.Suppose that b1andb2are eigenvectors corresponding to distinct eigenvalues 1and
2, respectively, and suppose that b3andb4are linearly independent eigenvectors
corresponding to a third distinct eigenvalue 3. Does it necessarily follow that
fb1;b2;b3;b4gis a linearly independent set? [ Hint: Consider the equation c1b1C
c2b2C.c3b3Cc4b4/D0.]
4.IfAis an nnmatrix and is an eigenvalue of A, show that 2is an eigenvalue
of2A.
5.1 EXERCISES
1.IsD2an eigenvalue of3 2
3 8
? Why or why not?
2.IsD  2an eigenvalue of7 3
3 1
? Why or why not?
3.Is1
4
an eigenvector of 3 1
 3 8
? If so, Ô¨Ånd the eigen-
value.
4.Is
 1Cp
2
1
an eigenvector of2 1
1 4
? If so, Ô¨Ånd the
eigenvalue.
5.Is2
44
 3
13
5an eigenvector of2
43 7 9
 4 5 1
2 4 43
5? If so, Ô¨Ånd
the eigenvalue.6.Is2
41
 2
13
5an eigenvector of2
43 6 7
3 3 7
5 6 53
5? If so, Ô¨Ånd the
eigenvalue.
7.IsD4an eigenvalue of2
43 0  1
2 3 1
 3 4 53
5? If so, Ô¨Ånd one
corresponding eigenvector.
8.IsD3an eigenvalue of2
41 2 2
3 2 1
0 1 13
5? If so, Ô¨Ånd one
corresponding eigenvector.
In Exercises 9‚Äì16, Ô¨Ånd a basis for the eigenspace corresponding
to each listed eigenvalue.
SECOND REVISED PAGES


--- Page 291 ---
274 CHAPTER 5 Eigenvalues and Eigenvectors
9.AD5 0
2 1
,D1; 5
10.AD10 9
4 2
,D4
11.AD4 2
 3 9
,D10
12.AD7 4
 3 1
,D1; 5
13.AD2
44 0 1
 2 1 0
 2 0 13
5,D1; 2; 3
14.AD2
41 0  1
1 3 0
4 13 13
5,D  2
15.AD2
44 2 3
 1 1  3
2 4 93
5,D3
16.AD2
6643 0 2 0
1 3 1 0
0 1 1 0
0 0 0 43
775,D4
Find the eigenvalues of the matrices in Exercises 17 and 18.
17.2
40 0 0
0 2 5
0 0  13
5 18.2
44 0 0
0 0 0
1 0  33
5
19. ForAD2
41 2 3
1 2 3
1 2 33
5, Ô¨Ånd one eigenvalue, with no cal-
culation. Justify your answer.
20. Without calculation, Ô¨Ånd one eigenvalue and two linearly
independent eigenvectors of AD2
45 5 5
5 5 5
5 5 53
5. Justify
your answer.
In Exercises 21 and 22, Ais annnmatrix. Mark each statement
True or False. Justify each answer.
21. a.IfAxDxfor some vector x, then is an eigenvalue of
A.
b.A matrix Ais not invertible if and only if 0 is an eigen-
value of A.
c.A number cis an eigenvalue of Aif and only if the
equation .A cI/xD0has a nontrivial solution.d.Finding an eigenvector of Amay be difÔ¨Åcult, but check-
ing whether a given vector is in fact an eigenvector is
easy.
e.To Ô¨Ånd the eigenvalues of A, reduce Ato echelon form.
22. a.IfAxDxfor some scalar , then xis an eigenvector
ofA.
b.Ifv1andv2are linearly independent eigenvectors, then
they correspond to distinct eigenvalues.
c.A steady-state vector for a stochastic matrix is actually an
eigenvector.
d.The eigenvalues of a matrix are on its main diagonal.
e.An eigenspace of Ais a null space of a certain matrix.
23. Explain why a 22matrix can have at most two distinct
eigenvalues. Explain why an nnmatrix can have at most
ndistinct eigenvalues.
24. Construct an example of a 22matrix with only one distinct
eigenvalue.
25. Letbe an eigenvalue of an invertible matrix A. Show that
 1is an eigenvalue of A 1. [Hint: Suppose a nonzero x
satisÔ¨Åes AxDx.]
26. Show that if A2is the zero matrix, then the only eigenvalue
ofAis 0.
27. Show that is an eigenvalue of Aif and only if is an
eigenvalue of AT. [Hint: Find out how A IandAT I
are related.]
28. Use Exercise 27 to complete the proof of Theorem 1 for the
case when Ais lower triangular.
29. Consider an nnmatrix Awith the property that the row
sums all equal the same number s. Show that sis an eigen-
value of A. [Hint: Find an eigenvector.]
30. Consider an nnmatrix Awith the property that the col-
umn sums all equal the same number s. Show that sis an
eigenvalue of A. [Hint: Use Exercises 27 and 29.]
In Exercises 31 and 32, let Abe the matrix of the linear transfor-
mation T. Without writing A, Ô¨Ånd an eigenvalue of Aand describe
the eigenspace.
31.Tis the transformation on R2that reÔ¨Çects points across some
line through the origin.
32.Tis the transformation on R3that rotates points about some
line through the origin.
33. Letuandvbe eigenvectors of a matrix A, with corresponding
eigenvalues and, and let c1andc2be scalars. DeÔ¨Åne
xkDc1kuCc2kv.kD0; 1; 2; : : :/
a.What is xkC1, by deÔ¨Ånition?
b.Compute Axkfrom the formula for xk, and show that
AxkDxkC1. This calculation will prove that the se-
quence fxkgdeÔ¨Åned above satisÔ¨Åes the difference equa-
tionxkC1DAxk.kD0; 1; 2; : : :/ .
SECOND REVISED PAGES


--- Page 292 ---
5.1 Eigenvectors and Eigenvalues 275
34. Describe how you might try to build a solution of a difference
equation xkC1DAxk.kD0; 1; 2; : : :/ if you were given the
initial x0and this vector did not happen to be an eigenvector
ofA. [Hint: How might you relate x0to eigenvectors of A?]
35. Letuandvbe the vectors shown in the Ô¨Ågure, and suppose u
andvare eigenvectors of a 22matrix Athat correspond
to eigenvalues 2 and 3, respectively. Let TWR2!R2be
the linear transformation given by T .x/DAxfor each xin
R2, and let wDuCv. Make a copy of the Ô¨Ågure, and on
the same coordinate system, carefully plot the vectors T .u/,
T .v/, and T .w/.
x1x2
v
u
36. Repeat Exercise 35, assuming uandvare eigenvectors of A
that correspond to eigenvalues  1and 3, respectively.[M] In Exercises 37‚Äì40, use a matrix program to Ô¨Ånd the eigen-
values of the matrix. Then use the method of Example 4 with a
row reduction routine to produce a basis for each eigenspace.
37.2
48 10 5
2 17 2
 9 18 43
5
38.2
6649 4 2 4
 56 32  28 44
 14 14 6  14
42 33 21  453
775
39.2
666644 9 7 8 2
 7 9 0 7 14
5 10 5  5 10
 2 3 7 0 4
 3 13 7 10 113
77775
40.2
66664 4 4 20  8 1
14 12 46 18 2
6 4  18 8 1
11 7  37 17 2
18 12  60 24 53
77775
SOLUTIONS TO PRACTICE PROBLEMS
1.The number 5 is an eigenvalue of Aif and only if the equation .A 5I/xD0has a
nontrivial solution. Form
A 5ID2
46 3 1
3 0 5
2 2 63
5 2
45 0 0
0 5 0
0 0 53
5D2
41 3 1
3 5 5
2 2 13
5
and row reduce the augmented matrix:
2
41 3 1 0
3 5 5 0
2 2 1 03
52
41 3 1 0
0 4 2 0
0 8  1 03
52
41 3 1 0
0 4 2 0
0 0  5 03
5
At this point, it is clear that the homogeneous system has no free variables. Thus
A 5Iis an invertible matrix, which means that 5 is notan eigenvalue of A.
2.Ifxis an eigenvector of Acorresponding to , then AxDxand so
A2xDA.x/DAxD2x
Again, A3xDA.A2x/DA.2x/D2AxD3x. The general pattern, AkxDkx,
is proved by induction.
3.Yes. Suppose c1b1Cc2b2C.c3b3Cc4b4/D0. Since any linear combination of
eigenvectors corresponding to the same eigenvalue is in the eigenspace for that
eigenvalue, c3b3Cc4b4is either 0or an eigenvector for 3. Ifc3b3Cc4b4were
an eigenvector for 3, then by Theorem 2, fb1;b2; c3b3Cc4b4gwould be a linearly
independent set, which would force c1Dc2D0andc3b3Cc4b4D0, contradicting
thatc3b3Cc4b4is an eigenvector. Thus c3b3Cc4b4must be 0, implying that
c1b1Cc2b2D0also. By Theorem 2, fb1;b2gis a linearly independent set so
c1Dc2D0. Moreover, fb3;b4gis a linearly independent set so c3Dc4D0. Since
all of the coefÔ¨Åcients c1,c2,c3, and c4must be zero, it follows that fb1,b2,b3,b4g
is a linearly independent set.
SECOND REVISED PAGES


--- Page 293 ---
276 CHAPTER 5 Eigenvalues and Eigenvectors
4.Since is an eigenvalue of A, there is a nonzero vector xinRnsuch that AxDx.
Multiplying both sides of this equation by 2 results in the equation 2.Ax/D2.x/.
Thus .2A/ xD.2/xand hence 2is an eigenvalue of 2A.
5.2 THE CHARACTERISTIC EQUATION
Useful information about the eigenvalues of a square matrix Ais encoded in a special
scalar equation called the characteristic equation of A. A simple example will lead to
the general case.
EXAMPLE 1 Find the eigenvalues of AD2 3
3 6
.
SOLUTION We must Ô¨Ånd all scalars such that the matrix equation
.A I/xD0
has a nontrivial solution. By the Invertible Matrix Theorem in Section 2.3, this problem
is equivalent to Ô¨Ånding all such that the matrix A Iisnotinvertible, where
A ID2 3
3 6
  0
0 
D2  3
3 6 
By Theorem 4 in Section 2.2, this matrix fails to be invertible precisely when its
determinant is zero. So the eigenvalues of Aare the solutions of the equation
det.A I/Ddet2  3
3 6 
D0
Recall that
deta b
c d
Dad bc
So
det.A I/D.2 /. 6 / .3/.3/
D  12C6 2C2 9
D2C4 21
D. 3/.C7/
If det .A I/D0, then D3orD  7. So the eigenvalues of Aare 3 and  7.
The determinant in Example 1 transformed the matrix equation .A I/xD0,
which involves twounknowns .andx/, into the scalar equation 2C4 21D0,
which involves only one unknown. The same idea works for nnmatrices. However,
before turning to larger matrices, we summarize the properties of determinants needed
to study eigenvalues.
Determinants
LetAbe an nnmatrix, let Ube any echelon form obtained from Aby row
replacements and row interchanges (without scaling), and let rbe the number of such
row interchanges. Then the determinant ofA, written as det A, is. 1/rtimes the
product of the diagonal entries u11; : : : ; u nninU. IfAis invertible, then u11; : : : ; u nn
SECOND REVISED PAGES


--- Page 294 ---
5.2 The Characteristic Equation 277
are all pivots (because AInand the uiihave not been scaled to 1‚Äôs). Otherwise, at
leastunnis zero, and the product u11unnis zero. Thus1
detAD8
¬à<
¬à:. 1/r 
product of
pivots in U!
;when Ais invertible
0; when Ais not invertible(1)
EXAMPLE 2 Compute det AforAD2
41 5 0
2 4  1
0 2 03
5.
SOLUTION The following row reduction uses one row interchange:
A2
41 5 0
0 6 1
0 2 03
52
41 5 0
0 2 0
0 6 13
52
41 5 0
0 2 0
0 0  13
5DU1
So det Aequals . 1/1.1/. 2/. 1/D  2. The following alternative row reduction
avoids the row interchange and produces a different echelon form. The last step adds
 1=3times row 2 to row 3:
A2
41 5 0
0 6 1
0 2 03
52
41 5 0
0 6 1
0 0 1=33
5DU2
This time det Ais. 1/0.1/. 6/.1=3/ D  2, the same as before.
Formula (1) for the determinant shows that Ais invertible if and only if det Ais
nonzero. This fact, and the characterization of invertibility found in Section 5.1, can be
added to the Invertible Matrix Theorem.
T H E O R E M The Invertible Matrix Theorem (continued)
LetAbe an nnmatrix. Then Ais invertible if and only if:
s.The number 0 is notan eigenvalue of A.
t.The determinant of Aisnotzero.
When Ais a33matrix, jdetAjturns out to be the volume of the parallelepiped
determined by the columns a1,a2, and a3ofA, as in Figure 1. (See Section 3.3 for
details.) This volume is nonzero if and only if the vectors a1,a2, and a3are linearly
independent, in which case the matrix Ais invertible. (If the vectors are nonzero and
linearly dependent, they lie in a plane or along a line.)
The next theorem lists facts needed from Sections 3.1 and 3.2. Part (a) is included
here for convenient reference.
x1x2x3
a2
a3
a1 FIGURE 1
1Formula (1) was derived in Section 3.2. Readers who have not studied Chapter 3 may use this formula as
the deÔ¨Ånition of det A. It is a remarkable and nontrivial fact that any echelon form Uobtained from A
without scaling gives the same value for det A.
SECOND REVISED PAGES


--- Page 295 ---
278 CHAPTER 5 Eigenvalues and Eigenvectors
T H E O R E M 3 Properties of Determinants
LetAandBbennmatrices.
a.Ais invertible if and only if det A¬§0.
b.detABD.detA/.detB/.
c.detATDdetA.
d.IfAis triangular, then det Ais the product of the entries on the main diagonal
ofA.
e.A row replacement operation on Adoes not change the determinant. A row
interchange changes the sign of the determinant. A row scaling also scales the
determinant by the same scalar factor.
The Characteristic Equation
Theorem 3(a) shows how to determine when a matrix of the form A Iisnot
invertible. The scalar equation det .A I/D0is called the characteristic equation
ofA, and the argument in Example 1 justiÔ¨Åes the following fact.
A scalar is an eigenvalue of an nnmatrix Aif and only if satisÔ¨Åes the
characteristic equation
det.A I/D0
EXAMPLE 3 Find the characteristic equation of
AD2
6645 2 6  1
0 3  8 0
0 0 5 4
0 0 0 13
775
SOLUTION Form A I, and use Theorem 3(d):
det.A I/Ddet2
6645  2 6  1
0 3   8 0
0 0 5   4
0 0 0 1  3
775
D.5 /.3 /.5 /.1 /
The characteristic equation is
.5 /2.3 /.1 /D0
or
. 5/2. 3/. 1/D0
Expanding the product, we can also write
4 143C682 130C75D0
In Examples 1 and 3, det .A I/is a polynomial in . It can be shown that if Ais
annnmatrix, then det .A I/is a polynomial of degree ncalled the characteristic
polynomial ofA.
The eigenvalue 5 in Example 3 is said to have multiplicity 2 because . 5/
occurs two times as a factor of the characteristic polynomial. In general, the ( algebraic )
multiplicity of an eigenvalue is its multiplicity as a root of the characteristic equation.
SECOND REVISED PAGES


--- Page 296 ---
5.2 The Characteristic Equation 279
EXAMPLE 4 The characteristic polynomial of a 66matrix is 6 45 124.
Find the eigenvalues and their multiplicities.
SOLUTION Factor the polynomial
6 45 124D4.2 4 12/D4. 6/.C2/
The eigenvalues are 0 (multiplicity 4), 6 (multiplicity 1), and  2(multiplicity 1).
We could also list the eigenvalues in Example 4 as 0; 0; 0; 0; 6 , and 2, so that the
eigenvalues are repeated according to their multiplicities.
Because the characteristic equation for an nnmatrix involves an nth-degree
polynomial, the equation has exactly nroots, counting multiplicities, provided complex
roots are allowed. Such complex roots, called complex eigenvalues , will be discussed
in Section 5.5. Until then, we consider only real eigenvalues, and scalars will continue
to be real numbers.
The characteristic equation is important for theoretical purposes. In practical work,
however, eigenvalues of any matrix larger than 22should be found by a computer,
unless the matrix is triangular or has other special properties. Although a 33charac-
teristic polynomial is easy to compute by hand, factoring it can be difÔ¨Åcult (unless the
matrix is carefully chosen). See the Numerical Notes at the end of this section.
SGFactoring a
Polynomial 5‚Äì8
Similarity
The next theorem illustrates one use of the characteristic polynomial, and it provides
the foundation for several iterative methods that approximate eigenvalues. If Aand
Barennmatrices, then Ais similar to Bif there is an invertible matrix P
such that P 1APDB, or, equivalently, ADPBP 1. Writing QforP 1, we have
Q 1BQDA. SoBis also similar to A, and we say simply that AandBare similar .
Changing AintoP 1APis called a similarity transformation .
T H E O R E M 4 Ifnnmatrices AandBare similar, then they have the same characteristic
polynomial and hence the same eigenvalues (with the same multiplicities).
PROOF IfBDP 1AP, then
B IDP 1AP P 1PDP 1.AP P /DP 1.A I/P
Using the multiplicative property (b) in Theorem 3, we compute
det.B I/Ddet¬åP 1.A I/P ¬ç
Ddet.P 1/det.A I/det.P / (2)
Since det .P 1/det.P /Ddet.P 1P /DdetID1, we see from equation (2) that
det.B I/Ddet.A I/.
WARNINGS:
1.The matrices 2 1
0 2
and2 0
0 2
are not similar even though they have the same eigenvalues.
2.Similarity is not the same as row equivalence. (If Ais row equivalent to B, then
BDEAfor some invertible matrix E.) Row operations on a matrix usually
change its eigenvalues.
SECOND REVISED PAGES


--- Page 297 ---
280 CHAPTER 5 Eigenvalues and Eigenvectors
Application to Dynamical Systems
Eigenvalues and eigenvectors hold the key to the discrete evolution of a dynamical
system, as mentioned in the chapter introduction.
EXAMPLE 5 LetAD:95 :03
:05 :97
. Analyze the long-term behavior of the dynam-
ical system deÔ¨Åned by xkC1DAxk.kD0; 1; 2; : : :/ , with x0D:6
:4
.
SOLUTION The Ô¨Årst step is to Ô¨Ånd the eigenvalues of Aand a basis for each eigenspace.
The characteristic equation for Ais
0Ddet:95  :03
:05 :97  
D.:95 /.:97  / .:03/.:05/
D2 1:92C:92
By the quadratic formula
D1:92p
.1:92/2 4.:92/
2D1:92p
:0064
2
D1:92:08
2D1or:92
It is readily checked that eigenvectors corresponding to D1andD:92are multiples
of
v1D3
5
and v2D1
 1
respectively.
The next step is to write the given x0in terms of v1andv2. This can be done because
fv1;v2gis obviously a basis for R2. (Why?) So there exist weights c1andc2such that
x0Dc1v1Cc2v2D¬åv1v2¬çc1
c2
(3)
In fact,
c1
c2
D¬åv1v2¬ç 1x0D3 1
5 1 1:60
:40
D1
 8 1 1
 5 3:60
:40
D:125
:225
(4)
Because v1andv2in (3) are eigenvectors of A, with Av1Dv1andAv2D:92v2, we
easily compute each xk:
x1DAx0Dc1Av1Cc2Av2 Using linearity of x7!Ax
Dc1v1Cc2.:92/ v2 v1andv2are eigenvectors.
x2DAx1Dc1Av1Cc2.:92/A v2
Dc1v1Cc2.:92/2v2
and so on. In general,
xkDc1v1Cc2.:92/kv2.kD0; 1; 2; : : :/
Using c1andc2from (4),
xkD:1253
5
C:225.:92/k1
 1
.kD0; 1; 2; : : :/ (5)
SECOND REVISED PAGES


--- Page 298 ---
5.2 The Characteristic Equation 281
This explicit formula for xkgives the solution of the difference equation xkC1DAxk.
Ask! 1 ,.:92/ktends to zero and xktends to:375
:625
D:125v1.
The calculations in Example 5 have an interesting application to a Markov chain
discussed in Section 4.9. Those who read that section may recognize that matrix A
in Example 5 above is the same as the migration matrix Min Section 4.9, x0is the
initial population distribution between city and suburbs, and xkrepresents the population
distribution after kyears.
Theorem 18 in Section 4.9 stated that for a matrix such as A, the sequence xktends
to a steady-state vector. Now we know why thexkbehave this way, at least for the
migration matrix. The steady-state vector is :125v1, a multiple of the eigenvector v1,
and formula (5) for xkshows precisely why xk!:125v1.
N U M E R I C A L N O T E S
1.Computer software such as Mathematica and Maple can use symbolic calcu-
lations to Ô¨Ånd the characteristic polynomial of a moderate-sized matrix. But
there is no formula or Ô¨Ånite algorithm to solve the characteristic equation of a
general nnmatrix for n5.
2.The best numerical methods for Ô¨Ånding eigenvalues avoid the characteristic
polynomial entirely. In fact, MATLAB Ô¨Ånds the characteristic polynomial
of a matrix Aby Ô¨Årst computing the eigenvalues 1; : : : ;  nofAand then
expanding the product . 1/. 2/. n/.
3.Several common algorithms for estimating the eigenvalues of a matrix A
are based on Theorem 4. The powerful QR algorithm is discussed in the
exercises. Another technique, called Jacobi‚Äô s method , works when ADAT
and computes a sequence of matrices of the form
A1DAand AkC1DP 1
kAkPk.kD1; 2; : : :/
Each matrix in the sequence is similar to Aand so has the same eigenvalues
asA. The nondiagonal entries of AkC1tend to zero as kincreases, and the
diagonal entries tend to approach the eigenvalues of A.
4.Other methods of estimating eigenvalues are discussed in Section 5.8.
PRACTICE PROBLEM
Find the characteristic equation and eigenvalues of AD1 4
4 2
.
5.2 EXERCISES
Find the characteristic polynomial and the eigenvalues of the
matrices in Exercises 1‚Äì8.
1.2 7
7 2
2.5 3
3 5
3.3 2
1 1
4.5 3
 4 35.2 1
 1 4
6.3 4
4 8
7.5 3
 4 4
8.7 2
2 3
Exercises 9‚Äì14 require techniques from Section 3.1. Find the
characteristic polynomial of each matrix, using either a cofactor
expansion or the special formula for 33determinants described
SECOND REVISED PAGES


--- Page 299 ---
282 CHAPTER 5 Eigenvalues and Eigenvectors
prior to Exercises 15‚Äì18 in Section 3.1. [ Note: Finding the char-
acteristic polynomial of a 33matrix is not easy to do with just
row operations, because the variable is involved.]
9.2
41 0  1
2 3  1
0 6 03
5 10.2
40 3 1
3 0 2
1 2 03
5
11.2
44 0 0
5 3 2
 2 0 23
5 12.2
4 1 0 1
 3 4 1
0 0 23
5
13.2
46 2 0
 2 9 0
5 8 33
5 14.2
45 2 3
0 1 0
6 7  23
5
For the matrices in Exercises 15‚Äì17, list the eigenvalues, repeated
according to their multiplicities.
15.2
6644 7 0 2
0 3  4 6
0 0 3  8
0 0 0 13
77516.2
6645 0 0 0
8 4 0 0
0 7 1 0
1 5 2 13
775
17.2
666643 0 0 0 0
 5 1 0 0 0
3 8 0 0 0
0 7 2 1 0
 4 1 9  2 33
77775
18. It can be shown that the algebraic multiplicity of an eigen-
value is always greater than or equal to the dimension of the
eigenspace corresponding to . Find hin the matrix Abelow
such that the eigenspace for D5is two-dimensional:
AD2
6645 2 6  1
0 3 h 0
0 0 5 4
0 0 0 13
775
19. LetAbe an nnmatrix, and suppose Ahasnreal eigenval-
ues,1; : : : ;  n, repeated according to multiplicities, so that
det.A I/D.1 /. 2 /.n /
Explain why det Ais the product of the neigenvalues of
A. (This result is true for any square matrix when complex
eigenvalues are considered.)
20. Use a property of determinants to show that AandAThave
the same characteristic polynomial.
In Exercises 21 and 22, AandBarennmatrices. Mark each
statement True or False. Justify each answer.
21. a.The determinant of Ais the product of the diagonal entries
inA.
b.An elementary row operation on Adoes not change the
determinant.
c..detA/.detB/DdetAB
d.IfC5is a factor of the characteristic polynomial of A,
then 5 is an eigenvalue of A.22. a.IfAis33, with columns a1,a2, and a3, then det A
equals the volume of the parallelepiped determined by a1,
a2anda3.
b.detATD. 1/detA.
c.The multiplicity of a root rof the characteristic equation
ofAis called the algebraic multiplicity of ras an eigen-
value of A.
d.A row replacement operation on Adoes not change the
eigenvalues.
A widely used method for estimating eigenvalues of a general
matrix Ais the QRalgorithm . Under suitable conditions, this al-
gorithm produces a sequence of matrices, all similar to A, that be-
come almost upper triangular, with diagonal entries that approach
the eigenvalues of A. The main idea is to factor A(or another
matrix similar to A) in the form ADQ1R1, where QT
1DQ 1
1
andR1is upper triangular. The factors are interchanged to form
A1DR1Q1, which is again factored as A1DQ2R2; then to form
A2DR2Q2, and so on. The similarity of A; A 1; : : :follows from
the more general result in Exercise 23.
23. Show that if ADQRwithQinvertible, then Ais similar to
A1DRQ.
24. Show that if AandBare similar, then det ADdetB.
25. LetAD:6 :3
:4 :7
,v1D3=7
4=7
,x0D:5
:5
. [Note: Ais
the stochastic matrix studied in Example 5 of Section 4.9.]
a.Find a basis for R2consisting of v1and another eigenvec-
torv2ofA.
b.Verify that x0may be written in the form x0Dv1Ccv2.
c.ForkD1; 2; : : : ; deÔ¨Åne xkDAkx0. Compute x1andx2,
and write a formula for xk. Then show that xk!v1ask
increases.
26. LetADa b
c d
. Use formula (1) for a determinant
(given before Example 2) to show that det ADad bc.
Consider two cases: a¬§0andaD0.
27. LetAD2
4:5 :2 :3
:3 :8 :3
:2 0 :43
5,v1D2
4:3
:6
:13
5,v2D2
41
 3
23
5,
v3D2
4 1
0
13
5, and wD2
41
1
13
5.
a.Show that v1,v2, and v3are eigenvectors of A. [Note: Ais
the stochastic matrix studied in Example 3 of Section 4.9.]
b.Letx0be any vector in R3with nonnegative entries whose
sum is 1. (In Section 4.9, x0was called a probability
vector.) Explain why there are constants c1,c2, and c3
such that x0Dc1v1Cc2v2Cc3v3. Compute wTx0, and
deduce that c1D1.
c.ForkD1; 2; : : : ; deÔ¨Åne xkDAkx0, with x0as in part
(b). Show that xk!v1askincreases.
SECOND REVISED PAGES


--- Page 300 ---
5.3 Diagonalization 283
28. [M] Construct a random integer-valued 44matrix A, and
verify that AandAThave the same characteristic polynomial
(the same eigenvalues with the same multiplicities). Do A
andAThave the same eigenvectors? Make the same analysis
of a55matrix. Report the matrices and your conclusions.
29. [M] Construct a random integer-valued 44matrix A.
a.Reduce Ato echelon form Uwith no row scaling, and use
Uin formula (1) (before Example 2) to compute det A. (If
Ahappens to be singular, start over with a new random
matrix.)
b.Compute the eigenvalues of Aand the product of these
eigenvalues (as accurately as possible).c.List the matrix A, and, to four decimal places, list the
pivots in Uand the eigenvalues of A. Compute det Awith
your matrix program, and compare it with the products
you found in (a) and (b).
30. [M] Let AD2
4 6 28 21
4 15 12
 8 a 253
5. For each value of ain
the set f32; 31:9; 31:8; 32:1; 32:2 g, compute the characteris-
tic polynomial of Aand the eigenvalues. In each case, create
a graph of the characteristic polynomial p.t/Ddet.A tI/
for0t3. If possible, construct all graphs on one coordi-
nate system. Describe how the graphs reveal the changes in
the eigenvalues as achanges.
SOLUTION TO PRACTICE PROBLEM
The characteristic equation is
0Ddet.A I/Ddet1  4
4 2  
D.1 /.2 / . 4/.4/D2 3C18
From the quadratic formula,
D3p
. 3/2 4.18/
2D3p
 63
2
It is clear that the characteristic equation has no real solutions, so Ahas no real
eigenvalues. The matrix Ais acting on the real vector space R2, and there is no nonzero
vector vinR2such that AvDvfor some scalar .
5.3 DIAGONALIZATION
In many cases, the eigenvalue‚Äìeigenvector information contained within a matrix Acan
be displayed in a useful factorization of the form ADPDP 1where Dis a diagonal ma-
trix. In this section, the factorization enables us to compute Akquickly for large values
ofk, a fundamental idea in several applications of linear algebra. Later, in Sections 5.6
and 5.7, the factorization will be used to analyze (and decouple ) dynamical systems.
The following example illustrates that powers of a diagonal matrix are easy to
compute.
EXAMPLE 1 IfDD5 0
0 3
, then D2D5 0
0 35 0
0 3
D520
0 32
and
D3DDD2D5 0
0 3520
0 32
D530
0 33
In general,
DkD5k0
0 3k
fork1
IfADPDP 1for some invertible Pand diagonal D, then Akis also easy to
compute, as the next example shows.
SECOND REVISED PAGES


--- Page 301 ---
284 CHAPTER 5 Eigenvalues and Eigenvectors
EXAMPLE 2 LetAD7 2
 4 1
. Find a formula for Ak, given that ADPDP 1,
where
PD1 1
 1 2
and DD5 0
0 3
SOLUTION The standard formula for the inverse of a 22matrix yields
P 1D2 1
 1 1
Then, by associativity of matrix multiplication,
A2D.PDP 1/.PDP 1/DPD.P 1P /¬Ñ¬É¬Ç¬Ö
IDP 1DPDDP 1
DPD2P 1D1 1
 1 2520
0 322 1
 1 1
Again,
A3D.PDP 1/A2D.PDP 1/P¬Ñ¬É¬Ç¬Ö
ID2P 1DPDD2P 1DPD3P 1
In general, for k1,
AkDPDkP 1D1 1
 1 25k0
0 3k2 1
 1 1
D25k 3k5k 3k
23k 25k23k 5k
A square matrix Ais said to be diagonalizable ifAis similar to a diagonal matrix,
that is, if ADPDP 1for some invertible matrix Pand some diagonal matrix D.
The next theorem gives a characterization of diagonalizable matrices and tells how to
construct a suitable factorization.
T H E O R E M 5 The Diagonalization Theorem
Annnmatrix Ais diagonalizable if and only if Ahasnlinearly independent
eigenvectors.
In fact, ADPDP 1, with Da diagonal matrix, if and only if the columns of
Parenlinearly independent eigenvectors of A. In this case, the diagonal entries
ofDare eigenvalues of Athat correspond, respectively, to the eigenvectors in P.
In other words, Ais diagonalizable if and only if there are enough eigenvectors to
form a basis of Rn. We call such a basis an eigenvector basis ofRn.
PROOF First, observe that if Pis any nnmatrix with columns v1; : : : ; vn, and if D
is any diagonal matrix with diagonal entries 1; : : : ;  n, then
APDA¬åv1v2 vn¬çD¬åAv1Av2Avn¬ç (1)
while
PDDP2
66641 0 0
0  2 0
:::::::::
0 0  n3
7775D¬å1v12v2nvn¬ç (2)
SECOND REVISED PAGES


--- Page 302 ---
5.3 Diagonalization 285
Now suppose Ais diagonalizable and ADPDP 1. Then right-multiplying this relation
byP, we have APDPD. In this case, equations (1) and (2) imply that
¬åAv1Av2Avn¬çD¬å1v12v2nvn¬ç (3)
Equating columns, we Ô¨Ånd that
Av1D1v1; A v2D2v2; : : : ; A vnDnvn (4)
Since Pis invertible, its columns v1; : : : ; vnmust be linearly independent. Also, since
these columns are nonzero, the equations in (4) show that 1; : : : ;  nare eigenvalues
andv1; : : : ; vnare corresponding eigenvectors. This argument proves the ‚Äúonly if‚Äù parts
of the Ô¨Årst and second statements, along with the third statement, of the theorem.
Finally, given any neigenvectors v1; : : : ; vn, use them to construct the columns
ofPand use corresponding eigenvalues 1; : : : ;  nto construct D. By equations (1)‚Äì
(3),APDPD. This is true without any condition on the eigenvectors. If, in fact, the
eigenvectors are linearly independent, then Pis invertible (by the Invertible Matrix
Theorem), and APDPDimplies that ADPDP 1.
Diagonalizing Matrices
EXAMPLE 3 Diagonalize the following matrix, if possible.
AD2
41 3 3
 3 5 3
3 3 13
5
That is, Ô¨Ånd an invertible matrix Pand a diagonal matrix Dsuch that ADPDP 1.
SOLUTION There are four steps to implement the description in Theorem 5.
Step 1 .Find the eigenvalues of A. As mentioned in Section 5.2, the mechanics of
this step are appropriate for a computer when the matrix is larger than 22. To avoid
unnecessary distractions, the text will usually supply information needed for this step.
In the present case, the characteristic equation turns out to involve a cubic polynomial
that can be factored:
0Ddet.A I/D  3 32C4
D  . 1/.C2/2
The eigenvalues are D1andD  2.
Step 2 .Find three linearly independent eigenvectors of A. Three vectors are needed
because Ais a33matrix. This is the critical step. If it fails, then Theorem 5 says
thatAcannot be diagonalized. The method in Section 5.1 produces a basis for each
eigenspace:
Basis for D1Wv1D2
41
 1
13
5
Basis for D  2Wv2D2
4 1
1
03
5and v3D2
4 1
0
13
5
You can check that fv1;v2;v3gis a linearly independent set.
SECOND REVISED PAGES


--- Page 303 ---
286 CHAPTER 5 Eigenvalues and Eigenvectors
Step 3 .Construct P from the vectors in step 2. The order of the vectors is unimportant.
Using the order chosen in step 2, form
PDv1v2v3
D2
41 1 1
 1 1 0
1 0 13
5
Step 4 .Construct D from the corresponding eigenvalues. In this step, it is essential that
the order of the eigenvalues matches the order chosen for the columns of P. Use the
eigenvalue D  2twice, once for each of the eigenvectors corresponding to D  2:
DD2
41 0 0
0 2 0
0 0  23
5
It is a good idea to check that PandDreally work. To avoid computing P 1,
simply verify that APDPD. This is equivalent to ADPDP 1when Pis invertible.
(However, be sure that Pis invertible!) Compute
APD2
41 3 3
 3 5 3
3 3 13
52
41 1 1
 1 1 0
1 0 13
5D2
41 2 2
 1 2 0
1 0  23
5
PDD2
41 1 1
 1 1 0
1 0 13
52
41 0 0
0 2 0
0 0  23
5D2
41 2 2
 1 2 0
1 0  23
5
EXAMPLE 4 Diagonalize the following matrix, if possible.
AD2
42 4 3
 4 6 3
3 3 13
5
SOLUTION The characteristic equation of Aturns out to be exactly the same as that in
Example 3:
0Ddet.A I/D  3 32C4D  . 1/.C2/2
The eigenvalues are D1andD  2. However, it is easy to verify that each
eigenspace is only one-dimensional:
Basis for D1W v1D2
41
 1
13
5
Basis for D  2W v2D2
4 1
1
03
5
There are no other eigenvalues, and every eigenvector of Ais a multiple of either v1
orv2. Hence it is impossible to construct a basis of R3using eigenvectors of A. By
Theorem 5, Aisnotdiagonalizable.
The following theorem provides a sufÔ¨Åcient condition for a matrix to be
diagonalizable.
T H E O R E M 6 Annnmatrix with ndistinct eigenvalues is diagonalizable.
SECOND REVISED PAGES


--- Page 304 ---
5.3 Diagonalization 287
PROOF Letv1; : : : ; vnbe eigenvectors corresponding to the ndistinct eigenvalues of
a matrix A. Then fv1; : : : ; vngis linearly independent, by Theorem 2 in Section 5.1.
Hence Ais diagonalizable, by Theorem 5.
It is not necessary for an nnmatrix to have ndistinct eigenvalues in order to be
diagonalizable. The 33matrix in Example 3 is diagonalizable even though it has only
two distinct eigenvalues.
EXAMPLE 5 Determine if the following matrix is diagonalizable.
AD2
45 8 1
0 0 7
0 0  23
5
SOLUTION This is easy! Since the matrix is triangular, its eigenvalues are obviously 5,
0, and 2. Since Ais a33matrix with three distinct eigenvalues, Ais diagonalizable.
Matrices Whose Eigenvalues Are Not Distinct
If annnmatrix Ahasndistinct eigenvalues, with corresponding eigenvectors v1; : : : ;
vn, and if PD¬åv1 vn¬ç, then Pis automatically invertible because its columns
are linearly independent, by Theorem 2. When Ais diagonalizable but has fewer than n
distinct eigenvalues, it is still possible to build Pin a way that makes Pautomatically
invertible, as the next theorem shows.1
T H E O R E M 7 LetAbe an nnmatrix whose distinct eigenvalues are 1; : : : ;  p.
a.For1kp, the dimension of the eigenspace for kis less than or equal to
the multiplicity of the eigenvalue k.
b.The matrix Ais diagonalizable if and only if the sum of the dimensions of
the eigenspaces equals n, and this happens if and only if ( i) the characteristic
polynomial factors completely into linear factors and ( ii) the dimension of the
eigenspace for each kequals the multiplicity of k.
c.IfAis diagonalizable and Bkis a basis for the eigenspace corresponding to k
for each k, then the total collection of vectors in the sets B1; : : : ;Bpforms an
eigenvector basis for Rn.
EXAMPLE 6 Diagonalize the following matrix, if possible.
AD2
6645 0 0 0
0 5 0 0
1 4  3 0
 1 2 0  33
775
1The proof of Theorem 7 is somewhat lengthy but not difÔ¨Åcult. For instance, see S. Friedberg, A. Insel, and
L. Spence, Linear Algebra , 4th ed. (Englewood Cliffs, NJ: Prentice-Hall, 2002), Section 5.2.
SECOND REVISED PAGES


--- Page 305 ---
288 CHAPTER 5 Eigenvalues and Eigenvectors
SOLUTION Since Ais a triangular matrix, the eigenvalues are 5 and  3, each with
multiplicity 2. Using the method in Section 5.1, we Ô¨Ånd a basis for each eigenspace.
Basis for D5Wv1D2
664 8
4
1
03
775and v2D2
664 16
4
0
13
775
Basis for D  3Wv3D2
6640
0
1
03
775and v4D2
6640
0
0
13
775
The set fv1; : : : ; v4gis linearly independent, by Theorem 7. So the matrix PD
¬åv1 v4¬çis invertible, and ADPDP 1, where
PD2
664 8 16 0 0
4 4 0 0
1 0 1 0
0 1 0 13
775and DD2
6645 0 0 0
0 5 0 0
0 0  3 0
0 0 0  33
775
PRACTICE PROBLEMS
1.Compute A8, where AD4 3
2 1
.
2.LetAD 3 12
 2 7
,v1D3
1
, and v2D2
1
. Suppose you are told that v1and
v2are eigenvectors of A. Use this information to diagonalize A.
3.LetAbe a44matrix with eigenvalues 5, 3, and  2, and suppose you know that
the eigenspace for D3is two-dimensional. Do you have enough information to
determine if Ais diagonalizable?
WEB
5.3 EXERCISES
In Exercises 1 and 2, let ADPDP 1and compute A4.
1.PD5 7
2 3
,DD2 0
0 1
2.PD2 3
 3 5
,DD1 0
0 1=2
In Exercises 3 and 4, use the factorization ADPDP 1to com-
puteAk, where krepresents an arbitrary positive integer.
3.a 0
3.a b/ b
D1 0
3 1a 0
0 b1 0
 3 1
4. 2 12
 1 5
D3 4
1 12 0
0 1 1 4
1 3
In Exercises 5 and 6, the matrix Ais factored in the form PDP 1.
Use the Diagonalization Theorem to Ô¨Ånd the eigenvalues of Aand
a basis for each eigenspace.5.2
42 2 1
1 3 1
1 2 23
5D
2
41 1 2
1 0  1
1 1 03
52
45 0 0
0 1 0
0 0 13
52
41=4 1=2 1=4
1=4 1=2  3=4
1=4 1=2 1=43
5
6.2
44 0  2
2 5 4
0 0 53
5D
2
4 2 0  1
0 1 2
1 0 03
52
45 0 0
0 5 0
0 0 43
52
40 0 1
2 1 4
 1 0  23
5
Diagonalize the matrices in Exercises 7‚Äì20, if possible. The
eigenvalues for Exercises 11‚Äì16 are as follows: (11) D1; 2; 3 ;
(12)D2; 8; (13) D5; 1; (14) D5; 4; (15) D3; 1; (16)
D2; 1. For Exercise 18, one eigenvalue is D5and one
eigenvector is . 2; 1; 2/ .
SECOND REVISED PAGES


--- Page 306 ---
5.3 Diagonalization 289
7.1 0
6 1
8.5 1
0 5
9.3 1
1 5
10.2 3
4 1
11.2
4 1 4  2
 3 4 0
 3 1 33
5 12.2
44 2 2
2 4 2
2 2 43
5
13.2
42 2  1
1 3  1
 1 2 23
5 14.2
44 0  2
2 5 4
0 0 53
5
15.2
47 4 16
2 5 8
 2 2 53
5 16.2
40 4 6
 1 0  3
1 2 53
5
17.2
44 0 0
1 4 0
0 0 53
5 18.2
4 7 16 4
6 13  2
12 16 13
5
19.2
6645 3 0 9
0 3 1  2
0 0 2 0
0 0 0 23
77520.2
6644 0 0 0
0 4 0 0
0 0 2 0
1 0 0 23
775
In Exercises 21 and 22, A,B,P, and Darennmatrices.
Mark each statement True or False. Justify each answer. (Study
Theorems 5 and 6 and the examples in this section carefully before
you try these exercises.)
21. a.Ais diagonalizable if ADPDP 1for some matrix D
and some invertible matrix P.
b.IfRnhas a basis of eigenvectors of A, then Ais diago-
nalizable.
c.Ais diagonalizable if and only if Ahasneigenvalues,
counting multiplicities.
d.IfAis diagonalizable, then Ais invertible.
22. a.Ais diagonalizable if Ahasneigenvectors.
b.IfAis diagonalizable, then Ahasndistinct eigenvalues.
c.IfAPDPD, with Ddiagonal, then the nonzero columns
ofPmust be eigenvectors of A.
d.IfAis invertible, then Ais diagonalizable.
23.Ais a55matrix with two eigenvalues. One eigenspace
is three-dimensional, and the other eigenspace is two-
dimensional. Is Adiagonalizable? Why?24.Ais a33matrix with two eigenvalues. Each eigenspace is
one-dimensional. Is Adiagonalizable? Why?
25.Ais a44matrix with three eigenvalues. One eigenspace
is one-dimensional, and one of the other eigenspaces is two-
dimensional. Is it possible that Aisnotdiagonalizable?
Justify your answer.
26.Ais a77matrix with three eigenvalues. One eigenspace is
two-dimensional, and one of the other eigenspaces is three-
dimensional. Is it possible that Aisnotdiagonalizable?
Justify your answer.
27. Show that if Ais both diagonalizable and invertible, then so
isA 1.
28. Show that if Ahasnlinearly independent eigenvectors, then
so does AT. [Hint: Use the Diagonalization Theorem.]
29.Afactorization ADPDP 1is not unique. Demonstrate this
for the matrix Ain Example 2. With D1D3 0
0 5
, use
the information in Example 2 to Ô¨Ånd a matrix P1such that
ADP1D1P 1
1.
30. With AandDas in Example 2, Ô¨Ånd an invertible P2unequal
to the Pin Example 2, such that ADP2DP 1
2.
31. Construct a nonzero 22matrix that is invertible but not
diagonalizable.
32. Construct a nondiagonal 22matrix that is diagonalizable
but not invertible.
[M] Diagonalize the matrices in Exercises 33‚Äì36. Use your ma-
trix program‚Äôs eigenvalue command to Ô¨Ånd the eigenvalues, and
then compute bases for the eigenspaces as in Section 5.1.
33.2
664 6 4 0 9
 3 0 1 6
 1 2 1 0
 4 4 0 73
77534.2
6640 13 8 4
4 9 8 4
8 6 12 8
0 5 0  43
775
35.2
6666411 6 4  10 4
 3 5  2 4 1
 8 12  3 12 4
1 6  2 3  1
8 18 8  14 13
77775
36.2
666644 4 2 3  2
0 1  2 2 2
6 12 11 2  4
9 20 10 10  6
15 28 14 5  33
77775
SECOND REVISED PAGES


--- Page 307 ---
290 CHAPTER 5 Eigenvalues and Eigenvectors
SOLUTIONS TO PRACTICE PROBLEMS
1.det.A I/D2 3C2D. 2/. 1/. The eigenvalues are 2 and 1, and
the corresponding eigenvectors are v1D3
2
andv2D1
1
. Next, form
PD3 1
2 1
; D D2 0
0 1
;and P 1D1 1
 2 3
Since ADPDP 1,
A8DPD8P 1D3 1
2 1280
0 181 1
 2 3
D3 1
2 1256 0
0 11 1
 2 3
D766 765
510 509
2.Compute Av1D 3 12
 2 73
1
D3
1
D1v1, and
Av2D 3 12
 2 72
1
D6
3
D3v2
So,v1andv2are eigenvectors for the eigenvalues 1 and 3, respectively. Thus
ADPDP 1;where PD3 2
1 1
and DD1 0
0 3
3.Yes,Ais diagonalizable. There is a basis fv1;v2gfor the eigenspace corresponding
toD3. In addition, there will be at least one eigenvector for D5and one
forD  2. Call them v3andv4. Then fv1;v2;v3;v4gis linearly independent by
Theorem 2 and Practice Problem 3 in Section 5.1. There can be no additional
eigenvectors that are linearly independent from v1,v2,v3,v4, because the vectors are
all inR4. Hence the eigenspaces for D5andD  2are both one-dimensional.
It follows that Ais diagonalizable by Theorem 7(b).
SGMastering: Eigenvalue
and Eigenspace 5‚Äì14
5.4 EIGENVECTORS AND LINEAR TRANSFORMATIONS
The goal of this section is to understand the matrix factorization ADPDP 1as a
statement about linear transformations. We shall see that the transformation x7!Ax
is essentially the same as the very simple mapping u7!Du, when viewed from the
proper perspective. A similar interpretation will apply to AandDeven when Dis not
a diagonal matrix.
Recall from Section 1.9 that any linear transformation TfromRntoRmcan be
implemented via left-multiplication by a matrix A, called the standard matrix ofT.
Now we need the same sort of representation for any linear transformation between two
Ô¨Ånite-dimensional vector spaces.
SECOND REVISED PAGES


--- Page 308 ---
5.4 Eigenvectors and Linear Transformations 291
The Matrix of a Linear Transformation
LetVbe an n-dimensional vector space, let Wbe an m-dimensional vector space, and
letTbe any linear transformation from VtoW. To associate a matrix with T, choose
(ordered) bases BandCforVandW, respectively.
Given any xinV, the coordinate vector ¬åx¬çBis inRnand the coordinate vector of
its image, ¬åT .x/¬çC, is inRm, as shown in Figure 1.
[x]B
/H11938n
/H11938mxV T W
T(x)
[T(x)]C
FIGURE 1 A linear transformation from VtoW.
The connection between ¬åx¬çBand¬åT .x/¬çCis easy to Ô¨Ånd. Let fb1; : : : ; bngbe the
basis BforV. IfxDr1b1C  C rnbn, then
¬åx¬çBD2
64r1
:::
rn3
75
and
T .x/DT .r 1b1C  C rnbn/Dr1T .b1/C  C rnT .bn/ (1)
because Tis linear. Now, since the coordinate mapping from WtoRmis linear
(Theorem 8 in Section 4.4), equation (1) leads to
¬åT .x/¬çCDr1¬åT .b1/¬çCC  C rn¬åT .bn/¬çC(2)
Since C-coordinate vectors are in Rm, the vector equation (2) can be written as a matrix
equation, namely,
¬åT .x/¬çCDM¬åx¬çB(3)
where
MD¬åT .b1/¬çC¬åT .b2/¬çC ¬åT .bn/¬çC
(4)
The matrix Mis a matrix representation of T, called the matrix for Trelative to the
basesBandC. See Figure 2.
[T(x)]CT(x)Tx
Multiplication
by M[x]B FIGURE 2
Equation (3) says that, so far as coordinate vectors are concerned, the action of T
onxmay be viewed as left-multiplication by M.
EXAMPLE 1 Suppose BD fb1;b2gis a basis for VandCD fc1;c2;c3gis a basis
forW. LetTWV!Wbe a linear transformation with the property that
T .b1/D3c1 2c2C5c3and T .b2/D4c1C7c2 c3
Find the matrix MforTrelative to BandC.
SECOND REVISED PAGES


--- Page 309 ---
292 CHAPTER 5 Eigenvalues and Eigenvectors
SOLUTION TheC-coordinate vectors of the images ofb1andb2are
¬åT .b1/¬çCD2
43
 2
5
?3
5and ¬åT .b2/¬çCD2
44
7
 1
?3
5
Hence
MD2
43 4
 2 7
5 13
5
IfBandCare bases for the same space Vand if Tis the identity transformation
T .x/DxforxinV, then matrix Min (4) is just a change-of-coordinates matrix (see
Section 4.7).
Linear Transformations from VintoV
In the common case where Wis the same as Vand the basis Cis the same as B, the
xTT(x)
[T(x)]BMultiplication
by [T]B[x]B
FIGURE 3matrix Min (4) is called the matrix for Trelative to B, or simply the B-matrix for T,
and is denoted by ¬åT¬çB. See Figure 3.
TheB-matrix for TWV!VsatisÔ¨Åes
¬åT .x/¬çBD¬åT¬çB¬åx¬çB;for all xinV (5)
EXAMPLE 2 The mapping TWP2!P2deÔ¨Åned by
T .a 0Ca1tCa2t2/Da1C2a2t
is a linear transformation. (Calculus students will recognize Tas the differentiation
operator.)
a.Find the B-matrix for T, when Bis the basis f1; t; t2g.
b.Verify that ¬åT .p/¬çBD¬åT¬çB¬åp¬çBfor each pinP2.
SOLUTION
a.Compute the images of the basis vectors:
T .1/D0 The zero polynomial
T .t/D1 The polynomial whose value is always 1
T .t2/D2t
Then write the B-coordinate vectors of T .1/ ,T .t/, and T .t2/(which are found by
inspection in this example) and place them together as the B-matrix for T:
¬åT .1/ ¬çBD2
40
0
0
?3
5; ¬åT .t/¬çBD2
41
0
0
?3
5; ¬åT .t2/¬çBD2
40
2
0
?3
5
¬åT¬çBD2
40 1 0
0 0 2
0 0 03
5
SECOND REVISED PAGES


--- Page 310 ---
5.4 Eigenvectors and Linear Transformations 293
b.For a general p.t/Da0Ca1tCa2t2,
¬åT .p/¬çBD¬åa1C2a2t¬çBD2
4a1
2a2
03
5
D2
40 1 0
0 0 2
0 0 03
52
4a0
a1
a23
5D¬åT¬çB¬åp¬çB
See Figure 4.
a0 + a1t + a2t2
a0
Multiplication
by [T]Ba1
a12a2
0a2T
/H119362
/H119383
/H119383/H119362a1 + 2a2t
FIGURE 4 Matrix representation of a linear
transformation.
WEB
Linear Transformations on Rn
In an applied problem involving Rn, a linear transformation Tusually appears Ô¨Årst as
a matrix transformation, x7!Ax. IfAis diagonalizable, then there is a basis BforRn
consisting of eigenvectors of A. Theorem 8 below shows that, in this case, the B-matrix
forTis diagonal. Diagonalizing Aamounts to Ô¨Ånding a diagonal matrix representation
ofx7!Ax.
T H E O R E M 8 Diagonal Matrix Representation
Suppose ADPDP 1, where Dis a diagonal nnmatrix. If Bis the basis for
Rnformed from the columns of P, then Dis the B-matrix for the transformation
x7!Ax.
PROOF Denote the columns of Pbyb1; : : : ; bn, so that BD fb1; : : : ; bngandPD
¬åb1 bn¬ç. In this case, Pis the change-of-coordinates matrix PBdiscussed in
Section 4.4, where
P ¬åx¬çBDxand ¬åx¬çBDP 1x
IfT .x/DAxforxinRn, then
¬åT¬çBD¬åT .b1/¬çB ¬åT .bn/¬çB
DeÔ¨Ånition of ¬åT¬çB
D¬åAb1¬çB ¬åAbn¬çB
Since T .x/DAx
D¬åP 1Ab1P 1Abn¬ç Change of coordinates
DP 1A¬åb1 bn¬ç Matrix multiplication
DP 1AP (6)
Since ADPDP 1, we have ¬åT¬çBDP 1APDD.
SECOND REVISED PAGES


--- Page 311 ---
294 CHAPTER 5 Eigenvalues and Eigenvectors
EXAMPLE 3 DeÔ¨Åne TWR2!R2byT .x/DAx, where AD7 2
 4 1
. Find a
basis BforR2with the property that the B-matrix for Tis a diagonal matrix.
SOLUTION From Example 2 in Section 5.3, we know that ADPDP 1, where
PD1 1
 1 2
and DD5 0
0 3
The columns of P, call them b1andb2, are eigenvectors of A. By Theorem 8, Dis the
B-matrix for Twhen BD fb1;b2g. The mappings x7!Axandu7!Dudescribe the
same linear transformation, relative to different bases.
Similarity of Matrix Representations
The proof of Theorem 8 did not use the information that Dwas diagonal. Hence,
ifAis similar to a matrix C, with ADP CP 1, then Cis the B-matrix for the
transformation x7!Axwhen the basis Bis formed from the columns of P. The
factorization ADP CP 1is shown in Figure 5.
Multiplication
by A
Multiplication
by CMultiplication
by P‚Äì1
[x]BMultiplication
by P
[Ax]BAx x
FIGURE 5 Similarity of two matrix representations:
ADPCP 1.
Conversely, if TWRn!Rnis deÔ¨Åned by T .x/DAx, and if Bis any basis for
Rn, then the B-matrix for Tis similar to A. In fact, the calculations in the proof of
Theorem 8 show that if Pis the matrix whose columns come from the vectors in B,
then¬åT ¬çBDP 1AP. Thus, the set of all matrices similar to a matrix Acoincides with
the set of all matrix representations of the transformation x7!Ax.
EXAMPLE 4 LetAD4 9
4 8
,b1D3
2
, and b2D2
1
. The characteristic
polynomial of Ais.C2/2, but the eigenspace for the eigenvalue  2is only one-
dimensional; so Ais not diagonalizable. However, the basis BD fb1;b2ghas the
property that the B-matrix for the transformation x7!Axis a triangular matrix called
theJordan form ofA.1Find this B-matrix.
SOLUTION IfPD¬åb1b2¬ç, then the B-matrix is P 1AP. Compute
APD4 9
4 83 2
2 1
D 6 1
 4 0
P 1APD 1 2
2 3 6 1
 4 0
D 2 1
0 2
Notice that the eigenvalue of Ais on the diagonal.
1Every square matrix Ais similar to a matrix in Jordan form. The basis used to produce a Jordan form
consists of eigenvectors and so-called ‚Äúgeneralized eigenvectors‚Äù of A. See Chapter 9 of Applied Linear
Algebra , 3rd ed. (Englewood Cliffs, NJ: Prentice-Hall, 1988), by B. Noble and J. W. Daniel.
SECOND REVISED PAGES


--- Page 312 ---
5.4 Eigenvectors and Linear Transformations 295
N U M E R I C A L N O T E
An efÔ¨Åcient way to compute a B-matrix P 1APis to compute APand then to row
reduce the augmented matrix ¬åPAP¬çto¬åI P 1AP¬ç. A separate computation
ofP 1is unnecessary. See Exercise 12 in Section 2.2.
PRACTICE PROBLEMS
1.Find T .a 0Ca1tCa2t2/, ifTis the linear transformation from P2toP2whose
matrix relative to BD f1; t; t2gis
¬åT¬çBD2
43 4 0
0 5  1
1 2 73
5
2.LetA,B, and Cbennmatrices. The text has shown that if Ais similar to B,
thenBis similar to A. This property, together with the statements below, shows that
‚Äúsimilar to‚Äù is an equivalence relation . (Row equivalence is another example of an
equivalence relation.) Verify parts (a) and (b).
a.Ais similar to A.
b.IfAis similar to BandBis similar to C, then Ais similar to C.
5.4 EXERCISES
1.LetBD fb1;b2;b3gandDD fd1;d2gbe bases for vector
spaces VandW, respectively. Let TWV!Wbe a linear
transformation with the property that
T .b1/D3d1 5d2; T . b2/D  d1C6d2; T . b3/D4d2
Find the matrix for Trelative to BandD.
2.LetDD fd1;d2gandBD fb1;b2gbe bases for vector spaces
VandW, respectively. Let TWV!Wbe a linear transfor-
mation with the property that
T .d1/D2b1 3b2; T . d2/D  4b1C5b2
Find the matrix for Trelative to DandB.
3.LetED fe1;e2;e3gbe the standard basis for R3,
BD fb1;b2;b3gbe a basis for a vector space V, and
TWR3!Vbe a linear transformation with the property
that
T .x 1; x2; x3/D.x3 x2/b1 .x1Cx3/b2C.x1 x2/b3
a.Compute T .e1/,T .e2/, and T .e3/.
b.Compute ¬åT .e1/¬çB,¬åT .e2/¬çB, and ¬åT .e3/¬çB.
c.Find the matrix for Trelative to EandB.
4.LetBD fb1;b2;b3gbe a basis for a vector space Vand
TWV!R2be a linear transformation with the property that
T .x 1b1Cx2b2Cx3b3/D2x1 4x2C5x3
 x2C3x3Find the matrix for Trelative to Band the standard basis for
R2.
5.LetTWP2!P3be the transformation that maps a polyno-
mial p.t/into the polynomial .tC5/p.t/.
a.Find the image of p.t/D2 tCt2.
b.Show that Tis a linear transformation.
c.Find the matrix for Trelative to the bases f1; t; t2gand
f1; t; t2; t3g.
6.LetTWP2!P4be the transformation that maps a polyno-
mial p.t/into the polynomial p.t/Ct2p.t/.
a.Find the image of p.t/D2 tCt2.
b.Show that Tis a linear transformation.
c.Find the matrix for Trelative to the bases f1; t; t2gand
f1; t; t2; t3; t4g.
7.Assume the mapping TWP2!P2deÔ¨Åned by
T .a 0Ca1tCa2t2/D3a0C.5a0 2a1/tC.4a 1Ca2/t2
is linear. Find the matrix representation of Trelative to the
basisBD f1; t; t2g.
8.LetBD fb1;b2;b3gbe a basis for a vector space V. Find
T .3b1 4b2/when Tis a linear transformation from Vto
Vwhose matrix relative to Bis
¬åT ¬çBD2
40 6 1
0 5  1
1 2 73
5
SECOND REVISED PAGES


--- Page 313 ---
296 CHAPTER 5 Eigenvalues and Eigenvectors
9.DeÔ¨Åne TWP2!R3byT .p/D2
4p. 1/
p.0/
p.1/3
5.
a.Find the image under Tofp.t/D5C3t.
b.Show that Tis a linear transformation.
c.Find the matrix for Trelative to the basis f1; t; t2gforP2
and the standard basis for R3.
10. DeÔ¨Åne TWP3!R4byT .p/D2
664p. 3/
p. 1/
p.1/
p.3/3
775.
a.Show that Tis a linear transformation.
b.Find the matrix for Trelative to the basis f1; t; t2; t3gfor
P3and the standard basis for R4.
In Exercises 11 and 12, Ô¨Ånd the B-matrix for the transformation
x7!Ax, when BD fb1;b2g.
11.AD3 4
 1 1
,b1D2
 1
,b2D1
2
12.AD 1 4
 2 3
,b1D3
2
,b2D 1
1
In Exercises 13‚Äì16, deÔ¨Åne TWR2!R2byT .x/DAx. Find a
basisBforR2with the property that ¬åT ¬çBis diagonal.
13.AD0 1
 3 4
14.AD5 3
 7 1
15.AD4 2
 1 3
16.AD2 6
 1 3
17. LetAD1 1
 1 3
andBD fb1;b2g, for b1D1
1
,
b2D5
4
. DeÔ¨Åne TWR2!R2byT .x/DAx.
a.Verify that b1is an eigenvector of AbutAis not diago-
nalizable.
b.Find the B-matrix for T.
18. DeÔ¨Åne TWR3!R3byT .x/DAx, where Ais a33
matrix with eigenvalues 5 and  2. Does there exist a basis
BforR3such that the B-matrix for Tis a diagonal matrix?
Discuss.
Verify the statements in Exercises 19‚Äì24. The matrices are square.
19. IfAis invertible and similar to B, then Bis invertible and
A 1is similar to B 1. [Hint: P 1APDBfor some invert-
ibleP. Explain why Bis invertible. Then Ô¨Ånd an invertible
Qsuch that Q 1A 1QDB 1.]
20. IfAis similar to B, then A2is similar to B2.
21. IfBis similar to AandCis similar to A, then Bis similar
toC.22. IfAis diagonalizable and Bis similar to A, then Bis also
diagonalizable.
23. IfBDP 1APandxis an eigenvector of Acorresponding
to an eigenvalue , then P 1xis an eigenvector of Bcorre-
sponding also to .
24. IfAandBare similar, then they have the same rank. [ Hint:
Refer to Supplementary Exercises 13 and 14 for Chapter 4.]
25. The trace of a square matrix Ais the sum of the diagonal
entries in Aand is denoted by tr A. It can be veriÔ¨Åed that
tr.F G/ Dtr.GF / for any two nnmatrices FandG.
Show that if AandBare similar, then tr ADtrB.
26. It can be shown that the trace of a matrix Aequals the sum of
the eigenvalues of A. Verify this statement for the case when
Ais diagonalizable.
27. LetVbeRnwith a basis BD fb1; : : : ; bng; letWbeRn
with the standard basis, denoted here by E; and consider the
identity transformation IWV!W, where I.x/Dx. Find
the matrix for Irelative to BandE. What was this matrix
called in Section 4.4?
28. LetVbe a vector space with a basis BD fb1; : : : ; bng; W
be the same space as Vwith a basis CD fc1; : : : ; cng, and I
be the identity transformation IWV!W. Find the matrix
forIrelative to BandC. What was this matrix called in
Section 4.7?
29. LetVbe a vector space with a basis BD fb1; : : : ; bng. Find
theB-matrix for the identity transformation IWV!V.
[M] In Exercises 30 and 31, Ô¨Ånd the B-matrix for the transforma-
tionx7!Axwhen BD fb1;b2;b3g.
30.AD2
4 14 4  14
 33 9  31
11 4 113
5,
b1D2
4 1
 2
13
5,b2D2
4 1
 1
13
5,b3D2
4 1
 2
03
5
31.AD2
4 7 48 16
1 14 6
 3 45 193
5,
b1D2
4 3
1
 33
5,b2D2
4 2
1
 33
5,b3D2
43
 1
03
5
32. [M] Let Tbe the transformation whose standard matrix is
given below. Find a basis for R4with the property thatT
B
is diagonal.
AD2
66415 66 44 33
0 13 21  15
1 15 21 12
2 18 22 83
775
SECOND REVISED PAGES


--- Page 314 ---
5.5 Complex Eigenvalues 297
SOLUTIONS TO PRACTICE PROBLEMS
1.Letp.t/Da0Ca1tCa2t2and compute
¬åT .p/¬çBD¬åT¬çB¬åp¬çBD2
43 4 0
0 5  1
1 2 73
52
4a0
a1
a23
5D2
43a0C4a1
5a1 a2
a0 2a1C7a23
5
SoT .p/D.3a0C4a1/C.5a1 a2/tC.a0 2a1C7a2/t2.
2.a.AD.I/ 1AI, soAis similar to A.
b.By hypothesis, there exist invertible matrices PandQwith the property that
BDP 1APandCDQ 1BQ. Substitute the formula for Binto the formula
forC, and use a fact about the inverse of a product:
CDQ 1BQDQ 1.P 1AP/QD.PQ/ 1A.PQ/
This equation has the proper form to show that Ais similar to C.
5.5 COMPLEX EIGENVALUES
Since the characteristic equation of an nnmatrix involves a polynomial of degree n,
the equation always has exactly nroots, counting multiplicities, provided that possibly
complex roots are included . This section shows that if the characteristic equation of
a real matrix Ahas some complex roots, then these roots provide critical information
about A. The key is to let Aact on the space Cnofn-tuples of complex numbers.1
Our interest in Cndoes not arise from a desire to ‚Äúgeneralize‚Äù the results of the
earlier chapters, although that would in fact open up signiÔ¨Åcant new applications of
linear algebra.2Rather, this study of complex eigenvalues is essential in order to uncover
‚Äúhidden‚Äù information about certain matrices with real entries that arise in a variety of
real-life problems. Such problems include many real dynamical systems that involve
periodic motion, vibration, or some type of rotation in space.
The matrix eigenvalue‚Äìeigenvector theory already developed for Rnapplies
equally well to Cn. So a complex scalar satisÔ¨Åes det .A I/D0if and only if there
is a nonzero vector xinCnsuch that AxDx. We call a (complex )eigenvalue and
xa (complex )eigenvector corresponding to .
EXAMPLE 1 IfAD0 1
1 0
, then the linear transformation x7!AxonR2
rotates the plane counterclockwise through a quarter-turn. The action of Ais periodic,
since after four quarter-turns, a vector is back where it started. Obviously, no nonzero
vector is mapped into a multiple of itself, so Ahas no eigenvectors in R2and hence no
real eigenvalues. In fact, the characteristic equation of Ais
2C1D0
1Refer to Appendix B for a brief discussion of complex numbers. Matrix algebra and concepts about
real vector spaces carry over to the case with complex entries and scalars. In particular, A.cxCdy/D
cAxCdAy, forAanmnmatrix with complex entries, x,yinCn, and c,dinC.
2A second course in linear algebra often discusses such topics. They are of particular importance in
electrical engineering.
SECOND REVISED PAGES


--- Page 315 ---
298 CHAPTER 5 Eigenvalues and Eigenvectors
The only roots are complex: DiandD  i. However, if we permit Ato act on C2,
then 0 1
1 01
 i
Di
1
Di1
 i
0 1
1 01
i
D i
1
D  i1
i
Thus iand iare eigenvalues, with1
 i
and1
i
as corresponding eigenvectors.
(A method for Ô¨Ånding complex eigenvectors is discussed in Example 2.)
The main focus of this section will be on the matrix in the next example.
EXAMPLE 2 LetAD:5 :6
:75 1:1
. Find the eigenvalues of A, and Ô¨Ånd a basis
for each eigenspace.
SOLUTION The characteristic equation of Ais
0Ddet:5   :6
:75 1:1  
D.:5 /.1:1  / . :6/.:75/
D2 1:6C1
From the quadratic formula, D1
2¬å1:6p
. 1:6/2 4¬çD:8:6i. For the eigen-
value D:8 :6i, construct
A .:8 :6i/ID:5 :6
:75 1:1
 :8 :6i 0
0 :8  :6i
D :3C:6i  :6
:75 :3 C:6i
(1)
Row reduction of the usual augmented matrix is quite unpleasant by hand because of the
complex arithmetic. However, here is a nice observation that really simpliÔ¨Åes matters:
Since :8 :6iis an eigenvalue, the system
. :3C:6i/x 1  :6x2D0
:75x 1C.:3C:6i/x 2D0(2)
has a nontrivial solution (with x1andx2possibly complex numbers). Therefore, both
equations in (2) determine the same relationship between x1andx2, and either equation
can be used to express one variable in terms of the other .3
The second equation in (2) leads to
:75x 1D. :3 :6i/x 2
x1D. :4 :8i/x 2
Choose x2D5to eliminate the decimals, and obtain x1D  2 4i. A basis for the
eigenspace corresponding to D:8 :6iis
v1D 2 4i
5
3Another way to see this is to realize that the matrix in equation (1) is not invertible, so its rows are linearly
dependent (as vectors in C2/, and hence one row is a (complex) multiple of the other.
SECOND REVISED PAGES


--- Page 316 ---
5.5 Complex Eigenvalues 299
Analogous calculations for D:8C:6iproduce the eigenvector
v2D 2C4i
5
As a check on the work, compute
Av2D:5 :6
:75 1:1 2C4i
5
D 4C2i
4C3i
D.:8C:6i/v2
Surprisingly, the matrix Ain Example 2 determines a transformation x7!Axthat
is essentially a rotation. This fact becomes evident when appropriate points are plotted.
EXAMPLE 3 One way to see how multiplication by the matrix Ain Example 2
affects points is to plot an arbitrary initial point‚Äîsay, x0D.2; 0/ ‚Äîand then to plot
successive images of this point under repeated multiplications by A. That is, plot
x1DAx0D:5 :6
:75 1:12
0
D1:0
1:5
x2DAx1D:5 :6
:75 1:11:0
1:5
D :4
2:4
x3DAx2; : : :
Figure 1 shows x0; : : : ; x8as larger dots. The smaller dots are the locations of x9; : : : ;
x100. The sequence lies along an elliptical orbit.
x1x2
x2 x3
x4
x5
x6
x7x8x1
x0
FIGURE 1 Iterates of a point x0
under the action of a matrix with a
complex eigenvalue.
Of course, Figure 1 does not explain why the rotation occurs. The secret to the
rotation is hidden in the real and imaginary parts of a complex eigenvector.
Real and Imaginary Parts of Vectors
The complex conjugate of a complex vector xinCnis the vector xinCnwhose entries
are the complex conjugates of the entries in x. The real andimaginary parts of a
complex vector xare the vectors Re xand Im xinRnformed from the real and imaginary
parts of the entries of x.
SECOND REVISED PAGES


--- Page 317 ---
300 CHAPTER 5 Eigenvalues and Eigenvectors
EXAMPLE 4 IfxD2
43 i
i
2C5i3
5D2
43
0
23
5Ci2
4 1
1
53
5, then
RexD2
43
0
23
5;ImxD2
4 1
1
53
5;and xD2
43
0
23
5 i2
4 1
1
53
5D2
43Ci
 i
2 5i3
5
IfBis an mnmatrix with possibly complex entries, then Bdenotes the matrix
whose entries are the complex conjugates of the entries in B. Properties of conjugates
for complex numbers carry over to complex matrix algebra:
rxDrx;BxDBx;BCDBC ; and rBDrB
Eigenvalues and Eigenvectors of a Real Matrix
That Acts on Cn
LetAbe an nnmatrix whose entries are real. Then AxDAxDAx. Ifis an
eigenvalue of Aandxis a corresponding eigenvector in Cn, then
AxDAxDxDx
Hence is also an eigenvalue of A, with xa corresponding eigenvector. This shows that
when Ais real, its complex eigenvalues occur in conjugate pairs . (Here and elsewhere,
we use the term complex eigenvalue to refer to an eigenvalue DaCbi, with b¬§0.)
EXAMPLE 5 The eigenvalues of the real matrix in Example 2 are complex conju-
gates, namely, :8 :6iand:8C:6i. The corresponding eigenvectors found in Exam-
ple 2 are also conjugates:
v1D 2 4i
5
and v2D 2C4i
5
Dv1
The next example provides the basic ‚Äúbuilding block‚Äù for all real 22matrices
with complex eigenvalues.
EXAMPLE 6 IfCDa b
b a
, where aandbare real and not both zero, then the
eigenvalues of CareDabi. (See the Practice Problem at the end of this section.)
Also, if rD jj Dp
a2Cb2, then
CDra=r b=r
b=r a=r
Dr 0
0 rcos' sin'
sin' cos'
where 'is the angle between the positive x-axis and the ray from .0; 0/ through .a; b/ .
See Figure 2 and Appendix B. The angle 'is called the argument ofDaCbi. Thus
the transformation x7!Cxmay be viewed as the composition of a rotation through the
angle 'and a scaling by jj(see Figure 3).
b(a, b)
aœïr
Re zIm z FIGURE 2
Finally, we are ready to uncover the rotation that is hidden within a real matrix
having a complex eigenvalue.
SECOND REVISED PAGES


--- Page 318 ---
5.5 Complex Eigenvalues 301
x2
x1Axx
œïScaling
Rotation
FIGURE 3 A rotation followed by a
scaling.
EXAMPLE 7 LetAD:5 :6
:75 1:1
,D:8 :6i, and v1D 2 4i
5
, as in
Example 2. Also, let Pbe the 22real matrix
PDRev1 Imv1
D 2 4
5 0
and let
CDP 1APD1
200 4
 5 2:5 :6
:75 1:1 2 4
5 0
D:8 :6
:6 :8
By Example 6, Cis a pure rotation because jj2D.:8/2C.:6/2D1. From
CDP 1AP, we obtain
ADP CP 1DP:8 :6
:6 :8
P 1
Here is the rotation ‚Äúinside‚Äù A! The matrix Pprovides a change of variable, say,
xDPu. The action of Aamounts to a change of variable from xtou, followed by
a rotation, and then a return to the original variable. See Figure 4. The rotation produces
an ellipse, as in Figure 1, instead of a circle, because the coordinate system determined
by the columns of Pis not rectangular and does not have equal unit lengths on the two
axes.
P‚Äì1A
PAx
Cu ux
Change o f
variableChange of
variable
C
Rotation
FIGURE 4 Rotation due to a complex eigenvalue.
The next theorem shows that the calculations in Example 7 can be carried out for
any22real matrix Ahaving a complex eigenvalue . The proof uses the fact that if
the entries in Aare real, then A.Rex/DRe.Ax/andA.Imx/DIm.Ax/, and if xis an
eigenvector for a complex eigenvalue, then Re xand Im xare linearly independent in
R2. (See Exercises 25 and 26.) The details are omitted.
T H E O R E M 9 LetAbe a real 22matrix with a complex eigenvalue Da bi(b¬§0) and
an associated eigenvector vinC2. Then
ADPCP 1;where PD¬åRevImv¬çand CDa b
b a
SECOND REVISED PAGES


--- Page 319 ---
302 CHAPTER 5 Eigenvalues and Eigenvectors
The phenomenon displayed in Example 7 persists in higher dimensions. For
w0 w1w2 x2
x1
w10
x3
x0x10
x1x2
FIGURE 5
Iterates of two points under the
action of a 33matrix with a
complex eigenvalue.instance, if Ais a33matrix with a complex eigenvalue, then there is a plane in
R3on which Aacts as a rotation (possibly combined with scaling). Every vector in that
plane is rotated into another point on the same plane. We say that the plane is invariant
under A.
EXAMPLE 8 The matrix AD2
4:8 :6 0
:6 :8 0
0 0 1:073
5has eigenvalues :8:6iand
1.07. Any vector w0in the x1x2-plane (with third coordinate 0) is rotated by Ainto
another point in the plane. Any vector x0not in the plane has its x3-coordinate multiplied
by 1.07. The iterates of the points w0D.2; 0; 0/ andx0D.2; 0; 1/ under multiplication
byAare shown in Figure 5.
PRACTICE PROBLEM
Show that if aandbare real, then the eigenvalues of ADa b
b a
areabi, with
corresponding eigenvectors1
 i
and1
i
.
5.5 EXERCISES
Let each matrix in Exercises 1‚Äì6 act on C2. Find the eigenvalues
and a basis for each eigenspace in C2.
1.1 2
1 3
2.5 5
1 1
3.1 5
 2 3
4.5 2
1 3
5.0 1
 8 4
6.4 3
 3 4
In Exercises 7‚Äì12, use Example 6 to list the eigenvalues of A.
In each case, the transformation x7!Axis the composition of
a rotation and a scaling. Give the angle 'of the rotation, where
  < ' , and give the scale factor r.
7.p
3 1
1p
3
8.p
3 3
 3p
3
9. p
3=2 1=2
 1=2 p
3=2
10. 5 5
5 5
11.:1 :1
 :1 :1
12.0 :3
 :3 0
In Exercises 13‚Äì20, Ô¨Ånd an invertible matrix Pand a matrix
Cof the forma b
b a
such that the given matrix has the
form ADP CP 1. For Exercises 13‚Äì16, use information from
Exercises 1‚Äì4.
13.1 2
1 3
14.5 5
1 115.1 5
 2 3
16.5 2
1 3
17.1 :8
4 2:2
18.1 1
:4 :6
19.1:52 :7
:56 :4
20. 1:64  2:4
1:92 2:2
21. In Example 2, solve the Ô¨Årst equation in (2) for x2in terms of
x1, and from that produce the eigenvector yD2
 1C2i
for the matrix A. Show that this yis a (complex) multiple of
the vector v1used in Example 2.
22. LetAbe a complex (or real) nnmatrix, and let xinCnbe
an eigenvector corresponding to an eigenvalue inC. Show
that for each nonzero complex scalar , the vector xis an
eigenvector of A.
Chapter 7 will focus on matrices Awith the property that ATDA.
Exercises 23 and 24 show that every eigenvalue of such a matrix
is necessarily real.
23. LetAbe an nnreal matrix with the property that ATDA,
letxbe any vector in Cn, and let qDxTAx. The equalities
below show that qis a real number by verifying that qDq.
Give a reason for each step.
qDxTAxDxTAxDxTAxD.xTAx/TDxTATxDq
(a) (b) (c) (d) (e)
SECOND REVISED PAGES


--- Page 320 ---
5.6 Discrete Dynamical Systems 303
24. LetAbe an nnreal matrix with the property that ATDA.
Show that if AxDxfor some nonzero vector xinCn, then,
in fact, is real and the real part of xis an eigenvector of A.
[Hint: Compute xTAx, and use Exercise 23. Also, examine
the real and imaginary parts of Ax.]
25. LetAbe a real nnmatrix, and let xbe a vector in Cn. Show
that Re .Ax/DA.Rex/and Im .Ax/DA.Imx/.
26. LetAbe a real 22matrix with a complex eigenvalue
Da bi(b¬§0) and an associated eigenvector vinC2.
a.Show that A.Rev/DaRevCbImvandA.Imv/D
 bRevCaImv. [Hint: Write vDRevCiImv, and
compute Av.]
b.Verify that if PandCare given as in Theorem 9, then
APDP C.[M] In Exercises 27 and 28, Ô¨Ånd a factorization of the given
matrix Ain the form ADP CP 1, where Cis a block-diagonal
matrix with 22blocks of the form shown in Example 6. (For
each conjugate pair of eigenvalues, use the real and imaginary
parts of one eigenvector in C4to create two columns of P.)
27.2
664:7 1:1 2:0 1:7
 2:0 4:0 8:6 7:4
0 :5 1:0 1:0
1:0 2:8 6:0 5:33
775
28.2
664 1:4 2:0 2:0 2:0
 1:3  :8 :1 :6
:3 1:9 1:6 1:4
2:0 3:3 2:3 2:63
775
SOLUTION TO PRACTICE PROBLEM
Remember that it is easy to test whether a vector is an eigenvector. There is no need to
examine the characteristic equation. Compute
AxDa b
b a1
 i
DaCbi
b ai
D.aCbi/1
 i
Thus1
 i
is an eigenvector corresponding to DaCbi. From the discussion in this
section,1
i
must be an eigenvector corresponding to Da bi.
5.6 DISCRETE DYNAMICAL SYSTEMS
Eigenvalues and eigenvectors provide the key to understanding the long-term behavior,
orevolution , of a dynamical system described by a difference equation xkC1DAxk.
Such an equation was used to model population movement in Section 1.10, various
Markov chains in Section 4.9, and the spotted owl population in the introductory
example for this chapter. The vectors xkgive information about the system as time
(denoted by k) passes. In the spotted owl example, for instance, xklisted the numbers
of owls in three age classes at time k.
The applications in this section focus on ecological problems because they are easier
to state and explain than, say, problems in physics or engineering. However, dynamical
systems arise in many scientiÔ¨Åc Ô¨Åelds. For instance, standard undergraduate courses
in control systems discuss several aspects of dynamical systems. The modern state-
space design method in such courses relies heavily on matrix algebra.1Thesteady-state
response of a control system is the engineering equivalent of what we call here the
‚Äúlong-term behavior‚Äù of the dynamical system xkC1DAxk.
1See G. F. Franklin, J. D. Powell, and A. Emami-Naeimi, Feedback Control of Dynamic Systems , 5th ed.
(Upper Saddle River, NJ: Prentice-Hall, 2006). This undergraduate text has a nice introduction to dynamic
models (Chapter 2). State-space design is covered in Chapters 7 and 8.
SECOND REVISED PAGES


--- Page 321 ---
304 CHAPTER 5 Eigenvalues and Eigenvectors
Until Example 6, we assume that Ais diagonalizable, with nlinearly indepen-
dent eigenvectors, v1; : : : ; vn, and corresponding eigenvalues, 1; : : : ;  n. For conve-
nience, assume the eigenvectors are arranged so that j1j  j2j    j nj. Since
fv1; : : : ; vngis a basis for Rn, any initial vector x0can be written uniquely as
x0Dc1v1C  C cnvn (1)
This eigenvector decomposition ofx0determines what happens to the sequence fxkg.
The next calculation generalizes the simple case examined in Example 5 of Section 5.2.
Since the viare eigenvectors,
x1DAx0Dc1Av1C  C cnAvn
Dc11v1C  C cnnvn
In general,
xkDc1.1/kv1C  C cn.n/kvn .kD0; 1; 2; : : :/ (2)
The examples that follow illustrate what can happen in (2) as k! 1 .
A Predator‚ÄìPrey System
Deep in the redwood forests of California, dusky-footed wood rats provide up to 80% of
the diet for the spotted owl, the main predator of the wood rat. Example 1 uses a linear
dynamical system to model the physical system of the owls and the rats. (Admittedly,
the model is unrealistic in several respects, but it can provide a starting point for the
study of more complicated nonlinear models used by environmental scientists.)
EXAMPLE 1 Denote the owl and wood rat populations at time kbyxkDOk
Rk
,
where kis the time in months, Okis the number of owls in the region studied, and Rk
is the number of rats (measured in thousands). Suppose
OkC1D.:5/O kC.:4/R k
RkC1D  pOkC.1:1/R k(3)
where pis a positive parameter to be speciÔ¨Åed. The .:5/O kin the Ô¨Årst equation says
that with no wood rats for food, only half of the owls will survive each month, while the
.1:1/R kin the second equation says that with no owls as predators, the rat population
will grow by 10% per month. If rats are plentiful, the .:4/R kwill tend to make the
owl population rise, while the negative term  pOkmeasures the deaths of rats due
to predation by owls. (In fact, 1000p is the average number of rats eaten by one owl in
one month.) Determine the evolution of this system when the predation parameter pis
.104.
SOLUTION When pD:104, the eigenvalues of the coefÔ¨Åcient matrix Afor the equa-
tions in (3) turn out to be 1D1:02 and2D:58. Corresponding eigenvectors are
v1D10
13
; v2D5
1
An initial x0can be written as x0Dc1v1Cc2v2. Then, for k0,
xkDc1.1:02/kv1Cc2.:58/kv2
Dc1.1:02/k10
13
Cc2.:58/k5
1
SECOND REVISED PAGES


--- Page 322 ---
5.6 Discrete Dynamical Systems 305
Ask! 1 ,.:58/krapidly approaches zero. Assume c1> 0. Then, for all sufÔ¨Åciently
large k,xkis approximately the same as c1.1:02/kv1, and we write
xkc1.1:02/k10
13
(4)
The approximation in (4) improves as kincreases, and so for large k,
xkC1c1.1:02/kC110
13
D.1:02/c 1.1:02/k10
13
1:02xk (5)
The approximation in (5) says that eventually both entries of xk(the numbers of owls
and rats) grow by a factor of almost 1.02 each month, a 2% monthly growth rate. By
(4),xkis approximately a multiple of .10; 13/ , so the entries in xkare nearly in the same
ratio as 10 to 13. That is, for every 10 owls there are about 13 thousand rats.
Example 1 illustrates two general facts about a dynamical system xkC1DAxkin
which Aisnn, its eigenvalues satisfy j1j 1and1 >jjjforjD2; : : : ; n , and v1
is an eigenvector corresponding to 1. Ifx0is given by equation (1), with c1¬§0, then
for all sufÔ¨Åciently large k,
xkC11xk (6)
and
xkc1.1/kv1 (7)
The approximations in (6) and (7) can be made as close as desired by taking k
sufÔ¨Åciently large. By (6), the xkeventually grow almost by a factor of 1each time,
so1determines the eventual growth rate of the system. Also, by (7), the ratio of any
two entries in xk(for large k) is nearly the same as the ratio of the corresponding entries
inv1. The case in which 1D1is illustrated in Example 5 in Section 5.2.
Graphical Description of Solutions
When Ais22, algebraic calculations can be supplemented by a geometric description
of a system‚Äôs evolution. We can view the equation xkC1DAxkas a description of what
happens to an initial point x0inR2as it is transformed repeatedly by the mapping
x7!Ax. The graph of x0;x1; : : :is called a trajectory of the dynamical system.
EXAMPLE 2 Plot several trajectories of the dynamical system xkC1DAxk, when
AD:80 0
0 :64
SOLUTION The eigenvalues of Aare .8 and .64, with eigenvectors v1D1
0
and
v2D0
1
. Ifx0Dc1v1Cc2v2, then
xkDc1.:8/k1
0
Cc2.:64/k0
1
Of course, xktends to 0because .:8/kand.:64/kboth approach 0 as k! 1 . But the way
xkgoes toward 0is interesting. Figure 1 shows the Ô¨Årst few terms of several trajectories
that begin at points on the boundary of the box with corners at .3;3/. The points on
each trajectory are connected by a thin curve, to make the trajectory easier to see.
SECOND REVISED PAGES


--- Page 323 ---
306 CHAPTER 5 Eigenvalues and Eigenvectors
x2x1
x1x2
x0
x2x1x0
x2x1x0
33
FIGURE 1 The origin as an attractor.
In Example 2, the origin is called an attractor of the dynamical system because
all trajectories tend toward 0. This occurs whenever both eigenvalues are less than 1
in magnitude. The direction of greatest attraction is along the line through 0and the
eigenvector v2for the eigenvalue of smaller magnitude.
In the next example, both eigenvalues of Aare larger than 1 in magnitude, and 0
is called a repeller of the dynamical system. All solutions of xkC1DAxkexcept the
(constant) zero solution are unbounded and tend away from the origin.2
x1x2
FIGURE 2 The origin as a repeller.
EXAMPLE 3 Plot several typical solutions of the equation xkC1DAxk, where
AD1:44 0
0 1:2
2The origin is the only possible attractor or repeller in a linear dynamical system, but there can be multiple
attractors and repellers in a more general dynamical system for which the mapping xk7!xkC1is not linear.
In such a system, attractors and repellers are deÔ¨Åned in terms of the eigenvalues of a special matrix (with
variable entries) called the Jacobian matrix of the system.
SECOND REVISED PAGES


--- Page 324 ---
5.6 Discrete Dynamical Systems 307
SOLUTION The eigenvalues of Aare 1.44 and 1.2. If x0Dc1
c2
, then
xkDc1.1:44/k1
0
Cc2.1:2/k0
1
Both terms grow in size, but the Ô¨Årst term grows faster. So the direction of greatest re-
pulsion is the line through 0and the eigenvector for the eigenvalue of larger magnitude.
Figure 2 shows several trajectories that begin at points quite close to 0.
In the next example, 0is called a saddle point because the origin attracts solutions
from some directions and repels them in other directions. This occurs whenever one
eigenvalue is greater than 1 in magnitude and the other is less than 1 in magnitude. The
direction of greatest attraction is determined by an eigenvector for the eigenvalue of
smaller magnitude. The direction of greatest repulsion is determined by an eigenvector
for the eigenvalue of greater magnitude.
EXAMPLE 4 Plot several typical solutions of the equation ykC1DDyk, where
DD2:0 0
0 0:5
(We write Dandyhere instead of Aandxbecause this example will be used later.)
Show that a solution fykgis unbounded if its initial point is not on the x2-axis.
SOLUTION The eigenvalues of Dare 2 and .5. If y0Dc1
c2
, then
ykDc12k1
0
Cc2.:5/k0
1
(8)
Ify0is on the x2-axis, then c1D0andyk!0ask! 1 . But if y0is not on the x2-axis,
then the Ô¨Årst term in the sum for ykbecomes arbitrarily large, and so fykgis unbounded.
Figure 3 shows ten trajectories that begin near or on the x2-axis.
x3x2x1
x1x2
x0
x3x2
x1
x0
FIGURE 3 The origin as a saddle point.
SECOND REVISED PAGES


--- Page 325 ---
308 CHAPTER 5 Eigenvalues and Eigenvectors
Change of Variable
The preceding three examples involved diagonal matrices. To handle the nondiagonal
case, we return for a moment to the nncase in which eigenvectors of Aform a
basisfv1; : : : ; vngforRn. Let PD¬åv1 vn¬ç, and let Dbe the diagonal matrix
with the corresponding eigenvalues on the diagonal. Given a sequence fxkgsatisfying
xkC1DAxk, deÔ¨Åne a new sequence fykgby
ykDP 1xk;or equivalently ;xkDPyk
Substituting these relations into the equation xkC1DAxkand using the fact that AD
PDP 1, we Ô¨Ånd that
PykC1DAPykD.PDP 1/PykDPDyk
Left-multiplying both sides by P 1, we obtain
ykC1DDyk
If we write ykasy.k/and denote the entries in y.k/byy1.k/; : : : ; y n.k/, then
2
66664y1.kC1/
y2.kC1/
:::
yn.kC1/3
77775D2
666641 0 0
0  2:::
::::::0
0 0  n3
777752
66664y1.k/
y2.k/
:::
yn.k/3
77775
The change of variable from xktoykhasdecoupled the system of difference equations.
The evolution of y1.k/, for example, is unaffected by what happens to y2.k/; : : : ; y n.k/,
because y1.kC1/D1y1.k/for each k.
The equation xkDPyksays that ykis the coordinate vector of xkwith respect to
the eigenvector basis fv1; : : : ; vng. We can decouple the system xkC1DAxkby making
calculations in the new eigenvector coordinate system. When nD2, this amounts to
using graph paper with axes in the directions of the two eigenvectors.
EXAMPLE 5 Show that the origin is a saddle point for solutions of xkC1DAxk,
where
AD1:25  :75
 :75 1:25
Find the directions of greatest attraction and greatest repulsion.
SOLUTION Using standard techniques, we Ô¨Ånd that Ahas eigenvalues 2 and .5, with
corresponding eigenvectors v1D1
 1
andv2D1
1
, respectively. Since j2j> 1and
j:5j< 1, the origin is a saddle point of the dynamical system. If x0Dc1v1Cc2v2, then
xkDc12kv1Cc2.:5/kv2 (9)
This equation looks just like equation (8) in Example 4, with v1andv2in place of the
standard basis.
On graph paper, draw axes through 0and the eigenvectors v1andv2. See Figure 4.
Movement along these axes corresponds to movement along the standard axes in
Figure 3. In Figure 4, the direction of greatest repulsion is the line through 0and the
eigenvector v1whose eigenvalue is greater than 1 in magnitude. If x0is on this line, the
c2in (9) is zero and xkmoves quickly away from 0. The direction of greatest attraction
is determined by the eigenvector v2whose eigenvalue is less than 1 in magnitude.
A number of trajectories are shown in Figure 4. When this graph is viewed in
terms of the eigenvector axes, the picture ‚Äúlooks‚Äù essentially the same as the picture
in Figure 3.
SECOND REVISED PAGES


--- Page 326 ---
5.6 Discrete Dynamical Systems 309
x3x2v2
v1x1
xy
x0x3
x2x1x0
FIGURE 4 The origin as a saddle point.
Complex Eigenvalues
When a real 22matrix Ahas complex eigenvalues, Ais not diagonalizable (when
acting on R2/, but the dynamical system xkC1DAxkis easy to describe. Example 3
of Section 5.5 illustrated the case in which the eigenvalues have absolute value 1. The
iterates of a point x0spiraled around the origin along an elliptical trajectory.
IfAhas two complex eigenvalues whose absolute value is greater than 1, then 0is
a repeller and iterates of x0will spiral outward around the origin. If the absolute values
of the complex eigenvalues are less than 1, then the origin is an attractor and the iterates
ofx0spiral inward toward the origin, as in the following example.
EXAMPLE 6 It can be veriÔ¨Åed that the matrix
AD:8 :5
 :1 1:0
has eigenvalues :9:2i, with eigenvectors12i
1
. Figure 5 shows three trajectories
of the system xkC1DAxk, with initial vectors0
2:5
,3
0
, and0
 2:5
.
Survival of the Spotted Owls
Recall from this chapter‚Äôs introductory example that the spotted owl population in the
Willow Creek area of California was modeled by a dynamical system xkC1DAxkin
which the entries in xkD.jk; sk; ak/listed the numbers of females (at time k) in the
juvenile, subadult, and adult life stages, respectively, and Ais the stage-matrix
AD2
40 0 :33
:18 0 0
0 :71 :943
5 (10)
SECOND REVISED PAGES


--- Page 327 ---
310 CHAPTER 5 Eigenvalues and Eigenvectors
x3x2 x1x3x2x1x1x2
x0x0x3x2x1x0
FIGURE 5 Rotation associated with complex
eigenvalues.
MATLAB shows that the eigenvalues of Aare approximately 1D:98,
2D  :02C:21i, and 3D  :02 :21i. Observe that all three eigenvalues are
less than 1 in magnitude, because j2j2D j3j2D. :02/2C.:21/2D:0445 .
For the moment, let Aact on the complex vector space C3. Then, because Ahas
three distinct eigenvalues, the three corresponding eigenvectors are linearly independent
and form a basis for C3. Denote the eigenvectors by v1,v2, and v3. Then the general
solution of xkC1DAxk(using vectors in C3) has the form
xkDc1.1/kv1Cc2.2/kv2Cc3.3/kv3 (11)
Ifx0is a real initial vector, then x1DAx0is real because Ais real. Similarly, the
equation xkC1DAxkshows that each xkon the left side of (11) is real, even though
it is expressed as a sum of complex vectors. However, each term on the right side
of (11) is approaching the zero vector, because the eigenvalues are all less than 1 in
magnitude. Therefore the real sequence xkapproaches the zero vector, too. Sadly, this
model predicts that the spotted owls will eventually all perish.
Is there hope for the spotted owl? Recall from the introductory example that the
18% entry in the matrix Ain (10) comes from the fact that although 60% of the juvenile
owls live long enough to leave the nest and search for new home territories, only 30%
of that group survive the search and Ô¨Ånd new home ranges. Search survival is strongly
inÔ¨Çuenced by the number of clear-cut areas in the forest, which make the search more
difÔ¨Åcult and dangerous.
Some owl populations live in areas with few or no clear-cut areas. It may be that
a larger percentage of the juvenile owls there survive and Ô¨Ånd new home ranges. Of
course, the problem of the spotted owl is more complex than we have described, but the
Ô¨Ånal example provides a happy ending to the story.
EXAMPLE 7 Suppose the search survival rate of the juvenile owls is 50%, so the
.2; 1/ -entry in the stage-matrix Ain (10) is .3 instead of .18. What does the stage-matrix
model predict about this spotted owl population?
SOLUTION Now the eigenvalues of Aturn out to be approximately 1D1:01,2D
 :03C:26i, and 3D  :03 :26i. An eigenvector for 1is approximately v1D
.10; 3; 31/ . Let v2and v3be (complex) eigenvectors for 2and3. In this case,
SECOND REVISED PAGES


--- Page 328 ---
5.6 Discrete Dynamical Systems 311
equation (11) becomes
xkDc1.1:01/kv1Cc2. :03C:26i/kv2Cc3. :03 :26i/kv3
Ask! 1 , the second two vectors tend to zero. So xkbecomes more and more like
the (real) vector c1.1:01/kv1. The approximations in equations (6) and (7), following
Example 1, apply here. Also, it can be shown that the constant c1in the initial
decomposition of x0is positive when the entries in x0are nonnegative. Thus the owl
population will grow slowly, with a long-term growth rate of 1.01. The eigenvector v1
describes the eventual distribution of the owls by life stages: for every 31 adults, there
will be about 10 juveniles and 3 subadults.
Further Reading
Franklin, G. F., J. D. Powell, and M. L. Workman. Digital Control of Dynamic Systems ,
3rd ed. Reading, MA: Addison-Wesley, 1998.
Sandefur, James T. Discrete Dynamical Systems‚ÄîTheory and Applications . Oxford:
Oxford University Press, 1990.
Tuchinsky, Philip. Management of a Buffalo Herd , UMAP Module 207. Lexington, MA:
COMAP, 1980.
PRACTICE PROBLEMS
1.The matrix Abelow has eigenvalues 1,2
3, and1
3, with corresponding eigenvectors
v1,v2, and v3:
AD1
92
47 2 0
 2 6 2
0 2 53
5;v1D2
4 2
2
13
5;v2D2
42
1
23
5;v3D2
41
2
 23
5
Find the general solution of the equation xkC1DAxkifx0D2
41
11
 23
5.
2.What happens to the sequence fxkgin Practice Problem 1 as k! 1 ?
5.6 EXERCISES
1.LetAbe a 22matrix with eigenvalues 3 and 1=3 and
corresponding eigenvectors v1D1
1
andv2D 1
1
. Let
fxkgbe a solution of the difference equation xkC1DAxk,
x0D9
1
.
a.Compute x1DAx0. [Hint: You do not need to know A
itself.]
b.Find a formula for xkinvolving kand the eigenvectors v1
andv2.2.Suppose the eigenvalues of a 33matrix Aare 3, 4=5, and
3=5, with corresponding eigenvectors2
41
0
 33
5,2
42
1
 53
5, and
2
4 3
 3
73
5. Let x0D2
4 2
 5
33
5. Find the solution of the equation
xkC1DAxkfor the speciÔ¨Åed x0, and describe what happens
ask! 1 .
SECOND REVISED PAGES


--- Page 329 ---
312 CHAPTER 5 Eigenvalues and Eigenvectors
In Exercises 3‚Äì6, assume that any initial vector x0has an eigen-
vector decomposition such that the coefÔ¨Åcient c1in equation (1)
of this section is positive.3
3.Determine the evolution of the dynamical system in Exam-
ple 1 when the predation parameter pis .2 in equation (3).
(Give a formula for xk:/Does the owl population grow or
decline? What about the wood rat population?
4.Determine the evolution of the dynamical system in Example
1 when the predation parameter pis:125. (Give a formula
forxk.) As time passes, what happens to the sizes of the owl
and wood rat populations? The system tends toward what is
sometimes called an unstable equilibrium. What do you think
might happen to the system if some aspect of the model (such
as birth rates or the predation rate) were to change slightly?
5.In old-growth forests of Douglas Ô¨År, the spotted owl dines
mainly on Ô¨Çying squirrels. Suppose the predator‚Äìprey matrix
for these two populations is AD:4 :3
 p 1:2
. Show that
if the predation parameter pis .325, both populations grow.
Estimate the long-term growth rate and the eventual ratio of
owls to Ô¨Çying squirrels.
6.Show that if the predation parameter pin Exercise 5 is .5,
both the owls and the squirrels will eventually perish. Find a
value of pfor which populations of both owls and squirrels
tend toward constant levels. What are the relative population
sizes in this case?
7.LetAhave the properties described in Exercise 1.
a.Is the origin an attractor, a repeller, or a saddle point of
the dynamical system xkC1DAxk?
b.Find the directions of greatest attraction and/or repulsion
for this dynamical system.
c.Make a graphical description of the system, showing
the directions of greatest attraction or repulsion. Include
a rough sketch of several typical trajectories (without
computing speciÔ¨Åc points).
8.Determine the nature of the origin (attractor, repeller, or
saddle point) for the dynamical system xkC1DAxkifAhas
the properties described in Exercise 2. Find the directions of
greatest attraction or repulsion.
In Exercises 9‚Äì14, classify the origin as an attractor, repeller,
or saddle point of the dynamical system xkC1DAxk. Find the
directions of greatest attraction and/or repulsion.
9.AD1:7 :3
 1:2 :8
10.AD:3 :4
 :3 1:1
3One of the limitations of the model in Example 1 is that there always exist
initial population vectors x0with positive entries such that the coefÔ¨Åcient
c1is negative. The approximation (7) is still valid, but the entries in xk
eventually become negative.11.AD:4 :5
 :4 1:3
12.AD:5 :6
 :3 1:4
13.AD:8 :3
 :4 1:5
14.AD1:7 :6
 :4 :7
15. LetAD2
4:4 0 :2
:3 :8 :3
:3 :2 :53
5. The vector v1D2
4:1
:6
:33
5is an
eigenvector for A, and two eigenvalues are .5 and .2. Con-
struct the solution of the dynamical system xkC1DAxkthat
satisÔ¨Åes x0D.0; :3; :7/ . What happens to xkask! 1 ?
16. [M] Produce the general solution of the dynamical system
xkC1DAxkwhen Ais the stochastic matrix for the Hertz
Rent A Car model in Exercise 16 of Section 4.9.
17. Construct a stage-matrix model for an animal species that has
two life stages: juvenile (up to 1 year old) and adult. Suppose
the female adults give birth each year to an average of 1.6
female juveniles. Each year, 30% of the juveniles survive
to become adults and 80% of the adults survive. For k0,
letxkD.jk; ak/, where the entries in xkare the numbers of
female juveniles and female adults in year k.
a.Construct the stage-matrix Asuch that xkC1DAxkfor
k0.
b.Show that the population is growing, compute the even-
tual growth rate of the population, and give the eventual
ratio of juveniles to adults.
c.[M] Suppose that initially there are 15 juveniles and 10
adults in the population. Produce four graphs that show
how the population changes over eight years: (a) the
number of juveniles, (b) the number of adults, (c) the
total population, and (d) the ratio of juveniles to adults
(each year). When does the ratio in (d) seem to stabilize?
Include a listing of the program or keystrokes used to
produce the graphs for (c) and (d).
18. A herd of American buffalo (bison) can be modeled by a stage
matrix similar to that for the spotted owls. The females can be
divided into calves (up to 1 year old), yearlings (1 to 2 years),
and adults. Suppose an average of 42 female calves are
born each year per 100 adult females. (Only adults produce
offspring.) Each year, about 60% of the calves survive, 75%
of the yearlings survive, and 95% of the adults survive. For
k0, let xkD.ck; yk; ak/, where the entries in xkare the
numbers of females in each life stage at year k.
a.Construct the stage-matrix Afor the buffalo herd, such
thatxkC1DAxkfork0.
b.[M] Show that the buffalo herd is growing, determine
the expected growth rate after many years, and give the
expected numbers of calves and yearlings present per 100
adults.
SECOND REVISED PAGES


--- Page 330 ---
5.7 Applications to Differential Equations 313
SOLUTIONS TO PRACTICE PROBLEMS
1.The Ô¨Årst step is to write x0as a linear combination of v1,v2, and v3. Row reduction
of¬åv1v2v3x0¬çproduces the weights c1D2,c2D1, and c3D3, so that
x0D2v1C1v2C3v3
Since the eigenvalues are 1,2
3, and1
3, the general solution is
xkD21kv1C12
3k
v2C31
3k
v3
D22
4 2
2
13
5C2
3k2
42
1
23
5C31
3k2
41
2
 23
5 (12)
2.Ask! 1 , the second and third terms in (12) tend to the zero vector, and
xkD2v1C2
3k
v2C31
3k
v3!2v1D2
4 4
4
23
5
5.7 APPLICATIONS TO DIFFERENTIAL EQUATIONS
This section describes continuous analogues of the difference equations studied in
Section 5.6. In many applied problems, several quantities are varying continuously in
time, and they are related by a system of differential equations:
x0
1Da11x1C  C a1nxn
x0
2Da21x1C  C a2nxn
:::
x0
nDan1x1C  C annxn
Here x1; : : : ; x nare differentiable functions of t, with derivatives x0
1; : : : ; x0
n, and the aij
are constants. The crucial feature of this system is that it is linear . To see this, write the
system as a matrix differential equation
x0.t/DAx.t/ (1)
where
x.t/D2
64x1.t/
:::
xn.t/3
75;x0.t/D2
64x0
1.t/
:::
x0
n.t/3
75;and AD2
64a11 a1n
::::::
an1 ann3
75
Asolution of equation (1) is a vector-valued function that satisÔ¨Åes (1) for all tin some
interval of real numbers, such as t0.
Equation (1) is linear because both differentiation of functions and multiplication of
vectors by a matrix are linear transformations. Thus, if uandvare solutions of x0DAx,
thencuCdvis also a solution, because
.cuCdv/0Dcu0Cdv0
DcAuCdAvDA.cuCdv/
SECOND REVISED PAGES


--- Page 331 ---
314 CHAPTER 5 Eigenvalues and Eigenvectors
(Engineers call this property superposition of solutions.) Also, the identically zero
function is a (trivial) solution of (1). In the terminology of Chapter 4, the set of all
solutions of (1) is a subspace of the set of all continuous functions with values in Rn.
Standard texts on differential equations show that there always exists what is called
afundamental set of solutions to (1). If Aisnn, then there are nlinearly independent
functions in a fundamental set, and each solution of (1) is a unique linear combination
of these nfunctions. That is, a fundamental set of solutions is a basis for the set of
all solutions of (1), and the solution set is an n-dimensional vector space of functions.
If a vector x0is speciÔ¨Åed, then the initial value problem is to construct the (unique)
function xsuch that x0DAxandx.0/Dx0.
When Ais a diagonal matrix, the solutions of (1) can be produced by elementary
calculus. For instance, consider
"
x0
1.t/
x0
2.t/#
D"
3 0
0 5#"
x1.t/
x2.t/#
(2)
that is,
x0
1.t/ D3x1.t/
x0
2.t/D  5x2.t/(3)
The system (2) is said to be decoupled because each derivative of a function depends
only on the function itself, not on some combination or ‚Äúcoupling‚Äù of both x1.t/and
x2.t/. From calculus, the solutions of (3) are x1.t/Dc1e3tandx2.t/Dc2e 5t, for any
constants c1andc2. Each solution of equation (2) can be written in the form
x1.t/
x2.t/
Dc1e3t
c2e 5t
Dc11
0
e3tCc20
1
e 5t
This example suggests that for the general equation x0DAx, a solution might be a
linear combination of functions of the form
x.t/Dvet(4)
for some scalar and some Ô¨Åxed nonzero vector v. [If vD0, the function x.t/is
identically zero and hence satisÔ¨Åes x0DAx.] Observe that
x0.t/DvetBy calculus, since vis a constant vector
Ax.t/DAvetMultiplying both sides of (4) by A
Since etis never zero, x0.t/will equal Ax.t/if and only if vDAv, that is, if and only
ifis an eigenvalue of Aandvis a corresponding eigenvector. Thus each eigenvalue‚Äì
eigenvector pair provides a solution (4) of x0DAx. Such solutions are sometimes called
eigenfunctions of the differential equation. Eigenfunctions provide the key to solving
systems of differential equations.
EXAMPLE 1 The circuit in Figure 1 can be described by the differential equation
"
x0
1.t/
x0
2.t/#
D"
 .1=R 1C1=R 2/=C 1 1=.R 2C1/
1=.R 2C2/  1=.R 2C2/#"
x1.t/
x2.t/#
where x1.t/andx2.t/are the voltages across the two capacitors at time t. Suppose
resistor R1is 1 ohm, R2is 2 ohms, capacitor C1is 1 farad, and C2is .5 farad, and
suppose there is an initial charge of 5 volts on capacitor C1and 4 volts on capacitor C2.
Find formulas for x1.t/andx2.t/that describe how the voltages change over time.
R1
R2C1
C2+
+ FIGURE 1
SECOND REVISED PAGES


--- Page 332 ---
5.7 Applications to Differential Equations 315
SOLUTION LetAdenote the matrix displayed above, and let x.t/Dx1.t/
x2.t/
. For the
data given, AD 1:5 :5
1 1
, and x.0/D5
4
. The eigenvalues of Aare1D  :5
and2D  2, with corresponding eigenvectors
v1D1
2
and v2D 1
1
The eigenfunctions x1.t/Dv1e1tandx2.t/Dv2e2tboth satisfy x0DAx, and so does
any linear combination of x1andx2. Set
x.t/Dc1v1e1tCc2v2e2tDc11
2
e :5tCc2 1
1
e 2t
and note that x.0/Dc1v1Cc2v2. Since v1andv2are obviously linearly independent
and hence span R2,c1andc2can be found to make x.0/equal to x0. In fact, the equation
c11
2
6v1Cc2 1
1
6v2D5
4
6x0
leads easily to c1D3andc2D  2. Thus the desired solution of the differential equation
x0DAxis
x.t/D31
2
e :5t 2 1
1
e 2t
orx1.t/
x2.t/
D"
3e :5tC2e 2t
6e :5t 2e 2t#
Figure 2 shows the graph, or trajectory , ofx.t/, fort0, along with trajectories for
some other initial points. The trajectories of the two eigenfunctions x1andx2lie in the
eigenspaces of A.
The functions x1and x2both decay to zero as t! 1 , but the values of x2
decay faster because its exponent is more negative. The entries in the corresponding
eigenvector v2show that the voltages across the capacitors will decay to zero as rapidly
as possible if the initial voltages are equal in magnitude but opposite in sign.
54x0
v2v1
FIGURE 2 The origin as an attractor.
SECOND REVISED PAGES


--- Page 333 ---
316 CHAPTER 5 Eigenvalues and Eigenvectors
In Figure 2, the origin is called an attractor , orsink, of the dynamical system
because all trajectories are drawn into the origin. The direction of greatest attraction
is along the trajectory of the eigenfunction x2(along the line through 0and v2/
corresponding to the more negative eigenvalue, D  2. Trajectories that begin at points
not on this line become asymptotic to the line through 0andv1because their components
in the v2direction decay so rapidly.
If the eigenvalues in Example 1 were positive instead of negative, the corresponding
trajectories would be similar in shape, but the trajectories would be traversed away from
the origin. In such a case, the origin is called a repeller , orsource , of the dynamical
system, and the direction of greatest repulsion is the line containing the trajectory of the
eigenfunction corresponding to the more positive eigenvalue.
EXAMPLE 2 Suppose a particle is moving in a planar force Ô¨Åeld and its position
vector xsatisÔ¨Åes x0DAxandx.0/Dx0, where
AD4 5
 2 1
; x0D2:9
2:6
Solve this initial value problem for t0, and sketch the trajectory of the particle.
SOLUTION The eigenvalues of Aturn out to be 1D6and2D  1, with correspond-
ing eigenvectors v1D. 5; 2/ andv2D.1; 1/ . For any constants c1andc2, the function
x.t/Dc1v1e1tCc2v2e2tDc1 5
2
e6tCc21
1
e t
is a solution of x0DAx. We want c1andc2to satisfy x.0/Dx0, that is,
c1 5
2
Cc21
1
D2:9
2:6
or 5 1
2 1c1
c2
D2:9
2:6
Calculations show that c1D  3=70 andc2D188=70 , and so the desired function is
x.t/D 3
70 5
2
e6tC188
701
1
e t
Trajectories of xand other solutions are shown in Figure 3.
In Figure 3, the origin is called a saddle point of the dynamical system because
some trajectories approach the origin at Ô¨Årst and then change direction and move away
from the origin. A saddle point arises whenever the matrix Ahas both positive and
negative eigenvalues. The direction of greatest repulsion is the line through v1and0,
corresponding to the positive eigenvalue. The direction of greatest attraction is the line
through v2and0, corresponding to the negative eigenvalue.
x0
v2v1
FIGURE 3 The origin as a saddle point.
SECOND REVISED PAGES


--- Page 334 ---
5.7 Applications to Differential Equations 317
Decoupling a Dynamical System
The following discussion shows that the method of Examples 1 and 2 produces a
fundamental set of solutions for any dynamical system described by x0DAxwhen A
isnnand has nlinearly independent eigenvectors, that is, when Ais diagonalizable.
Suppose the eigenfunctions for Aare
v1e1t; : : : ; vnent
with v1; : : : ; vnlinearly independent eigenvectors. Let PD¬åv1 vn¬ç, and let Dbe
the diagonal matrix with entries 1; : : : ;  n, so that ADPDP 1. Now make a change
of variable , deÔ¨Åning a new function yby
y.t/DP 1x.t/ or, equivalently ;x.t/DPy.t/
The equation x.t/DPy.t/says that y.t/is the coordinate vector of x.t/relative to the
eigenvector basis. Substitution of Pyforxin the equation x0DAxgives
d
dt.Py/DA.P y/D.PDP 1/PyDPDy (5)
Since Pis a constant matrix, the left side of (5) is Py0. Left-multiply both sides of (5)
byP 1and obtain y0DDy, or
2
66664y0
1.t/
y0
2.t/
:::
y0
n.t/3
77775D2
666641 0 0
0  2:::
::::::0
0 0  n3
777752
66664y1.t/
y2.t/
:::
yn.t/3
77775
The change of variable from xtoyhasdecoupled the system of differential equations,
because the derivative of each scalar function ykdepends only on yk. (Review the anal-
ogous change of variables in Section 5.6.) Since y0
1D1y1, we have y1.t/Dc1e1t,
with similar formulas for y2; : : : ; y n. Thus
y.t/D2
64c1e1t
:::
cnent3
75;where2
64c1
:::
cn3
75Dy.0/DP 1x.0/DP 1x0
To obtain the general solution xof the original system, compute
x.t/DPy.t/D¬åv1 vn¬çy.t/
Dc1v1e1tC  C cnvnent
This is the eigenfunction expansion constructed as in Example 1.
Complex Eigenvalues
In the next example, a real matrix Ahas a pair of complex eigenvalues and, with
associated complex eigenvectors vandv. (Recall from Section 5.5 that for a real matrix,
complex eigenvalues and associated eigenvectors come in conjugate pairs.) So two
solutions of x0DAxare
x1.t/Dvetand x2.t/Dvet(6)
It can be shown that x2.t/Dx1.t/by using a power series representation for the
complex exponential function. Although the complex eigenfunctions x1andx2are
convenient for some calculations (particularly in electrical engineering), real functions
are more appropriate for many purposes. Fortunately, the real and imaginary parts of x1
SECOND REVISED PAGES


--- Page 335 ---
318 CHAPTER 5 Eigenvalues and Eigenvectors
are (real) solutions of x0DAx, because they are linear combinations of the solutions
in (6):
Re.vet/D1
2¬åx1.t/Cx1.t/¬ç; Im.vet/D1
2i¬åx1.t/ x1.t/¬ç
To understand the nature of Re .vet/, recall from calculus that for any number x,
the exponential function excan be computed from the power series:
exD1CxC1
2¬äx2C  C1
n¬äxnC 
This series can be used to deÔ¨Åne etwhen is complex:
etD1C.t/C1
2¬ä.t/2C  C1
n¬ä.t/nC 
By writing DaCbi(with aandbreal), and using similar power series for the cosine
and sine functions, one can show that
e.aCbi/tDeateibtDeat.cosbtCisinbt/ (7)
Hence
vetD.RevCiImv/eat.cosbtCisinbt/
D¬å.Rev/cosbt .Imv/sinbt¬çeat
Ci¬å.Rev/sinbtC.Imv/cosbt¬çeat
So two real solutions of x0DAxare
y1.t/DRex1.t/D¬å.Rev/cosbt .Imv/sinbt¬ç eat
y2.t/DImx1.t/D¬å.Rev/sinbtC.Imv/cosbt¬ç eat
It can be shown that y1andy2are linearly independent functions (when b¬§0).1
EXAMPLE 3 The circuit in Figure 4 can be described by the equation
R1
R2C
L+
iL
FIGURE 4"
i0
L
v0
C#
D"
 R2=L  1=L
1=C  1=.R 1C /#"
iL
vC#
where iLis the current passing through the inductor LandvCis the voltage drop across
the capacitor C. Suppose R1is 5 ohms, R2is .8 ohm, Cis .1 farad, and Lis .4 henry.
Find formulas for iLandvC, if the initial current through the inductor is 3 amperes and
the initial voltage across the capacitor is 3 volts.
SOLUTION For the data given, AD 2 2:5
10 2
andx0D3
3
. The method dis-
cussed in Section 5.5 produces the eigenvalue D  2C5iand the corresponding
eigenvector v1Di
2
. The complex solutions of x0DAxare complex linear combi-
nations of
x1.t/Di
2
e. 2C5i/tand x2.t/D i
2
e. 2 5i/t
1Since x2.t/is the complex conjugate of x1.t/, the real and imaginary parts of x2.t/arey1.t/and y2.t/,
respectively. Thus one can use either x1.t/orx2.t/, but not both, to produce two real linearly independent
solutions of x0DAx.
SECOND REVISED PAGES


--- Page 336 ---
5.7 Applications to Differential Equations 319
Next, use equation (7) to write
x1.t/Di
2
e 2t.cos5tCisin5t/
The real and imaginary parts of x1provide real solutions:
y1.t/D sin5t
2cos5t
e 2t; y2.t/Dcos5t
2sin5t
e 2t
Since y1andy2are linearly independent functions, they form a basis for the two-
dimensional real vector space of solutions of x0DAx. Thus the general solution is
x.t/Dc1 sin5t
2cos5t
e 2tCc2cos5t
2sin5t
e 2t
To satisfy x.0/D3
3
, we need c10
2
Cc21
0
D3
3
, which leads to c1D1:5and
c2D3. Thus
x.t/D1:5 sin5t
2cos5t
e 2tC3cos5t
2sin5t
e 2t
or iL.t/
vC.t/
D 1:5sin5tC3cos5t
3cos5tC6sin5t
e 2t
See Figure 5.
x0FIGURE 5
The origin as a spiral point.
In Figure 5, the origin is called a spiral point of the dynamical system. The rotation
is caused by the sine and cosine functions that arise from a complex eigenvalue. The
trajectories spiral inward because the factor e 2ttends to zero. Recall that  2is the real
part of the eigenvalue in Example 3. When Ahas a complex eigenvalue with positive
real part, the trajectories spiral outward. If the real part of the eigenvalue is zero, the
trajectories form ellipses around the origin.
PRACTICE PROBLEMS
A real 33matrix Ahas eigenvalues  :5,:2C:3i, and :2 :3i, with corresponding
eigenvectors
v1D2
41
 2
13
5;v2D2
41C2i
4i
23
5;and v3D2
41 2i
 4i
23
5
1.IsAdiagonalizable as ADPDP 1, using complex matrices?
2.Write the general solution of x0DAxusing complex eigenfunctions, and then Ô¨Ånd
the general real solution.
3.Describe the shapes of typical trajectories.
5.7 EXERCISES
1.A particle moving in a planar force Ô¨Åeld has a position vector
xthat satisÔ¨Åes x0DAx. The 22matrix Ahas eigenvalues
4 and 2, with corresponding eigenvectors v1D 3
1
andv2D 1
1
. Find the position of the particle at time t,
assuming that x.0/D 6
1
.
SECOND REVISED PAGES


--- Page 337 ---
320 CHAPTER 5 Eigenvalues and Eigenvectors
2.LetAbe a22matrix with eigenvalues  3and 1and
corresponding eigenvectors v1D 1
1
andv2D1
1
. Let
x.t/be the position of a particle at time t. Solve the initial
value problem x0DAx,x.0/D2
3
.
In Exercises 3‚Äì6, solve the initial value problem x0.t/DAx.t/
fort0, with x.0/D.3; 2/ . Classify the nature of the origin
as an attractor, repeller, or saddle point of the dynamical system
described by x0DAx. Find the directions of greatest attraction
and/or repulsion. When the origin is a saddle point, sketch typical
trajectories.
3.AD2 3
 1 2
4.AD 2 5
1 4
5.AD7 1
3 3
6.AD1 2
3 4
In Exercises 7 and 8, make a change of variable that decouples the
equation x0DAx. Write the equation x.t/DPy.t/and show the
calculation that leads to the uncoupled system y0DDy, specify-
ingPandD.
7.Aas in Exercise 5 8.Aas in Exercise 6
In Exercises 9‚Äì18, construct the general solution of x0DAx
involving complex eigenfunctions and then obtain the general real
solution. Describe the shapes of typical trajectories.
9.AD 3 2
 1 1
10.AD3 1
 2 1
11.AD 3 9
2 3
12.AD 7 10
 4 5
13.AD4 3
6 2
14.AD 2 1
 8 2
15. [M]AD2
4 8 12 6
2 1 2
7 12 53
516. [M]AD2
4 6 11 16
2 5  4
 4 5 103
5
17. [M]AD2
430 64 23
 11 23 9
6 15 43
5
18. [M]AD2
453 30 2
90 52 3
20 10 23
5
19. [M] Find formulas for the voltages v1andv2(as functions of
timet) for the circuit in Example 1, assuming that R1D1=5
ohm, R2D1=3ohm, C1D4farads, C2D3farads, and the
initial charge on each capacitor is 4 volts.
20. [M] Find formulas for the voltages v1andv2for the circuit in
Example 1, assuming that R1D1=15 ohm, R2D1=3ohm,
C1D9farads, C2D2farads, and the initial charge on each
capacitor is 3 volts.
21. [M] Find formulas for the current iLand the voltage vC
for the circuit in Example 3, assuming that R1D1ohm,
R2D:125 ohm, CD:2farad, LD:125 henry, the initial
current is 0 amp, and the initial voltage is 15 volts.
22. [M] The circuit in the Ô¨Ågure is described by the equation
"
i0
L
v0
C#
D"
0 1=L
 1=C  1=.RC /#"
iL
vC#
where iLis the current through the inductor LandvCis the
voltage drop across the capacitor C. Find formulas for iL
andvCwhen RD:5ohm, CD2:5farads, LD:5henry,
the initial current is 0 amp, and the initial voltage is 12 volts.
R
C+
L
SOLUTIONS TO PRACTICE PROBLEMS
1.Yes, the 33matrix is diagonalizable because it has three distinct eigenvalues.
Theorem 2 in Section 5.1 and Theorem 5 in Section 5.3 are valid when complex
scalars are used. (The proofs are essentially the same as for real scalars.)
2.The general solution has the form
x.t/Dc12
41
 2
13
5e :5tCc22
41C2i
4i
23
5e.:2C:3i/tCc32
41 2i
 4i
23
5e.:2 :3i/t
The scalars c1,c2, and c3here can be any complex numbers. The Ô¨Årst term in x.t/is
real. Two more real solutions can be produced using the real and imaginary parts of
SECOND REVISED PAGES


--- Page 338 ---
5.8 Iterative Estimates for Eigenvalues 321
the second term in x.t/:
2
41C2i
4i
23
5e:2t.cos:3tCisin:3t/
The general real solution has the following form, with realscalars c1,c2, and c3:
c12
41
 2
13
5e :5tCc22
4cos:3t 2sin:3t
 4sin:3t
2cos:3t3
5e:2tCc32
4sin:3tC2cos:3t
4cos:3t
2sin:3t3
5e:2t
3.Any solution with c2Dc3D0is attracted to the origin because of the negative
exponential factor. Other solutions have components that grow without bound, and
the trajectories spiral outward.
Be careful not to mistake this problem for one in Section 5.6. There the condition
for attraction toward 0was that an eigenvalue be less than 1 in magnitude, to make
jjk!0. Here the real part of the eigenvalue must be negative, to make et!0.
5.8 ITERATIVE ESTIMATES FOR EIGENVALUES
In scientiÔ¨Åc applications of linear algebra, eigenvalues are seldom known precisely.
Fortunately, a close numerical approximation is usually quite satisfactory. In fact, some
applications require only a rough approximation to the largest eigenvalue. The Ô¨Årst
algorithm described below can work well for this case. Also, it provides a foundation
for a more powerful method that can give fast estimates for other eigenvalues as well.
The Power Method
The power method applies to an nnmatrix Awith a strictly dominant eigenvalue
1, which means that 1must be larger in absolute value than all the other eigenvalues.
In this case, the power method produces a scalar sequence that approaches 1and a
vector sequence that approaches a corresponding eigenvector. The background for the
method rests on the eigenvector decomposition used at the beginning of Section 5.6.
Assume for simplicity that Ais diagonalizable and Rnhas a basis of eigenvectors
v1; : : : ; vn, arranged so their corresponding eigenvalues 1; : : : ;  ndecrease in size, with
the strictly dominant eigenvalue Ô¨Årst. That is,
j1j>j2j  j3j    j nj
Strictly larger-(1)
As we saw in equation (2) of Section 5.6, if xinRnis written as xDc1v1C  C cnvn,
then
AkxDc1.1/kv1Cc2.2/kv2C  C cn.n/kvn.kD1; 2; : : :/
Assume c1¬§0. Then, dividing by .1/k,
1
.1/kAkxDc1v1Cc22
1k
v2C  C cnn
1k
vn.kD1; 2; : : :/ (2)
From inequality (1), the fractions 2=1; : : : ;  n=1are all less than 1 in magnitude and
so their powers go to zero. Hence
.1/ kAkx!c1v1ask! 1 (3)
SECOND REVISED PAGES


--- Page 339 ---
322 CHAPTER 5 Eigenvalues and Eigenvectors
Thus, for large k, a scalar multiple of Akxdetermines almost the same direction as the
eigenvector c1v1. Since positive scalar multiples do not change the direction of a vector,
Akxitself points almost in the same direction as v1or v1, provided c1¬§0.
EXAMPLE 1 LetAD1:8 :8
:2 1:2
,v1D4
1
, and xD :5
1
. Then Ahas
eigenvalues 2 and 1, and the eigenspace for 1D2is the line through 0andv1. For
kD0; : : : ; 8 , compute Akxand construct the line through 0andAkx. What happens as
kincreases?
SOLUTION The Ô¨Årst three calculations are
AxD1:8 :8
:2 1:2 :5
1
D :1
1:1
A2xDA.Ax/D1:8 :8
:2 1:2 :1
1:1
D:7
1:3
A3xDA.A2x/D1:8 :8
:2 1:2:7
1:3
D2:3
1:7
Analogous calculations complete Table 1.
TABLE 1 Iterates of a Vector
k 0 1 2 3 4 5 6 7 8
Akx :5
1  :1
1:1 :7
1:3 2:3
1:7 5:5
2:5 11:9
4:1 24:7
7:3 50:3
13:7 101:5
26:5
The vectors x,Ax; : : : ; A4xare shown in Figure 1. The other vectors are growing
too long to display. However, line segments are drawn showing the directions of those
vectors. In fact, the directions of the vectors are what we really want to see, not the vec-
tors themselves. The lines seem to be approaching the line representing the eigenspace
spanned by v1. More precisely, the angle between the line (subspace) determined by
Akxand the line (eigenspace) determined by v1goes to zero as k! 1 .
1EigenspaceA4x
A3xA2xAx
x
10x1x2
v1
4 1
FIGURE 1 Directions determined by x,Ax,A2x; : : : ; A7x.
The vectors .1/ kAkxin (3) are scaled to make them converge to c1v1, provided
c1¬§0. We cannot scale Akxin this way because we do not know 1. But we can scale
eachAkxto make its largest entry a 1. It turns out that the resulting sequence fxkgwill
converge to a multiple of v1whose largest entry is 1. Figure 2 shows the scaled sequence
SECOND REVISED PAGES


--- Page 340 ---
5.8 Iterative Estimates for Eigenvalues 323
for Example 1. The eigenvalue 1can be estimated from the sequence fxkg, too. When
xkis close to an eigenvector for 1, the vector Axkis close to 1xk, with each entry in
Axkapproximately 1times the corresponding entry in xk. Because the largest entry in
xkis 1, the largest entry in Axkis close to 1. (Careful proofs of these statements are
omitted.)
EigenspaceA3x
A2x
Ax
x1x2x3
x4
x1x2
4 1Multiple of v112
x = x0
FIGURE 2 Scaled multiples of x,Ax,A2x; : : : ; A7x.
THE POWER METHOD FOR ESTIMATING A STRICTLY DOMINANT EIGENVALUE
1.Select an initial vector x0whose largest entry is 1.
2.ForkD0; 1; : : : ;
a.Compute Axk.
b.Letkbe an entry in Axkwhose absolute value is as large as possible.
c.Compute xkC1D.1= k/Axk.
3.For almost all choices of x0, the sequence fkgapproaches the dominant
eigenvalue, and the sequence fxkgapproaches a corresponding eigenvector.
EXAMPLE 2 Apply the power method to AD6 5
1 2
with x0D0
1
. Stop
when kD5, and estimate the dominant eigenvalue and a corresponding eigenvector
ofA.
SOLUTION Calculations in this example and the next were made with MATLAB,
which computes with 16-digit accuracy, although we show only a few signiÔ¨Åcant Ô¨Ågures
here. To begin, compute Ax0and identify the largest entry 0inAx0:
Ax0D6 5
1 20
1
D5
2
;  0D5
Scale Ax0by1= 0to get x1, compute Ax1, and identify the largest entry in Ax1:
x1D1
0Ax0D1
55
2
D1
:4
Ax1D6 5
1 21
:4
D8
1:8
;  1D8
Scale Ax1by1= 1to get x2, compute Ax2, and identify the largest entry in Ax2:
x2D1
1Ax1D1
88
1:8
D1
:225
Ax2D6 5
1 21
:225
D7:125
1:450
;  2D7:125
SECOND REVISED PAGES


--- Page 341 ---
324 CHAPTER 5 Eigenvalues and Eigenvectors
Scale Ax2by1= 2to get x3, and so on. The results of MATLAB calculations for the
Ô¨Årst Ô¨Åve iterations are arranged in Table 2.
TABLE 2 The Power Method for Example 2
k 0 1 2 3 4 5
xk0
1 1
:4 1
:225 1
:2035 1
:2005 1
:20007
Axk5
2 8
1:8 7:125
1:450 7:0175
1:4070 7:0025
1:4010 7:00036
1:40014
k 5 8 7.125 7.0175 7.0025 7.00036
The evidence from Table 2 strongly suggests that fxkgapproaches .1; :2/ andfkg
approaches 7. If so, then .1; :2/ is an eigenvector and 7 is the dominant eigenvalue. This
is easily veriÔ¨Åed by computing
A1
:2
D6 5
1 21
:2
D7
1:4
D71
:2
The sequence fkgin Example 2 converged quickly to 1D7because the second
eigenvalue of Awas much smaller. (In fact, 2D1.) In general, the rate of convergence
depends on the ratio j2=1j, because the vector c2.2=1/kv2in equation (2) is the main
source of error when using a scaled version of Akxas an estimate of c1v1. (The other
fractions j=1are likely to be smaller.) If j2=1jis close to 1, then fkgandfxkgcan
converge very slowly, and other approximation methods may be preferred.
With the power method, there is a slight chance that the chosen initial vector x
will have no component in the v1direction (when c1D0). But computer rounding
errors during the calculations of the xkare likely to create a vector with at least a small
component in the direction of v1. If that occurs, the xkwill start to converge to a multiple
ofv1.
The Inverse Power Method
This method provides an approximation for anyeigenvalue, provided a good initial
estimate of the eigenvalue is known. In this case, we let BD.A I/ 1and apply
the power method to B. It can be shown that if the eigenvalues of Aare1; : : : ;  n, then
the eigenvalues of Bare
1
1 ;1
2 ; : : : ;1
n 
and the corresponding eigenvectors are the same as those for A. (See Exercises 15 and
16.)
Suppose, for example, that is closer to 2than to the other eigenvalues of A.
Then 1=. 2 /will be a strictly dominant eigenvalue of B. Ifis really close to 2,
then1=. 2 /ismuch larger than the other eigenvalues of B, and the inverse power
method produces a very rapid approximation to 2for almost all choices of x0. The
following algorithm gives the details.
SECOND REVISED PAGES


--- Page 342 ---
5.8 Iterative Estimates for Eigenvalues 325
THE INVERSE POWER METHOD FOR ESTIMATING AN EIGENVALUE OFA
1.Select an initial estimate sufÔ¨Åciently close to .
2.Select an initial vector x0whose largest entry is 1.
3.ForkD0; 1; : : : ;
a.Solve .A I/ykDxkforyk.
b.Letkbe an entry in ykwhose absolute value is as large as possible.
c.Compute kDC.1= k/.
d.Compute xkC1D.1= k/yk.
4.For almost all choices of x0, the sequence fkgapproaches the eigenvalue 
ofA, and the sequence fxkgapproaches a corresponding eigenvector.
Notice that B, or rather .A I/ 1, does not appear in the algorithm. Instead of
computing .A I/ 1xkto get the next vector in the sequence, it is better to solve
the equation .A I/ykDxkforyk(and then scale ykto produce xkC1/. Since this
equation for ykmust be solved for each k, an LU factorization of A Iwill speed up
the process.
EXAMPLE 3 It is not uncommon in some applications to need to know the smallest
eigenvalue of a matrix Aand to have at hand rough estimates of the eigenvalues.
Suppose 21, 3.3, and 1.9 are estimates for the eigenvalues of the matrix Abelow. Find
the smallest eigenvalue, accurate to six decimal places.
AD2
410 8 4
 8 13 4
 4 5 43
5
SOLUTION The two smallest eigenvalues seem close together, so we use the inverse
power method for A 1:9I . Results of a MATLAB calculation are shown in Table 3.
Here x0was chosen arbitrarily, ykD.A 1:9I/ 1xk,kis the largest entry in yk,
kD1:9C1= k, and xkC1D.1= k/yk. As it turns out, the initial eigenvalue estimate
was fairly good, and the inverse power sequence converged quickly. The smallest
eigenvalue is exactly 2.
TABLE 3 The Inverse Power Method
k 0 1 2 3 4
xk2
41
1
13
52
4:5736
:0646
13
52
4:5054
:0045
13
52
4:5004
:0003
13
52
4:50003
:00002
13
5
yk2
44:45
:50
7:763
52
45:0131
:0442
9:91973
52
45:0012
:0031
9:99493
52
45:0001
:0002
9:99963
52
45:000006
:000015
9:9999753
5
k 7.76 9.9197 9.9949 9.9996 9.999975
k 2.03 2.0008 2.00005 2.000004 2.0000002
If an estimate for the smallest eigenvalue of a matrix is not available, one can simply
takeD0in the inverse power method. This choice of works reasonably well if the
smallest eigenvalue is much closer to zero than to the other eigenvalues.
SECOND REVISED PAGES


--- Page 343 ---
326 CHAPTER 5 Eigenvalues and Eigenvectors
The two algorithms presented in this section are practical tools for many simple
situations, and they provide an introduction to the problem of eigenvalue estimation. A
more robust and widely used iterative method is the QR algorithm. For instance, it is
the heart of the MATLAB command eig(A) , which rapidly computes eigenvalues and
eigenvectors of A. A brief description of the QR algorithm was given in the exercises
for Section 5.2. Further details are presented in most modern numerical analysis texts.
PRACTICE PROBLEM
How can you tell if a given vector xis a good approximation to an eigenvector of a
matrix A? If it is, how would you estimate the corresponding eigenvalue? Experiment
with
AD2
45 8 4
8 3  1
4 1 23
5and xD2
41:0
 4:3
8:13
5
5.8 EXERCISES
In Exercises 1‚Äì4, the matrix Ais followed by a sequence fxkg
produced by the power method. Use these data to estimate the
largest eigenvalue of A, and give a corresponding eigenvector.
1.AD4 3
1 2
;
1
0
;1
:25
;1
:3158
;1
:3298
;1
:3326
2.AD1:8  :8
 3:2 4:2
;
1
0
; :5625
1
; :3021
1
; :2601
1
; :2520
1
3.AD:5 :2
:4 :7
;
1
0
;1
:8
;:6875
1
;:5577
1
;:5188
1
4.AD4:1  6
3 4:4
;
1
1
;1
:7368
;1
:7541
;1
:7490
;1
:7502
5.LetAD15 16
 20 21
. The vectors x; : : : ; A5xare1
1
,
31
 41
; 191
241
;991
 1241
; 4991
6241
;24991
 31241
:
Find a vector with a 1 in the second entry that is close to
an eigenvector of A. Use four decimal places. Check your
estimate, and give an estimate for the dominant eigenvalue
ofA.6.LetAD 2 3
6 7
. Repeat Exercise 5, using the following
sequence x,Ax; : : : ; A5x.
1
1
; 5
13
; 29
61
; 125
253
; 509
1021
; 2045
4093
[M] Exercises 7‚Äì12 require MATLAB or other computational aid.
In Exercises 7 and 8, use the power method with the x0given. List
fxkgandfkgforkD1; : : : ; 5 . In Exercises 9 and 10, list 5and
6.
7.AD6 7
8 5
,x0D1
0
8.AD2 1
4 5
,x0D1
0
9.AD2
48 0 12
1 2 1
0 3 03
5,x0D2
41
0
03
5
10.AD2
41 2  2
1 1 9
0 1 93
5,x0D2
41
0
03
5
Another estimate can be made for an eigenvalue when an ap-
proximate eigenvector is available. Observe that if AxDx, then
xTAxDxT.x/D.xTx/, and the Rayleigh quotient
R.x/DxTAx
xTx
equals . Ifxis close to an eigenvector for , then this quotient is
close to . When Ais a symmetric matrix .ATDA/, the Rayleigh
quotient R.xk/D.xT
kAxk/=.xT
kxk/will have roughly twice as
many digits of accuracy as the scaling factor kin the power
method. Verify this increased accuracy in Exercises 11 and 12 by
computing kandR.xk/forkD1; : : : ; 4 .
SECOND REVISED PAGES


--- Page 344 ---
5.8 Iterative Estimates for Eigenvalues 327
11.AD5 2
2 2
,x0D1
0
12.AD 3 2
2 0
,x0D1
0
Exercises 13 and 14 apply to a 33matrix Awhose eigenvalues
are estimated to be 4,  4, and 3.
13. If the eigenvalues close to 4 and  4are known to have
different absolute values, will the power method work? Is it
likely to be useful?
14. Suppose the eigenvalues close to 4 and  4are known to have
exactly the same absolute value. Describe how one might
obtain a sequence that estimates the eigenvalue close to 4.
15. Suppose AxDxwith x¬§0. Let be a scalar different
from the eigenvalues of A, and let BD.A I/ 1. Subtract
xfrom both sides of the equation AxDx, and use algebra
to show that 1=. /is an eigenvalue of B, with xa
corresponding eigenvector.
16. Suppose is an eigenvalue of the Bin Exercise 15, and that
xis a corresponding eigenvector, so that .A I/ 1xDx.
Use this equation to Ô¨Ånd an eigenvalue of Ain terms of and
. [Note: ¬§0because Bis invertible.]
17. [M] Use the inverse power method to estimate the middle
eigenvalue of the Ain Example 3, with accuracy to four
decimal places. Set x0D.1; 0; 0/ .18. [M] Let Abe as in Exercise 9. Use the inverse power method
with x0D.1; 0; 0/ to estimate the eigenvalue of Anear
D  1:4, with an accuracy to four decimal places.
[M] In Exercises 19 and 20, Ô¨Ånd (a) the largest eigenvalue and (b)
the eigenvalue closest to zero. In each case, set x0D.1; 0; 0; 0/
and carry out approximations until the approximating sequence
seems accurate to four decimal places. Include the approximate
eigenvector.
19.AD2
66410 7 8 7
7 5 6 5
8 6 10 9
7 5 9 103
775
20.AD2
6641 2 3 2
2 12 13 11
 2 3 0 2
4 5 7 23
775
21. A common misconception is that if Ahas a strictly dominant
eigenvalue, then, for any sufÔ¨Åciently large value of k, the
vector Akxis approximately equal to an eigenvector of A. For
the three matrices below, study what happens to Akxwhen
xD.:5; :5/ , and try to draw general conclusions (for a 22
matrix).
a.AD:8 0
0 :2
b.AD1 0
0 :8
c.AD8 0
0 2
SOLUTION TO PRACTICE PROBLEM
For the given Aandx,
AxD2
45 8 4
8 3  1
4 1 23
52
41:00
 4:30
8:103
5D2
43:00
 13:00
24:503
5
IfAxis nearly a multiple of x, then the ratios of corresponding entries in the two vectors
should be nearly constant. So compute:
fentry in Axg  f entry in xg D f ratiog
3:00 1:00 3:000
 13:00  4:30 3:023
24:50 8:10 3:025
Each entry in Axis about 3 times the corresponding entry in x, so xis close to an
eigenvector. Any of the ratios above is an estimate for the eigenvalue. (To Ô¨Åve decimal
places, the eigenvalue is 3.02409.)
WEB
SECOND REVISED PAGES


--- Page 345 ---
328 CHAPTER 5 Eigenvalues and Eigenvectors
CHAPTER 5 SUPPLEMENTARY EXERCISES
Throughout these supplementary exercises, AandBrepresent
square matrices of appropriate sizes.
1.Mark each statement as True or False. Justify each answer.
a.IfAis invertible and 1 is an eigenvalue for A, then 1 is
also an eigenvalue of A 1.
b.IfAis row equivalent to the identity matrix I, then Ais
diagonalizable.
c.IfAcontains a row or column of zeros, then 0 is an
eigenvalue of A.
d.Each eigenvalue of Ais also an eigenvalue of A2.
e.Each eigenvector of Ais also an eigenvector of A2.
f.Each eigenvector of an invertible matrix Ais also an
eigenvector of A 1.
g.Eigenvalues must be nonzero scalars.
h.Eigenvectors must be nonzero vectors.
i.Two eigenvectors corresponding to the same eigenvalue
are always linearly dependent.
j.Similar matrices always have exactly the same eigen-
values.
k.Similar matrices always have exactly the same eigen-
vectors.
l.The sum of two eigenvectors of a matrix Ais also an
eigenvector of A.
m.The eigenvalues of an upper triangular matrix Aare
exactly the nonzero entries on the diagonal of A.
n.The matrices AandAThave the same eigenvalues,
counting multiplicities.
o.If a55matrix Ahas fewer than 5 distinct eigenvalues,
thenAis not diagonalizable.
p.There exists a 22matrix that has no eigenvectors in
R2.
q.IfAis diagonalizable, then the columns of Aare linearly
independent.
r.A nonzero vector cannot correspond to two different
eigenvalues of A.
s.A (square) matrix Ais invertible if and only if there is a
coordinate system in which the transformation x7!Ax
is represented by a diagonal matrix.
t.If each vector ejin the standard basis for Rnis an
eigenvector of A, then Ais a diagonal matrix.
u.IfAis similar to a diagonalizable matrix B, then Ais
also diagonalizable.
v.IfAandBare invertible nnmatrices, then ABis
similar to BA.
w.Annnmatrix with nlinearly independent eigenvec-
tors is invertible.x.IfAis an nndiagonalizable matrix, then each vector
inRncan be written as a linear combination of eigenvec-
tors of A.
2.Show that if xis an eigenvector of the matrix product ABand
Bx¬§0, then Bxis an eigenvector of BA.
3.Suppose xis an eigenvector of Acorresponding to an eigen-
value .
a.Show that xis an eigenvector of 5I A. What is the
corresponding eigenvalue?
b.Show that xis an eigenvector of 5I 3ACA2. What is
the corresponding eigenvalue?
4.Use mathematical induction to show that if is an eigenvalue
of an nnmatrix A, with xa corresponding eigenvector,
then, for each positive integer m,mis an eigenvalue of Am,
with xa corresponding eigenvector.
5.Ifp.t/Dc0Cc1tCc2t2C  C cntn, deÔ¨Åne p.A/ to be the
matrix formed by replacing each power of tinp.t/ by the
corresponding power of A(with A0DI). That is,
p.A/Dc0ICc1ACc2A2C  C cnAn
Show that if is an eigenvalue of A, then one eigenvalue of
p.A/ isp./ .
6.Suppose ADPDP 1, where Pis22andDD2 0
0 7
.
a.LetBD5I 3ACA2. Show that Bis diagonalizable
by Ô¨Ånding a suitable factorization of B.
b.Given p.t/ andp.A/ as in Exercise 5, show that p.A/ is
diagonalizable.
7.Suppose Ais diagonalizable and p.t/ is the characteristic
polynomial of A. DeÔ¨Åne p.A/ as in Exercise 5, and show
thatp.A/ is the zero matrix. This fact, which is also true for
anysquare matrix, is called the Cayley‚ÄìHamilton theorem .
8.a.LetAbe a diagonalizable nnmatrix. Show that if the
multiplicity of an eigenvalue isn, then ADI.
b.Use part (a) to show that the matrix AD3 1
0 3
is not
diagonalizable.
9.Show that I Ais invertible when all the eigenvalues of A
are less than 1 in magnitude. [ Hint: What would be true if
I Awere not invertible?]
10. Show that if Ais diagonalizable, with all eigenvalues less
than 1 in magnitude, then Aktends to the zero matrix as
k! 1 . [Hint: Consider Akxwhere xrepresents any one of
the columns of I.]
11.Letube an eigenvector of Acorresponding to an eigenvalue
, and let Hbe the line in Rnthrough uand the origin.
a.Explain why His invariant under Ain the sense that Ax
is inHwhenever xis inH.
SECOND REVISED PAGES


--- Page 346 ---
Chapter 5 Supplementary Exercises 329
b.LetKbe a one-dimensional subspace of Rnthat is invari-
ant under A. Explain why Kcontains an eigenvector of
A.
12. LetGDA X
0 B
. Use formula (1) for the determinant
in Section 5.2 to explain why det GD.detA/.detB/. From
this, deduce that the characteristic polynomial of Gis the
product of the characteristic polynomials of AandB.
Use Exercise 12 to Ô¨Ånd the eigenvalues of the matrices in Exer-
cises 13 and 14.
13.AD2
43 2 8
0 5  2
0 4 33
5
14.AD2
6641 5  6 7
2 4 5 2
0 0  7 4
0 0 3 13
775
15. LetJbe the nnmatrix of all 1‚Äôs, and consider
AD.a b/ICbJ; that is,
AD2
666664a b b  b
b a b  b
b b a  b
:::::::::::::::
b b b  a3
777775
Use the results of Exercise 16 in the Supplementary Exercises
for Chapter 3 to show that the eigenvalues of Aarea band
aC.n 1/b. What are the multiplicities of these eigenval-
ues?
16. Apply the result of Exercise 15 to Ô¨Ånd the eigenvalues of the
matrices2
41 2 2
2 1 2
2 2 13
5and2
666647 3 3 3 3
3 7 3 3 3
3 3 7 3 3
3 3 3 7 3
3 3 3 3 73
77775.
17. LetADa11 a12
a21 a22
. Recall from Exercise 25 in Section
5.4 that tr A(the trace of A) is the sum of the diagonal entries
inA. Show that the characteristic polynomial of Ais
2 .trA/CdetA
Then show that the eigenvalues of a 22matrix Aare both
real if and only if det AtrA
22
.
18. Let AD:4 :3
:4 1:2
. Explain why Akapproaches
 :5 :75
1:0 1:50
ask! 1 .
Exercises 19‚Äì23 concern the polynomial
p.t/Da0Ca1tC  C an 1tn 1Ctnand an nnmatrix Cpcalled the companion matrix ofp:
CpD2
6666640 1 0  0
0 0 1 0
::::::
0 0 0 1
 a0 a1 a2   an 13
777775
19. Write the companion matrix Cpforp.t/D6 5tCt2, and
then Ô¨Ånd the characteristic polynomial of Cp.
20. Letp.t/D.t 2/.t 3/.t 4/D  24C26t 9t2Ct3.
Write the companion matrix for p.t/, and use techniques
from Chapter 3 to Ô¨Ånd its characteristic polynomial.
21. Use mathematical induction to prove that for n2,
det.Cp I/D. 1/n.a0Ca1C  C an 1n 1Cn/
D. 1/np./
[Hint: Expanding by cofactors down the Ô¨Årst column, show
that det .Cp I/has the form . /BC. 1/na0, where B
is a certain polynomial (by the induction assumption).]
22. Letp.t/Da0Ca1tCa2t2Ct3, and let be a zero of p.
a.Write the companion matrix for p.
b.Explain why 3D  a0 a1 a22, and show that
.1; ; 2/is an eigenvector of the companion matrix for
p.
23. Letpbe the polynomial in Exercise 22, and suppose the
equation p.t/D0has distinct roots 1,2,3. LetVbe the
Vandermonde matrix
VD2
641 1 1
123
2
12
22
33
75
(The transpose of Vwas considered in Supplementary Ex-
ercise 11 in Chapter 2.) Use Exercise 22 and a theorem from
this chapter to deduce that Vis invertible (but do not compute
V 1/. Then explain why V 1CpVis a diagonal matrix.
24. [M] The MATLAB command roots(p) computes the
roots of the polynomial equation p.t/D0. Read a MATLAB
manual, and then describe the basic idea behind the algorithm
for the roots command.
25. [M] Use a matrix program to diagonalize
AD2
4 3 2 0
14 7  1
 6 3 13
5
if possible. Use the eigenvalue command to create the diag-
onal matrix D. If the program has a command that produces
eigenvectors, use it to create an invertible matrix P. Then
compute AP PDandPDP 1. Discuss your results.
26. [M] Repeat Exercise 25 for AD2
664 8 5  2 0
 5 2 1  2
10 8 6  3
3 2 1 03
775.
SECOND REVISED PAGES


--- Page 347 ---
SECOND REVISED PAGES


--- Page 348 ---
6Orthogonality and
Least Squares
INTRODUCTORY EXAMPLE
The North American Datum
and GPS Navigation
Imagine starting a massive project that you estimate will
take ten years and require the efforts of scores of people
to construct and solve a 1,800,000-by-900,000 system
of linear equations. That is exactly what the National
Geodetic Survey did in 1974, when it set out to update
the North American Datum (NAD)‚Äîa network of 268,000
precisely located reference points that span the entire North
American continent, together with Greenland, Hawaii, the
Virgin Islands, Puerto Rico, and other Caribbean islands.
The recorded latitudes and longitudes in the NAD
must be determined to within a few centimeters because
they form the basis for all surveys, maps, legal property
boundaries, and layouts of civil engineering projects
such as highways and public utility lines. However,
more than 200,000 new points had been added to the
datum since the last adjustment in 1927, and errors had
gradually accumulated over the years, due to imprecise
measurements and shifts in the earth‚Äôs crust. Data gathering
for the NAD readjustment was completed in 1983.
The system of equations for the NAD had no solution
in the ordinary sense, but rather had a least-squares
solution , which assigned latitudes and longitudes to the
reference points in a way that corresponded best to the
1.8 million observations. The least-squares solution was
found in 1986 by solving a related system of so-callednormal equations , which involved 928,735 equations in
928,735 variables.1
More recently, knowledge of reference points on the
ground has become crucial for accurately determining
the locations of satellites in the satellite-based Global
Positioning System (GPS) . A GPS satellite calculates its
position relative to the earth by measuring the time it takes
for signals to arrive from three ground transmitters. To do
this, the satellites use precise atomic clocks that have been
synchronized with ground stations (whose locations are
known accurately because of the NAD).
The Global Positioning System is used both for
determining the locations of new reference points on the
ground and for Ô¨Ånding a user‚Äôs position on the ground
relative to established maps. When a car driver (or a
mountain climber) turns on a GPS receiver, the receiver
measures the relative arrival times of signals from at
least three satellites. This information, together with the
transmitted data about the satellites‚Äô locations and message
times, is used to adjust the GPS receiver‚Äôs time and to
determine its approximate location on the earth. Given
information from a fourth satellite, the GPS receiver can
even establish its approximate altitude.
1A mathematical discussion of the solution strategy (along with details
of the entire NAD project) appears in North American Datum of 1983 ,
Charles R. Schwarz (ed.), National Geodetic Survey, National Oceanic
and Atmospheric Administration (NOAA) Professional Paper NOS 2,
1989.
SECOND REVISED PAGES
331

--- Page 349 ---
332 CHAPTER 6 Orthogonality and Least Squares
Both the NAD and GPS problems are solved by Ô¨Ånding
a vector that ‚Äúapproximately satisÔ¨Åes‚Äù an inconsistent
system of equations. A careful explanation of this apparentcontradiction will require ideas developed in the Ô¨Årst Ô¨Åve
sections of this chapter.
WEB
In order to Ô¨Ånd an approximate solution to an inconsistent system of equations that has
no actual solution, a well-deÔ¨Åned notion of nearness is needed. Section 6.1 introduces
the concepts of distance and orthogonality in a vector space. Sections 6.2 and 6.3 show
how orthogonality can be used to identify the point within a subspace Wthat is nearest
to a point ylying outside of W. By taking Wto be the column space of a matrix,
Section 6.5 develops a method for producing approximate (‚Äúleast-squares‚Äù) solutions
for inconsistent linear systems, such as the system solved for the NAD report.
Section 6.4 provides another opportunity to see orthogonal projections at work,
creating a matrix factorization widely used in numerical linear algebra. The remaining
sections examine some of the many least-squares problems that arise in applications,
including those in vector spaces more general than Rn.
6.1 INNER PRODUCT, LENGTH, AND ORTHOGONALITY
Geometric concepts of length, distance, and perpendicularity, which are well known for
R2andR3, are deÔ¨Åned here for Rn. These concepts provide powerful geometric tools for
solving many applied problems, including the least-squares problems mentioned above.
All three notions are deÔ¨Åned in terms of the inner product of two vectors.
The Inner Product
Ifuandvare vectors in Rn, then we regard uandvasn1matrices. The transpose
uTis a1nmatrix, and the matrix product uTvis a11matrix, which we write as
a single real number (a scalar) without brackets. The number uTvis called the inner
product ofuandv, and often it is written as uv. This inner product, mentioned in the
exercises for Section 2.1, is also referred to as a dot product . If
uD2
6664u1
u2
:::
un3
7775and vD2
6664v1
v2
:::
vn3
7775
then the inner product of uandvis
¬åu1u2  un¬ç2
6664v1
v2
:::
vn3
7775Du1v1Cu2v2C    C unvn
SECOND REVISED PAGES


--- Page 350 ---
6.1 Inner Product, Length, and Orthogonality 333
EXAMPLE 1 Compute uvandvuforuD2
42
 5
 13
5andvD2
43
2
 33
5.
SOLUTION
uvDuTvD¬å2 5 1¬ç2
43
2
 33
5D.2/.3/ C. 5/.2/ C. 1/. 3/D  1
vuDvTuD¬å3 2  3¬ç2
42
 5
 13
5D.3/.2/ C.2/. 5/C. 3/. 1/D  1
It is clear from the calculations in Example 1 why uvDvu. This commutativity
of the inner product holds in general. The following properties of the inner product are
easily deduced from properties of the transpose operation in Section 2.1. (See Exer-
cises 21 and 22 at the end of this section.)
T H E O R E M 1 Letu,v, and wbe vectors in Rn, and let cbe a scalar. Then
a.uvDvu
b..uCv/wDuwCvw
c..cu/vDc.uv/Du.cv/
d.uu0, and uuD0if and only if uD0
Properties (b) and (c) can be combined several times to produce the following useful
rule:
.c1u1C    C cpup/wDc1.u1w/C    C cp.upw/
The Length of a Vector
Ifvis inRn, with entries v1; : : : ; v n, then the square root of vvis deÔ¨Åned because vv
is nonnegative.
D E F I N I T I O N Thelength (ornorm ) ofvis the nonnegative scalar kvkdeÔ¨Åned by
kvk DpvvDq
v2
1Cv2
2C    C v2n;and kvk2Dvv
Suppose vis inR2, say, vDa
b
. If we identify vwith a geometric point in the
|a||b|
x1x2
(a, b)
a2 + b2‚àö‚éØ‚éØ‚éØ‚éØ‚éØ
0
FIGURE 1
Interpretation of kvkas length.plane, as usual, then kvkcoincides with the standard notion of the length of the line
segment from the origin to v. This follows from the Pythagorean Theorem applied to a
triangle such as the one in Figure 1.
A similar calculation with the diagonal of a rectangular box shows that the deÔ¨Ånition
of length of a vector vinR3coincides with the usual notion of length.
For any scalar c, the length of cvisjcjtimes the length of v. That is,
kcvk D j cjkvk
(To see this, compute kcvk2D.cv/.cv/Dc2vvDc2kvk2and take square roots.)
SECOND REVISED PAGES


--- Page 351 ---
334 CHAPTER 6 Orthogonality and Least Squares
A vector whose length is 1 is called a unit vector . If we divide a nonzero vector v
by its length‚Äîthat is, multiply by 1=kvk‚Äîwe obtain a unit vector ubecause the length
ofuis.1=kvk/kvk. The process of creating ufrom vis sometimes called normalizing
v, and we say that uisin the same direction asv.
Several examples that follow use the space-saving notation for (column) vectors.
EXAMPLE 2 LetvD.1; 2; 2; 0/ . Find a unit vector uin the same direction as v.
SOLUTION First, compute the length of v:
kvk2DvvD.1/2C. 2/2C.2/2C.0/2D9
kvk Dp
9D3
Then, multiply vby1=kvkto obtain
uD1
kvkvD1
3vD1
32
6641
 2
2
03
775D2
6641=3
 2=3
2=3
03
775
To check that kuk D1, it sufÔ¨Åces to show that kuk2D1.
kuk2DuuD 1
32C 
 2
32C 2
32C.0/2
D1
9C4
9C4
9C0D1
EXAMPLE 3 LetWbe the subspace of R2spanned by xD.2
3; 1/. Find a unit vector
zthat is a basis for W.
SOLUTION Wconsists of all multiples of x, as in Figure 2(a). Any nonzero vector in
Wis a basis for W. To simplify the calculation, ‚Äúscale‚Äù xto eliminate fractions. That
is, multiply xby 3 to get
yD2
3
Now compute kyk2D22C32D13,kyk Dp
13, and normalize yto get
zD1p
132
3
D2=p
13
3=p
13
See Figure 2(b). Another unit vector is . 2=p
13; 3=p
13/.
(a)x1x2
xW
11
(b)x1x2
y
z
11FIGURE 2
Normalizing a vector to produce a
unit vector.
Distance in Rn
We are ready now to describe how close one vector is to another. Recall that if aand
bare real numbers, the distance on the number line between aandbis the number
ja bj. Two examples are shown in Figure 3. This deÔ¨Ånition of distance in Rhas a
direct analogue in Rn.
|2 ‚Äì 8| = |‚Äì6| = 6   or   |8 ‚Äì 2| = |6| = 6 |(‚Äì3) ‚Äì 4| = |‚Äì7| = 7   or   |4 ‚Äì (‚Äì3)| = |7| = 76 units aparta b a b
7 units apart132 456789 1 3 02 ‚Äì1 ‚Äì3‚Äì24 5
FIGURE 3 Distances in R.
SECOND REVISED PAGES


--- Page 352 ---
6.1 Inner Product, Length, and Orthogonality 335
D E F I N I T I O N ForuandvinRn, the distance between u and v , written as dist .u;v/, is the
length of the vector u v. That is,
dist.u;v/D ku vk
InR2andR3, this deÔ¨Ånition of distance coincides with the usual formulas for the
Euclidean distance between two points, as the next two examples show.
EXAMPLE 4 Compute the distance between the vectors uD.7; 1/ andvD.3; 2/ .
SOLUTION Calculate
u vD7
1
 3
2
D4
 1
ku vk Dp
42C. 1/2Dp
17
The vectors u,v, and u vare shown in Figure 4. When the vector u vis added
tov, the result is u. Notice that the parallelogram in Figure 4 shows that the distance
from utovis the same as the distance from u vto0.
||u ‚Äì v||
x1x2
v
u
u ‚Äì v
‚Äìv11
FIGURE 4 The distance between uandvis
the length of u v.
EXAMPLE 5 IfuD.u1; u2; u3/andvD.v1; v2; v3/, then
dist.u;v/D ku vk Dp
.u v/.u v/
Dp
.u1 v1/2C.u2 v2/2C.u3 v3/2
Orthogonal Vectors
The rest of this chapter depends on the fact that the concept of perpendicular lines in
||u ‚Äì( ‚Äì v)||||u ‚Äì v||
v
0u
‚Äìv
FIGURE 5ordinary Euclidean geometry has an analogue in Rn.
Consider R2orR3and two lines through the origin determined by vectors uand
v. The two lines shown in Figure 5 are geometrically perpendicular if and only if the
distance from utovis the same as the distance from uto v. This is the same as
requiring the squares of the distances to be the same. Now
¬ådist.u; v/¬ç2D ku . v/k2D kuCvk2
D.uCv/.uCv/
Du.uCv/Cv.uCv/ Theorem 1(b)
DuuCuvCvuCvv Theorem 1(a), (b)
D kuk2C kvk2C2uv Theorem 1(a) (1)
SECOND REVISED PAGES


--- Page 353 ---
336 CHAPTER 6 Orthogonality and Least Squares
The same calculations with vand vinterchanged show that
¬ådist.u;v/¬ç2D kuk2C k   vk2C2u. v/
D kuk2C kvk2 2uv
The two squared distances are equal if and only if 2uvD  2uv, which happens if and
only if uvD0.
This calculation shows that when vectors uandvare identiÔ¨Åed with geometric
points, the corresponding lines through the points and the origin are perpendicular if
and only if uvD0. The following deÔ¨Ånition generalizes to Rnthis notion of perpen-
dicularity (or orthogonality , as it is commonly called in linear algebra).
D E F I N I T I O N Two vectors uandvinRnareorthogonal (to each other) if uvD0.
Observe that the zero vector is orthogonal to every vector in Rnbecause 0TvD0
for all v.
The next theorem provides a useful fact about orthogonal vectors. The proof follows
immediately from the calculation in (1) above and the deÔ¨Ånition of orthogonality. The
right triangle shown in Figure 6 provides a visualization of the lengths that appear in
the theorem.
T H E O R E M 2 The Pythagorean Theorem
Two vectors uandvare orthogonal if and only if kuCvk2D kuk2C kvk2.
Orthogonal Complements
To provide practice using inner products, we introduce a concept here that will be of use
in Section 6.3 and elsewhere in the chapter. If a vector zis orthogonal to every vector
in a subspace WofRn, then zis said to be orthogonal to W. The set of all vectors z
that are orthogonal to Wis called the orthogonal complement ofWand is denoted by
W?(and read as ‚Äú Wperpendicular‚Äù or simply ‚Äú Wperp‚Äù).
vu + v
||u + v|| u||v||
||u||
0 FIGURE 6
EXAMPLE 6 LetWbe a plane through the origin in R3, and let Lbe the line
through the origin and perpendicular to W. Ifzandware nonzero, zis on L, and w
zLw
0
W
FIGURE 7
A plane and line through 0as
orthogonal complements.is inW, then the line segment from 0tozis perpendicular to the line segment from 0to
w; that is, zwD0. See Figure 7. So each vector on Lis orthogonal to every winW.
In fact, Lconsists of allvectors that are orthogonal to the w‚Äôs inW, and Wconsists of
all vectors orthogonal to the z‚Äôs inL. That is,
LDW?and WDL?
The following two facts about W?, with Wa subspace of Rn, are needed later
in the chapter. Proofs are suggested in Exercises 29 and 30. Exercises 27‚Äì31 provide
excellent practice using properties of the inner product.
1.A vector xis inW?if and only if xis orthogonal to every vector in a set that
spans W.
2.W?is a subspace of Rn.
The next theorem and Exercise 31 verify the claims made in Section 4.6 concerning
the subspaces shown in Figure 8. (Also see Exercise 28 in Section 4.6.)
SECOND REVISED PAGES


--- Page 354 ---
6.1 Inner Product, Length, and Orthogonality 337
A
00
Row ANulA
Col
ANulAT
FIGURE 8 The fundamental subspaces determined
by an mnmatrix A.
Remark: A common way to prove that two sets, say SandT, are equal is to show
thatSis a subset of TandTis a subset of S. The proof of the next theorem that
NulAD(Row A)?is established by showing that Nul Ais a subset of (Row A)?and
(Row A)?is a subset of Nul A. That is, an arbitrary element xin Nul Ais shown to be in
(Row A)?, and then an arbitrary element xin (Row A)?is shown to be in Nul A.
T H E O R E M 3 LetAbe an mnmatrix. The orthogonal complement of the row space of Ais
the null space of A, and the orthogonal complement of the column space of Ais
the null space of AT:
.RowA/?DNulAand .ColA/?DNulAT
PROOF The row‚Äìcolumn rule for computing Axshows that if xis in Nul A, then xis
orthogonal to each row of A(with the rows treated as vectors in Rn/. Since the rows
ofAspan the row space, xis orthogonal to Row A. Conversely, if xis orthogonal to
RowA, then xis certainly orthogonal to each row of A, and hence AxD0. This proves
the Ô¨Årst statement of the theorem. Since this statement is true for any matrix, it is true
forAT. That is, the orthogonal complement of the row space of ATis the null space of
AT. This proves the second statement, because Row ATDColA.
Angles in R2andR3(Optional)
Ifuandvare nonzero vectors in either R2orR3, then there is a nice connection between
their inner product and the angle #between the two line segments from the origin to the
points identiÔ¨Åed with uandv. The formula is
uvD kuk kvkcos# (2)
To verify this formula for vectors in R2, consider the triangle shown in Figure 9, with
sides of lengths kuk,kvk, andku vk. By the law of cosines,
ku vk2D kuk2C kvk2 2kuk kvkcos#
(u1, u2)
(v1, v2)||u ‚Äì v||
||v||||u|| /H9277
FIGURE 9 The angle between two vectors.
SECOND REVISED PAGES


--- Page 355 ---
338 CHAPTER 6 Orthogonality and Least Squares
which can be rearranged to produce
kuk kvkcos#D1
2
kuk2C kvk2  ku vk2
D1
2
u2
1Cu2
2Cv2
1Cv2
2 .u1 v1/2 .u2 v2/2
Du1v1Cu2v2
Duv
The veriÔ¨Åcation for R3is similar. When n > 3 , formula (2) may be used to deÔ¨Åne the
angle between two vectors in Rn. In statistics, for instance, the value of cos #deÔ¨Åned
by (2) for suitable vectors uandvis what statisticians call a correlation coefÔ¨Åcient .
PRACTICE PROBLEMS
1.LetaD 2
1
andbD 3
1
. Computeab
aaandab
aa
a.
2.LetcD2
44=3
 1
2=33
5anddD2
45
6
 13
5.
a.Find a unit vector uin the direction of c.
b.Show that dis orthogonal to c.
c.Use the results of (a) and (b) to explain why dmust be orthogonal to the unit
vector u.
3.LetWbe a subspace of Rn. Exercise 30 establishes that W?is also a subspace of
Rn. Prove that dim WCdimW?Dn.
6.1 EXERCISES
Compute the quantities in Exercises 1‚Äì8 using the vectors
uD 1
2
,vD4
6
,wD2
43
 1
 53
5,xD2
46
 2
33
5
1.uu,vu, andvu
uu2. w w,xw, andxw
ww
3.1
www 4.1
uuu
5.uv
vv
v 6.xw
xx
x
7.kwk 8.kxk
In Exercises 9‚Äì12, Ô¨Ånd a unit vector in the direction of the given
vector.
9. 30
40
10.2
4 6
4
 33
5
11.2
47=4
1=2
13
5 12.8=3
2
13. Find the distance between xD10
 3
andyD 1
 5
.14. Find the distance between uD2
40
 5
23
5andzD2
4 4
 1
83
5.
Determine which pairs of vectors in Exercises 15‚Äì18 are orthog-
onal.
15. aD8
 5
,bD 2
 3
16. u D2
412
3
 53
5,vD2
42
 3
33
5
17. uD2
6643
2
 5
03
775,vD2
664 4
1
 2
63
77518. y D2
664 3
7
4
03
775,zD2
6641
 8
15
 73
775
In Exercises 19 and 20, all vectors are in Rn. Mark each statement
True or False. Justify each answer.
19. a.vvD kvk2.
b.For any scalar c,u.cv/Dc.uv/.
c.If the distance from utovequals the distance from uto
 v, then uandvare orthogonal.
d.For a square matrix A, vectors in Col Aare orthogonal to
vectors in Nul A.
SECOND REVISED PAGES


--- Page 356 ---
6.1 Inner Product, Length, and Orthogonality 339
e.If vectors v1; : : : ; vpspan a subspace Wand if xis
orthogonal to each vjforjD1; : : : ; p , then xis inW?.
20. a.uv vuD0.
b.For any scalar c,kcvk Dckvk.
c.Ifxis orthogonal to every vector in a subspace W, then x
is inW?.
d.Ifkuk2C kvk2D kuCvk2, then uandvare orthogonal.
e.For an mnmatrix A, vectors in the null space of Aare
orthogonal to vectors in the row space of A.
21. Use the transpose deÔ¨Ånition of the inner product to verify
parts (b) and (c) of Theorem 1. Mention the appropriate facts
from Chapter 2.
22. Let uD.u1; u2; u3/. Explain why uu0. When is
uuD0?
23. LetuD2
42
 5
 13
5andvD2
4 7
 4
63
5. Compute and compare
uv,kuk2,kvk2, andkuCvk2. Do not use the Pythagorean
Theorem.
24. Verify the parallelogram law for vectors uandvinRn:
kuCvk2C ku vk2D2kuk2C2kvk2
25. LetvDa
b
. Describe the set Hof vectorsx
y
that are
orthogonal to v. [Hint: Consider vD0andv¬§0.]
26. LetuD2
45
 6
73
5, and let Wbe the set of all xinR3such that
uxD0. What theorem in Chapter 4 can be used to show that
Wis a subspace of R3? Describe Win geometric language.
27. Suppose a vector yis orthogonal to vectors uandv. Show
thatyis orthogonal to the vector uCv.
28. Suppose yis orthogonal to uandv. Show that yis orthogonal
to every win Span fu;vg. [Hint: An arbitrary win Span fu;vg
has the form wDc1uCc2v. Show that yis orthogonal to
such a vector w.]
w
0
y v
Span{u, v}
29. LetWDSpanfv1; : : : ; vpg. Show that if xis orthogonal to
each vj, for1jp, then xis orthogonal to every vector
inW.30. LetWbe a subspace of Rn, and let W?be the set of all
vectors orthogonal to W. Show that W?is a subspace of Rn
using the following steps.
a.Take zinW?, and let urepresent any element of W. Then
zuD0. Take any scalar cand show that czis orthogonal
tou. (Since uwas an arbitrary element of W, this will
show that czis inW?.)
b.Take z1and z2inW?, and let ube any element of
W. Show that z1Cz2is orthogonal to u. What can you
conclude about z1Cz2? Why?
c.Finish the proof that W?is a subspace of Rn.
31. Show that if xis in both WandW?, then xD0.
32. [M] Construct a pair u,vof random vectors in R4, and let
AD2
664:5 :5 :5 :5
:5 :5  :5 :5
:5 :5 :5  :5
:5 :5 :5 :53
775
a.Denote the columns of Abya1; : : : ; a4. Compute
the length of each column, and compute a1a2,
a1a3;a1a4;a2a3;a2a4, and a3a4.
b.Compute and compare the lengths of u,Au,v, and Av.
c.Use equation (2) in this section to compute the cosine of
the angle between uandv. Compare this with the cosine
of the angle between AuandAv.
d.Repeat parts (b) and (c) for two other pairs of random
vectors. What do you conjecture about the effect of Aon
vectors?
33. [M] Generate random vectors x,y, and vinR4with integer
entries (and v¬§0), and compute the quantities
xv
vv
v;yv
vv
v;.xCy/v
vvv;.10x/v
vvv
Repeat the computations with new random vectors xand
y. What do you conjecture about the mapping x7!T .x/D
xv
vv
v(forv¬§0)? Verify your conjecture algebraically.
34. [M] Let AD2
66664 6 3  27 33 13
6 5 25 28 14
8 6 34 38 18
12 10 50 41 23
14 21 49 29 333
77775. Construct
a matrix Nwhose columns form a basis for Nul A, and
construct a matrix Rwhose rows form a basis for Row A(see
Section 4.6 for details). Perform a matrix computation with
NandRthat illustrates a fact from Theorem 3.
SECOND REVISED PAGES


--- Page 357 ---
340 CHAPTER 6 Orthogonality and Least Squares
SOLUTIONS TO PRACTICE PROBLEMS
1.abD7,aaD5. Henceab
aaD7
5, andab
aa
aD7
5aD 14=5
7=5
.
2.a.Scale c, multiplying by 3 to get yD2
44
 3
23
5. Compute kyk2D29andkyk Dp
29.
The unit vector in the direction of both candyisuD1
kykyD2
44=p
29
 3=p
29
2=p
293
5.
b.dis orthogonal to c, because
dcD2
45
6
 13
52
44=3
 1
2=33
5D20
3 6 2
3D0
c.dis orthogonal to u, because uhas the form kcfor some k, and
duDd.kc/Dk.dc/Dk.0/D0
3.IfW¬§ f0g, letfb1; : : : ; bpgbe a basis for W, where 1pn. LetAbe the pn
matrix having rows bT
1; : : : ; bT
p. It follows that Wis the row space of A. Theorem
3 implies that W?D(Row A/?DNulAand hence dim W?Ddim Nul A. Thus,
dimWCdimW?Ddim Row ACdim Nul ADrankACdim Nul ADn, by the
Rank Theorem. If WD f0g, then W?DRn, and the result follows.
6.2 ORTHOGONAL SETS
A set of vectors fu1; : : : ; upginRnis said to be an orthogonal set if each pair of distinct
vectors from the set is orthogonal, that is, if uiujD0whenever i¬§j.
EXAMPLE 1 Show that fu1;u2;u3gis an orthogonal set, where
u1D2
43
1
13
5;u2D2
4 1
2
13
5;u3D2
4 1=2
 2
7=23
5
SOLUTION Consider the three possible pairs of distinct vectors, namely, fu1;u2g,
fu1;u3g, andfu2;u3g.
u1u2D3. 1/C1.2/C1.1/D0
u1u3D3 
 1
2
C1. 2/C1 7
2
D0
u2u3D  1 
 1
2
C2. 2/C1 7
2
D0
Each pair of distinct vectors is orthogonal, and so fu1;u2;u3gis an orthogonal set. See
Figure 1; the three line segments there are mutually perpendicular.
u1u2u3
x2
x1x3 FIGURE 1
T H E O R E M 4 IfSD fu1; : : : ; upgis an orthogonal set of nonzero vectors in Rn, then Sis
linearly independent and hence is a basis for the subspace spanned by S.
SECOND REVISED PAGES


--- Page 358 ---
6.2 Orthogonal Sets 341
PROOF If0Dc1u1C    C cpupfor some scalars c1; : : : ; c p, then
0D0u1D.c1u1Cc2u2C    C cpup/u1
D.c1u1/u1C.c2u2/u1C    C .cpup/u1
Dc1.u1u1/Cc2.u2u1/C    C cp.upu1/
Dc1.u1u1/
because u1is orthogonal to u2; : : : ; up. Since u1is nonzero, u1u1is not zero and so
c1D0. Similarly, c2; : : : ; c pmust be zero. Thus Sis linearly independent.
D E F I N I T I O N Anorthogonal basis for a subspace WofRnis a basis for Wthat is also an
orthogonal set.
The next theorem suggests why an orthogonal basis is much nicer than other bases.
The weights in a linear combination can be computed easily.
T H E O R E M 5 Letfu1; : : : ; upgbe an orthogonal basis for a subspace WofRn. For each yinW,
the weights in the linear combination
yDc1u1C    C cpup
are given by
cjDyuj
ujuj.jD1; : : : ; p/
PROOF As in the preceding proof, the orthogonality of fu1; : : : ; upgshows that
yu1D.c1u1Cc2u2C    C cpup/u1Dc1.u1u1/
Since u1u1is not zero, the equation above can be solved for c1. To Ô¨Ånd cjfor
jD2; : : : ; p , compute yujand solve for cj.
EXAMPLE 2 The set SD fu1;u2;u3gin Example 1 is an orthogonal basis for R3.
Express the vector yD2
46
1
 83
5as a linear combination of the vectors in S.
SOLUTION Compute
yu1D11; yu2D  12; yu3D  33
u1u1D11; u2u2D6; u3u3D33=2
By Theorem 5,
yDyu1
u1u1u1Cyu2
u2u2u2Cyu3
u3u3u3
D11
11u1C 12
6u2C 33
33=2u3
Du1 2u2 2u3
Notice how easy it is to compute the weights needed to build yfrom an orthogonal
basis. If the basis were not orthogonal, it would be necessary to solve a system of linear
equations in order to Ô¨Ånd the weights, as in Chapter 1.
We turn next to a construction that will become a key step in many calculations
involving orthogonality, and it will lead to a geometric interpretation of Theorem 5.
SECOND REVISED PAGES


--- Page 359 ---
342 CHAPTER 6 Orthogonality and Least Squares
An Orthogonal Projection
Given a nonzero vector uinRn, consider the problem of decomposing a vector yinRn
into the sum of two vectors, one a multiple of uand the other orthogonal to u. We wish
to write
yDOyCz (1)
where OyDufor some scalar andzis some vector orthogonal to u. See Figure 2.
yÀÜ z /H11005 y /H11002 y
ÀÜy /H11005 projW y0
W
FIGURE 2
Finding to make y Oy
orthogonal to u.Given any scalar , letzDy u, so that (1) is satisÔ¨Åed. Then y Oyis orthogonal to
uif and only if
0D.y u/uDyu .u/uDyu .uu/
That is, (1) is satisÔ¨Åed with zorthogonal to uif and only if Dyu
uuandOyDyu
uuu.
The vector Oyis called the orthogonal projection of y onto u , and the vector zis called
thecomponent of y orthogonal to u .
Ifcis any nonzero scalar and if uis replaced by cuin the deÔ¨Ånition of Oy, then the
orthogonal projection of yontocuis exactly the same as the orthogonal projection of
yonto u(Exercise 31). Hence this projection is determined by the subspace Lspanned
byu(the line through uand0). Sometimes Oyis denoted by projLyand is called the
orthogonal projection of y onto L. That is,
OyDprojLyDyu
uuu (2)
EXAMPLE 3 LetyD7
6
anduD4
2
. Find the orthogonal projection of yonto
u. Then write yas the sum of two orthogonal vectors, one in Span fugand one orthogonal
tou.
SOLUTION Compute
yuD7
6
4
2
D40
uuD4
2
4
2
D20
The orthogonal projection of yonto uis
OyDyu
uuuD40
20uD24
2
D8
4
and the component of yorthogonal to uis
y OyD7
6
 8
4
D 1
2
The sum of these two vectors is y. That is,
7
6
"
yD8
4
"
OyC 1
2
"
.y Oy/
This decomposition of yis illustrated in Figure 3. Note: If the calculations above are
correct, then fOy;y Oygwill be an orthogonal set. As a check, compute
Oy.y Oy/D8
4
 1
2
D  8C8D0
SECOND REVISED PAGES


--- Page 360 ---
6.2 Orthogonal Sets 343
x1x2y
uyÀÜL = Span{ u}
3
186
yy ‚ÄìÀÜ
FIGURE 3 The orthogonal projection of yonto a
lineLthrough the origin.
Since the line segment in Figure 3 between yandOyis perpendicular to L, by con-
struction of Oy, the point identiÔ¨Åed with Oyis the closest point of Ltoy. (This can be proved
from geometry. We will assume this for R2now and prove it for Rnin Section 6.3.)
EXAMPLE 4 Find the distance in Figure 3 from ytoL.
SOLUTION The distance from ytoLis the length of the perpendicular line segment
from yto the orthogonal projection Oy. This length equals the length of y Oy. Thus the
distance is
ky Oyk Dp
. 1/2C22Dp
5
A Geometric Interpretation of Theorem 5
The formula for the orthogonal projection Oyin (2) has the same appearance as each of the
terms in Theorem 5. Thus Theorem 5 decomposes a vector yinto a sum of orthogonal
projections onto one-dimensional subspaces.
It is easy to visualize the case in which WDR2DSpanfu1;u2g, with u1andu2
orthogonal. Any yinR2can be written in the form
yDyu1
u1u1u1Cyu2
u2u2u2 (3)
The Ô¨Årst term in (3) is the projection of yonto the subspace spanned by u1(the line
through u1and the origin), and the second term is the projection of yonto the subspace
spanned by u2. Thus (3) expresses yas the sum of its projections onto the (orthogonal)
axes determined by u1andu2. See Figure 4.
0y
u1u2
ÀÜy2 = projection onto u2
ÀÜy1 = projection onto u1
FIGURE 4 A vector decomposed into
the sum of two projections.
SECOND REVISED PAGES


--- Page 361 ---
344 CHAPTER 6 Orthogonality and Least Squares
Theorem 5 decomposes each yin Span fu1; : : : ; upginto the sum of pprojections
onto one-dimensional subspaces that are mutually orthogonal.
Decomposing a Force into Component Forces
The decomposition in Figure 4 can occur in physics when some sort of force is applied to
an object. Choosing an appropriate coordinate system allows the force to be represented
by a vector yinR2orR3. Often the problem involves some particular direction of
interest, which is represented by another vector u. For instance, if the object is moving
in a straight line when the force is applied, the vector umight point in the direction of
movement, as in Figure 5. A key step in the problem is to decompose the force into
a component in the direction of uand a component orthogonal to u. The calculations
would be analogous to those made in Example 3 above.
uy
FIGURE 5
Orthonormal Sets
A set fu1; : : : ; upgis an orthonormal set if it is an orthogonal set of unit vectors. If W
is the subspace spanned by such a set, then fu1; : : : ; upgis an orthonormal basis for
W, since the set is automatically linearly independent, by Theorem 4.
The simplest example of an orthonormal set is the standard basis fe1; : : : ; engfor
Rn. Any nonempty subset of fe1; : : : ; engis orthonormal, too. Here is a more compli-
cated example.
EXAMPLE 5 Show that fv1;v2;v3gis an orthonormal basis of R3, where
v1D2
643=p
11
1=p
11
1=p
113
75;v2D2
64 1=p
6
2=p
6
1=p
63
75;v3D2
64 1=p
66
 4=p
66
7=p
663
75
SOLUTION Compute
v1v2D  3=p
66C2=p
66C1=p
66D0
v1v3D  3=p
726 4=p
726C7=p
726D0
v2v3D1=p
396 8=p
396C7=p
396D0
Thusfv1;v2;v3gis an orthogonal set. Also,
v1v1D9=11C1=11C1=11D1
v2v2D1=6C4=6C1=6D1
v3v3D1=66C16=66 C49=66 D1
which shows that v1,v2, and v3are unit vectors. Thus fv1;v2;v3gis an orthonormal set.
Since the set is linearly independent, its three vectors form a basis for R3. See Figure 6.
v3
v2
x1v1x3
x2 FIGURE 6
SECOND REVISED PAGES


--- Page 362 ---
6.2 Orthogonal Sets 345
When the vectors in an orthogonal set of nonzero vectors are normalized to have
unit length, the new vectors will still be orthogonal, and hence the new set will be
an orthonormal set. See Exercise 32. It is easy to check that the vectors in Figure 6
(Example 5) are simply the unit vectors in the directions of the vectors in Figure 1
(Example 1).
Matrices whose columns form an orthonormal set are important in applications and
in computer algorithms for matrix computations. Their main properties are given in
Theorems 6 and 7.
T H E O R E M 6 Anmnmatrix Uhas orthonormal columns if and only if UTUDI.
PROOF To simplify notation, we suppose that Uhas only three columns, each a vector
inRm. The proof of the general case is essentially the same. Let UD¬åu1u2u3¬ç
and compute
UTUD2
64uT
1
uT
2
uT
33
75u1u2u3
D2
64uT
1u1uT
1u2uT
1u3
uT
2u1uT
2u2uT
2u3
uT
3u1uT
3u2uT
3u33
75 (4)
The entries in the matrix at the right are inner products, using transpose notation. The
columns of Uare orthogonal if and only if
uT
1u2DuT
2u1D0; uT
1u3DuT
3u1D0; uT
2u3DuT
3u2D0 (5)
The columns of Uall have unit length if and only if
uT
1u1D1; uT
2u2D1; uT
3u3D1 (6)
The theorem follows immediately from (4)‚Äì(6).
T H E O R E M 7 LetUbe an mnmatrix with orthonormal columns, and let xandybe inRn.
Then
a.kUxk D k xk
b..Ux/.Uy/Dxy
c..Ux/.Uy/D0if and only if xyD0
Properties (a) and (c) say that the linear mapping x7!Uxpreserves lengths and or-
thogonality. These properties are crucial for many computer algorithms. See Exercise 25
for the proof of Theorem 7.
EXAMPLE 6 LetUD2
641=p
2 2=3
1=p
2 2=3
0 1=33
75andxDp
2
3
. Notice that Uhas or-
thonormal columns and
UTUD
1=p
2 1=p
2 0
2=3  2=3 1=32
41=p
2 2=3
1=p
2 2=3
0 1=33
5D1 0
0 1
Verify that kUxk D k xk.
SECOND REVISED PAGES


--- Page 363 ---
346 CHAPTER 6 Orthogonality and Least Squares
SOLUTION
UxD2
41=p
2 2=3
1=p
2 2=3
0 1=33
5p
2
3
D2
43
 1
13
5
kUxk Dp
9C1C1Dp
11
kxk Dp
2C9Dp
11
Theorems 6 and 7 are particularly useful when applied to square matrices. An
orthogonal matrix is a square invertible matrix Usuch that U 1DUT. By Theo-
rem 6, such a matrix has orthonormal columns.1It is easy to see that any square matrix
with orthonormal columns is an orthogonal matrix. Surprisingly, such a matrix must
have orthonormal rows , too. See Exercises 27 and 28. Orthogonal matrices will appear
frequently in Chapter 7.
EXAMPLE 7 The matrix
UD2
643=p
11 1=p
6 1=p
66
1=p
11 2=p
6 4=p
66
1=p
11 1=p
6 7=p
663
75
is an orthogonal matrix because it is square and because its columns are orthonormal,
by Example 5. Verify that the rows are orthonormal, too!
PRACTICE PROBLEMS
1.Letu1D 1=p
5
2=p
5
andu2D2=p
5
1=p
5
. Show that fu1;u2gis an orthonormal basis
forR2.
2.LetyandLbe as in Example 3 and Figure 3. Compute the orthogonal projection Oy
ofyontoLusing uD2
1
instead of the uin Example 3.
3.LetUandxbe as in Example 6, and let yD
 3p
2
6
. Verify that UxUyDxy.
4.LetUbe an nnmatrix with orthonormal columns. Show that det UD¬±1.
6.2 EXERCISES
In Exercises 1‚Äì6, determine which sets of vectors are orthogonal.
1.2
4 1
4
 33
5,2
45
2
13
5,2
43
 4
 73
5 2.2
41
 2
13
5,2
40
1
23
5,2
4 5
 2
13
5
3.2
42
 7
 13
5,2
4 6
 3
93
5,2
43
1
 13
5 4.2
42
 5
 33
5,2
40
0
03
5,2
44
 2
63
55.2
6643
 2
1
33
775,2
664 1
3
 3
43
775,2
6643
8
7
03
7756.2
6645
 4
0
33
775,2
664 4
1
 3
83
775,2
6643
3
5
 13
775
In Exercises 7‚Äì10, show that fu1;u2gorfu1;u2;u3gis an orthog-
onal basis for R2orR3, respectively. Then express xas a linear
combination of the u‚Äôs.
7.u1D2
 3
,u2D6
4
, and xD9
 7
1A better name might be orthonormal matrix , and this term is found in some statistics texts. However,
orthogonal matrix is the standard term in linear algebra.
SECOND REVISED PAGES


--- Page 364 ---
6.2 Orthogonal Sets 347
8.u1D3
1
,u2D 2
6
, and xD 6
3
9.u1D2
41
0
13
5,u2D2
4 1
4
13
5,u3D2
42
1
 23
5, and xD2
48
 4
 33
5
10. u1D2
43
 3
03
5,u2D2
42
2
 13
5,u3D2
41
1
43
5, and xD2
45
 3
13
5
11.Compute the orthogonal projection of1
7
onto the line
through 4
2
and the origin.
12. Compute the orthogonal projection of1
 1
onto the line
through 1
3
and the origin.
13. LetyD2
3
anduD4
 7
. Write yas the sum of two
orthogonal vectors, one in Span fugand one orthogonal to u.
14. LetyD2
6
anduD7
1
. Write yas the sum of a vector
in Span fugand a vector orthogonal to u.
15. LetyD3
1
anduD8
6
. Compute the distance from yto
the line through uand the origin.
16. LetyD 3
9
anduD1
2
. Compute the distance from y
to the line through uand the origin.
In Exercises 17‚Äì22, determine which sets of vectors are orthonor-
mal. If a set is only orthogonal, normalize the vectors to produce
an orthonormal set.
17.2
41=3
1=3
1=33
5,2
4 1=2
0
1=23
5 18.2
40
1
03
5,2
40
 1
03
5
19. :6
:8
,:8
:6
20.2
4 2=3
1=3
2=33
5,2
41=3
2=3
03
5
21.2
41=p
10
3=p
20
3=p
203
5,2
43=p
10
 1=p
20
 1=p
203
5,2
40
 1=p
2
1=p
23
5
22.2
41=p
18
4=p
18
1=p
183
5,2
41=p
2
0
 1=p
23
5,2
4 2=3
1=3
 2=33
5
In Exercises 23 and 24, all vectors are in Rn. Mark each statement
True or False. Justify each answer.
23. a.Not every linearly independent set in Rnis an orthogonal
set.b.Ifyis a linear combination of nonzero vectors from an
orthogonal set, then the weights in the linear combination
can be computed without row operations on a matrix.
c.If the vectors in an orthogonal set of nonzero vectors are
normalized, then some of the new vectors may not be
orthogonal.
d.A matrix with orthonormal columns is an orthogonal
matrix.
e.IfLis a line through 0and if Oyis the orthogonal projection
ofyontoL, then kOykgives the distance from ytoL.
24. a.Not every orthogonal set in Rnis linearly independent.
b.If a set SD fu1; : : : ; upghas the property that uiujD0
whenever i¬§j, then Sis an orthonormal set.
c.If the columns of an mnmatrix Aare orthonormal, then
the linear mapping x7!Axpreserves lengths.
d.The orthogonal projection of yonto vis the same as the
orthogonal projection of yontocvwhenever c¬§0.
e.An orthogonal matrix is invertible.
25. Prove Theorem 7. [ Hint: For (a), compute kUxk2, or prove
(b) Ô¨Årst.]
26. Suppose Wis a subspace of Rnspanned by nnonzero
orthogonal vectors. Explain why WDRn.
27. LetUbe a square matrix with orthonormal columns. Explain
whyUis invertible. (Mention the theorems you use.)
28. LetUbe an nnorthogonal matrix. Show that the rows of
Uform an orthonormal basis of Rn.
29. LetUandVbennorthogonal matrices. Explain why
UV is an orthogonal matrix. [That is, explain why UV is
invertible and its inverse is .UV /T.]
30. LetUbe an orthogonal matrix, and construct Vby inter-
changing some of the columns of U. Explain why Vis an
orthogonal matrix.
31. Show that the orthogonal projection of a vector yonto a line
Lthrough the origin in R2does not depend on the choice
of the nonzero uinLused in the formula for Oy. To do this,
suppose yand uare given and Oyhas been computed by
formula (2) in this section. Replace uin that formula by cu,
where cis an unspeciÔ¨Åed nonzero scalar. Show that the new
formula gives the same Oy.
32. Letfv1;v2gbe an orthogonal set of nonzero vectors, and let
c1,c2be any nonzero scalars. Show that fc1v1; c2v2gis also
an orthogonal set. Since orthogonality of a set is deÔ¨Åned in
terms of pairs of vectors, this shows that if the vectors in
an orthogonal set are normalized, the new set will still be
orthogonal.
33. Given u¬§0inRn, letLDSpanfug. Show that the mapping
x7!projLxis a linear transformation.
34. Given u¬§0inRn, let LDSpanfug. For yinRn, the
reÔ¨Çection of y in Lis the point reÔ¨Ç LydeÔ¨Åned by
SECOND REVISED PAGES


--- Page 365 ---
348 CHAPTER 6 Orthogonality and Least Squares
reÔ¨ÇLyD2projLy y
See the Ô¨Ågure, which shows that reÔ¨Ç Lyis the sum of
OyDprojLyandOy y. Show that the mapping y7!reÔ¨ÇLy
is a linear transformation.
x1x2y
uyÀÜL = Span{ u}
yy ‚ÄìÀÜ reflL y
yy ‚ÄìÀÜ
The reÔ¨Çection of yin a line through the origin.
35. [M] Show that the columns of the matrix Aare orthogonal
by making an appropriate matrix calculation. State the calcu-
lation you use.AD2
66666666664 6 3 6 1
 1 2 1  6
3 6 3  2
6 3 6  1
2 1 2 3
 3 6 3 2
 2 1 2  3
1 2 1 63
77777777775
36. [M] In parts (a)‚Äì(d), let Ube the matrix formed by normal-
izing each column of the matrix Ain Exercise 35.
a.Compute UTUandU UT. How do they differ?
b.Generate a random vector yinR8, and compute
pDU UTyandzDy p. Explain why pis in Col A.
Verify that zis orthogonal to p.
c.Verify that zis orthogonal to each column of U.
d.Notice that yDpCz, with pin Col A. Explain why zis
in.ColA/?. (The signiÔ¨Åcance of this decomposition of
ywill be explained in the next section.)
SOLUTIONS TO PRACTICE PROBLEMS
1.The vectors are orthogonal because
u1u2D  2=5C2=5D0
They are unit vectors because
ku1k2D. 1=p
5/2C.2=p
5/2D1=5C4=5D1
ku2k2D.2=p
5/2C.1=p
5/2D4=5C1=5D1
In particular, the set fu1;u2gis linearly independent, and hence is a basis for R2since
there are two vectors in the set.
2.When yD7
6
anduD2
1
,
OyDyu
uuuD20
52
1
D42
1
D8
4
This is the same Oyfound in Example 3. The orthogonal projection does not seem to
depend on the uchosen on the line. See Exercise 31.
3.UyD2
41=p
2 2=3
1=p
2 2=3
0 1=33
5
 3p
2
6
D2
41
 7
23
5
Also, from Example 6, xDp
2
3
andUxD2
43
 1
13
5. Hence
UxUyD3C7C2D12; and xyD  6C18D12
4.Since Uis an nnmatrix with orthonormal columns, by Theorem 6, UTUDI.
Taking the determinant of the left side of this equation, and applying Theorems 5
and 6 from Section 3.2 results in det UTUD.detUT/.detU /D.detU /.detU /D
.detU /2. Recall det ID1. Putting the two sides of the equation back together results
in (det U)2D1and hence det UD¬±1.
SGMastering: Orthogonal
Basis 6‚Äì4
SECOND REVISED PAGES


--- Page 366 ---
6.3 Orthogonal Projections 349
6.3 ORTHOGONAL PROJECTIONS
The orthogonal projection of a point in R2onto a line through the origin has an important
analogue in Rn. Given a vector yand a subspace WinRn, there is a vector OyinWsuch
that (1) Oyis the unique vector in Wfor which y Oyis orthogonal to W, and (2) Oyis
the unique vector in Wclosest to y. See Figure 1. These two properties of Oyprovide the
key to Ô¨Ånding least-squares solutions of linear systems, mentioned in the introductory
example for this chapter. The full story will be told in Section 6.5.
To prepare for the Ô¨Årst theorem, observe that whenever a vector yis written as a
linear combination of vectors u1; : : : ; uninRn, the terms in the sum for ycan be grouped
y
ÀÜy /H11005 projW y0
W
FIGURE 1into two parts so that ycan be written as
yDz1Cz2
where z1is a linear combination of some of the uiandz2is a linear combination of
the rest of the ui. This idea is particularly useful when fu1; : : : ; ungis an orthogonal
basis. Recall from Section 6.1 that W?denotes the set of all vectors orthogonal to a
subspace W.
EXAMPLE 1 Letfu1; : : : ; u5gbe an orthogonal basis for R5and let
yDc1u1C    C c5u5
Consider the subspace WDSpanfu1;u2g, and write yas the sum of a vector z1inW
and a vector z2inW?.
SOLUTION Write
yDc1u1Cc2u2¬Ñ¬É¬Ç ¬Ö
z1Cc3u3Cc4u4Cc5u5¬Ñ ¬É¬Ç ¬Ö
z2
z1Dc1u1Cc2u2is in Span fu1;u2g where
z2Dc3u3Cc4u4Cc5u5is in Span fu3;u4;u5g: and
To show that z2is inW?, it sufÔ¨Åces to show that z2is orthogonal to the vectors in the
basisfu1;u2gforW. (See Section 6.1.) Using properties of the inner product, compute
z2u1D.c3u3Cc4u4Cc5u5/u1
Dc3u3u1Cc4u4u1Cc5u5u1
D0
because u1is orthogonal to u3,u4, and u5. A similar calculation shows that z2u2D0.
Thus z2is inW?.
The next theorem shows that the decomposition yDz1Cz2in Example 1 can be
computed without having an orthogonal basis for Rn. It is enough to have an orthogonal
basis only for W.
SECOND REVISED PAGES


--- Page 367 ---
350 CHAPTER 6 Orthogonality and Least Squares
T H E O R E M 8 The Orthogonal Decomposition Theorem
LetWbe a subspace of Rn. Then each yinRncan be written uniquely in the form
yDOyCz (1)
where Oyis inWandzis inW?. In fact, if fu1; : : : ; upgis any orthogonal basis of
W, then
OyDyu1
u1u1u1C    Cyup
upupup (2)
andzDy Oy.
The vector Oyin (1) is called the orthogonal projection of y onto Wand often is
written as projWy. See Figure 2. When Wis a one-dimensional subspace, the formula
forOymatches the formula given in Section 6.2.
yÀÜ z /H11005 y /H11002 y
ÀÜy /H11005 projW y0
W
FIGURE 2 The orthogonal projection
ofyontoW.
PROOF Letfu1; : : : ; upgbe any orthogonal basis for W, and deÔ¨Åne Oyby (2).1Then Oy
is inWbecause Oyis a linear combination of the basis u1; : : : ; up. Let zDy Oy. Since
u1is orthogonal to u2; : : : ; up, it follows from (2) that
zu1D.y Oy/u1Dyu1 yu1
u1u1
u1u1 0       0
Dyu1 yu1D0
Thus zis orthogonal to u1. Similarly, zis orthogonal to each ujin the basis for W.
Hence zis orthogonal to every vector in W. That is, zis inW?.
To show that the decomposition in (1) is unique, suppose ycan also be written as
yDOy1Cz1, with Oy1inWandz1inW?. Then OyCzDOy1Cz1(since both sides equal
y/, and so
Oy Oy1Dz1 z
This equality shows that the vector vDOy Oy1is inWand in W?(because z1andz
are both in W?, and W?is a subspace). Hence vvD0, which shows that vD0. This
proves that OyDOy1and also z1Dz.
The uniqueness of the decomposition (1) shows that the orthogonal projection Oy
depends only on Wand not on the particular basis used in (2).
1We may assume that Wis not the zero subspace, for otherwise W?DRnand (1) is simply yD0Cy.
The next section will show that any nonzero subspace of Rnhas an orthogonal basis.
SECOND REVISED PAGES


--- Page 368 ---
6.3 Orthogonal Projections 351
EXAMPLE 2 Letu1D2
42
5
 13
5,u2D2
4 2
1
13
5, and yD2
41
2
33
5. Observe that fu1;u2g
is an orthogonal basis for WDSpanfu1;u2g. Write yas the sum of a vector in Wand
a vector orthogonal to W.
SOLUTION The orthogonal projection of yontoWis
OyDyu1
u1u1u1Cyu2
u2u2u2
D9
302
42
5
 13
5C3
62
4 2
1
13
5D9
302
42
5
 13
5C15
302
4 2
1
13
5D2
4 2=5
2
1=53
5
Also
y OyD2
41
2
33
5 2
4 2=5
2
1=53
5D2
47=5
0
14=53
5
Theorem 8 ensures that y Oyis inW?. To check the calculations, however, it is a good
idea to verify that y Oyis orthogonal to both u1andu2and hence to all of W. The
desired decomposition of yis
yD2
41
2
33
5D2
4 2=5
2
1=53
5C2
47=5
0
14=53
5
A Geometric Interpretation of the Orthogonal Projection
When Wis a one-dimensional subspace, the formula (2) for projWycontains just one
term. Thus, when dim W > 1 , each term in (2) is itself an orthogonal projection of y
onto a one-dimensional subspace spanned by one of the u‚Äôs in the basis for W. Figure 3
illustrates this when Wis a subspace of R3spanned by u1andu2. Here Oy1andOy2denote
the projections of yonto the lines spanned by u1andu2, respectively. The orthogonal
projection OyofyontoWis the sum of the projections of yonto one-dimensional sub-
spaces that are orthogonal to each other. The vector Oyin Figure 3 corresponds to the
vector yin Figure 4 of Section 6.2, because now it is Oythat is in W.
y2x3x2
0ÀÜ
y u1 ÀÜ ÀÜ ÀÜ
y1u1u2
y . u1
u1 . u1
ÀÜ‚Äì‚Äì‚Äì‚Äì‚Äì u2 y1 y2y . u2
u2 . u2‚Äì‚Äì‚Äì‚Äì‚Äì
x1
FIGURE 3 The orthogonal projection of yis the
sum of its projections onto one-dimensional
subspaces that are mutually orthogonal.
SECOND REVISED PAGES


--- Page 369 ---
352 CHAPTER 6 Orthogonality and Least Squares
Properties of Orthogonal Projections
Iffu1; : : : ; upgis an orthogonal basis for Wand if yhappens to be in W, then the
formula for projWyis exactly the same as the representation of ygiven in Theorem 5
in Section 6.2. In this case, projWyDy.
Ifyis inWDSpanfu1; : : : ; upg, then projWyDy.
This fact also follows from the next theorem.
T H E O R E M 9 The Best Approximation Theorem
LetWbe a subspace of Rn, letybe any vector in Rn, and let Oybe the orthogonal
projection of yontoW. Then Oyis the closest point in Wtoy, in the sense that
ky Oyk<ky vk (3)
for all vinWdistinct from Oy.
The vector Oyin Theorem 9 is called the best approximation to y by elements of W.
Later sections in the text will examine problems where a given ymust be replaced, or
approximated , by a vector vin some Ô¨Åxed subspace W. The distance from ytov, given
byky vk, can be regarded as the ‚Äúerror‚Äù of using vin place of y. Theorem 9 says that
this error is minimized when vDOy.
Inequality (3) leads to a new proof that Oydoes not depend on the particular orthogo-
nal basis used to compute it. If a different orthogonal basis for Wwere used to construct
an orthogonal projection of y, then this projection would also be the closest point in W
toy, namely, Oy.
PROOF Take vinWdistinct from Oy. See Figure 4. Then Oy vis inW. By the Orthogo-
nal Decomposition Theorem, y Oyis orthogonal to W. In particular, y Oyis orthogonal
toOy v(which is in W). Since
y vD.y Oy/C.Oy v/
the Pythagorean Theorem gives
ky vk2D ky Oyk2C kOy vk2
(See the colored right triangle in Figure 4. The length of each side is labeled.) Now
kOy vk2> 0because Oy v¬§0, and so inequality (3) follows immediately.
y
v|| ||y ‚Äì vÀÜ0||y /H11002 y||ÀÜ
yÀÜ||y /H11002 v||
W
FIGURE 4 The orthogonal projection
ofyontoWis the closest point in W
toy.
SECOND REVISED PAGES


--- Page 370 ---
6.3 Orthogonal Projections 353
EXAMPLE 3 Ifu1D2
42
5
 13
5,u2D2
4 2
1
13
5,yD2
41
2
33
5, and WDSpanfu1;u2g,
as in Example 2, then the closest point in Wtoyis
OyDyu1
u1u1u1Cyu2
u2u2u2D2
4 2=5
2
1=53
5
EXAMPLE 4 The distance from a point yinRnto a subspace Wis deÔ¨Åned as the
distance from yto the nearest point in W. Find the distance from ytoWDSpanfu1;u2g,
where
yD2
4 1
 5
103
5;u1D2
45
 2
13
5;u2D2
41
2
 13
5
SOLUTION By the Best Approximation Theorem, the distance from ytoWisky Oyk,
where OyDprojWy. Since fu1;u2gis an orthogonal basis for W,
OyD15
30u1C 21
6u2D1
22
45
 2
13
5 7
22
41
2
 13
5D2
4 1
 8
43
5
y OyD2
4 1
 5
103
5 2
4 1
 8
43
5D2
40
3
63
5
ky Oyk2D32C62D45
The distance from ytoWisp
45D3p
5.
The Ô¨Ånal theorem in this section shows how formula (2) for projWyis simpliÔ¨Åed
when the basis for Wis an orthonormal set.
T H E O R E M 1 0 Iffu1; : : : ; upgis an orthonormal basis for a subspace WofRn, then
projWyD.yu1/u1C.yu2/u2C    C .yup/up (4)
IfUD¬åu1u2   up¬ç, then
projWyDU UTyfor all yinRn(5)
PROOF Formula (4) follows immediately from (2) in Theorem 8. Also, (4) shows
that projWyis a linear combination of the columns of Uusing the weights yu1,
yu2; : : : ; yup. The weights can be written as uT
1y;uT
2y; : : : ; uT
py, showing that they
are the entries in UTyand justifying (5).
Suppose Uis annpmatrix with orthonormal columns, and let Wbe the column
WEB
space of U. Then
UTUxDIpxDxfor all xinRpTheorem 6
U UTyDprojWy for all yinRnTheorem 10
IfUis an nn(square) matrix with orthonormal columns, then Uis an orthogonal
matrix, the column space Wis all of Rn, and U UTyDIyDyfor all yinRn.
Although formula (4) is important for theoretical purposes, in practice it usually
involves calculations with square roots of numbers (in the entries of the ui/. Formula
(2) is recommended for hand calculations.
SECOND REVISED PAGES


--- Page 371 ---
354 CHAPTER 6 Orthogonality and Least Squares
PRACTICE PROBLEMS
1.Letu1D2
4 7
1
43
5,u2D2
4 1
1
 23
5,yD2
4 9
1
63
5, and WDSpanfu1;u2g. Use the fact
thatu1andu2are orthogonal to compute projWy.
2.LetWbe a subspace of Rn. Let xandybe vectors in Rnand let zDxCy. Ifuis
the projection of xontoWandvis the projection of yontoW, show that uCvis
the projection of zontoW.
6.3 EXERCISES
In Exercises 1 and 2, you may assume that fu1; : : : ; u4gis an
orthogonal basis for R4.
1.u1D2
6640
1
 4
 13
775,u2D2
6643
5
1
13
775,u3D2
6641
0
1
 43
775,u4D2
6645
 3
 1
13
775,
xD2
66410
 8
2
03
775. Write xas the sum of two vectors, one in
Spanfu1;u2;u3gand the other in Span fu4g.
2.u1D2
6641
2
1
13
775,u2D2
664 2
1
 1
13
775,u3D2
6641
1
 2
 13
775,u4D2
664 1
1
1
 23
775,
vD2
6644
5
 3
33
775. Write vas the sum of two vectors, one in
Spanfu1gand the other in Span fu2;u3;u4g.
In Exercises 3‚Äì6, verify that fu1;u2gis an orthogonal set, and then
Ô¨Ånd the orthogonal projection of yonto Span fu1;u2g.
3.yD2
4 1
4
33
5,u1D2
41
1
03
5,u2D2
4 1
1
03
5
4.yD2
46
3
 23
5,u1D2
43
4
03
5,u2D2
4 4
3
03
5
5.yD2
4 1
2
63
5,u1D2
43
 1
23
5,u2D2
41
 1
 23
5
6.yD2
46
4
13
5,u1D2
4 4
 1
13
5,u2D2
40
1
13
5
In Exercises 7‚Äì10, let Wbe the subspace spanned by the u‚Äôs, and
write yas the sum of a vector in Wand a vector orthogonal to W.
7.yD2
41
3
53
5,u1D2
41
3
 23
5,u2D2
45
1
43
58.yD2
4 1
4
33
5,u1D2
41
1
13
5,u2D2
4 1
3
 23
5
9.yD2
6644
3
3
 13
775,u1D2
6641
1
0
13
775,u2D2
664 1
3
1
 23
775,u3D2
664 1
0
1
13
775
10. yD2
6643
4
5
63
775,u1D2
6641
1
0
 13
775,u2D2
6641
0
1
13
775,u3D2
6640
 1
1
 13
775
In Exercises 11 and 12, Ô¨Ånd the closest point to yin the subspace
Wspanned by v1andv2.
11.yD2
6643
1
5
13
775,v1D2
6643
1
 1
13
775,v2D2
6641
 1
1
 13
775
12. yD2
6643
 1
1
133
775,v1D2
6641
 2
 1
23
775,v2D2
664 4
1
0
33
775
In Exercises 13 and 14, Ô¨Ånd the best approximation to zby vectors
of the form c1v1Cc2v2.
13. zD2
6643
 7
2
33
775,v1D2
6642
 1
 3
13
775,v2D2
6641
1
0
 13
775
14. zD2
6642
4
0
 13
775,v1D2
6642
0
 1
 33
775,v2D2
6645
 2
4
23
775
15. LetyD2
45
 9
53
5,u1D2
4 3
 5
13
5,u2D2
4 3
2
13
5. Find the dis-
tance from yto the plane in R3spanned by u1andu2.
16. Lety,v1, and v2be as in Exercise 12. Find the distance from
yto the subspace of R4spanned by v1andv2.
SECOND REVISED PAGES


--- Page 372 ---
6.3 Orthogonal Projections 355
17. Let yD2
44
8
13
5,u1D2
42=3
1=3
2=33
5,u2D2
4 2=3
2=3
1=33
5, and
WDSpanfu1;u2g.
a.LetUD¬åu1u2¬ç. Compute UTUandU UT.
b.Compute projWyand.U UT/y.
18. LetyD7
9
,u1D1=p
10
 3=p
10
, and WDSpanfu1g.
a.LetUbe the 21matrix whose only column is u1.
Compute UTUandU UT.
b.Compute projWyand.U UT/y.
19. Letu1D2
41
1
 23
5,u2D2
45
 1
23
5, and u3D2
40
0
13
5. Note that
u1andu2are orthogonal but that u3is not orthogonal to u1or
u2. It can be shown that u3is not in the subspace Wspanned
byu1andu2. Use this fact to construct a nonzero vector vin
R3that is orthogonal to u1andu2.
20. Letu1andu2be as in Exercise 19, and let u4D2
40
1
03
5. It can
be shown that u4is not in the subspace Wspanned by u1and
u2. Use this fact to construct a nonzero vector vinR3that is
orthogonal to u1andu2.
In Exercises 21 and 22, all vectors and subspaces are in Rn. Mark
each statement True or False. Justify each answer.
21. a.Ifzis orthogonal to u1and to u2and if WD
Spanfu1;u2g, then zmust be in W?.
b.For each yand each subspace W, the vector y projWy
is orthogonal to W.
c.The orthogonal projection Oyofyonto a subspace Wcan
sometimes depend on the orthogonal basis for Wused to
compute Oy.
d.Ifyis in a subspace W, then the orthogonal projection of
yontoWisyitself.e.If the columns of an npmatrix Uare orthonormal, then
U UTyis the orthogonal projection of yonto the column
space of U.
22. a.IfWis a subspace of Rnand if vis in both WandW?,
then vmust be the zero vector.
b.In the Orthogonal Decomposition Theorem, each term in
formula (2) for Oyis itself an orthogonal projection of y
onto a subspace of W.
c.IfyDz1Cz2, where z1is in a subspace Wandz2is in
W?, then z1must be the orthogonal projection of yonto
W.
d.The best approximation to yby elements of a subspace
Wis given by the vector y projWy.
e.If an npmatrix Uhas orthonormal columns, then
U UTxDxfor all xinRn.
23. LetAbe an mnmatrix. Prove that every vector xinRn
can be written in the form xDpCu, where pis in Row A
anduis in Nul A. Also, show that if the equation AxDb
is consistent, then there is a unique pin Row Asuch that
ApDb.
24. LetWbe a subspace of Rnwith an orthogonal basis
fw1; : : : ; wpg, and let fv1; : : : ; vqgbe an orthogonal basis for
W?.
a.Explain why fw1; : : : ; wp;v1; : : : ; vqgis an orthogonal
set.
b.Explain why the set in part (a) spans Rn.
c.Show that dim WCdimW?Dn.
25. [M] Let Ube the 84matrix in Exercise 36 in Section 6.2.
Find the closest point to yD.1; 1; 1; 1; 1; 1; 1; 1/ in Col U.
Write the keystrokes or commands you use to solve this
problem.
26. [M] Let Ube the matrix in Exercise 25. Find the distance
from bD.1; 1; 1; 1;  1; 1; 1; 1/to Col U.
SOLUTION TO PRACTICE PROBLEMS
1.Compute
projWyDyu1
u1u1u1Cyu2
u2u2u2D88
66u1C 2
6u2
D4
32
4 7
1
43
5 1
32
4 1
1
 23
5D2
4 9
1
63
5Dy
In this case, yhappens to be a linear combination of u1andu2, soyis inW. The
closest point in Wtoyisyitself.
2.Using Theorem 10, let Ube a matrix whose columns consist of an orthonormal
basis for W. Then proj WzDU UTzDU UT(xCy)DU UTxCU UTyD
projWxCprojWyDuCv.
SECOND REVISED PAGES


--- Page 373 ---
356 CHAPTER 6 Orthogonality and Least Squares
6.4 THE GRAM SCHMIDT PROCESS
The Gram‚ÄìSchmidt process is a simple algorithm for producing an orthogonal or
orthonormal basis for any nonzero subspace of Rn. The Ô¨Årst two examples of the process
are aimed at hand calculation.
EXAMPLE 1 LetWDSpanfx1;x2g, where x1D2
43
6
03
5and x2D2
41
2
23
5. Con-
x3
v1 /H11005 x10
x2W
x2v2
p
x1
FIGURE 1
Construction of an orthogonal
basisfv1;v2g.struct an orthogonal basis fv1;v2gforW.
SOLUTION The subspace Wis shown in Figure 1, along with x1,x2, and the projection
pofx2onto x1. The component of x2orthogonal to x1isx2 p, which is in Wbecause
it is formed from x2and a multiple of x1. Let v1Dx1and
v2Dx2 pDx2 x2x1
x1x1x1D2
41
2
23
5 15
452
43
6
03
5D2
40
0
23
5
Then fv1;v2gis an orthogonal set of nonzero vectors in W. Since dim WD2, the set
fv1;v2gis a basis for W.
The next example fully illustrates the Gram‚ÄìSchmidt process. Study it carefully.
EXAMPLE 2 Letx1D2
6641
1
1
13
775,x2D2
6640
1
1
13
775, and x3D2
6640
0
1
13
775. Then fx1;x2;x3gis
clearly linearly independent and thus is a basis for a subspace WofR4. Construct an
orthogonal basis for W.
SOLUTION
Step 1 .Letv1Dx1andW1DSpanfx1g DSpanfv1g.
Step 2 .Letv2be the vector produced by subtracting from x2its projection onto the
subspace W1. That is, let
v2Dx2 projW1x2
Dx2 x2v1
v1v1v1 Since v1Dx1
D2
6640
1
1
13
775 3
42
6641
1
1
13
775D2
664 3=4
1=4
1=4
1=43
775
As in Example 1, v2is the component of x2orthogonal to x1, and fv1;v2gis an
orthogonal basis for the subspace W2spanned by x1andx2.
Step 20(optional). If appropriate, scale v2to simplify later computations. Since v2has
fractional entries, it is convenient to scale it by a factor of 4 and replace fv1;v2gby the
orthogonal basis
v1D2
6641
1
1
13
775;v0
2D2
664 3
1
1
13
775
SECOND REVISED PAGES


--- Page 374 ---
6.4 The Gram‚ÄìSchmidt Process 357
Step 3 .Letv3be the vector produced by subtracting from x3its projection onto the
subspace W2. Use the orthogonal basis fv1;v0
2gto compute this projection onto W2:
projW2x3DProjection of
x3onto v1
?
x3v1
v1v1v1CProjection of
x3onto v0
2
?
x3v0
2
v0
2v0
2v0
2D2
42
6641
1
1
13
775C2
122
664 3
1
1
13
775D2
6640
2=3
2=3
2=33
775
Then v3is the component of x3orthogonal to W2, namely,
v3Dx3 projW2x3D2
6640
0
1
13
775 2
6640
2=3
2=3
2=33
775D2
6640
 2=3
1=3
1=33
775
See Figure 2 for a diagram of this construction. Observe that v3is inW, because x3
and proj W2x3are both in W. Thus fv1;v0
2;v3gis an orthogonal set of nonzero vectors
and hence a linearly independent set in W. Note that Wis three-dimensional since it
was deÔ¨Åned by a basis of three vectors. Hence, by the Basis Theorem in Section 4.5,
fv1;v0
2;v3gis an orthogonal basis for W.
v3
v1v/H110322x3
projW2x30
W2 /H11005 Span{ v1, v/H110322}
FIGURE 2 The construction of v3from
x3andW2.
The proof of the next theorem shows that this strategy really works. Scaling of
vectors is not mentioned because that is used only to simplify hand calculations.
T H E O R E M 1 1 The Gram--Schmidt Process
Given a basis fx1; : : : ; xpgfor a nonzero subspace WofRn, deÔ¨Åne
v1Dx1
v2Dx2 x2v1
v1v1v1
v3Dx3 x3v1
v1v1v1 x3v2
v2v2v2
:::
vpDxp xpv1
v1v1v1 xpv2
v2v2v2      xpvp 1
vp 1vp 1vp 1
Then fv1; : : : ; vpgis an orthogonal basis for W. In addition
Spanfv1; : : : ; vkg DSpanfx1; : : : ; xkg for1kp (1)
SECOND REVISED PAGES


--- Page 375 ---
358 CHAPTER 6 Orthogonality and Least Squares
PROOF For1kp, letWkDSpanfx1; : : : ; xkg. Set v1Dx1, so that Span fv1g D
Spanfx1g. Suppose, for some k < p , we have constructed v1; : : : ; vkso that fv1; : : : ; vkg
is an orthogonal basis for Wk. DeÔ¨Åne
vkC1DxkC1 projWkxkC1 (2)
By the Orthogonal Decomposition Theorem, vkC1is orthogonal to Wk. Note that
projWkxkC1is inWkand hence also in WkC1. Since xkC1is inWkC1, so is vkC1(because
WkC1is a subspace and is closed under subtraction). Furthermore, vkC1¬§0because
xkC1is not in WkDSpanfx1; : : : ; xkg. Hence fv1; : : : ; vkC1gis an orthogonal set of
nonzero vectors in the .kC1/-dimensional space WkC1. By the Basis Theorem in Sec-
tion 4.5, this set is an orthogonal basis for WkC1. Hence WkC1DSpanfv1; : : : ; vkC1g.
When kC1Dp, the process stops.
Theorem 11 shows that any nonzero subspace WofRnhas an orthogonal basis, be-
cause an ordinary basis fx1; : : : ; xpgis always available (by Theorem 11 in Section 4.5),
and the Gram‚ÄìSchmidt process depends only on the existence of orthogonal projections
onto subspaces of Wthat already have orthogonal bases.
Orthonormal Bases
An orthonormal basis is constructed easily from an orthogonal basis fv1; : : : ; vpg:
simply normalize (i.e., ‚Äúscale‚Äù) all the vk. When working problems by hand, this is
easier than normalizing each vkas soon as it is found (because it avoids unnecessary
writing of square roots).
EXAMPLE 3 Example 1 constructed the orthogonal basis
v1D2
43
6
03
5;v2D2
40
0
23
5
An orthonormal basis is
u1D1
kv1kv1D1p
452
43
6
03
5D2
41=p
5
2=p
5
03
5
u2D1
kv2kv2D2
40
0
13
5
QR Factorization of Matrices
If an mnmatrix Ahas linearly independent columns x1; : : : ; xn, then applying the
WEB
Gram‚ÄìSchmidt process (with normalizations) to x1; : : : ; xnamounts to factoring A, as
described in the next theorem. This factorization is widely used in computer algorithms
for various computations, such as solving equations (discussed in Section 6.5) and
Ô¨Ånding eigenvalues (mentioned in the exercises for Section 5.2).
SECOND REVISED PAGES


--- Page 376 ---
6.4 The Gram‚ÄìSchmidt Process 359
T H E O R E M 1 2 The QR Factorization
IfAis anmnmatrix with linearly independent columns, then Acan be factored
asADQR, where Qis an mnmatrix whose columns form an orthonormal
basis for Col AandRis annnupper triangular invertible matrix with positive
entries on its diagonal.
PROOF The columns of Aform a basis fx1; : : : ; xngfor Col A. Construct an orthonor-
mal basis fu1; : : : ; ungforWDColAwith property (1) in Theorem 11. This basis may
be constructed by the Gram‚ÄìSchmidt process or some other means. Let
QD¬åu1u2   un¬ç
ForkD1; : : : ; n; xkis in Span fx1; : : : ; xkg DSpanfu1; : : : ; ukg. So there are con-
stants, r1k; : : : ; r kk, such that
xkDr1ku1C    C rkkukC0ukC1C    C 0un
We may assume that rkk0. (Ifrkk< 0, multiply both rkkandukby 1.) This shows
thatxkis a linear combination of the columns of Qusing as weights the entries in the
vector
rkD2
66666664r1k
:::
rkk
0:::
03
77777775
That is, xkDQrkforkD1; : : : ; n . LetRD¬år1   rn¬ç. Then
AD¬åx1   xn¬çD¬åQr1  Qrn¬çDQR
The fact that Ris invertible follows easily from the fact that the columns of Aare linearly
independent (Exercise 19). Since Ris clearly upper triangular, its nonnegative diagonal
entries must be positive.
EXAMPLE 4 Find a QR factorization of AD2
6641 0 0
1 1 0
1 1 1
1 1 13
775.
SOLUTION The columns of Aare the vectors x1,x2, and x3in Example 2. An
orthogonal basis for Col ADSpanfx1;x2;x3gwas found in that example:
v1D2
6641
1
1
13
775;v0
2D2
664 3
1
1
13
775;v3D2
6640
 2=3
1=3
1=33
775
To simplify the arithmetic that follows, scale v3by letting v0
3D3v3. Then normalize
the three vectors to obtain u1,u2, and u3, and use these vectors as the columns of Q:
QD2
66641=2  3=p
12 0
1=2 1=p
12  2=p
6
1=2 1=p
12 1=p
6
1=2 1=p
12 1=p
63
7775
SECOND REVISED PAGES


--- Page 377 ---
360 CHAPTER 6 Orthogonality and Least Squares
By construction, the Ô¨Årst kcolumns of Qare an orthonormal basis of Span fx1; : : : ; xkg.
From the proof of Theorem 12, ADQRfor some R. To Ô¨Ånd R, observe that QTQDI,
because the columns of Qare orthonormal. Hence
QTADQT.QR/DIRDR
and
RD2
41=2 1=2 1=2 1=2
 3=p
12 1=p
12 1=p
12 1=p
12
0  2=p
6 1=p
6 1=p
63
52
6641 0 0
1 1 0
1 1 1
1 1 13
775
D2
42 3=2 1
0 3=p
12 2=p
12
0 0 2=p
63
5
N U M E R I C A L N O T E S
1.When the Gram‚ÄìSchmidt process is run on a computer, roundoff error can
build up as the vectors ukare calculated, one by one. For jandklarge but
unequal, the inner products uT
jukmay not be sufÔ¨Åciently close to zero. This
loss of orthogonality can be reduced substantially by rearranging the order
of the calculations.1However, a different computer-based QR factorization is
usually preferred to this modiÔ¨Åed Gram‚ÄìSchmidt method because it yields a
more accurate orthonormal basis, even though the factorization requires about
twice as much arithmetic.
2.To produce a QR factorization of a matrix A, a computer program usually
left-multiplies Aby a sequence of orthogonal matrices until Ais transformed
into an upper triangular matrix. This construction is analogous to the left-
multiplication by elementary matrices that produces an LU factorization of A.
PRACTICE PROBLEMS
1.LetWDSpanfx1;x2g, where x1D2
41
1
13
5andx2D2
41=3
1=3
 2=33
5. Construct an or-
thonormal basis for W.
2.Suppose ADQR, where Qis an mnmatrix with orthogonal columns and Ris
annnmatrix. Show that if the columns of Aare linearly dependent, then Rcannot
be invertible.
6.4 EXERCISES
In Exercises 1‚Äì6, the given set is a basis for a subspace W. Use
the Gram‚ÄìSchmidt process to produce an orthogonal basis for W.
1.2
43
0
 13
5,2
48
5
 63
5 2.2
40
4
23
5,2
45
6
 73
53.2
42
 5
13
5,2
44
 1
23
5 4.2
43
 4
53
5,2
4 3
14
 73
5
5.2
6641
 4
0
13
775,2
6647
 7
 4
13
7756.2
6643
 1
2
 13
775,2
664 5
9
 9
33
775
1SeeFundamentals of Matrix Computations , by David S. Watkins (New York: John Wiley & Sons, 1991),
pp. 167‚Äì180.
SECOND REVISED PAGES


--- Page 378 ---
6.4 The Gram‚ÄìSchmidt Process 361
7.Find an orthonormal basis of the subspace spanned by the
vectors in Exercise 3.
8.Find an orthonormal basis of the subspace spanned by the
vectors in Exercise 4.
Find an orthogonal basis for the column space of each matrix in
Exercises 9‚Äì12.
9.2
6643 5 1
1 1 1
 1 5  2
3 7 83
77510.2
664 1 6 6
3 8 3
1 2 6
1 4 33
775
11.2
666641 2 5
 1 1  4
 1 4  3
1 4 7
1 2 13
7777512.2
666641 3 5
 1 3 1
0 2 3
1 5 2
1 5 83
77775
In Exercises 13 and 14, the columns of Qwere obtained by
applying the Gram‚ÄìSchmidt process to the columns of A. Find an
upper triangular matrix Rsuch that ADQR. Check your work.
13.AD2
6645 9
1 7
 3 5
1 53
775,QD2
6645=6  1=6
1=6 5=6
 3=6 1=6
1=6 3=63
775
14.AD2
664 2 3
5 7
2 2
4 63
775,QD2
664 2=7 5=7
5=7 2=7
2=7  4=7
4=7 2=73
775
15. Find a QR factorization of the matrix in Exercise 11.
16. Find a QR factorization of the matrix in Exercise 12.
In Exercises 17 and 18, all vectors and subspaces are in Rn. Mark
each statement True or False. Justify each answer.
17. a.Iffv1;v2;v3gis an orthogonal basis for W, then mul-
tiplying v3by a scalar cgives a new orthogonal basis
fv1;v2; cv3g.
b.The Gram‚ÄìSchmidt process produces from a linearly in-
dependent set fx1; : : : ; xpgan orthogonal set fv1; : : : ; vpg
with the property that for each k, the vectors v1; : : : ; vk
span the same subspace as that spanned by x1; : : : ; xk.
c.IfADQR, where Qhas orthonormal columns, then
RDQTA.
18. a.IfWDSpanfx1;x2;x3gwithfx1;x2;x3glinearly inde-
pendent, and if fv1;v2;v3gis an orthogonal set in W, then
fv1;v2;v3gis a basis for W.
b.Ifxis not in a subspace W, then x projWxis not zero.
c.In a QR factorization, say ADQR (when Ahas lin-
early independent columns), the columns of Qform an
orthonormal basis for the column space of A.19. Suppose ADQR, where QismnandRisnn. Show
that if the columns of Aare linearly independent, then Rmust
be invertible. [ Hint: Study the equation RxD0and use the
fact that ADQR.]
20. Suppose ADQR, where Ris an invertible matrix. Show
thatAandQhave the same column space. [ Hint: Given yin
ColA, show that yDQxfor some x. Also, given yin Col Q,
show that yDAxfor some x.]
21. Given ADQRas in Theorem 12, describe how to Ô¨Ånd an
orthogonal mm(square) matrix Q1and an invertible nn
upper triangular matrix Rsuch that
ADQ1R
0
The MATLAB qrcommand supplies this ‚Äúfull‚Äù QR factor-
ization when rank ADn.
22. Letu1; : : : ; upbe an orthogonal basis for a subspace Wof
Rn, and let TWRn!Rnbe deÔ¨Åned by T .x/DprojWx.
Show that Tis a linear transformation.
23. Suppose ADQR is a QR factorization of an mnma-
trixA(with linearly independent columns). Partition Aas
¬åA1A2¬ç, where A1haspcolumns. Show how to obtain a
QR factorization of A1, and explain why your factorization
has the appropriate properties.
24. [M] Use the Gram‚ÄìSchmidt process as in Example 2 to
produce an orthogonal basis for the column space of
AD2
66664 10 13 7  11
2 1  5 3
 6 3 13  3
16 16  2 5
2 1  5 73
77775
25. [M] Use the method in this section to produce a QR factor-
ization of the matrix in Exercise 24.
26. [M] For a matrix program, the Gram‚ÄìSchmidt process works
better with orthonormal vectors. Starting with x1; : : : ; xpas
in Theorem 11, let AD¬åx1  xp¬ç. Suppose Qis an
nkmatrix whose columns form an orthonormal basis for
the subspace Wkspanned by the Ô¨Årst kcolumns of A. Then
forxinRn,QQTxis the orthogonal projection of xontoWk
(Theorem 10 in Section 6.3). If xkC1is the next column of A,
then equation (2) in the proof of Theorem 11 becomes
vkC1DxkC1 Q.QTxkC1/
(The parentheses above reduce the number of arithmetic
operations.) Let ukC1DvkC1=kvkC1k. The new Qfor the
next step is ¬åQ ukC1¬ç. Use this procedure to compute the
QR factorization of the matrix in Exercise 24. Write the
keystrokes or commands you use.
WEB
SECOND REVISED PAGES


--- Page 379 ---
362 CHAPTER 6 Orthogonality and Least Squares
SOLUTION TO PRACTICE PROBLEMS
1.Letv1Dx1D2
41
1
13
5andv2Dx2 x2v1
v1v1v1Dx2 0v1Dx2. Sofx1;x2gis al-
ready orthogonal. All that is needed is to normalize the vectors. Let
u1D1
kv1kv1D1p
32
41
1
13
5D2
41=p
3
1=p
3
1=p
33
5
Instead of normalizing v2directly, normalize v0
2D3v2instead:
u2D1
kv0
2kv0
2D1p
12C12C. 2/22
41
1
 23
5D2
41=p
6
1=p
6
 2=p
63
5
Then fu1;u2gis an orthonormal basis for W.
2.Since the columns of Aare linearly dependent, there is a nontrivial vector xsuch
thatAxD0. But then QRxD0. Applying Theorem 7 from Section 6.2 results in
kRxk D k QRxk D k 0k D0. But kRxk D0implies RxD0, by Theorem 1 from
Section 6.1. Thus there is a nontrivial vector xsuch that RxD0and hence, by the
Invertible Matrix Theorem, Rcannot be invertible.
6.5 LEAST-SQUARES PROBLEMS
The chapter‚Äôs introductory example described a massive problem AxDbthat had no
solution. Inconsistent systems arise often in applications, though usually not with such
an enormous coefÔ¨Åcient matrix. When a solution is demanded and none exists, the best
one can do is to Ô¨Ånd an xthat makes Axas close as possible to b.
Think of Axas an approximation tob. The smaller the distance between bandAx,
given by kb Axk, the better the approximation. The general least-squares problem
is to Ô¨Ånd an xthat makes kb Axkas small as possible. The adjective ‚Äúleast-squares‚Äù
arises from the fact that kb Axkis the square root of a sum of squares.
D E F I N I T I O N IfAismnandbis inRm, aleast-squares solution ofAxDbis an OxinRn
such that
kb AOxk  k b Axk
for all xinRn.
The most important aspect of the least-squares problem is that no matter what xwe
select, the vector Axwill necessarily be in the column space, Col A. So we seek an x
that makes Axthe closest point in Col Atob. See Figure 1. (Of course, if bhappens to
be in Col A, then bisAxfor some x, and such an xis a ‚Äúleast-squares solution.‚Äù)
Solution of the General Least-Squares Problem
Given Aandbas above, apply the Best Approximation Theorem in Section 6.3 to the
subspace Col A. Let
ObDprojColAb
SECOND REVISED PAGES


--- Page 380 ---
6.5 Least-Squares Problems 363
AxÀÜ0
 
Ax1
Col Ab
Ax2
FIGURE 1 The vector bis closer to AOx
than to Axfor other x.
Because Obis in the column space of A, the equation AxDObisconsistent, and there is
anOxinRnsuch that
AOxDOb (1)
Since Obis the closest point in Col Atob, a vector Oxis a least-squares solution of AxDb
if and only if OxsatisÔ¨Åes (1). Such an OxinRnis a list of weights that will build Obout of
the columns of A. See Figure 2. [There are many solutions of (1) if the equation has free
variables.]
ÀÜx n0
subspace of mbb ‚Äì AxÀÜ
b = AxÀÜ ÀÜCol A
 
A
FIGURE 2 The least-squares solution Oxis inRn.
Suppose OxsatisÔ¨Åes AOxDOb. By the Orthogonal Decomposition Theorem in
Section 6.3, the projection Obhas the property that b Obis orthogonal to Col A,
sob AOxis orthogonal to each column of A. If ajis any column of A, then
aj.b AOx/D0, and aT
j.b AOx/D0. Since each aT
jis a row of AT,
AT.b AOx/D0 (2)
(This equation also follows from Theorem 3 in Section 6.1.) Thus
ATb ATAOxD0
ATAOxDATb
These calculations show that each least-squares solution of AxDbsatisÔ¨Åes the equation
ATAxDATb (3)
The matrix equation (3) represents a system of equations called the normal equations
forAxDb. A solution of (3) is often denoted by Ox.
T H E O R E M 1 3 The set of least-squares solutions of AxDbcoincides with the nonempty set of
solutions of the normal equations ATAxDATb.
SECOND REVISED PAGES


--- Page 381 ---
364 CHAPTER 6 Orthogonality and Least Squares
PROOF As shown above, the set of least-squares solutions is nonempty and each
least-squares solution OxsatisÔ¨Åes the normal equations. Conversely, suppose OxsatisÔ¨Åes
ATAOxDATb. Then OxsatisÔ¨Åes (2) above, which shows that b AOxis orthogonal to the
rows of ATand hence is orthogonal to the columns of A. Since the columns of Aspan
ColA, the vector b AOxis orthogonal to all of Col A. Hence the equation
bDAOxC.b AOx/
is a decomposition of binto the sum of a vector in Col Aand a vector orthogonal to
ColA. By the uniqueness of the orthogonal decomposition, AOxmust be the orthogonal
projection of bonto Col A. That is, AOxDOb, and Oxis a least-squares solution.
EXAMPLE 1 Find a least-squares solution of the inconsistent system AxDbfor
AD2
44 0
0 2
1 13
5;bD2
42
0
113
5
SOLUTION To use normal equations (3), compute:
ATAD4 0 1
0 2 12
44 0
0 2
1 13
5D17 1
1 5
ATbD4 0 1
0 2 12
42
0
113
5D19
11
Then the equation ATAxDATbbecomes
17 1
1 5x1
x2
D19
11
Row operations can be used to solve this system, but since ATAis invertible and 22,
it is probably faster to compute
.ATA/ 1D1
845 1
 1 17
and then to solve ATAxDATbas
OxD.ATA/ 1ATb
D1
845 1
 1 1719
11
D1
8484
168
D1
2
In many calculations, ATAis invertible, but this is not always the case. The next
example involves a matrix of the sort that appears in what are called analysis of variance
problems in statistics.
EXAMPLE 2 Find a least-squares solution of AxDbfor
AD2
66666641 1 0 0
1 1 0 0
1 0 1 0
1 0 1 0
1 0 0 1
1 0 0 13
7777775;bD2
6666664 3
 1
0
2
5
13
7777775
SECOND REVISED PAGES


--- Page 382 ---
6.5 Least-Squares Problems 365
SOLUTION Compute
ATAD2
6641 1 1 1 1 1
1 1 0 0 0 0
0 0 1 1 0 0
0 0 0 0 1 13
7752
66666641 1 0 0
1 1 0 0
1 0 1 0
1 0 1 0
1 0 0 1
1 0 0 13
7777775D2
6646 2 2 2
2 2 0 0
2 0 2 0
2 0 0 23
775
ATbD2
6641 1 1 1 1 1
1 1 0 0 0 0
0 0 1 1 0 0
0 0 0 0 1 13
7752
6666664 3
 1
0
2
5
13
7777775D2
6644
 4
2
63
775
The augmented matrix for ATAxDATbis
2
6646 2 2 2 4
2 2 0 0  4
2 0 2 0 2
2 0 0 2 63
7752
6641 0 0 1 3
0 1 0  1 5
0 0 1  1 2
0 0 0 0 03
775
The general solution is x1D3 x4,x2D  5Cx4,x3D  2Cx4, and x4is free. So
the general least-squares solution of AxDbhas the form
OxD2
6643
 5
 2
03
775Cx42
664 1
1
1
13
775
The next theorem gives useful criteria for determining when there is only one least-
squares solution of AxDb. (Of course, the orthogonal projection Obis always unique.)
T H E O R E M 1 4 LetAbe an mnmatrix. The following statements are logically equivalent:
a.The equation AxDbhas a unique least-squares solution for each binRm.
b.The columns of Aare linearly independent.
c.The matrix ATAis invertible.
When these statements are true, the least-squares solution Oxis given by
OxD.ATA/ 1ATb (4)
The main elements of a proof of Theorem 14 are outlined in Exercises 19‚Äì21, which
also review concepts from Chapter 4. Formula (4) for Oxis useful mainly for theoretical
purposes and for hand calculations when ATAis a22invertible matrix.
When a least-squares solution Oxis used to produce AOxas an approximation to b,
the distance from btoAOxis called the least-squares error of this approximation.
EXAMPLE 3 Given Aandbas in Example 1, determine the least-squares error in
the least-squares solution of AxDb.
SECOND REVISED PAGES


--- Page 383 ---
366 CHAPTER 6 Orthogonality and Least Squares
SOLUTION From Example 1,
(2, 0, 11)b
084
(0, 2, 1)
(4, 0, 1)Ax ÀÜ
x1x2x3
Col A
FIGURE 3bD2
42
0
113
5and AOxD2
44 0
0 2
1 13
51
2
D2
44
4
33
5
Hence
b AOxD2
42
0
113
5 2
44
4
33
5D2
4 2
 4
83
5
and
kb AOxk Dp
. 2/2C. 4/2C82Dp
84
The least-squares error isp
84. For any xinR2, the distance between band the vector
Axis at leastp
84. See Figure 3. Note that the least-squares solution Oxitself does not
appear in the Ô¨Ågure.
Alternative Calculations of Least-Squares Solutions
The next example shows how to Ô¨Ånd a least-squares solution of AxDbwhen the
columns of Aare orthogonal. Such matrices often appear in linear regression problems,
discussed in the next section.
EXAMPLE 4 Find a least-squares solution of AxDbfor
AD2
6641 6
1 2
1 1
1 73
775;bD2
664 1
2
1
63
775
SOLUTION Because the columns a1and a2ofAare orthogonal, the orthogonal
projection of bonto Col Ais given by
ObDba1
a1a1a1Cba2
a2a2a2D8
4a1C45
90a2 (5)
D2
6642
2
2
23
775C2
664 3
 1
1=2
7=23
775D2
664 1
1
5=2
11=23
775
Now that Obis known, we can solve AOxDOb. But this is trivial, since we already
know what weights to place on the columns of Ato produce Ob. It is clear from (5) that
OxD8=4
45=90
D2
1=2
In some cases, the normal equations for a least-squares problem can be ill-
conditioned ; that is, small errors in the calculations of the entries of ATAcan sometimes
cause relatively large errors in the solution Ox. If the columns of Aare linearly inde-
pendent, the least-squares solution can often be computed more reliably through a QR
factorization of A(described in Section 6.4).1
1The QR method is compared with the standard normal equation method in G. Golub and C. Van Loan,
Matrix Computations , 3rd ed. (Baltimore: Johns Hopkins Press, 1996), pp. 230‚Äì231.
SECOND REVISED PAGES


--- Page 384 ---
6.5 Least-Squares Problems 367
T H E O R E M 1 5 Given an mnmatrix Awith linearly independent columns, let ADQRbe a
QR factorization of Aas in Theorem 12. Then, for each binRm, the equation
AxDbhas a unique least-squares solution, given by
OxDR 1QTb (6)
PROOF LetOxDR 1QTb. Then
AOxDQROxDQRR 1QTbDQQTb
By Theorem 12, the columns of Qform an orthonormal basis for Col A. Hence, by
Theorem 10, QQTbis the orthogonal projection Obofbonto Col A. Then AOxDOb, which
shows that Oxis a least-squares solution of AxDb. The uniqueness of Oxfollows from
Theorem 14.
N U M E R I C A L N O T E
Since Rin Theorem 15 is upper triangular, Oxshould be calculated as the exact
solution of the equation
RxDQTb (7)
It is much faster to solve (7) by back-substitution or row operations than to
compute R 1and use (6).
EXAMPLE 5 Find the least-squares solution of AxDbfor
AD2
6641 3 5
1 1 0
1 1 2
1 3 33
775;bD2
6643
5
7
 33
775
SOLUTION The QR factorization of Acan be obtained as in Section 6.4.
ADQRD2
6641=2 1=2 1=2
1=2  1=2  1=2
1=2  1=2 1=2
1=2 1=2  1=23
7752
42 4 5
0 2 3
0 0 23
5
Then
QTbD2
41=2 1=2 1=2 1=2
1=2  1=2  1=2 1=2
1=2  1=2 1=2  1=23
52
6643
5
7
 33
775D2
46
 6
43
5
The least-squares solution OxsatisÔ¨Åes RxDQTb; that is,
2
42 4 5
0 2 3
0 0 23
52
4x1
x2
x33
5D2
46
 6
43
5
This equation is solved easily and yields OxD2
410
 6
23
5.
SECOND REVISED PAGES


--- Page 385 ---
368 CHAPTER 6 Orthogonality and Least Squares
PRACTICE PROBLEMS
1.LetAD2
41 3 3
1 5 1
1 7 23
5andbD2
45
 3
 53
5. Find a least-squares solution of AxDb,
and compute the associated least-squares error.
2.What can you say about the least-squares solution of AxDbwhen bis orthogonal
to the columns of A?
6.5 EXERCISES
In Exercises 1‚Äì4, Ô¨Ånd a least-squares solution of AxDbby
(a) constructing the normal equations for Oxand (b) solving for Ox.
1.AD2
4 1 2
2 3
 1 33
5,bD2
44
1
23
5
2.AD2
42 1
 2 0
2 33
5,bD2
4 5
8
13
5
3.AD2
6641 2
 1 2
0 3
2 53
775,bD2
6643
1
 4
23
775
4.AD2
41 3
1 1
1 13
5,bD2
45
1
03
5
In Exercises 5 and 6, describe all least-squares solutions of the
equation AxDb.
5.AD2
6641 1 0
1 1 0
1 0 1
1 0 13
775,bD2
6641
3
8
23
775
6.AD2
66666641 1 0
1 1 0
1 1 0
1 0 1
1 0 1
1 0 13
7777775,bD2
66666647
2
3
6
5
43
7777775
7.Compute the least-squares error associated with the least-
squares solution found in Exercise 3.
8.Compute the least-squares error associated with the least-
squares solution found in Exercise 4.
In Exercises 9‚Äì12, Ô¨Ånd (a) the orthogonal projection of bonto
ColAand (b) a least-squares solution of AxDb.
9.AD2
41 5
3 1
 2 43
5,bD2
44
 2
 33
510.AD2
41 2
 1 4
1 23
5,bD2
43
 1
53
5
11.AD2
6644 0 1
1 5 1
6 1 0
1 1 53
775,bD2
6649
0
0
03
775
12.AD2
6641 1 0
1 0  1
0 1 1
 1 1  13
775,bD2
6642
5
6
63
775
13. LetAD2
43 4
 2 1
3 43
5,bD2
411
 9
53
5,uD5
 1
, and vD
5
 2
. Compute AuandAv, and compare them with b.
Could upossibly be a least-squares solution of AxDb?
(Answer this without computing a least-squares solution.)
14. LetAD2
42 1
 3 4
3 23
5,bD2
45
4
43
5,uD4
 5
, and vD
6
 5
. Compute AuandAv, and compare them with b. Is
it possible that at least one of uorvcould be a least-squares
solution of AxDb? (Answer this without computing a least-
squares solution.)
In Exercises 15 and 16, use the factorization ADQRto Ô¨Ånd the
least-squares solution of AxDb.
15.AD2
42 3
2 4
1 13
5D2
42=3  1=3
2=3 2=3
1=3  2=33
53 5
0 1
,bD2
47
3
13
5
16.AD2
6641 1
1 4
1 1
1 43
775D2
6641=2  1=2
1=2 1=2
1=2  1=2
1=2 1=23
7752 3
0 5
;bD2
664 1
6
5
73
775
In Exercises 17 and 18, Ais anmnmatrix and bis inRm. Mark
each statement True or False. Justify each answer.
17. a.The general least-squares problem is to Ô¨Ånd an xthat
makes Axas close as possible to b.
SECOND REVISED PAGES


--- Page 386 ---
6.5 Least-Squares Problems 369
b.A least-squares solution of AxDbis a vector Oxthat
satisÔ¨Åes AOxDOb, where Obis the orthogonal projection of
bonto Col A.
c.A least-squares solution of AxDbis a vector Oxsuch that
kb Axk  k b AOxkfor all xinRn.
d.Any solution of ATAxDATbis a least-squares solution
ofAxDb.
e.If the columns of Aare linearly independent, then the
equation AxDbhas exactly one least-squares solution.
18. a.Ifbis in the column space of A, then every solution of
AxDbis a least-squares solution.
b.The least-squares solution of AxDbis the point in the
column space of Aclosest to b.
c.A least-squares solution of AxDbis a list of weights
that, when applied to the columns of A, produces the
orthogonal projection of bonto Col A.
d.IfOxis a least-squares solution of AxDb, then
OxD.ATA/ 1ATb.
e.The normal equations always provide a reliable method
for computing least-squares solutions.
f.IfAhas a QR factorization, say ADQR, then the best
way to Ô¨Ånd the least-squares solution of AxDbis to
compute OxDR 1QTb.
19. LetAbe an mnmatrix. Use the steps below to show that a
vector xinRnsatisÔ¨Åes AxD0if and only if ATAxD0. This
will show that Nul ADNulATA.
a.Show that if AxD0, then ATAxD0.
b.Suppose ATAxD0. Explain why xTATAxD0, and use
this to show that AxD0.
20. LetAbe an mnmatrix such that ATAis invertible. Show
that the columns of Aare linearly independent. [ Careful:
You may not assume that Ais invertible; it may not even be
square.]
21. LetAbe an mnmatrix whose columns are linearly inde-
pendent. [ Careful: Aneed not be square.]
a.Use Exercise 19 to show that ATAis an invertible matrix.
b.Explain why Amust have at least as many rows as
columns.
c.Determine the rank of A.
22. Use Exercise 19 to show that rank ATADrankA. [Hint: How
many columns does ATAhave? How is this connected with
the rank of ATA?]
23. Suppose Aismnwith linearly independent columns and
bis inRm. Use the normal equations to produce a formula
forOb, the projection of bonto Col A. [Hint: FindOxÔ¨Årst. The
formula does not require an orthogonal basis for Col A.]24. Find a formula for the least-squares solution of AxDbwhen
the columns of Aare orthonormal.
25. Describe all least-squares solutions of the system
xCyD2
xCyD4
26. [M] Example 3 in Section 4.8 displayed a low-pass linear
Ô¨Ålter that changed a signal fykgintofykC1gand changed a
higher-frequency signal fwkginto the zero signal, where
ykDcos.k=4/ andwkDcos.3k=4/ . The following cal-
culations will design a Ô¨Ålter with approximately those prop-
erties. The Ô¨Ålter equation is
a0ykC2Ca1ykC1Ca2ykD¬¥k for all k .8/
Because the signals are periodic, with period 8, it sufÔ¨Åces
to study equation (8) for kD0; : : : ; 7 . The action on the
two signals described above translates into two sets of eight
equations, shown below:
kD0
kD1:::
kD72
66666666664ykC2
0ykC1
.7yk
1
 :7 0 :7
 1 :7 0
 :7  1 :7
0 :7  1
:7 0  :7
1 :7 0
:7 1 :73
777777777752
4a0
a1
a23
5D2
66666666664ykC1
.7
0
 :7
 1
 :7
0
:7
13
77777777775
kD0
kD1:::
kD72
66666666664wkC2
0 wkC1
.7wk
1
:7 0  :7
 1 :7 0
:7  1 :7
0 :7  1
 :7 0 :7
1 :7 0
 :7 1  :73
777777777752
4a0
a1
a23
5D2
666666666640
0
0
0
0
0
0
03
77777777775
Write an equation AxDb, where Ais a163matrix formed
from the two coefÔ¨Åcient matrices above and where binR16is
formed from the two right sides of the equations. Find a0,a1,
anda2given by the least-squares solution of AxDb. (The
.7 in the data above was used as an approximation forp
2=2,
to illustrate how a typical computation in an applied problem
might proceed. If .707 were used instead, the resulting Ô¨Ålter
coefÔ¨Åcients would agree to at least seven decimal places
withp
2=4; 1=2 , andp
2=4, the values produced by exact
arithmetic calculations.)
WEB
SECOND REVISED PAGES


--- Page 387 ---
370 CHAPTER 6 Orthogonality and Least Squares
SOLUTIONS TO PRACTICE PROBLEMS
1.First, compute
ATAD2
41 1 1
 3 5 7
 3 1 23
52
41 3 3
1 5 1
1 7 23
5D2
43 9 0
9 83 28
0 28 143
5
ATbD2
41 1 1
 3 5 7
 3 1 23
52
45
 3
 53
5D2
4 3
 65
 283
5
Next, row reduce the augmented matrix for the normal equations, ATAxDATb:
2
43 9 0  3
9 83 28  65
0 28 14  283
52
41 3 0  1
0 56 28  56
0 28 14  283
5    2
41 0  3=2 2
0 1 1=2  1
0 0 0 03
5
The general least-squares solution is x1D2C3
2x3,x2D  1 1
2x3, with x3free.
For one speciÔ¨Åc solution, take x3D0(for example), and get
OxD2
42
 1
03
5
To Ô¨Ånd the least-squares error, compute
ObDAOxD2
41 3 3
1 5 1
1 7 23
52
42
 1
03
5D2
45
 3
 53
5
It turns out that ObDb, sokb Obk D0. The least-squares error is zero because b
happens to be in Col A.
2.Ifbis orthogonal to the columns of A, then the projection of bonto the column space
ofAis0. In this case, a least-squares solution OxofAxDbsatisÔ¨Åes AOxD0.
6.6 APPLICATIONS TO LINEAR MODELS
A common task in science and engineering is to analyze and understand relationships
among several quantities that vary. This section describes a variety of situations in which
data are used to build or verify a formula that predicts the value of one variable as a
function of other variables. In each case, the problem will amount to solving a least-
squares problem.
For easy application of the discussion to real problems that you may encounter later
in your career, we choose notation that is commonly used in the statistical analysis of
scientiÔ¨Åc and engineering data. Instead of AxDb, we write XDyand refer to Xas
thedesign matrix ,as the parameter vector , and yas the observation vector .
Least-Squares Lines
The simplest relation between two variables xand yis the linear equation
yD0C1x.1Experimental data often produce points .x1; y1/; : : : ; .x n; yn/that,
1This notation is commonly used for least-squares lines instead of yDmxCb.
SECOND REVISED PAGES


--- Page 388 ---
6.6 Applications to Linear Models 371
when graphed, seem to lie close to a line. We want to determine the parameters 0
and1that make the line as ‚Äúclose‚Äù to the points as possible.
Suppose 0and1are Ô¨Åxed, and consider the line yD0C1xin Figure 1.
Corresponding to each data point .xj; yj/there is a point .xj; 0C1xj/on the line
with the same x-coordinate. We call yjtheobserved value of yand0C1xjthe
predicted y-value (determined by the line). The difference between an observed y-value
and a predicted y-value is called a residual .
ResidualResidualPoint on lineData pointy
xjx1xnxy = /H92520 + /H92521x(xj, /H92520 + /H92521xj)(xj, yj)
FIGURE 1 Fitting a line to experimental data.
There are several ways to measure how ‚Äúclose‚Äù the line is to the data. The usual
choice (primarily because the mathematical calculations are simple) is to add the squares
of the residuals. The least-squares line is the line yD0C1xthat minimizes the
sum of the squares of the residuals. This line is also called a line of regression of y
on x, because any errors in the data are assumed to be only in the y-coordinates. The
coefÔ¨Åcients 0,1of the line are called (linear) regression coefÔ¨Åcients .2
If the data points were on the line, the parameters 0and1would satisfy the
equations
Predicted Observed
y-value y-value
0C1x1 = y1
0C1x2 = y2
::::::
0C1xn =yn
We can write this system as
XDy;where XD2
66641 x 1
1 x 2
::::::
1 x n3
7775;D0
1
;yD2
6664y1
y2
:::
yn3
7775(1)
Of course, if the data points don‚Äôt lie on a line, then there are no parameters 0,1for
which the predicted y-values in Xequal the observed y-values in y, and XDyhas
no solution. This is a least-squares problem, AxDb, with different notation!
The square of the distance between the vectors Xandyis precisely the sum of
the squares of the residuals. The that minimizes this sum also minimizes the distance
between Xandy.Computing the least-squares solution of XDyis equivalent to
Ô¨Ånding the that determines the least-squares line in Figure 1 .
2If the measurement errors are in xinstead of y, simply interchange the coordinates of the data .xj; yj/
before plotting the points and computing the regression line. If both coordinates are subject to possible error,
then you might choose the line that minimizes the sum of the squares of the orthogonal (perpendicular)
distances from the points to the line. See the Practice Problems for Section 7.5.
SECOND REVISED PAGES


--- Page 389 ---
372 CHAPTER 6 Orthogonality and Least Squares
EXAMPLE 1 Find the equation yD0C1xof the least-squares line that best Ô¨Åts
the data points .2; 1/ ,.5; 2/ ,.7; 3/ , and .8; 3/ .
SOLUTION Use the x-coordinates of the data to build the design matrix Xin (1) and
they-coordinates to build the observation vector y:
XD2
6641 2
1 5
1 7
1 83
775;yD2
6641
2
3
33
775
For the least-squares solution of XDy, obtain the normal equations (with the new
notation):
XTXDXTy
That is, compute
XTXD1 1 1 1
2 5 7 82
6641 2
1 5
1 7
1 83
775D4 22
22 142
XTyD1 1 1 1
2 5 7 82
6641
2
3
33
775D9
57
The normal equations are
4 22
22 1420
1
D9
57
Hence
0
1
D4 22
22 142 19
57
D1
84142 22
 22 49
57
D1
8424
30
D2=7
5=14
Thus the least-squares line has the equation
yD2
7C5
14x
See Figure 2.
123123
45 789 6y
x
FIGURE 2 The least-squares line
yD2
7C5
14x.
A common practice before computing a least-squares line is to compute the average
xof the original x-values and form a new variable xDx x. The new x-data are said
to be in mean-deviation form . In this case, the two columns of the design matrix will
be orthogonal. Solution of the normal equations is simpliÔ¨Åed, just as in Example 4 in
Section 6.5. See Exercises 17 and 18.
SECOND REVISED PAGES


--- Page 390 ---
6.6 Applications to Linear Models 373
The General Linear Model
In some applications, it is necessary to Ô¨Åt data points with something other than a straight
line. In the examples that follow, the matrix equation is still XDy, but the speciÔ¨Åc
form of Xchanges from one problem to the next. Statisticians usually introduce a
residual vector , deÔ¨Åned by Dy X, and write
yDXC
Any equation of this form is referred to as a linear model . Once Xandyare determined,
the goal is to minimize the length of , which amounts to Ô¨Ånding a least-squares solution
ofXDy. In each case, the least-squares solution Ois a solution of the normal
equations
XTXDXTy
Least-Squares Fitting of Other Curves
When data points .x1; y1/; : : : ; .x n; yn/on a scatter plot do not lie close to any line, it
may be appropriate to postulate some other functional relationship between xandy.
The next two examples show how to Ô¨Åt data by curves that have the general form
yD0f0.x/C1f1.x/C    C kfk.x/ (2)
where f0; : : : ; f kare known functions and 0; : : : ;  kare parameters that must be
determined. As we will see, equation (2) describes a linear model because it is linear in
the unknown parameters.
For a particular value of x, (2) gives a predicted, or ‚ÄúÔ¨Åtted,‚Äù value of y. The
difference between the observed value and the predicted value is the residual. The
parameters 0; : : : ;  kmust be determined so as to minimize the sum of the squares
of the residuals.
EXAMPLE 2 Suppose data points .x1; y1/; : : : ; .x n; yn/appear to lie along some
sort of parabola instead of a straight line. For instance, if the x-coordinate denotes the
production level for a company, and ydenotes the average cost per unit of operating at
a level of xunits per day, then a typical average cost curve looks like a parabola that
opens upward (Figure 3). In ecology, a parabolic curve that opens downward is used
Units producedAverage cost
per unit
xyFIGURE 3
Average cost curve.
to model the net primary production of nutrients in a plant, as a function of the surface
area of the foliage (Figure 4). Suppose we wish to approximate the data by an equation
of the form
yD0C1xC2x2(3)
Describe the linear model that produces a ‚Äúleast-squares Ô¨Åt‚Äù of the data by equation (3).
SOLUTION Equation (3) describes the ideal relationship. Suppose the actual values of
Surface area
of folia gexyNet pr imary
production
FIGURE 4
Production of nutrients.the parameters are 0,1,2. Then the coordinates of the Ô¨Årst data point .x1; y1/satisfy
an equation of the form
y1D0C1x1C2x2
1C1
where 1is the residual error between the observed value y1and the predicted y-value
0C1x1C2x2
1. Each data point determines a similar equation:
y1D0C1x1C2x2
1C1
y2D0C1x2C2x2
2C2
::::::
ynD0C1xnC2x2
nCn
SECOND REVISED PAGES


--- Page 391 ---
374 CHAPTER 6 Orthogonality and Least Squares
It is a simple matter to write this system of equations in the form yDXC. To Ô¨Ånd
X, inspect the Ô¨Årst few rows of the system and look for the pattern.
2
6664y1
y2
:::
yn3
7775D2
666641 x 1x2
1
1 x 2x2
2
:::::::::
1 x nx2
n3
777752
640
1
23
75C2
666641
2
:::
n3
77775
yD X C 
EXAMPLE 3 If data points tend to follow a pattern such as in Figure 5, then an
xy
FIGURE 5
Data points along a cubic curve.appropriate model might be an equation of the form
yD0C1xC2x2C3x3
Such data, for instance, could come from a company‚Äôs total costs, as a function of the
level of production. Describe the linear model that gives a least-squares Ô¨Åt of this type
to data .x1; y1/; : : : ; .x n; yn/.
SOLUTION By an analysis similar to that in Example 2, we obtain
Observation Design Parameter Residual
vector matrix vector vector
yD2
66664y1
y2
:::
yn3
77775; X D2
666641 x 1x2
1x3
1
1 x 2x2
2x3
2
::::::::::::
1 x nx2
nx3
n3
77775;D2
666640
1
2
33
77775;D2
666641
2
:::
n3
77775
Multiple Regression
Suppose an experiment involves two independent variables‚Äîsay, uandv‚Äîand one
dependent variable, y. A simple equation for predicting yfrom uandvhas the form
yD0C1uC2v (4)
A more general prediction equation might have the form
yD0C1uC2vC3u2C4uvC5v2(5)
This equation is used in geology, for instance, to model erosion surfaces, glacial cirques,
soil pH, and other quantities. In such cases, the least-squares Ô¨Åt is called a trend surface .
Equations (4) and (5) both lead to a linear model because they are linear in the
unknown parameters (even though uandvare multiplied). In general, a linear model
will arise whenever yis to be predicted by an equation of the form
yD0f0.u; v/ C1f1.u; v/ C    C kfk.u; v/
withf0; : : : ; f kany sort of known functions and 0; : : : ;  kunknown weights.
EXAMPLE 4 In geography, local models of terrain are constructed from data
.u1; v1; y1/; : : : ; .u n; vn; yn/, where uj,vj, and yjare latitude, longitude, and altitude,
respectively. Describe the linear model based on (4) that gives a least-squares Ô¨Åt to such
data. The solution is called the least-squares plane . See Figure 6.
SECOND REVISED PAGES


--- Page 392 ---
6.6 Applications to Linear Models 375
FIGURE 6 A least-squares plane.
SOLUTION We expect the data to satisfy the following equations:
y1D0C1u1C2v1C1
y2D0C1u2C2v2C2
::::::
ynD0C1unC2vnCn
This system has the matrix form yDXC, where
Observation Design Parameter Residual
vector matrix vector vector
yD2
6664y1
y2
:::
yn3
7775; X D2
66641 u 1v1
1 u 2v2
:::::::::
1 u nvn3
7775;D2
40
1
23
5;D2
66641
2
:::
n3
7775
Example 4 shows that the linear model for multiple regression has the same abstract
form as the model for the simple regression in the earlier examples. Linear algebra gives
us the power to understand the general principle behind all the linear models. Once X
is deÔ¨Åned properly, the normal equations for have the same matrix form, no matter
how many variables are involved. Thus, for any linear model where XTXis invertible,
the least-squares Ois given by .XTX/ 1XTy.
SGThe Geometry of a
Linear Model 6‚Äì19
Further Reading
Ferguson, J., Introduction to Linear Algebra in Geology (New York: Chapman & Hall,
1994).
Krumbein, W. C., and F. A. Graybill, An Introduction to Statistical Models in Geology
(New York: McGraw-Hill, 1965).
Legendre, P., and L. Legendre, Numerical Ecology (Amsterdam: Elsevier, 1998).
Unwin, David J., An Introduction to Trend Surface Analysis , Concepts and Techniques
in Modern Geography, No. 5 (Norwich, England: Geo Books, 1975).
PRACTICE PROBLEM
When the monthly sales of a product are subject to seasonal Ô¨Çuctuations, a curve that
approximates the sales data might have the form
yD0C1xC2sin.2x=12/
where xis the time in months. The term 0C1xgives the basic sales trend, and the
sine term reÔ¨Çects the seasonal changes in sales. Give the design matrix and the parameter
vector for the linear model that leads to a least-squares Ô¨Åt of the equation above. Assume
the data are .x1; y1/; : : : ; .x n; yn/.
SECOND REVISED PAGES


--- Page 393 ---
$)"15&3  2UWKRJRQDOLW\DQG/HDVW6TXDUHV
&9&3$*4&4
,Q([HUFLVHV¬≤ √ÄQGWKHHTXDWLRQ yDÀá0CÀá1xRIWKHOHDVW
VTXDUHVOLQHWKDWEHVW√ÄWVWKHJLYHQGDWDSRLQWV
.0; 1/.1; 1/.2; 2/.3; 2/
.1; 0/.2; 1/.4; 2/.5; 3/
./NUL1; 0/.0; 1/.1; 2/.2; 4/
.2; 3/.3; 2/.5; 1/.6; 0/
/HWXEHWKHGHVLJQPDWUL[XVHGWR√ÄQGWKHOHDVWVTXDUHVOLQH
WR√ÄWGDWD .x1;y1/ ;:::;. x n;yn/8VHDWKHRUHPLQ6HFWLRQ
WRVKRZWKDWWKHQRUPDOHTXDWLRQVKDYHDXQLTXHVROXWLRQLIDQGRQO\LIWKHGDWDLQFOXGHDWOHDVWWZRGDWDSRLQWVZLWKGLIIHUHQW
xFRRUGLQDWHV
/HWXEHWKHGHVLJQPDWUL[LQ([DPSOH FRUUHVSRQGLQJWR
DOHDVWVTXDUHV√ÄWRIDSDUDERODWRGDWD .x1;y1/ ;:::;. x n;yn/
6XSSRVH x1x2DQG x3DUHGLVWLQFW([SODLQZK\WKHUHLVRQO\
RQHSDUDERODWKDW√ÄWVWKHGDWDEHVW LQDOHDVWVTXDUHVVHQVH6HH([HUFLVH 
$ FHUWDLQH[SHULPHQWSURGXFHVWKHGDWD
.1; 1:8/.2; 2:7/
.3; 3:4/.4; 3:8/.5; 3:9/ 'HVFULEHWKHPRGHOWKDWSURGXFHV
DOHDVWVTXDUHV√ÄWRIWKHVHSRLQWVE\DIXQFWLRQRIWKHIRUP
yDÀá1xCÀá2x2
6XFKDIXQFWLRQPLJKWDULVHIRUH[DPSOHDVWKHUHYHQXHIURP
WKHVDOHRI xXQLWVRIDSURGXFWZKHQWKHDPRXQWRIIHUHGIRU
VDOHDIIHFWVWKHSULFHWREHVHWIRUWKHSURGXFW
D *LYHWKHGHVLJQPDWUL[ WKHREVHUYDWLRQYHFWRU DQGWKH
XQNQRZQSDUDPHWHUYHFWRU
E >0@ )LQGWKHDVVRFLDWHGOHDVWVTXDUHVFXUYHIRUWKHGDWD
$ VLPSOHFXUYHWKDWRIWHQPDNHVDJRRGPRGHOIRUWKHYDUL
DEOHFRVWVRIDFRPSDQ\ DVDIXQFWLRQRIWKHVDOHVOHYHO x
KDVWKHIRUP yDÀá1xCÀá2x2CÀá3x3 7KHUHLVQRFRQVWDQW
WHUPEHFDXVH√Ä[HGFRVWVDUHQRWLQFOXGHG
D *LYHWKHGHVLJQPDWUL[DQGWKHSDUDPHWHUYHFWRUIRUWKH
OLQHDUPRGHOWKDWOHDGVWRDOHDVWVTXDUHV√ÄWRIWKHHTXDWLRQDERYHZLWKGDWD
.x1;y1/ ;:::;. x n;yn/
E >0@ )LQGWKHOHDVWVTXDUHVFXUYHRIWKHIRUPDERYHWR√ÄW
WKHGDWD .4; 1:58/.6; 2:08/.8; 2:5/.10; 2:8/.12; 3:1/
.14; 3:4/.16; 3:8/D Q G .18; 4:32/  ZLWKYDOXHVLQWKRX
VDQGV ,ISRVVLEOH SURGXFHDJUDSKWKDWVKRZVWKHGDWD
SRLQWVDQGWKHJUDSKRIWKHFXELFDSSUR[LPDWLRQ
$FHUWDLQH[SHULPHQWSURGXFHVWKHGDWD .1; 7:9/.2; 5:4/DQG
.3;/NUL:9/'HVFULEHWKHPRGHOWKDWSURGXFHVDOHDVWVTXDUHV√ÄW
RIWKHVHSRLQWVE\DIXQFWLRQRIWKHIRUP
yDAFRVxCBVLQx
6XSSRVHUDGLRDFWLYHVXEVWDQFHV$ DQG% KDYHGHFD\FRQ
VWDQWVRIDQG UHVSHFWLYHO\ ,IDPL[WXUHRIWKHVHWZRVXEVWDQFHVDWWLPH
tD0FRQWDLQV M$JUDPVRI$ DQG M%
JUDPVRI%WKHQDPRGHOIRUWKHWRWDODPRXQW yRIWKHPL[WXUH
SUHVHQWDWWLPH tLV
yDM$e/NUL:02tCM%e/NUL:07t.6/6XSSRVH WKH LQLWLDO DPRXQWV M$DQGM%DUH XQNQRZQ
EXW D VFLHQWLVW LV DEOH WR PHDVXUH WKH WRWDO DPRXQWVSUHVHQWDWVHYHUDOWLPHVDQGUHFRUGVWKHIROORZLQJSRLQWV
.ti;yi/.10; 21:34/ .11; 20:68/ .12; 20:05/ .14; 18:87/ 
DQG.15; 18:30/ 
D 'HVFULEHDOLQHDUPRGHOWKDWFDQEHXVHGWRHVWLPDWH M$
DQGM%
E >0@ )LQGWKHOHDVWVTXDUHVFXUYHEDVHGRQ
+DOOH\¬∑V&RPHWODVWDSSHDUHGLQDQGZLOOUHDSSHDULQ

>0@ $FFRUGLQJWR.HSOHU¬∑V√ÄUVWODZ DFRPHWVKRXOGKDYH
DQHOOLSWLF SDUDEROLF RUK\SHUEROLFRUELWZLWKJUDYLWDWLRQDODWWUDFWLRQVIURPWKHSODQHWVLJQRUHG ,QVXLWDEOHSRODUFRRUGLQDWHVWKHSRVLWLRQ
.r; #/RIDFRPHWVDWLV√ÄHVDQHTXDWLRQRI
WKHIRUP
rDÀáCe.r/SOHFRV#/
ZKHUH ÀáLVDFRQVWDQWDQG eLVWKHHFFHQWULFLW\ RIWKHRUELW
ZLWK0/DC4e<1IRUDQHOOLSVH eD1IRUDSDUDERODDQG e>1
IRUDK\SHUEROD6XSSRVHREVHUYDWLRQVRIDQHZO\GLVFRYHUHG
FRPHWSURYLGHWKHGDWDEHORZ 'HWHUPLQHWKHW\SHRIRUELWDQGSUHGLFWZKHUHWKHFRPHWZLOOEHZKHQ
#D4:6UDGLDQV
#              
r    
>0@ $KHDOWK\FKLOG¬∑VV\VWROLFEORRGSUHVVXUH pLQPLOOLPH
WHUVRIPHUFXU\DQGZHLJKW wLQSRXQGVDUHDSSUR[LPDWHO\
UHODWHGE\WKHHTXDWLRQ
Àá0CÀá1OQwDp
8VHWKHIROORZLQJH[SHULPHQWDOGDWDWRHVWLPDWHWKHV\VWROLF
EORRGSUHVVXUHRIDKHDOWK\FKLOGZHLJKLQJSRXQGV
37KHEDVLFLGHDRIOHDVWVTXDUHV√ÄWWLQJRIGDWDLVGXHWR. ) *D XVV
DQG LQGHSHQGHQWO\ WR$/HJHQGUH ZKRVHLQLWLDOULVHWRIDPHR FFXUUHG
LQZKHQKHXVHGWKHPHWKRGWRGHWHUPLQHWKHSDWKRIWKHDV WHURLG
&HUHV)RUW\GD\VDIWHUWKHDVWHURLGZDVGLVFRYHUHGLWGLVDSSHDUHGEHK LQG
WKHVXQ *DXVVSUHGLFWHGLWZRXOGDSSHDUWHQPRQWKVODWHUDQGJD YHLWV
ORFDWLRQ7KHDFFXUDF\RIWKHSUHGLFWLRQDVWRQLVKHGWKH(XURSHDQ VFLHQWL√ÄF
FRPPXQLW\

--- Page 394 ---
6.6 Applications to Linear Models 377
w 44 61 81 113 131
lnw 3.78 4.11 4.39 4.73 4.88
p 91 98 103 110 112
13. [M] To measure the takeoff performance of an airplane, the
horizontal position of the plane was measured every second,
from tD0totD12. The positions (in feet) were: 0, 8.8,
29.9, 62.0, 104.7, 159.1, 222.0, 294.5, 380.4, 471.1, 571.7,
686.8, and 809.2.
a.Find the least-squares cubic curve yD0C1tC
2t2C3t3for these data.
b.Use the result of part (a) to estimate the velocity of the
plane when tD4:5seconds.
14. LetxD1
n.x1C    C xn/andyD1
n.y1C    C yn/. Show
that the least-squares line for the data .x1; y1/; : : : ; .x n; yn/
must pass through .x;y/. That is, show that xandysatisfy
the linear equation yDO0CO1x. [Hint: Derive this equa-
tion from the vector equation yDXOC. Denote the Ô¨Årst
column of Xby1. Use the fact that the residual vector is
orthogonal to the column space of Xand hence is orthogonal
to1.]
Given data for a least-squares problem, .x1; y1/; : : : ; .x n; yn/, the
following abbreviations are helpful:
PxDPn
iD1xi;Px2DPn
iD1x2
i;
PyDPn
iD1yi;PxyDPn
iD1xiyi
The normal equations for a least-squares line yDO0CO1xmay
be written in the form
nO0CO1PxDPy
O0PxCO1Px2DPxy.7/
15. Derive the normal equations (7) from the matrix form given
in this section.
16. Use a matrix inverse to solve the system of equations in (7)
and thereby obtain formulas for O0andO1that appear in many
statistics texts.17. a.Rewrite the data in Example 1 with new x-coordinates
in mean deviation form. Let Xbe the associated design
matrix. Why are the columns of Xorthogonal?
b.Write the normal equations for the data in part (a), and
solve them to Ô¨Ånd the least-squares line, yD0C1x,
where xDx 5:5.
18. Suppose the x-coordinates of the data .x1; y1/; : : : ; .x n; yn/
are in mean deviation form, so thatPxiD0. Show that if
Xis the design matrix for the least-squares line in this case,
thenXTXis a diagonal matrix.
Exercises 19 and 20 involve a design matrix Xwith two or more
columns and a least-squares solution OofyDX. Consider the
following numbers.
(i)kXOk2‚Äîthe s um of the s quares of the ‚Äúr egression term.‚Äù
Denote this number by SS(R).
(ii)ky XOk2‚Äîthe s um of the s quares for e rror term. Denote
this number by SS(E).
(iii)kyk2‚Äîthe ‚Äút otal‚Äù s um of the s quares of the y-values. Denote
this number by SS(T).
Every statistics text that discusses regression and the linear model
yDXCintroduces these numbers, though terminology and
notation vary somewhat. To simplify matters, assume that the
mean of the y-values is zero. In this case, SS(T) is proportional to
what is called the variance of the set of y-values.
19. Justify the equation SS(T) DSS(R) CSS(E). [ Hint: Use a
theorem, and explain why the hypotheses of the theorem are
satisÔ¨Åed.] This equation is extremely important in statistics,
both in regression theory and in the analysis of variance.
20. Show that kXOk2=OTXTy. [Hint: Rewrite the left side
and use the fact that OsatisÔ¨Åes the normal equations.] This
formula for SS(R) is used in statistics. From this and from
Exercise 19, obtain the standard formula for SS(E):
SS(E) DyTy OTXTy
SOLUTION TO PRACTICE PROBLEM
Construct Xandso that the kth row of Xis the predicted y-value that corresponds
xy
Sales trend with seasonal
Ô¨Çuctuations.to the data point .xk; yk/, namely,
0C1xkC2sin.2x k=12/
It should be clear that
XD2
641 x 1 sin.2x 1=12/
:::::::::
1 x n sin.2x n=12/3
75;D2
40
1
23
5
SECOND REVISED PAGES


--- Page 395 ---
378 CHAPTER 6 Orthogonality and Least Squares
6.7 INNER PRODUCT SPACES
Notions of length, distance, and orthogonality are often important in applications
involving a vector space. For Rn, these concepts were based on the properties of the
inner product listed in Theorem 1 of Section 6.1. For other spaces, we need analogues of
the inner product with the same properties. The conclusions of Theorem 1 now become
axioms in the following deÔ¨Ånition.
D E F I N I T I O N Aninner product on a vector space Vis a function that, to each pair of vectors
uandvinV, associates a real number hu;viand satisÔ¨Åes the following axioms,
for all u,v, and winVand all scalars c:
1.hu;vi D h v;ui
2.huCv;wi D h u;wi C h v;wi
3.hcu;vi Dchu;vi
4.hu;ui 0and hu;ui D0if and only if uD0
A vector space with an inner product is called an inner product space .
The vector space Rnwith the standard inner product is an inner product space, and
nearly everything discussed in this chapter for Rncarries over to inner product spaces.
The examples in this section and the next lay the foundation for a variety of applications
treated in courses in engineering, physics, mathematics, and statistics.
EXAMPLE 1 Fix any two positive numbers‚Äîsay, 4 and 5‚Äîand for vectors
uD.u1; u2/andvD.v1; v2/inR2, set
hu;vi D4u1v1C5u2v2 (1)
Show that equation (1) deÔ¨Ånes an inner product.
SOLUTION Certainly Axiom 1 is satisÔ¨Åed, because hu;vi D4u1v1C5u2v2D
4v1u1C5v2u2D hv;ui. IfwD.w1; w2/, then
huCv;wi D4.u1Cv1/w1C5.u2Cv2/w2
D4u1w1C5u2w2C4v1w1C5v2w2
D hu;wi C h v;wi
This veriÔ¨Åes Axiom 2. For Axiom 3, compute
hcu;vi D4.cu 1/v1C5.cu 2/v2Dc.4u 1v1C5u2v2/Dchu;vi
For Axiom 4, note that hu;ui D4u2
1C5u2
20, and4u2
1C5u2
2D0only if u1Du2D
0, that is, if uD0. Also, h0;0i D0. So (1) deÔ¨Ånes an inner product on R2.
Inner products similar to (1) can be deÔ¨Åned on Rn. They arise naturally in con-
nection with ‚Äúweighted least-squares‚Äù problems, in which weights are assigned to the
various entries in the sum for the inner product in such a way that more importance is
given to the more reliable measurements.
From now on, when an inner product space involves polynomials or other functions,
we will write the functions in the familiar way, rather than use the boldface type for
vectors. Nevertheless, it is important to remember that each function isa vector when it
is treated as an element of a vector space.
SECOND REVISED PAGES


--- Page 396 ---
6.7 Inner Product Spaces 379
EXAMPLE 2 Lett0; : : : ; t nbe distinct real numbers. For pandqinPn, deÔ¨Åne
hp; qi Dp.t0/q.t 0/Cp.t1/q.t 1/C    C p.tn/q.t n/ (2)
Inner product Axioms 1‚Äì3 are readily checked. For Axiom 4, note that
hp; pi D¬åp.t 0/¬ç2C¬åp.t 1/¬ç2C    C ¬åp.t n/¬ç20
Also, h0;0i D0. (The boldface zero here denotes the zero polynomial, the zero vector
inPn.) Ifhp; pi D0, then pmust vanish at nC1points: t0; : : : ; t n. This is possible
only if pis the zero polynomial, because the degree of pis less than nC1. Thus (2)
deÔ¨Ånes an inner product on Pn.
EXAMPLE 3 LetVbeP2, with the inner product from Example 2, where t0D0,
t1D1
2, and t2D1. Letp.t/D12t2andq.t/D2t 1. Compute hp; qiandhq; qi.
SOLUTION
hp; qi Dp.0/q.0/ Cp 1
2
q 1
2
Cp.1/q.1/
D.0/. 1/C.3/.0/ C.12/.1/ D12
hq; qi D¬åq.0/¬ç2C¬åq 1
2
¬ç2C¬åq.1/¬ç2
D. 1/2C.0/2C.1/2D2
Lengths, Distances, and Orthogonality
LetVbe an inner product space, with the inner product denoted by hu;vi. Just as in Rn,
we deÔ¨Åne the length , ornorm , of a vector vto be the scalar
kvk Dp
hv;vi
Equivalently, kvk2D hv;vi. (This deÔ¨Ånition makes sense because hv;vi 0, but the
deÔ¨Ånition does not say that hv;viis a ‚Äúsum of squares,‚Äù because vneed not be an element
ofRn.)
Aunit vector is one whose length is 1. The distance between u and v isku vk.
Vectors uandvareorthogonal ifhu;vi D0.
EXAMPLE 4 LetP2have the inner product (2) of Example 3. Compute the lengths
of the vectors p.t/D12t2andq.t/D2t 1.
SOLUTION
kpk2D hp; pi D¬åp.0/¬ç2C
p 1
22C¬åp.1/¬ç2
D0C¬å3¬ç2C¬å12¬ç2D153
kpk Dp
153
From Example 3, hq; qi D2. Hence kqk Dp
2.
The Gram‚ÄìSchmidt Process
The existence of orthogonal bases for Ô¨Ånite-dimensional subspaces of an inner product
space can be established by the Gram‚ÄìSchmidt process, just as in Rn. Certain orthogonal
bases that arise frequently in applications can be constructed by this process.
The orthogonal projection of a vector onto a subspace Wwith an orthogonal basis
can be constructed as usual. The projection does not depend on the choice of orthogonal
basis, and it has the properties described in the Orthogonal Decomposition Theorem and
the Best Approximation Theorem.
SECOND REVISED PAGES


--- Page 397 ---
380 CHAPTER 6 Orthogonality and Least Squares
EXAMPLE 5 LetVbeP4with the inner product in Example 2, involving evaluation
of polynomials at  2, 1, 0, 1, and 2, and view P2as a subspace of V. Produce an
orthogonal basis for P2by applying the Gram‚ÄìSchmidt process to the polynomials 1, t,
andt2.
SOLUTION The inner product depends only on the values of a polynomial at  2; : : : ; 2 ,
so we list the values of each polynomial as a vector in R5, underneath the name of the
polynomial:1
Polynomial: 1 t t2
Vector of values:2
666641
1
1
1
13
77775;2
66664 2
 1
0
1
23
77775;2
666644
1
0
1
43
77775
The inner product of two polynomials in Vequals the (standard) inner product of their
corresponding vectors in R5. Observe that tis orthogonal to the constant function 1. So
takep0.t/D1andp1.t/Dt. For p2, use the vectors in R5to compute the projection
oft2onto Span fp0; p1g:
ht2; p0i D h t2; 1i D4C1C0C1C4D10
hp0; p0i D5
ht2; p1i D h t2; ti D   8C. 1/C0C1C8D0
The orthogonal projection of t2onto Span f1; tgis10
5p0C0p1. Thus
p2.t/Dt2 2p0.t/Dt2 2
An orthogonal basis for the subspace P2ofVis:
Polynomial: p0 p1 p2
Vector of values:2
666641
1
1
1
13
77775;2
66664 2
 1
0
1
23
77775;2
666642
 1
 2
 1
23
77775(3)
Best Approximation in Inner Product Spaces
A common problem in applied mathematics involves a vector space Vwhose elements
are functions. The problem is to approximate a function finVby a function gfrom a
speciÔ¨Åed subspace WofV. The ‚Äúcloseness‚Äù of the approximation of fdepends on the
waykf gkis deÔ¨Åned. We will consider only the case in which the distance between
fandgis determined by an inner product. In this case, the best approximation to fby
functions in Wis the orthogonal projection of fonto the subspace W.
EXAMPLE 6 LetVbeP4with the inner product in Example 5, and let p0,p1,
andp2be the orthogonal basis found in Example 5 for the subspace P2. Find the best
approximation to p.t/D5 1
2t4by polynomials in P2.
1Each polynomial in P4is uniquely determined by its value at the Ô¨Åve numbers  2; : : : ; 2 . In fact, the
correspondence between pand its vector of values is an isomorphism, that is, a one-to-one mapping onto
R5that preserves linear combinations.
SECOND REVISED PAGES


--- Page 398 ---
6.7 Inner Product Spaces 381
SOLUTION The values of p0; p1, and p2at the numbers  2, 1, 0, 1, and 2 are listed
inR5vectors in (3) above. The corresponding values for pare 3, 9/2, 5, 9/2, and  3.
Compute
hp; p 0i D8; hp; p 1i D0; hp; p 2i D   31
hp0; p0i D5; hp2; p2i D14
Then the best approximation in Vtopby polynomials in P2is
OpDprojP2pDhp; p 0i
hp0; p0ip0Chp; p 1i
hp1; p1ip1Chp; p 2i
hp2; p2ip2
D8
5p0C 31
14p2D8
5 31
14.t2 2/:
This polynomial is the closest to pof all polynomials in P2, when the distance between
polynomials is measured only at  2, 1, 0, 1, and 2. See Figure 1.
ty
2
2
p(t)p(t)ÀÜ
FIGURE 1
The polynomials p0,p1, andp2in Examples 5 and 6 belong to a class of polynomi-
als that are referred to in statistics as orthogonal polynomials .2The orthogonality refers
to the type of inner product described in Example 2.
Two Inequalities
Given a vector vin an inner product space Vand given a Ô¨Ånite-dimensional subspace
W, we may apply the Pythagorean Theorem to the orthogonal decomposition of vwith
respect to Wand obtain
kvk2D kprojWvk2C kv projWvk2
See Figure 2. In particular, this shows that the norm of the projection of vontoWdoes
not exceed the norm of vitself. This simple observation leads to the following important
Wv
0
||projWv|| projW v||v /H11002 projW v||||v||FIGURE 2
The hypotenuse is the longest side.
inequality.
T H E O R E M 1 6 The Cauchy--Schwarz Inequality
For all u,vinV,
jhu;vij  k uk kvk (4)
2SeeStatistics and Experimental Design in Engineering and the Physical Sciences , 2nd ed., by Norman
L. Johnson and Fred C. Leone (New York: John Wiley & Sons, 1977). Tables there list ‚ÄúOrthogonal
Polynomials,‚Äù which are simply the values of the polynomial at numbers such as  2, 1, 0, 1, and 2.
SECOND REVISED PAGES


--- Page 399 ---
382 CHAPTER 6 Orthogonality and Least Squares
PROOF IfuD0, then both sides of (4) are zero, and hence the inequality is true in this
case. (See Practice Problem 1.) If u¬§0, letWbe the subspace spanned by u. Recall
thatkcuk D j cj kukfor any scalar c. Thus
kprojWvk Dhv;ui
hu;uiuDjhv;uij
jhu;uijkuk Djhv;uij
kuk2kuk Djhu;vij
kuk
Since kprojWvk  k vk, we havejhu;vij
kuk kvk, which gives (4).
The Cauchy‚ÄìSchwarz inequality is useful in many branches of mathematics. A few
simple applications are presented in the exercises. Our main need for this inequality here
is to prove another fundamental inequality involving norms of vectors. See Figure 3.
T H E O R E M 1 7 The Triangle Inequality
For all u;vinV,
kuCvk  k uk C k vk
PROOF kuCvk2D huCv;uCvi D h u;ui C2hu;vi C h v;vi
 kuk2C2jhu;vij C k vk2
 kuk2C2kuk kvk C k vk2Cauchy‚ÄìSchwarz
D.kuk C k vk/2
The triangle inequality follows immediately by taking square roots of both sides.
0 uv
||u + v||u + v
||v||
||u||FIGURE 3
The lengths of the sides of a
triangle.
An Inner Product for C ¬åa; b¬ç (Calculus required)
Probably the most widely used inner product space for applications is the vector space
C ¬åa; b¬ç of all continuous functions on an interval atb, with an inner product that
we will describe.
We begin by considering a polynomial pand any integer nlarger than or equal
to the degree of p. Then pis inPn, and we may compute a ‚Äúlength‚Äù for pusing the
inner product of Example 2 involving evaluation at nC1points in ¬åa; b¬ç . However, this
length of pcaptures the behavior at only those nC1points. Since pis inPnfor all
large n, we could use a much larger n, with many more points for the ‚Äúevaluation‚Äù inner
product. See Figure 4.
t
b at
b ap(t) p(t)
FIGURE 4 Using different numbers of evaluation points in ¬åa; b¬ç to compute
kpk2.
SECOND REVISED PAGES


--- Page 400 ---
6.7 Inner Product Spaces 383
Let us partition ¬åa; b¬ç intonC1subintervals of length ¬ÅtD.b a/=.n C1/, and
lett0; : : : ; t nbe arbitrary points in these subintervals.
a t0Œît
tj b tn
Ifnis large, the inner product on Pndetermined by t0; : : : ; t nwill tend to give a large
value to hp; pi, so we scale it down and divide by nC1. Observe that 1=.nC1/D
¬Åt=.b  a/, and deÔ¨Åne
hp; qi D1
nC1nX
jD0p.tj/q.t j/D1
b a2
4nX
jD0p.tj/q.t j/¬Åt3
5
Now, let nincrease without bound. Since polynomials pandqare continuous functions,
the expression in brackets is a Riemann sum that approaches a deÔ¨Ånite integral, and we
are led to consider the average value of p.t/q.t/ on the interval ¬åa; b¬ç :
1
b aZb
ap.t/q.t/ dt
This quantity is deÔ¨Åned for polynomials of any degree (in fact, for all continuous
functions), and it has all the properties of an inner product, as the next example shows.
The scale factor 1=.b a/is inessential and is often omitted for simplicity.
EXAMPLE 7 Forf,ginC ¬åa; b¬ç , set
hf; gi DZb
af .t/g.t/ dt (5)
Show that (5) deÔ¨Ånes an inner product on C ¬åa; b¬ç .
SOLUTION Inner product Axioms 1‚Äì3 follow from elementary properties of deÔ¨Ånite
integrals. For Axiom 4, observe that
hf; fi DZb
a¬åf .t/¬ç2dt0
The function ¬åf .t/¬ç2is continuous and nonnegative on ¬åa; b¬ç . If the deÔ¨Ånite integral of
¬åf .t/¬ç2is zero, then ¬åf .t/¬ç2must be identically zero on ¬åa; b¬ç , by a theorem in advanced
calculus, in which case fis the zero function. Thus hf; fi D0implies that fis the
zero function on ¬åa; b¬ç . So (5) deÔ¨Ånes an inner product on C ¬åa; b¬ç .
EXAMPLE 8 LetVbe the space C ¬å0; 1¬ç with the inner product of Example 7, and
letWbe the subspace spanned by the polynomials p1.t/D1,p2.t/D2t 1, and
p3.t/D12t2. Use the Gram‚ÄìSchmidt process to Ô¨Ånd an orthogonal basis for W.
SOLUTION Letq1Dp1, and compute
hp2; q1i DZ1
0.2t 1/.1/ dt D.t2 t/1
0D0
SECOND REVISED PAGES


--- Page 401 ---
384 CHAPTER 6 Orthogonality and Least Squares
Sop2is already orthogonal to q1, and we can take q2Dp2. For the projection of p3
ontoW2DSpanfq1; q2g, compute
hp3; q1i DZ1
012t21 dtD4t31
0D4
hq1; q1i DZ1
011 dtDt1
0D1
hp3; q2i DZ1
012t2.2t 1/ dt DZ1
0.24t3 12t2/ dtD2
hq2; q2i DZ1
0.2t 1/2dtD1
6.2t 1/31
0D1
3
Then
projW2p3Dhp3; q1i
hq1; q1iq1Chp3; q2i
hq2; q2iq2D4
1q1C2
1=3q2D4q1C6q2
and
q3Dp3 projW2p3Dp3 4q1 6q2
As a function, q3.t/D12t2 4 6.2t 1/D12t2 12tC2. The orthogonal basis
for the subspace Wisfq1; q2; q3g.
PRACTICE PROBLEMS
Use the inner product axioms to verify the following statements.
1.hv;0i D h 0;vi D0.
2.hu;vCwi D h u;vi C h u;wi.
6.7 EXERCISES
1.LetR2have the inner product of Example 1, and let
xD.1; 1/ andyD.5; 1/.
a.Findkxk,kyk, andjhx;yij2.
b.Describe all vectors .¬¥1; ¬¥2/that are orthogonal to y.
2.LetR2have the inner product of Example 1. Show that
the Cauchy‚ÄìSchwarz inequality holds for xD.3; 2/and
yD. 2; 1/. [Suggestion: Study jhx;yij2.]
Exercises 3‚Äì8 refer to P2with the inner product given by evalua-
tion at  1, 0, and 1. (See Example 2.)
3.Compute hp; qi, where p.t/D4Ct,q.t/D5 4t2.
4.Compute hp; qi, where p.t/D3t t2,q.t/D3C2t2.
5.Compute kpkandkqk, forpandqin Exercise 3.
6.Compute kpkandkqk, forpandqin Exercise 4.
7.Compute the orthogonal projection of qonto the subspace
spanned by p, forpandqin Exercise 3.
8.Compute the orthogonal projection of qonto the subspace
spanned by p, forpandqin Exercise 4.9.LetP3have the inner product given by evaluation at  3, 1,
1, and 3. Let p0.t/D1,p1.t/Dt, and p2.t/Dt2.
a.Compute the orthogonal projection of p2onto the sub-
space spanned by p0andp1.
b.Find a polynomial qthat is orthogonal to p0and
p1, such that fp0; p1; qgis an orthogonal basis for
Spanfp0; p1; p2g. Scale the polynomial qso that its vec-
tor of values at . 3; 1; 1; 3/ is.1; 1; 1; 1/.
10. LetP3have the inner product as in Exercise 9, with p0; p1,
andqthe polynomials described there. Find the best approx-
imation to p.t/Dt3by polynomials in Span fp0; p1; qg.
11.Letp0,p1, and p2be the orthogonal polynomials described
in Example 5, where the inner product on P4is given by eval-
uation at  2, 1, 0, 1, and 2. Find the orthogonal projection
oft3onto Span fp0; p1; p2g.
12. Find a polynomial p3such that fp0; p1; p2; p3g(see Ex-
ercise 11) is an orthogonal basis for the subspace P3of
P4. Scale the polynomial p3so that its vector of values is
. 1; 2; 0;  2; 1/.
SECOND REVISED PAGES


--- Page 402 ---
6.8 Applications of Inner Product Spaces 385
13. LetAbe any invertible nnmatrix. Show that for u,vin
Rn, the formula hu;vi D.Au/.Av/D.Au/T.Av/deÔ¨Ånes
an inner product on Rn.
14. LetTbe a one-to-one linear transformation from a vector
space VintoRn. Show that for u,vinV, the formula
hu;vi DT .u/T .v/deÔ¨Ånes an inner product on V.
Use the inner product axioms and other results of this section to
verify the statements in Exercises 15‚Äì18.
15.hu; cvi Dchu;vifor all scalars c.
16. Iffu;vgis an orthonormal set in V, then ku vk Dp
2.
17.hu;vi D1
4kuCvk2 1
4ku vk2.
18.kuCvk2C ku vk2D2kuk2C2kvk2.
19. Given a0andb0, let uDpap
b
andvDp
bpa
.
Use the Cauchy‚ÄìSchwarz inequality to compare the geomet-
ric meanp
abwith the arithmetic mean .aCb/=2 .
20. LetuDa
b
andvD1
1
. Use the Cauchy‚ÄìSchwarz in-
equality to show that
aCb
22
a2Cb2
2Exercises 21‚Äì24 refer to VDC ¬å0; 1¬ç , with the inner product
given by an integral, as in Example 7.
21. Compute hf; gi, where f .t/D1 3t2andg.t/Dt t3.
22. Compute hf; gi, where f .t/D5t 3andg.t/Dt3 t2.
23. Compute kfkforfin Exercise 21.
24. Compute kgkforgin Exercise 22.
25. LetVbe the space C ¬å 1; 1¬ç with the inner product of Ex-
ample 7. Find an orthogonal basis for the subspace spanned
by the polynomials 1,t, and t2. The polynomials in this basis
are called Legendre polynomials .
26. LetVbe the space C ¬å 2; 2¬ç with the inner product of Exam-
ple 7. Find an orthogonal basis for the subspace spanned by
the polynomials 1,t, and t2.
27. [M] LetP4have the inner product as in Example 5, and let
p0,p1,p2be the orthogonal polynomials from that example.
Using your matrix program, apply the Gram‚ÄìSchmidt proc-
ess to the set fp0; p1; p2; t3; t4gto create an orthogonal basis
forP4.
28. [M] Let Vbe the space C ¬å0; 2¬ç with the inner prod-
uct of Example 7. Use the Gram‚ÄìSchmidt process to
create an orthogonal basis for the subspace spanned by
f1;cost;cos2t;cos3tg. Use a matrix program or computa-
tional program to compute the appropriate deÔ¨Ånite integrals.
SOLUTIONS TO PRACTICE PROBLEMS
1.By Axiom 1, hv;0i D h 0;vi. Then h0;vi D h 0v;vi D0hv;vi, by Axiom 3, so
h0;vi D0.
2.By Axioms 1, 2, and then 1 again, hu;vCwi D h vCw;ui D h v;ui C h w;ui D
hu;vi C h u;wi.
6.8 APPLICATIONS OF INNER PRODUCT SPACES
The examples in this section suggest how the inner product spaces deÔ¨Åned in Section 6.7
arise in practical problems. The Ô¨Årst example is connected with the massive least-
squares problem of updating the North American Datum, described in the chapter‚Äôs
introductory example.
Weighted Least-Squares
Letybe a vector of nobservations, y1; : : : ; y n, and suppose we wish to approximate yby
a vector Oythat belongs to some speciÔ¨Åed subspace of Rn. (In Section 6.5, Oywas written
asAxso that Oywas in the column space of A.) Denote the entries in OybyOy1; : : : ; Oyn.
Then the sum of the squares for error , or SS(E), in approximating ybyOyis
SS(E) D.y1  Oy1/2C    C .yn  Oyn/2(1)
This is simply ky Oyk2, using the standard length in Rn.
SECOND REVISED PAGES


--- Page 403 ---
386 CHAPTER 6 Orthogonality and Least Squares
Now suppose the measurements that produced the entries in yare not equally
reliable. (This was the case for the North American Datum, since measurements were
made over a period of 140 years.) As another example, the entries in ymight be
computed from various samples of measurements, with unequal sample sizes.) Then
it becomes appropriate to weight the squared errors in (1) in such a way that more
importance is assigned to the more reliable measurements.1If the weights are denoted
byw2
1; : : : ; w2
n, then the weighted sum of the squares for error is
Weighted SS(E) Dw2
1.y1  Oy1/2C    C w2
n.yn  Oyn/2(2)
This is the square of the length of y Oy, where the length is derived from an inner
product analogous to that in Example 1 in Section 6.7, namely,
hx;yi Dw2
1x1y1C    C w2
nxnyn
It is sometimes convenient to transform a weighted least-squares problem into an
equivalent ordinary least-squares problem. Let Wbe the diagonal matrix with (positive)
w1; : : : ; w non its diagonal, so that
WyD2
6664w1 0    0
0 w 2
:::::::::
0    wn3
77752
6664y1
y2
:::
yn3
7775D2
6664w1y1
w2y2
:::
wnyn3
7775
with a similar expression for WOy. Observe that the jth term in (2) can be written as
w2
j.yj  Oyj/2D.wjyj wjOyj/2
It follows that the weighted SS(E) in (2) is the square of the ordinary length in Rnof
Wy WOy, which we write as kWy WOyk2.
Now suppose the approximating vector Oyis to be constructed from the columns of
a matrix A. Then we seek an Oxthat makes AOxDOyas close to yas possible. However,
the measure of closeness is the weighted error,
kWy WOyk2D kWy WAOxk2
Thus Oxis the (ordinary) least-squares solution of the equation
WAxDWy
The normal equation for the least-squares solution is
.WA/TWAxD.WA/TWy
EXAMPLE 1 Find the least-squares line yD0C1xthat best Ô¨Åts the data
. 2; 3/,. 1; 5/,.0; 5/ ,.1; 4/ , and .2; 3/ . Suppose the errors in measuring the y-values
of the last two data points are greater than for the other points. Weight these data half as
much as the rest of the data.
1Note for readers with a background in statistics: Suppose the errors in measuring the yiare independent
random variables with means equal to zero and variances of 2
1; : : : ; 2
n. Then the appropriate weights in (2)
arew2
iD1=2
i. The larger the variance of the error, the smaller the weight.
SECOND REVISED PAGES


--- Page 404 ---
6.8 Applications of Inner Product Spaces 387
SOLUTION As in Section 6.6, write Xfor the matrix Aandfor the vector x, and
obtain
XD2
666641 2
1 1
1 0
1 1
1 23
77775;D0
1
;yD2
666643
5
5
4
33
77775
For a weighting matrix, choose Wwith diagonal entries 2, 2, 2, 1, and 1. Left-
multiplication by Wscales the rows of Xandy:
WXD2
666642 4
2 2
2 0
1 1
1 23
77775; W yD2
666646
10
10
4
33
77775
For the normal equation, compute
.WX/TWXD14 9
 9 25
and .WX/TWyD59
 34
and solve 14 9
 9 250
1
D59
 34
The solution of the normal equation is (to two signiÔ¨Åcant digits) 0D4:3and1D:20.
The desired line is
yD4:3C:20x
In contrast, the ordinary least-squares line for these data is
yD4:0 :10x
Both lines are displayed in Figure 1.
22
‚Äì2y = 4 ‚Äì .1 xy = 4.3 + .2 xy
xFIGURE 1
Weighted and ordinary
least-squares lines.
Trend Analysis of Data
Letfrepresent an unknown function whose values are known (perhaps only approx-
imately) at t0; : : : ; t n. If there is a ‚Äúlinear trend‚Äù in the data f .t0/; : : : ; f .t n/, then we
might expect to approximate the values of fby a function of the form 0C1t.
If there is a ‚Äúquadratic trend‚Äù to the data, then we would try a function of the form
0C1tC2t2. This was discussed in Section 6.6, from a different point of view.
In some statistical problems, it is important to be able to separate the linear trend
from the quadratic trend (and possibly cubic or higher-order trends). For instance,
suppose engineers are analyzing the performance of a new car, and f .t/ represents
the distance between the car at time tand some reference point. If the car is traveling
at constant velocity, then the graph of f .t/ should be a straight line whose slope is the
car‚Äôs velocity. If the gas pedal is suddenly pressed to the Ô¨Çoor, the graph of f .t/ will
change to include a quadratic term and possibly a cubic term (due to the acceleration).
To analyze the ability of the car to pass another car, for example, engineers may want
to separate the quadratic and cubic components from the linear term.
If the function is approximated by a curve of the form yD0C1tC2t2, the
coefÔ¨Åcient 2may not give the desired information about the quadratic trend in the data,
because it may not be ‚Äúindependent‚Äù in a statistical sense from the other i. To make
SECOND REVISED PAGES


--- Page 405 ---
388 CHAPTER 6 Orthogonality and Least Squares
what is known as a trend analysis of the data, we introduce an inner product on the
spacePnanalogous to that given in Example 2 in Section 6.7. For p,qinPn, deÔ¨Åne
hp; qi Dp.t0/q.t 0/C    C p.tn/q.t n/
In practice, statisticians seldom need to consider trends in data of degree higher than
cubic or quartic. So let p0,p1,p2,p3denote an orthogonal basis of the subspace P3of
Pn, obtained by applying the Gram‚ÄìSchmidt process to the polynomials 1, t,t2, and t3.
By Supplementary Exercise 11 in Chapter 2, there is a polynomial ginPnwhose values
att0; : : : ; t ncoincide with those of the unknown function f. Let Ogbe the orthogonal
projection (with respect to the given inner product) of gontoP3, say,
OgDc0p0Cc1p1Cc2p2Cc3p3
Then Ogis called a cubic trend function , and c0; : : : ; c 3are the trend coefÔ¨Åcients of
the data. The coefÔ¨Åcient c1measures the linear trend, c2the quadratic trend, and c3the
cubic trend. It turns out that if the data have certain properties, these coefÔ¨Åcients are
statistically independent.
Since p0; : : : ; p 3are orthogonal, the trend coefÔ¨Åcients may be computed one
at a time, independently of one another. (Recall that ciD hg; p ii=hpi; pii.) We can
ignore p3andc3if we want only the quadratic trend. And if, for example, we needed
to determine the quartic trend, we would have to Ô¨Ånd (via Gram‚ÄìSchmidt) only a
polynomial p4inP4that is orthogonal to P3and compute hg; p 4i=hp4; p4i.
EXAMPLE 2 The simplest and most common use of trend analysis occurs when the
points t0; : : : ; t ncan be adjusted so that they are evenly spaced and sum to zero. Fit a
quadratic trend function to the data . 2; 3/,. 1; 5/,.0; 5/ ,.1; 4/ , and .2; 3/ .
SOLUTION Thet-coordinates are suitably scaled to use the orthogonal polynomials
found in Example 5 of Section 6.7:
Polynomial: p0 p1 p2 Data: g
Vector of values:2
666641
1
1
1
13
77775;2
66664 2
 1
0
1
23
77775;2
666642
 1
 2
 1
23
77775;2
666643
5
5
4
33
77775
The calculations involve only these vectors, not the speciÔ¨Åc formulas for the orthogonal
polynomials. The best approximation to the data by polynomials in P2is the orthogonal
projection given by
OpDhg; p 0i
hp0; p0ip0Chg; p 1i
hp1; p1ip1Chg; p 2i
hp2; p2ip2
D20
5p0 1
10p1 7
14p2
and
Op.t/D4 :1t :5.t2 2/ (3)
Since the coefÔ¨Åcient of p2is not extremely small, it would be reasonable to conclude
that the trend is at least quadratic. This is conÔ¨Årmed by the graph in Figure 2.
22
‚Äì2y
y = p(t)
xFIGURE 2
Approximation by a quadratic
trend function.
SECOND REVISED PAGES


--- Page 406 ---
6.8 Applications of Inner Product Spaces 389
Fourier Series (Calculus required)
Continuous functions are often approximated by linear combinations of sine and cosine
functions. For instance, a continuous function might represent a sound wave, an electric
signal of some type, or the movement of a vibrating mechanical system.
For simplicity, we consider functions on 0t2. It turns out that any function
inC ¬å0; 2¬ç can be approximated as closely as desired by a function of the form
a0
2Ca1costC    C ancosntCb1sintC    C bnsinnt (4)
for a sufÔ¨Åciently large value of n. The function (4) is called a trigonometric poly-
nomial . Ifanandbnare not both zero, the polynomial is said to be of order n. The
connection between trigonometric polynomials and other functions in C ¬å0; 2¬ç depends
on the fact that for any n1, the set
f1;cost;cos2t; : : : ; cosnt;sint;sin2t; : : : ; sinntg (5)
is orthogonal with respect to the inner product
hf; gi DZ2
0f .t/g.t/ dt (6)
This orthogonality is veriÔ¨Åed as in the following example and in Exercises 5 and 6.
EXAMPLE 3 LetC ¬å0; 2¬ç have the inner product (6), and let mandnbe unequal
positive integers. Show that cos mtand cos ntare orthogonal.
SOLUTION Use a trigonometric identity. When m¬§n,
hcosmt;cosnti DZ2
0cosmtcosnt dt
D1
2Z2
0¬åcos.mtCnt/Ccos.mt nt/¬ç dt
D1
2sin.mtCnt/
mCnCsin.mt nt/
m n2
0D0
LetWbe the subspace of C ¬å0; 2¬ç spanned by the functions in (5). Given fin
C ¬å0; 2¬ç , the best approximation to fby functions in Wis called the nth-order Fourier
approximation tofon¬å0; 2¬ç . Since the functions in (5) are orthogonal, the best
approximation is given by the orthogonal projection onto W. In this case, the coefÔ¨Åcients
akandbkin (4) are called the Fourier coefÔ¨Åcients off. The standard formula for an
orthogonal projection shows that
akDhf;coskti
hcoskt;coskti; b kDhf;sinkti
hsinkt;sinkti; k 1
Exercise 7 asks you to show that hcoskt;coskti Dandhsinkt;sinkti D. Thus
akD1
Z2
0f .t/ coskt dt; b kD1
Z2
0f .t/ sinkt dt (7)
The coefÔ¨Åcient of the (constant) function 1in the orthogonal projection is
hf; 1i
h1; 1iD1
2Z2
0f .t/ 1 dtD1
21
Z2
0f .t/ cos.0t/ dt
Da0
2
where a0is deÔ¨Åned by (7) for kD0. This explains why the constant term in (4) is written
asa0=2.
SECOND REVISED PAGES


--- Page 407 ---
390 CHAPTER 6 Orthogonality and Least Squares
EXAMPLE 4 Find the nth-order Fourier approximation to the function f .t/Dton
the interval ¬å0; 2¬ç .
SOLUTION Compute
a0
2D1
21
Z2
0t dtD1
2"
1
2t22
0#
D
and for k > 0 , using integration by parts,
akD1
Z2
0tcoskt dt D1
1
k2cosktCt
ksinkt2
0D0
bkD1
Z2
0tsinkt dt D1
1
k2sinkt t
kcoskt2
0D  2
k
Thus the nth-order Fourier approximation of f .t/Dtis
 2sint sin2t 2
3sin3t      2
nsinnt
Figure 3 shows the third- and fourth-order Fourier approximations of f.
(a) Third ordery
2/H9266
2/H9266y = t
/H9266
/H9266t
y
2/H9266
2/H9266y = t
/H9266
/H9266t
(b) Fourth order
FIGURE 3 Fourier approximations of the function f .t/Dt.
The norm of the difference between fand a Fourier approximation is called the
mean square error in the approximation. (The term mean refers to the fact that the norm
is determined by an integral.) It can be shown that the mean square error approaches
zero as the order of the Fourier approximation increases. For this reason, it is common
to write
f .t/Da0
2C1X
mD1.amcosmtCbmsinmt/
This expression for f .t/ is called the Fourier series forfon¬å0; 2¬ç . The term
amcosmt, for example, is the projection of fonto the one-dimensional subspace
spanned by cos mt.
PRACTICE PROBLEMS
1.Letq1.t/D1,q2.t/Dt, and q3.t/D3t2 4. Verify that fq1; q2; q3gis an orthog-
onal set in C ¬å 2; 2¬ç with the inner product of Example 7 in Section 6.7 (integration
from 2to 2).
2.Find the Ô¨Årst-order and third-order Fourier approximations to
f .t/D3 2sintC5sin2t 6cos2t
SECOND REVISED PAGES


--- Page 408 ---
6.8 Applications of Inner Product Spaces 391
6.8 EXERCISES
1.Find the least-squares line yD0C1xthat best Ô¨Åts the
data. 2; 0/,. 1; 0/,.0; 2/ ,.1; 4/ , and .2; 4/ , assuming that
the Ô¨Årst and last data points are less reliable. Weight them half
as much as the three interior points.
2.Suppose 5 out of 25 data points in a weighted least-squares
problem have a y-measurement that is less reliable than the
others, and they are to be weighted half as much as the other
20 points. One method is to weight the 20 points by a factor
of 1 and the other 5 by a factor of1
2. A second method is
to weight the 20 points by a factor of 2 and the other 5 by
a factor of 1. Do the two methods produce different results?
Explain.
3.Fit a cubic trend function to the data in Example 2. The
orthogonal cubic polynomial is p3.t/D5
6t3 17
6t.
4.To make a trend analysis of six evenly spaced data points, one
can use orthogonal polynomials with respect to evaluation at
the points tD  5; 3; 1; 1; 3 , and 5.
a.Show that the Ô¨Årst three orthogonal polynomials are
p0.t/D1; p 1.t/Dt;and p2.t/D3
8t2 35
8
(The polynomial p2has been scaled so that its values at
the evaluation points are small integers.)
b.Fit a quadratic trend function to the data
. 5; 1/; .  3; 1/; .  1; 4/; .1; 4/; .3; 6/; .5; 8/
In Exercises 5‚Äì14, the space is C ¬å0; 2¬ç with the inner product (6).
5.Show that sin mtand sin ntare orthogonal when m¬§n.
6.Show that sin mtand cos ntare orthogonal for all positive
integers mandn.
7.Show that kcosktk2Dandksinktk2Dfork > 0 .
8.Find the third-order Fourier approximation to f .t/Dt 1.9.Find the third-order Fourier approximation to f .t/D
2 t.
10. Find the third-order Fourier approximation to the square
wave function f .t/D1for0t <  andf .t/D  1for
t < 2 .
11.Find the third-order Fourier approximation to sin2t, without
performing any integration calculations.
12. Find the third-order Fourier approximation to cos3t, without
performing any integration calculations.
13. Explain why a Fourier coefÔ¨Åcient of the sum of two functions
is the sum of the corresponding Fourier coefÔ¨Åcients of the
two functions.
14. Suppose the Ô¨Årst few Fourier coefÔ¨Åcients of some function
finC ¬å0; 2¬ç area0,a1,a2, and b1,b2,b3. Which of the
following trigonometric polynomials is closer to f? Defend
your answer.
g.t/Da0
2Ca1costCa2cos2tCb1sint
h.t/Da0
2Ca1costCa2cos2tCb1sintCb2sin2t
15. [M] Refer to the data in Exercise 13 in Section 6.6, con-
cerning the takeoff performance of an airplane. Suppose the
possible measurement errors become greater as the speed of
the airplane increases, and let Wbe the diagonal weighting
matrix whose diagonal entries are 1, 1, 1, .9, .9, .8, .7, .6, .5,
.4, .3, .2, and .1. Find the cubic curve that Ô¨Åts the data with
minimum weighted least-squares error, and use it to estimate
the velocity of the plane when tD4:5seconds.
16. [M] Let f4andf5be the fourth-order and Ô¨Åfth-order Fourier
approximations in C ¬å0; 2¬ç to the square wave function in
Exercise 10. Produce separate graphs of f4andf5on the
interval ¬å0; 2¬ç , and produce a graph of f5on¬å 2; 2¬ç .
SG
The Linearity of an Orthogonal Projection 6‚Äì25
SOLUTIONS TO PRACTICE PROBLEMS
1.Compute
hq1; q2i DZ2
 21t dtD1
2t22
 2D0
hq1; q3i DZ2
 21.3t2 4/ dt D.t3 4t/2
 2D0
hq2; q3i DZ2
 2t.3t2 4/ dt D3
4t4 2t22
 2D0
SECOND REVISED PAGES


--- Page 409 ---
392 CHAPTER 6 Orthogonality and Least Squares
2.The third-order Fourier approximation to fis the best approximation in C ¬å0; 2¬ç
tofby functions (vectors) in the subspace spanned by 1, cos t, cos 2t, cos 3t,
sint, sin2t, and sin 3t. But fis obviously inthis subspace, so fis its own best
approximation:
f .t/D3 2sintC5sin2t 6cos2t
For the Ô¨Årst-order approximation, the closest function to fin the subspace WD
Spanf1;cost;sintgis3 2sint. The other two terms in the formula for f .t/ are
orthogonal to the functions in W, so they contribute nothing to the integrals that
give the Fourier coefÔ¨Åcients for a Ô¨Årst-order approximation.
y = 3 ‚Äì 2 sin t
y = f(t)
t39
‚Äì32œÄy
œÄ
First- and third-order
approximations to f .t/.
CHAPTER 6 SUPPLEMENTARY EXERCISES
1.The following statements refer to vectors in Rn(orRm/
with the standard inner product. Mark each statement True
or False. Justify each answer.
a.The length of every vector is a positive number.
b.A vector vand its negative,  v, have equal lengths.
c.The distance between uandvisku vk.
d.Ifris any scalar, then krvk Drkvk.
e.If two vectors are orthogonal, they are linearly indepen-
dent.
f.Ifxis orthogonal to both uand v, then xmust be
orthogonal to u v.
g.IfkuCvk2D kuk2C kvk2, then uandvare orthogonal.
h.Ifku vk2D kuk2C kvk2, then uandvare orthogonal.
i.The orthogonal projection of yonto uis a scalar multiple
ofy.
j.If a vector ycoincides with its orthogonal projection onto
a subspace W, then yis inW.
k.The set of all vectors in Rnorthogonal to one Ô¨Åxed vector
is a subspace of Rn.
l.IfWis a subspace of Rn, then WandW?have no
vectors in common.
m.Iffv1;v2;v3gis an orthogonal set and if c1,c2, and c3are
scalars, then fc1v1; c2v2; c3v3gis an orthogonal set.
n.If a matrix Uhas orthonormal columns, then U UTDI.
o.A square matrix with orthogonal columns is an orthogo-
nal matrix.
p.If a square matrix has orthonormal columns, then it also
has orthonormal rows.
q.IfWis a subspace, then kprojWvk2C kv projWvk2D
kvk2.r.A least-squares solution of AxDbis the vector AOxin
ColAclosest to b, so that kb AOxk  k b Axkfor
allx.
s.The normal equations for a least-squares solution of
AxDbare given by OxD.ATA/ 1ATb.
2.Letfv1; : : : ; vpgbe an orthonormal set. Verify the following
equality by induction, beginning with pD2. IfxDc1v1C
   C cpvp, then
kxk2D jc1j2C    C j cpj2
3.Letfv1; : : : ; vpgbe an orthonormal set in Rn. Verify the
following inequality, called Bessel‚Äô s inequality , which is true
for each xinRn:
kxk2 jxv1j2C jxv2j2C    C j xvpj2
4.LetUbe an nnorthogonal matrix. Show that if
fv1; : : : ; vngis an orthonormal basis for Rn, then so is
fUv1; : : : ; U vng.
5.Show that if an nnmatrix UsatisÔ¨Åes .Ux/.Uy/Dxy
for all xandyinRn, then Uis an orthogonal matrix.
6.Show that if Uis an orthogonal matrix, then any real eigen-
value of Umust be 1.
7.AHouseholder matrix , or an elementary reÔ¨Çector , has the
form QDI 2uuTwhere uis a unit vector. (See Exer-
cise 13 in the Supplementary Exercises for Chapter 2.) Show
thatQis an orthogonal matrix. (Elementary reÔ¨Çectors are of-
ten used in computer programs to produce a QR factorization
of a matrix A. IfAhas linearly independent columns, then
left-multiplication by a sequence of elementary reÔ¨Çectors can
produce an upper triangular matrix.)
SECOND REVISED PAGES


--- Page 410 ---
Chapter 6 Supplementary Exercises 393
8.LetTWRn!Rnbe a linear transformation that preserves
lengths; that is, kT .x/k D k xkfor all xinRn.
a.Show that Talso preserves orthogonality; that is,
T .x/T .y/D0whenever xyD0.
b.Show that the standard matrix of Tis an orthogonal
matrix.
9.Letuandvbe linearly independent vectors in Rnthat are
notorthogonal. Describe how to Ô¨Ånd the best approximation
tozinRnby vectors of the form x1uCx2vwithout Ô¨Årst
constructing an orthogonal basis for Span fu;vg.
10. Suppose the columns of Aare linearly independent. Deter-
mine what happens to the least-squares solution OxofAxDb
when bis replaced by cbfor some nonzero scalar c.
11.Ifa,b, and care distinct numbers, then the following
system is inconsistent because the graphs of the equations
are parallel planes. Show that the set of all least-squares
solutions of the system is precisely the plane whose equation
isx 2yC5¬¥D.aCbCc/=3.
x 2yC5¬¥Da
x 2yC5¬¥Db
x 2yC5¬¥Dc
12. Consider the problem of Ô¨Ånding an eigenvalue of an nn
matrix Awhen an approximate eigenvector vis known. Since
vis not exactly correct, the equation
AvDv .1/
will probably not have a solution. However, can be esti-
mated by a least-squares solution when (1) is viewed prop-
erly. Think of vas an n1matrix V, think of as a
vector in R1, and denote the vector Avby the symbol b.
Then (1) becomes bDV, which may also be written as
V Db. Find the least-squares solution of this system of
nequations in the one unknown , and write this solution
using the original symbols. The resulting estimate for 
is called a Rayleigh quotient . See Exercises 11 and 12 in
Section 5.8.
13. Use the steps below to prove the following relations among
the four fundamental subspaces determined by an mn
matrix A.
RowAD.NulA/?;ColAD.NulAT/?
a.Show that Row Ais contained in .NulA/?. (Show that
ifxis in Row A, then xis orthogonal to every uin
NulA.)
b.Suppose rank ADr. Find dim Nul Aand dim .NulA/?,
and then deduce from part (a) that Row AD.NulA/?.
[Hint: Study the exercises for Section 6.3.]
c.Explain why Col AD.NulAT/?.14. Explain why an equation AxDbhas a solution if and only
ifbis orthogonal to all solutions of the equation ATxD0.
Exercises 15 and 16 concern the (real) Schur factorization of an
nnmatrix Ain the form ADURUT, where Uis an orthogonal
matrix and Ris annnupper triangular matrix.1
15. Show that if Aadmits a (real) Schur factorization, AD
URUT, then Ahasnreal eigenvalues, counting multiplic-
ities.
16. LetAbe an nnmatrix with nreal eigenvalues, counting
multiplicities, denoted by 1; : : : ;  n. It can be shown that
Aadmits a (real) Schur factorization. Parts (a) and (b) show
the key ideas in the proof. The rest of the proof amounts to
repeating (a) and (b) for successively smaller matrices, and
then piecing together the results.
a.Letu1be a unit eigenvector corresponding to 1, let
u2; : : : ; unbe any other vectors such that fu1; : : : ; ung
is an orthonormal basis for Rn, and then let UD
¬åu1u2   un¬ç. Show that the Ô¨Årst column of
UTAUis1e1, where e1is the Ô¨Årst column of the nn
identity matrix.
b.Part (a) implies that UTAU has the form shown below.
Explain why the eigenvalues of A1are2; : : : ;  n. [Hint:
See the Supplementary Exercises for Chapter 5.]
UTAUD2
66641   
0
::: A1
03
7775
[M] When the right side of an equation AxDbis changed
slightly‚Äîsay, to AxDbC¬Åbfor some vector ¬Åb‚Äîthe solution
changes from xtoxC¬Åx, where ¬ÅxsatisÔ¨Åes A.¬Å x/D¬Åb.
The quotient k¬Åbk=kbkis called the relative change inb(or
therelative error inbwhen ¬Åbrepresents possible error in the
entries of b/. The relative change in the solution is k¬Åxk=kxk.
When Ais invertible, the condition number ofA, written as
cond.A/, produces a bound on how large the relative change in
xcan be:
k¬Åxk
kxkcond.A/ k¬Åbk
kbk.2/
In Exercises 17‚Äì20, solve AxDbandA.¬Å x/D¬Åb, and show
that the inequality (2) holds in each case. (See the discussion of
ill-conditioned matrices in Exercises 41‚Äì43 in Section 2.3.)
17.AD4:5 3:1
1:6 1:1
,bD19:249
6:843
,¬ÅbD:001
 :003
1If complex numbers are allowed, every nnmatrix Aadmits a
(complex) Schur factorization, ADURU 1, where Ris upper triangular
andU 1is the conjugate transpose of U. This very useful fact is discussed
inMatrix Analysis , by Roger A. Horn and Charles R. Johnson (Cambridge:
Cambridge University Press, 1985), pp. 79‚Äì100.
SECOND REVISED PAGES


--- Page 411 ---
394 CHAPTER 6 Orthogonality and Least Squares
18.AD4:5 3:1
1:6 1:1
,bD:500
 1:407
,¬ÅbD:001
 :003
19.AD2
6647 6 4 1
 5 1 0  2
10 11 7  3
19 9 7 13
775,bD2
664:100
2:888
 1:404
1:4623
775,
¬ÅbD10 42
664:49
 1:28
5:78
8:043
77520.AD2
6647 6 4 1
 5 1 0  2
10 11 7  3
19 9 7 13
775,bD2
6644:230
 11:043
49:991
69:5363
775,
¬ÅbD10 42
664:27
7:76
 3:77
3:933
775
SECOND REVISED PAGES


--- Page 412 ---
7Symmetric Matrices
and Quadratic Forms
INTRODUCTORY EXAMPLE
Multichannel Image Processing
Around the world in little more than 80 minutes , the two
Landsat satellites streak silently across the sky in near
polar orbits, recording images of terrain and coastline, in
swaths 185 kilometers wide. Every 16 days, each satellite
passes over almost every square kilometer of the earth‚Äôs
surface, so any location can be monitored every 8 days.
The Landsat images are useful for many purposes.
Developers and urban planners use them to study the rate
and direction of urban growth, industrial development, and
other changes in land usage. Rural countries can analyze
soil moisture, classify the vegetation in remote regions, and
locate inland lakes and streams. Governments can detect
and assess damage from natural disasters, such as forest
Ô¨Åres, lava Ô¨Çows, Ô¨Çoods, and hurricanes. Environmental
agencies can identify pollution from smokestacks and
measure water temperatures in lakes and rivers near power
plants.
Sensors aboard the satellite acquire seven simul-
taneous images of any region on earth to be studied. The
sensors record energy from separate wavelength bands‚Äî
three in the visible light spectrum and four in infrared and
thermal bands. Each image is digitized and stored as a
rectangular array of numbers, each number indicating the
signal intensity at a corresponding small point (or pixel )on the image. Each of the seven images is one channel of
amultichannel ormultispectral image .
The seven Landsat images of one Ô¨Åxed region typically
contain much redundant information, since some features
will appear in several images. Yet other features, because of
their color or temperature, may reÔ¨Çect light that is recorded
by only one or two sensors. One goal of multichannel
image processing is to view the data in a way that extracts
information better than studying each image separately.
Principal component analysis is an effective way
to suppress redundant information and provide in only
one or two composite images most of the information
from the initial data. Roughly speaking, the goal is to
Ô¨Ånd a special linear combination of the images, that is,
a list of weights that at each pixel combine all seven
corresponding image values into one new value. The
weights are chosen in a way that makes the range of light
intensities‚Äîthe scene variance ‚Äîin the composite image
(called the Ô¨Årst principal component ) greater than that in
any of the original images. Additional component images
can also be constructed, by criteria that will be explained
in Section 7.5.
SECOND REVISED PAGES
395

--- Page 413 ---
396 CHAPTER 7 Symmetric Matrices and Quadratic Forms
Principal component analysis is illustrated in the
photos below, taken over Railroad Valley, Nevada. Images
from three Landsat spectral bands are shown in (a)‚Äì(c).
The total information in the three bands is rearranged
in the three principal component images in (d)‚Äì(f). The
Ô¨Årst component (d) displays (or ‚Äúexplains‚Äù) 93.5% of the
scene variance present in the initial data. In this way, the
three-channel initial data have been reduced to one-channel
data, with a loss in some sense of only 6.5% of the scene
variance.
Earth Satellite Corporation of Rockville, Maryland,
which kindly supplied the photos shown here, is
experimenting with images from 224 separate spectral
bands. Principal component analysis, essential for such
massive data sets, typically reduces the data to about 15
usable principal components.
WEB
Symmetric matrices arise more often in applications, in one way or another, than any
other major class of matrices. The theory is rich and beautiful, depending in an essential
way on both diagonalization from Chapter 5 and orthogonality from Chapter 6. The
diagonalization of a symmetric matrix, described in Section 7.1, is the foundation for
the discussion in Sections 7.2 and 7.3 concerning quadratic forms. Section 7.3, in turn, is
needed for the Ô¨Ånal two sections on the singular value decomposition and on the image
processing described in the introductory example. Throughout the chapter, all vectors
and matrices have real entries.
SECOND REVISED PAGES


--- Page 414 ---
7.1Diagonalization of Symmetric Matrices 397
7.1 DIAGONALIZATION OF SYMMETRIC MATRICES
Asymmetric matrix is a matrix Asuch that AT=A. Such a matrix is necessarily square.
Its main diagonal entries are arbitrary, but its other entries occur in pairs‚Äîon opposite
sides of the main diagonal.
EXAMPLE 1 Of the following matrices, only the Ô¨Årst three are symmetric:
Symmetric:1 0
0 3
;2
40 1 0
 1 5 8
0 8  73
5;2
4a b c
b d e
c e f3
5
Nonsymmetric:1 3
3 0
;2
41 4 0
 6 1  4
0 6 13
5;2
45 4 3 2
4 3 2 1
3 2 1 03
5
To begin the study of symmetric matrices, it is helpful to review the diagonalization
process of Section 5.3.
EXAMPLE 2 If possible, diagonalize the matrix AD2
46 2 1
 2 6  1
 1 1 53
5.
SOLUTION The characteristic equation of Ais
0D  3C172 90C144D  . 8/. 6/. 3/
Standard calculations produce a basis for each eigenspace:
D8Wv1D2
4 1
1
03
5I D6Wv2D2
4 1
 1
23
5I D3Wv3D2
41
1
13
5
These three vectors form a basis for R3. In fact, it is easy to check that fv1;v2;v3gis an
orthogonal basis for R3. Experience from Chapter 6 suggests that an orthonormal basis
might be useful for calculations, so here are the normalized (unit) eigenvectors.
u1D2
4 1=p
2
1=p
2
03
5;u2D2
64 1=p
6
 1=p
6
2=p
63
75;u3D2
641=p
3
1=p
3
1=p
33
75
Let
PD2
64 1=p
2 1=p
6 1=p
3
1=p
2 1=p
6 1=p
3
0 2=p
6 1=p
33
75; D D2
48 0 0
0 6 0
0 0 33
5
Then ADPDP 1, as usual. But this time, since Pis square and has orthonormal
columns, Pis an orthogonal matrix, and P 1is simply PT. (See Section 6.2.)
Theorem 1 explains why the eigenvectors in Example 2 are orthogonal‚Äîthey cor-
respond to distinct eigenvalues.
T H E O R E M 1 IfAis symmetric, then any two eigenvectors from different eigenspaces are
orthogonal.
SECOND REVISED PAGES


--- Page 415 ---
398 CHAPTER 7 Symmetric Matrices and Quadratic Forms
PROOF Letv1andv2be eigenvectors that correspond to distinct eigenvalues, say, 1
and2. To show that v1v2D0, compute
1v1v2D.1v1/Tv2D.Av1/Tv2Since v1is an eigenvector
D.vT
1AT/v2DvT
1.Av2/ Since ATDA
DvT
1.2v2/ Since v2is an eigenvector
D2vT
1v2D2v1v2
Hence .1 2/v1v2D0. But 1 2¬§0, sov1v2D0.
The special type of diagonalization in Example 2 is crucial for the theory of sym-
metric matrices. An nnmatrix Ais said to be orthogonally diagonalizable if there
are an orthogonal matrix P(with P 1DPT) and a diagonal matrix Dsuch that
ADPDPTDPDP 1(1)
Such a diagonalization requires nlinearly independent and orthonormal eigenvec-
tors. When is this possible? If Ais orthogonally diagonalizable as in (1), then
ATD.PDPT/TDPT TDTPTDPDPTDA
Thus Ais symmetric! Theorem 2 below shows that, conversely, every symmetric matrix
is orthogonally diagonalizable. The proof is much harder and is omitted; the main idea
for a proof will be given after Theorem 3.
T H E O R E M 2 Annnmatrix Ais orthogonally diagonalizable if and only if Ais a symmetric
matrix.
This theorem is rather amazing, because the work in Chapter 5 would suggest that
it is usually impossible to tell when a matrix is diagonalizable. But this is not the case
for symmetric matrices.
The next example treats a matrix whose eigenvalues are not all distinct.
EXAMPLE 3 Orthogonally diagonalize the matrix AD2
43 2 4
 2 6 2
4 2 33
5, whose
characteristic equation is
0D  3C122 21 98D  . 7/2.C2/
SOLUTION The usual calculations produce bases for the eigenspaces:
D7Wv1D2
41
0
13
5;v2D2
4 1=2
1
03
5I D  2Wv3D2
4 1
 1=2
13
5
Although v1andv2are linearly independent, they are not orthogonal. Recall from
Section 6.2 that the projection of v2onto v1isv2v1
v1v1v1, and the component of v2
orthogonal to v1is
z2Dv2 v2v1
v1v1v1D2
4 1=2
1
03
5  1=2
22
41
0
13
5D2
4 1=4
1
1=43
5
SECOND REVISED PAGES


--- Page 416 ---
7.1Diagonalization of Symmetric Matrices 399
Then fv1;z2gis an orthogonal set in the eigenspace for D7. (Note that z2is a linear
combination of the eigenvectors v1andv2, soz2is in the eigenspace. This construction
ofz2is just the Gram‚ÄìSchmidt process of Section 6.4.) Since the eigenspace is two-
dimensional (with basis v1;v2/, the orthogonal set fv1;z2gis an orthogonal basis for
the eigenspace, by the Basis Theorem. (See Section 2.9 or 4.5.)
Normalize v1andz2to obtain the following orthonormal basis for the eigenspace
forD7:
u1D2
41=p
2
0
1=p
23
5;u2D2
64 1=p
18
4=p
18
1=p
183
75
An orthonormal basis for the eigenspace for D  2is
u3D1
k2v3k2v3D1
32
4 2
 1
23
5D2
4 2=3
 1=3
2=33
5
By Theorem 1, u3is orthogonal to the other eigenvectors u1andu2. Hence fu1;u2;u3g
is an orthonormal set. Let
PD¬åu1u2u3¬çD2
641=p
2 1=p
18 2=3
0 4=p
18 1=3
1=p
2 1=p
18 2=33
75; D D2
47 0 0
0 7 0
0 0  23
5
Then Porthogonally diagonalizes A, and ADPDP 1.
In Example 3, the eigenvalue 7 has multiplicity two and the eigenspace is two-
dimensional. This fact is not accidental, as the next theorem shows.
The Spectral Theorem
The set of eigenvalues of a matrix Ais sometimes called the spectrum ofA, and the
following description of the eigenvalues is called a spectral theorem.
T H E O R E M 3 The Spectral Theorem for Symmetric Matrices
Annnsymmetric matrix Ahas the following properties:
a.Ahasnreal eigenvalues, counting multiplicities.
b.The dimension of the eigenspace for each eigenvalue equals the multiplicity
ofas a root of the characteristic equation.
c.The eigenspaces are mutually orthogonal, in the sense that eigenvectors cor-
responding to different eigenvalues are orthogonal.
d.Ais orthogonally diagonalizable.
Part (a) follows from Exercise 24 in Section 5.5. Part (b) follows easily from part
(d). (See Exercise 31.) Part (c) is Theorem 1. Because of (a), a proof of (d) can be given
using Exercise 32 and the Schur factorization discussed in Supplementary Exercise 16
in Chapter 6. The details are omitted.
SECOND REVISED PAGES


--- Page 417 ---
400 CHAPTER 7 Symmetric Matrices and Quadratic Forms
Spectral Decomposition
Suppose ADPDP 1, where the columns of Pare orthonormal eigenvectors u1; : : : ; un
ofAand the corresponding eigenvalues 1; : : : ;  nare in the diagonal matrix D. Then,
since P 1DPT,
ADPDPTDu1   un2
641 0
:::
0 n3
752
64uT
1:::
uT
n3
75
D1u1   nun2
64uT
1:::
uT
n3
75
Using the column‚Äìrow expansion of a product (Theorem 10 in Section 2.4), we can
write
AD1u1uT
1C2u2uT
2C    C nunuT
n (2)
This representation of Ais called a spectral decomposition ofAbecause it breaks
upAinto pieces determined by the spectrum (eigenvalues) of A. Each term in (2) is
annnmatrix of rank 1. For example, every column of 1u1uT
1is a multiple of u1.
Furthermore, each matrix ujuT
jis aprojection matrix in the sense that for each xin
Rn, the vector .ujuT
j/xis the orthogonal projection of xonto the subspace spanned by
uj. (See Exercise 35.)
EXAMPLE 4 Construct a spectral decomposition of the matrix Athat has the or-
thogonal diagonalization
AD7 2
2 4
D2=p
5 1=p
5
1=p
5 2=p
58 0
0 32=p
5 1=p
5
 1=p
5 2=p
5
SOLUTION Denote the columns of Pbyu1andu2. Then
AD8u1uT
1C3u2uT
2
To verify this decomposition of A, compute
u1uT
1D2=p
5
1=p
5
2=p
5 1=p
5
D4=5 2=5
2=5 1=5
u2uT
2D 1=p
5
2=p
5
 1=p
5 2=p
5
D1=5 2=5
 2=5 4=5
and
8u1uT
1C3u2uT
2D32=5 16=5
16=5 8=5
C3=5  6=5
 6=5 12=5
D7 2
2 4
DA
SECOND REVISED PAGES


--- Page 418 ---
7.1Diagonalization of Symmetric Matrices 401
N U M E R I C A L N O T E
When Ais symmetric and not too large, modern high-performance computer al-
gorithms calculate eigenvalues and eigenvectors with great precision. They apply
a sequence of similarity transformations to Ainvolving orthogonal matrices. The
diagonal entries of the transformed matrices converge rapidly to the eigenvalues
ofA. (See the Numerical Notes in Section 5.2.) Using orthogonal matrices gener-
ally prevents numerical errors from accumulating during the process. When Ais
symmetric, the sequence of orthogonal matrices combines to form an orthogonal
matrix whose columns are eigenvectors of A.
A nonsymmetric matrix cannot have a full set of orthogonal eigenvectors, but
the algorithm still produces fairly accurate eigenvalues. After that, nonorthogonal
techniques are needed to calculate eigenvectors.
PRACTICE PROBLEMS
1.Show that if Ais a symmetric matrix, then A2is symmetric.
2.Show that if Ais orthogonally diagonalizable, then so is A2.
7.1 EXERCISES
Determine which of the matrices in Exercises 1‚Äì6 are symmetric.
1.3 5
5 7
2.3 5
 5 3
3.2 3
2 4
4.2
40 8 3
8 0  4
3 2 03
5
5.2
4 6 2 0
2 6 2
0 2  63
5 6.2
41 2 2 1
2 2 2 1
2 2 1 23
5
Determine which of the matrices in Exercises 7‚Äì12 are orthogo-
nal. If orthogonal, Ô¨Ånd the inverse.
7.:6 :8
:8 :6
8.
1 1
1 1
9. 4=5 3=5
3=5 4=5
10.2
41=3 2=3 2=3
2=3 1=3  2=3
2=3  2=3 1=33
5
11.2
42=3 2=3 1=3
0 1=3  2=3
5=3  4=3  2=33
5
12.2
664:5 :5  :5 :5
:5 :5 :5 :5
:5 :5 :5 :5
:5 :5 :5  :53
775
Orthogonally diagonalize the matrices in Exercises 13‚Äì22, giving
an orthogonal matrix Pand a diagonal matrix D. To save youtime, the eigenvalues in Exercises 17‚Äì22 are: (17)  4, 4,7; (18)
 3, 6,9; (19)  2, 7; (20)  3, 15; (21) 1, 5, 9; (22) 3, 5.
13.3 1
1 3
14.1 5
 5 1
15.3 4
4 9
16.6 2
 2 9
17.2
41 1 5
1 5 1
5 1 13
5 18.2
41 6 4
 6 2  2
4 2 33
5
19.2
43 2 4
 2 6 2
4 2 33
5 20.2
45 8  4
8 5  4
 4 4 13
5
21.2
6644 3 1 1
3 4 1 1
1 1 4 3
1 1 3 43
77522.2
6644 0 1 0
0 4 0 1
1 0 4 0
0 1 0 43
775
23. LetAD2
44 1 1
 1 4  1
 1 1 43
5andvD2
41
1
13
5. Verify that 5 is
an eigenvalue of Aandvis an eigenvector. Then orthogo-
nally diagonalize A.
24. Let AD2
42 1 1
 1 2  1
1 1 23
5,v1D2
4 1
0
13
5, and v2D
2
41
 1
13
5. Verify that v1andv2are eigenvectors of A. Then
orthogonally diagonalize A.
SECOND REVISED PAGES


--- Page 419 ---
402 CHAPTER 7 Symmetric Matrices and Quadratic Forms
In Exercises 25 and 26, mark each statement True or False. Justify
each answer.
25. a.Annnmatrix that is orthogonally diagonalizable must
be symmetric.
b.IfATDAand if vectors uandvsatisfy AuD3uand
AvD4v, then uvD0.
c.Annnsymmetric matrix has ndistinct real eigenval-
ues.
d.For a nonzero vinRn, the matrix vvTis called a projec-
tion matrix.
26. a.There are symmetric matrices that are not orthogonally
diagonalizable.
b.IfBDPDPT, where PTDP 1andDis a diagonal
matrix, then Bis a symmetric matrix.
c.An orthogonal matrix is orthogonally diagonalizable.
d.The dimension of an eigenspace of a symmetric matrix is
sometimes less than the multiplicity of the corresponding
eigenvalue.
27. Show that if Ais annnsymmetric matrix, then ( Ax/yD
x.Ay/for all x;yinRn.
28. Suppose Ais a symmetric nnmatrix and Bis any nm
matrix. Show that BTAB,BTB, and BBTare symmetric
matrices.
29. Suppose Ais invertible and orthogonally diagonalizable.
Explain why A 1is also orthogonally diagonalizable.
30. Suppose AandBare both orthogonally diagonalizable and
ABDBA. Explain why ABis also orthogonally diagonaliz-
able.
31. LetADPDP 1, where Pis orthogonal and Dis diagonal,
and let be an eigenvalue of Aof multiplicity k. Then
appears ktimes on the diagonal of D. Explain why the
dimension of the eigenspace for isk.
32. Suppose ADPRP 1, where Pis orthogonal and Ris upper
triangular. Show that if Ais symmetric, then Ris symmetric
and hence is actually a diagonal matrix.
33. Construct a spectral decomposition of Afrom Example 2.
34. Construct a spectral decomposition of Afrom Example 3.35. Letube a unit vector in Rn, and let BDuuT.
a.Given any xinRn, compute Bxand show that Bxis
the orthogonal projection of xonto u, as described in
Section 6.2.
b.Show that Bis a symmetric matrix and B2DB.
c.Show that uis an eigenvector of B. What is the corre-
sponding eigenvalue?
36. LetBbe an nnsymmetric matrix such that B2=B. Any
such matrix is called a projection matrix (or an orthogonal
projection matrix ). Given any yinRn, letOyDByand
zDy Oy.
a.Show that zis orthogonal to Oy.
b.LetWbe the column space of B. Show that yis the sum
of a vector in Wand a vector in W?. Why does this prove
thatByis the orthogonal projection of yonto the column
space of B?
[M] Orthogonally diagonalize the matrices in Exercises 37‚Äì40.
To practice the methods of this section, do not use an eigenvector
routine from your matrix program. Instead, use the program to Ô¨Ånd
the eigenvalues, and, for each eigenvalue , Ô¨Ånd an orthonormal
basis for Nul .A I/, as in Examples 2 and 3.
37.2
6646 2 9  6
2 6  6 9
9 6 6 2
 6 9 2 63
775
38.2
664:63 :18 :06 :04
 :18 :84  :04 :12
 :06 :04 :72  :12
 :04 :12  :12 :663
775
39.2
664:31 :58 :08 :44
:58 :56 :44  :58
:08 :44 :19  :08
:44 :58 :08 :313
775
40.2
666648 2 2  6 9
2 8 2  6 9
2 2 8  6 9
 6 6 6 24 9
9 9 9 9  213
77775
SOLUTIONS TO PRACTICE PROBLEMS
1..A2/TD.AA/TDATAT, by a property of transposes. By hypothesis, ATDA. So
.A2/TDAADA2, which shows that A2is symmetric.
2.IfAis orthogonally diagonalizable, then Ais symmetric, by Theorem 2. By Practice
Problem 1, A2is symmetric and hence is orthogonally diagonalizable (Theorem 2).
SECOND REVISED PAGES


--- Page 420 ---
7.2Quadratic Forms 403
7.2 QUADRATIC FORMS
Until now, our attention in this text has focused on linear equations, except for the sums
of squares encountered in Chapter 6 when computing xTx. Such sums and more general
expressions, called quadratic forms , occur frequently in applications of linear algebra
to engineering (in design criteria and optimization) and signal processing (as output
noise power). They also arise, for example, in physics (as potential and kinetic energy),
differential geometry (as normal curvature of surfaces), economics (as utility functions),
and statistics (in conÔ¨Ådence ellipsoids). Some of the mathematical background for such
applications Ô¨Çows easily from our work on symmetric matrices.
Aquadratic form onRnis a function QdeÔ¨Åned on Rnwhose value at a vector x
inRncan be computed by an expression of the form Q.x/DxTAx, where Ais annn
symmetric matrix. The matrix Ais called the matrix of the quadratic form .
The simplest example of a nonzero quadratic form is Q.x/DxTIxD kxk2. Exam-
ples 1 and 2 show the connection between any symmetric matrix Aand the quadratic
form xTAx.
EXAMPLE 1 LetxDx1
x2
. Compute xTAxfor the following matrices:
a.AD4 0
0 3
b.AD3 2
 2 7
SOLUTION
a.xTAxD¬åx1x2¬ç4 0
0 3x1
x2
D¬åx1x2¬ç4x1
3x2
D4x2
1C3x2
2.
b.There are two  2entries in A. Watch how they enter the calculations. The .1; 2/ -
entry in Ais in boldface type.
xTAxD¬åx1x2¬ç3 2
 2 7x1
x2
D¬åx1x2¬ç3x1 2x2
 2x1C7x2
Dx1.3x1 2x2/Cx2. 2x1C7x2/
D3x2
1 2x1x2 2x2x1C7x2
2
D3x2
1 4x1x2C7x2
2
The presence of  4x1x2in the quadratic form in Example 1(b) is due to the  2
entries off the diagonal in the matrix A. In contrast, the quadratic form associated with
the diagonal matrix Ain Example 1(a) has no x1x2cross-product term.
EXAMPLE 2 ForxinR3, letQ.x/D5x2
1C3x2
2C2x2
3 x1x2C8x2x3. Write this
quadratic form as xTAx.
SOLUTION The coefÔ¨Åcients of x2
1,x2
2,x2
3go on the diagonal of A. To make Asym-
metric, the coefÔ¨Åcient of xixjfori¬§jmust be split evenly between the .i; j / - and
.j; i/ -entries in A. The coefÔ¨Åcient of x1x3is 0. It is readily checked that
Q.x/DxTAxD¬åx1x2x3¬ç2
45 1=2 0
 1=2 3 4
0 4 23
52
4x1
x2
x33
5
SECOND REVISED PAGES


--- Page 421 ---
404 CHAPTER 7 Symmetric Matrices and Quadratic Forms
EXAMPLE 3 LetQ.x/Dx2
1 8x1x2 5x2
2. Compute the value of Q.x/forxD 3
1
,2
 2
, and1
 3
.
SOLUTION
Q. 3; 1/D. 3/2 8. 3/.1/  5.1/2D28
Q.2; 2/D.2/2 8.2/. 2/ 5. 2/2D16
Q.1; 3/D.1/2 8.1/. 3/ 5. 3/2D  20
In some cases, quadratic forms are easier to use when they have no cross-product
terms‚Äîthat is, when the matrix of the quadratic form is a diagonal matrix. Fortunately,
the cross-product term can be eliminated by making a suitable change of variable.
Change of Variable in a Quadratic Form
Ifxrepresents a variable vector in Rn, then a change of variable is an equation of the
form
xDPy; or equivalently ;yDP 1x (1)
where Pis an invertible matrix and yis a new variable vector in Rn. Here yis the
coordinate vector of xrelative to the basis of Rndetermined by the columns of P. (See
Section 4.4.)
If the change of variable (1) is made in a quadratic form xTAx, then
xTAxD.Py/TA.P y/DyTPTAPyDyT.PTAP / y (2)
and the new matrix of the quadratic form is PTAP. Since Ais symmetric, Theorem 2
guarantees that there is an orthogonal matrix Psuch that PTAPis a diagonal matrix D,
and the quadratic form in (2) becomes yTDy. This is the strategy of the next example.
EXAMPLE 4 Make a change of variable that transforms the quadratic form in Ex-
ample 3 into a quadratic form with no cross-product term.
SOLUTION The matrix of the quadratic form in Example 3 is
AD1 4
 4 5
The Ô¨Årst step is to orthogonally diagonalize A. Its eigenvalues turn out to be D3and
D  7. Associated unit eigenvectors are
D3W"
2=p
5
 1=p
5#
I D  7W"
1=p
5
2=p
5#
These vectors are automatically orthogonal (because they correspond to distinct eigen-
values) and so provide an orthonormal basis for R2. Let
PD"
2=p
5 1=p
5
 1=p
5 2=p
5#
; D D3 0
0 7
Then ADPDP 1andDDP 1APDPTAP, as pointed out earlier. A suitable change
of variable is
xDPy; where xDx1
x2
and yDy1
y2
SECOND REVISED PAGES


--- Page 422 ---
7.2Quadratic Forms 405
Then
x2
1 8x1x2 5x2
2DxTAxD.Py/TA.P y/
DyTPTAPyDyTDy
D3y2
1 7y2
2
To illustrate the meaning of the equality of quadratic forms in Example 4, we can
compute Q.x/forxD.2; 2/using the new quadratic form. First, since xDPy,
yDP 1xDPTx
so
yD"
2=p
5 1=p
5
1=p
5 2=p
5#2
 2
D"
6=p
5
 2=p
5#
Hence
3y2
1 7y2
2D3.6=p
5/2 7. 2=p
5/2D3.36=5/  7.4=5/
D80=5D16
This is the value of Q.x/in Example 3 when xD.2; 2/. See Figure 1.
/H11938/H119382
/H11938216 0Multiplication
by Px
yyTDyxTAx
FIGURE 1 Change of variable in xTAx.
Example 4 illustrates the following theorem. The proof of the theorem was essen-
tially given before Example 4.
T H E O R E M 4 The Principal Axes Theorem
LetAbe an nnsymmetric matrix. Then there is an orthogonal change of
variable, xDPy, that transforms the quadratic form xTAxinto a quadratic form
yTDywith no cross-product term.
The columns of Pin the theorem are called the principal axes of the quadratic
form xTAx. The vector yis the coordinate vector of xrelative to the orthonormal basis
ofRngiven by these principal axes.
A Geometric View of Principal Axes
Suppose Q.x/DxTAx, where Ais an invertible 22symmetric matrix, and let cbe a
constant. It can be shown that the set of all xinR2that satisfy
xTAxDc (3)
SECOND REVISED PAGES


--- Page 423 ---
406 CHAPTER 7 Symmetric Matrices and Quadratic Forms
either corresponds to an ellipse (or circle), a hyperbola, two intersecting lines, or a single
point, or contains no points at all. If Ais a diagonal matrix, the graph is in standard
position , such as in Figure 2. If Ais not a diagonal matrix, the graph of equation (3) is
x1x2
a
= 1,  a > b > 0
a2b2x2x2
2 1b
x1x2
ab
‚Äî‚Äî+
ellipse= 1,  a > b > 0
a2b2x2x2
2 1‚Äî‚Äî‚Äì
hyperbola
FIGURE 2 An ellipse and a hyperbola in standard position.
rotated out of standard position, as in Figure 3. Finding the principal axes (determined
by the eigenvectors of A) amounts to Ô¨Ånding a new coordinate system with respect to
which the graph is in standard position.
(a) 5 x2 ‚Äì 4x1x2 + 5x2 = 48x1x2 y1y2
11
12
x2
(b) x2 ‚Äì 8x1x2 ‚Äì 5x2 = 16x1
y1y2
11
12
FIGURE 3 An ellipse and a hyperbola notin standard position.
The hyperbola in Figure 3(b) is the graph of the equation xTAxD16, where Ais
the matrix in Example 4. The positive y1-axis in Figure 3(b) is in the direction of the
Ô¨Årst column of the matrix Pin Example 4, and the positive y2-axis is in the direction
of the second column of P.
EXAMPLE 5 The ellipse in Figure 3(a) is the graph of the equation 5x2
1 4x1x2C
5x2
2D48. Find a change of variable that removes the cross-product term from the
equation.
SOLUTION The matrix of the quadratic form is AD5 2
 2 5
. The eigenvalues of
Aturn out to be 3 and 7, with corresponding unit eigenvectors
u1D"
1=p
2
1=p
2#
;u2D"
 1=p
2
1=p
2#
SECOND REVISED PAGES


--- Page 424 ---
7.2Quadratic Forms 407
LetPD¬åu1u2¬çD"
1=p
2 1=p
2
1=p
2 1=p
2#
. Then Porthogonally diagonalizes A, so the
change of variable xDPyproduces the quadratic form yTDyD3y2
1C7y2
2. The new
axes for this change of variable are shown in Figure 3(a).
Classifying Quadratic Forms
When Ais an nnmatrix, the quadratic form Q.x/DxTAxis a real-valued function
with domain Rn. Figure 4 displays the graphs of four quadratic forms with domain R2.
For each point xD.x1; x2/in the domain of a quadratic form Q, the graph displays the
point .x1; x2; ¬¥/where ¬¥DQ.x/. Notice that except at xD0, the values of Q.x/are
all positive in Figure 4(a) and all negative in Figure 4(d). The horizontal cross-sections
of the graphs are ellipses in Figures 4(a) and 4(d) and hyperbolas in Figure 4(c).
(a)  z = 3 x2 + 7 x2
12x2
x1x3
(b)  z = 3 x2
1
x2
x1x3
(c)  z = 3 x2 ‚Äì 7x2
12
x2 x1x3
(d)  z = ‚Äì3x2 ‚Äì 7x2
12x3
x2x1
FIGURE 4 Graphs of quadratic forms.
The simple 22examples in Figure 4 illustrate the following deÔ¨Ånitions.
D E F I N I T I O N A quadratic form Qis:
a.positive deÔ¨Ånite ifQ.x/ > 0 for all x¬§0,
b.negative deÔ¨Ånite ifQ.x/ < 0 for all x¬§0,
c.indeÔ¨Ånite ifQ.x/assumes both positive and negative values.
Also, Qis said to be positive semideÔ¨Ånite ifQ.x/0for all x, and to be negative
semideÔ¨Ånite ifQ.x/0for all x. The quadratic forms in parts (a) and (b) of Figure 4
are both positive semideÔ¨Ånite, but the form in (a) is better described as positive deÔ¨Ånite.
Theorem 5 characterizes some quadratic forms in terms of eigenvalues.
T H E O R E M 5 Quadratic Forms and Eigenvalues
LetAbe an nnsymmetric matrix. Then a quadratic form xTAxis:
a.positive deÔ¨Ånite if and only if the eigenvalues of Aare all positive,
b.negative deÔ¨Ånite if and only if the eigenvalues of Aare all negative, or
c.indeÔ¨Ånite if and only if Ahas both positive and negative eigenvalues.
SECOND REVISED PAGES


--- Page 425 ---
408 CHAPTER 7 Symmetric Matrices and Quadratic Forms
PROOF By the Principal Axes Theorem, there exists an orthogonal change of variable
xDPysuch that
Q.x/DxTAxDyTDyD1y2
1C2y2
2C    C ny2
n (4)
where 1; : : : ;  nare the eigenvalues of A. Since Pis invertible, there is a one-to-
one correspondence between all nonzero xand all nonzero y. Thus the values of Q.x/
forx¬§0coincide with the values of the expression on the right side of (4), which
is obviously controlled by the signs of the eigenvalues 1; : : : ;  n, in the three ways
described in the theorem.
EXAMPLE 6 IsQ.x/D3x2
1C2x2
2Cx2
3C4x1x2C4x2x3positive deÔ¨Ånite?
SOLUTION Because of all the plus signs, this form ‚Äúlooks‚Äù positive deÔ¨Ånite. But the
matrix of the form is
x2x1x3
Positive definite
x2x1x3
Negative definite
x2x1x3
Indefinite
AD2
43 2 0
2 2 2
0 2 13
5
and the eigenvalues of Aturn out to be 5, 2, and  1. SoQis an indeÔ¨Ånite quadratic
form, not positive deÔ¨Ånite.
The classiÔ¨Åcation of a quadratic form is often carried over to the matrix of the form.
Thus a positive deÔ¨Ånite matrix Ais asymmetric matrix for which the quadratic form
xTAxis positive deÔ¨Ånite. Other terms, such as positive semideÔ¨Ånite matrix , are deÔ¨Åned
analogously.
WEB
N U M E R I C A L N O T E
A fast way to determine whether a symmetric matrix Ais positive deÔ¨Ånite is
to attempt to factor Ain the form ADRTR, where Ris upper triangular with
positive diagonal entries. (A slightly modiÔ¨Åed algorithm for an LU factorization
is one approach.) Such a Cholesky factorization is possible if and only if Ais
positive deÔ¨Ånite. See Supplementary Exercise 7 at the end of Chapter 7.
PRACTICE PROBLEM
Describe a positive semideÔ¨Ånite matrix Ain terms of its eigenvalues.
WEB
7.2 EXERCISES
1.Compute the quadratic form xTAx, when AD5 1=3
1=3 1
and
a.xDx1
x2
b.xD6
1
c.xD1
3
2.Compute the quadratic form xTAx, forAD2
43 2 0
2 2 1
0 1 03
5
anda.xD2
4x1
x2
x33
5 b.xD2
4 2
 1
53
5 c.xD2
641=p
2
1=p
2
1=p
23
75
3.Find the matrix of the quadratic form. Assume xis inR2.
a.3x2
1 4x1x2C5x2
2 b.3x2
1C2x1x2
4.Find the matrix of the quadratic form. Assume xis inR2.
a.5x2
1C16x 1x2 5x2
2 b.2x1x2
SECOND REVISED PAGES


--- Page 426 ---
7.2Quadratic Forms 409
5.Find the matrix of the quadratic form. Assume xis inR3.
a.3x2
1C2x2
2 5x2
3 6x1x2C8x1x3 4x2x3
b.6x1x2C4x1x3 10x 2x3
6.Find the matrix of the quadratic form. Assume xis inR3.
a.3x2
1 2x2
2C5x2
3C4x1x2 6x1x3
b.4x2
3 2x1x2C4x2x3
7.Make a change of variable, xDPy, that transforms the
quadratic form x2
1C10x 1x2Cx2
2into a quadratic form with
no cross-product term. Give Pand the new quadratic form.
8.LetAbe the matrix of the quadratic form
9x2
1C7x2
2C11x2
3 8x1x2C8x1x3
It can be shown that the eigenvalues of Aare 3, 9, and 15.
Find an orthogonal matrix Psuch that the change of variable
xDPytransforms xTAxinto a quadratic form with no cross-
product term. Give Pand the new quadratic form.
Classify the quadratic forms in Exercises 9‚Äì18. Then make a
change of variable, xDPy, that transforms the quadratic form
into one with no cross-product term. Write the new quadratic form.
Construct Pusing the methods of Section 7.1.
9.4x2
1 4x1x2C4x2
2 10.2x2
1C6x1x2 6x2
2
11.2x2
1 4x1x2 x2
2 12. x2
1 2x1x2 x2
2
13.x2
1 6x1x2C9x2
2 14.3x2
1C4x1x2
15. [M] 3x2
1 7x2
2 10x2
3 10x2
4C4x1x2C4x1x3C
4x1x4C6x3x4
16. [M]4x2
1C4x2
2C4x2
3C4x2
4C8x1x2C8x3x4 6x1x4C
6x2x3
17. [M]11x2
1C11x2
2C11x2
3C11x2
4C16x 1x2 12x 1x4C
12x 2x3C16x 3x4
18. [M]2x2
1C2x2
2 6x1x2 6x1x3 6x1x4 6x2x3 
6x2x4 2x3x4
19. What is the largest possible value of the quadratic
form 5x2
1C8x2
2ifxD.x1; x2/and xTxD1, that is, if
x2
1Cx2
2D1? (Try some examples of x.)
20. What is the largest value of the quadratic form 5x2
1 3x2
2if
xTxD1?
In Exercises 21 and 22, matrices are nnand vectors are in Rn.
Mark each statement True or False. Justify each answer.
21. a.The matrix of a quadratic form is a symmetric matrix.
b.A quadratic form has no cross-product terms if and only
if the matrix of the quadratic form is a diagonal matrix.
c.The principal axes of a quadratic form xTAxare eigenvec-
tors of A.d.A positive deÔ¨Ånite quadratic form QsatisÔ¨Åes Q.x/ > 0
for all xinRn.
e.If the eigenvalues of a symmetric matrix Aare all posi-
tive, then the quadratic form xTAxis positive deÔ¨Ånite.
f.A Cholesky factorization of a symmetric matrix Ahas
the form ADRTR, for an upper triangular matrix Rwith
positive diagonal entries.
22. a.The expression kxk2is not a quadratic form.
b.IfAis symmetric and Pis an orthogonal matrix, then
the change of variable xDPytransforms xTAxinto a
quadratic form with no cross-product term.
c.IfAis a22symmetric matrix, then the set of xsuch
thatxTAxDc(for a constant c) corresponds to either a
circle, an ellipse, or a hyperbola.
d.An indeÔ¨Ånite quadratic form is neither positive semidef-
inite nor negative semideÔ¨Ånite.
e.IfAis symmetric and the quadratic form xTAxhas only
negative values for x¬§0, then the eigenvalues of Aare
all positive.
Exercises 23 and 24 show how to classify a quadratic form
Q.x/DxTAx, when ADa b
b d
and det A¬§0, without Ô¨Ånd-
ing the eigenvalues of A.
23. If1and2are the eigenvalues of A, then the characteristic
polynomial of Acan be written in two ways: det .A I/
and. 1/. 2/. Use this fact to show that 1C2D
aCd(the diagonal entries of A) and 12DdetA.
24. Verify the following statements.
a.Qis positive deÔ¨Ånite if det A > 0 anda > 0 .
b.Qis negative deÔ¨Ånite if det A > 0 anda < 0 .
c.Qis indeÔ¨Ånite if det A < 0 .
25. Show that if Bismn, then BTBis positive semideÔ¨Ånite;
and if Bisnnand invertible, then BTBis positive deÔ¨Ånite.
26. Show that if an nnmatrix Ais positive deÔ¨Ånite, then there
exists a positive deÔ¨Ånite matrix Bsuch that ADBTB. [Hint:
Write ADPDPT, with PTDP 1. Produce a diagonal ma-
trixCsuch that DDCTC, and let BDPCPT. Show that
Bworks.]
27. LetAandBbe symmetric nnmatrices whose eigenvalues
are all positive. Show that the eigenvalues of ACBare all
positive. [ Hint: Consider quadratic forms.]
28. LetAbe an nninvertible symmetric matrix. Show that
if the quadratic form xTAxis positive deÔ¨Ånite, then so is the
quadratic form xTA 1x. [Hint: Consider eigenvalues.]
SG
Mastering: Diagonalization and Quadratic Forms 7‚Äì7
SECOND REVISED PAGES


--- Page 427 ---
410 CHAPTER 7 Symmetric Matrices and Quadratic Forms
SOLUTION TO PRACTICE PROBLEM
Make an orthogonal change of variable xDPy, and write
xTAxDyTDyD1y2
1C2y2
2C    C ny2
n
as in equation (4). If an eigenvalue‚Äîsay, i‚Äîwere negative, then xTAxwould be neg-
ative for the xcorresponding to yDei(theith column of In). So the eigenvalues
of a positive semideÔ¨Ånite quadratic form must all be nonnegative. Conversely, if the
eigenvalues are nonnegative, the expansion above shows that xTAxmust be positive
semideÔ¨Ånite.
x2x1x3
Positive semidefinite
7.3 CONSTRAINED OPTIMIZATION
Engineers, economists, scientists, and mathematicians often need to Ô¨Ånd the maximum
or minimum value of a quadratic form Q.x/forxin some speciÔ¨Åed set. Typically, the
problem can be arranged so that xvaries over the set of unit vectors. This constrained
optimization problem has an interesting and elegant solution. Example 6 below and the
discussion in Section 7.5 will illustrate how such problems arise in practice.
The requirement that a vector xinRnbe a unit vector can be stated in several
equivalent ways:
kxk D1; kxk2D1; xTxD1
and
x2
1Cx2
2C    C x2
nD1 (1)
The expanded version (1) of xTxD1is commonly used in applications.
When a quadratic form Qhas no cross-product terms, it is easy to Ô¨Ånd the maximum
and minimum of Q.x/forxTxD1.
EXAMPLE 1 Find the maximum and minimum values of Q.x/D9x2
1C4x2
2C3x2
3
subject to the constraint xTxD1.
SOLUTION Since x2
2andx2
3are nonnegative, note that
4x2
29x2
2 and 3x2
39x2
3
and hence
Q.x/D9x2
1C4x2
2C3x2
3
9x2
1C9x2
2C9x2
3
D9.x2
1Cx2
2Cx2
3/
D9
whenever x2
1Cx2
2Cx2
3D1. So the maximum value of Q.x/cannot exceed 9 when
xis a unit vector. Furthermore, Q.x/D9when xD.1; 0; 0/ . Thus 9 is the maximum
value of Q.x/forxTxD1.
To Ô¨Ånd the minimum value of Q.x/, observe that
9x2
13x2
1; 4x2
23x2
2
and hence
Q.x/3x2
1C3x2
2C3x2
3D3.x2
1Cx2
2Cx2
3/D3
whenever x2
1Cx2
2Cx2
3D1. Also, Q.x/D3when x1D0,x2D0, and x3D1. So 3
is the minimum value of Q.x/when xTxD1.
SECOND REVISED PAGES


--- Page 428 ---
7.3Constrained Optimization 411
It is easy to see in Example 1 that the matrix of the quadratic form Qhas eigen-
values 9, 4, and 3 and that the greatest and least eigenvalues equal, respectively, the
(constrained) maximum and minimum of Q.x/. The same holds true for any quadratic
form, as we shall see.
EXAMPLE 2 LetAD3 0
0 7
, and let Q.x/DxTAxforxinR2. Figure 1 dis-
plays the graph of Q. Figure 2 shows only the portion of the graph inside a cylinder;
the intersection of the cylinder with the surface is the set of points .x1; x2; ¬¥/such
that¬¥DQ.x 1; x2/andx2
1Cx2
2D1. The ‚Äúheights‚Äù of these points are the constrained
values of Q.x/. Geometrically, the constrained optimization problem is to locate the
highest and lowest points on the intersection curve.
The two highest points on the curve are 7 units above the x1x2-plane, occurring
where x1D0andx2D 1. These points correspond to the eigenvalue 7 of Aand
the eigenvectors xD.0; 1/ and xD.0; 1/. Similarly, the two lowest points on the
curve are 3 units above the x1x2-plane. They correspond to the eigenvalue 3 and the
eigenvectors .1; 0/ and. 1; 0/.
8
11(0, 1, 7)
(1, 0, 3)
11z
8z
x2x1
11 x2x1
FIGURE 1 ¬¥D3x2
1C7x2
2. FIGURE 2 The intersection of
¬¥D3x2
1C7x2
2and the cylinder
x2
1Cx2
2D1.
Every point on the intersection curve in Figure 2 has a ¬¥-coordinate between 3 and
7, and for any number tbetween 3 and 7, there is a unit vector xsuch that Q.x/Dt.
In other words, the set of all possible values of xTAx, forkxk D1, is the closed interval
3t7.
It can be shown that for any symmetric matrix A, the set of all possible values of
xTAx, forkxk D1, is a closed interval on the real axis. (See Exercise 13.) Denote the
left and right endpoints of this interval by mandM, respectively. That is, let
mDminfxTAxW kxk D1g; M DmaxfxTAxW kxk D1g (2)
Exercise 12 asks you to prove that if is an eigenvalue of A, then mM. The
next theorem says that mandMare themselves eigenvalues of A, just as in Example 2.1
1The use of minimum andmaximum in (2), and least andgreatest in the theorem, refers to the natural
ordering of the real numbers, not to magnitudes.
SECOND REVISED PAGES


--- Page 429 ---
412 CHAPTER 7 Symmetric Matrices and Quadratic Forms
T H E O R E M 6 LetAbe a symmetric matrix, and deÔ¨Åne mandMas in (2). Then Mis the greatest
eigenvalue 1ofAandmis the least eigenvalue of A. The value of xTAxisM
when xis a unit eigenvector u1corresponding to M. The value of xTAxismwhen
xis a unit eigenvector corresponding to m.
PROOF Orthogonally diagonalize AasPDP 1. We know that
xTAxDyTDywhen xDPy (3)
Also,
kxk D k Pyk D k ykfor all y
because PTPDIandkPyk2D.Py/T.Py/DyTPTPyDyTyD kyk2. In particular,
kyk D1if and only if kxk D1. Thus xTAxandyTDyassume the same set of values as
xandyrange over the set of all unit vectors.
To simplify notation, suppose that Ais a33matrix with eigenvalues abc.
Arrange the (eigenvector) columns of Pso that PD¬åu1u2u3¬çand
DD2
4a 0 0
0 b 0
0 0 c3
5
Given any unit vector yinR3with coordinates y1,y2,y3, observe that
ay2
1Day2
1
by2
2ay2
2
cy2
3ay2
3
and obtain these inequalities:
yTDyDay2
1Cby2
2Ccy2
3
ay2
1Cay2
2Cay2
3
Da.y2
1Cy2
2Cy2
3/
Dakyk2Da
Thus Ma, by deÔ¨Ånition of M. However, yTDyDawhen yDe1D.1; 0; 0/ , so in
factMDa. By (3), the xthat corresponds to yDe1is the eigenvector u1ofA, because
xDPe1Du1 u2 u32
41
0
03
5Du1
Thus MDaDeT
1De1DuT
1Au1, which proves the statement about M. A similar ar-
gument shows that mis the least eigenvalue, c, and this value of xTAxis attained when
xDPe3Du3.
EXAMPLE 3 LetAD2
43 2 1
2 3 1
1 1 43
5. Find the maximum value of the quadratic
form xTAxsubject to the constraint xTxD1, and Ô¨Ånd a unit vector at which this maxi-
mum value is attained.
SOLUTION By Theorem 6, the desired maximum value is the greatest eigenvalue of
A. The characteristic equation turns out to be
0D  3C102 27C18D  . 6/. 3/. 1/
The greatest eigenvalue is 6.
SECOND REVISED PAGES


--- Page 430 ---
7.3Constrained Optimization 413
The constrained maximum of xTAxis attained when xis a unit eigenvector for
D6. Solve .A 6I/xD0and Ô¨Ånd an eigenvector2
41
1
13
5. Set u1D2
641=p
3
1=p
3
1=p
33
75.
In Theorem 7 and in later applications, the values of xTAxare computed with addi-
tional constraints on the unit vector x.
T H E O R E M 7 LetA;  1, and u1be as in Theorem 6. Then the maximum value of xTAxsubject
to the constraints
xTxD1; xTu1D0
is the second greatest eigenvalue, 2, and this maximum is attained when xis an
eigenvector u2corresponding to 2.
Theorem 7 can be proved by an argument similar to the one above in which the
theorem is reduced to the case where the matrix of the quadratic form is diagonal. The
next example gives an idea of the proof for the case of a diagonal matrix.
EXAMPLE 4 Find the maximum value of 9x2
1C4x2
2C3x2
3subject to the con-
straints xTxD1andxTu1D0, where u1D.1; 0; 0/ . Note that u1is a unit eigenvector
corresponding to the greatest eigenvalue D9of the matrix of the quadratic form.
SOLUTION If the coordinates of xarex1,x2,x3, then the constraint xTu1D0means
simply that x1D0. For such a unit vector, x2
2Cx2
3D1, and
9x2
1C4x2
2C3x2
3D4x2
2C3x2
3
4x2
2C4x2
3
D4.x2
2Cx2
3/
D4
Thus the constrained maximum of the quadratic form does not exceed 4. And this value
is attained for xD.0; 1; 0/ , which is an eigenvector for the second greatest eigenvalue
of the matrix of the quadratic form.
EXAMPLE 5 LetAbe the matrix in Example 3 and let u1be a unit eigenvector
corresponding to the greatest eigenvalue of A. Find the maximum value of xTAxsubject
to the conditions
xTxD1; xTu1D0 (4)
SOLUTION From Example 3, the second greatest eigenvalue of AisD3. Solve
.A 3I/xD0to Ô¨Ånd an eigenvector, and normalize it to obtain
u2D2
641=p
6
1=p
6
 2=p
63
75
The vector u2is automatically orthogonal to u1because the vectors correspond to dif-
ferent eigenvalues. Thus the maximum of xTAxsubject to the constraints in (4) is 3,
attained when xDu2.
The next theorem generalizes Theorem 7 and, together with Theorem 6, gives a
useful characterization of allthe eigenvalues of A. The proof is omitted.
SECOND REVISED PAGES


--- Page 431 ---
414 CHAPTER 7 Symmetric Matrices and Quadratic Forms
T H E O R E M 8 LetAbe a symmetric nnmatrix with an orthogonal diagonalization
ADPDP 1, where the entries on the diagonal of Dare arranged so that
12     nand where the columns of Pare corresponding unit eigen-
vectors u1; : : : ; un. Then for kD2; : : : ; n , the maximum value of xTAxsubject to
the constraints
xTxD1; xTu1D0; : : : ; xTuk 1D0
is the eigenvalue k, and this maximum is attained at xDuk.
Theorem 8 will be helpful in Sections 7.4 and 7.5. The following application re-
quires only Theorem 6.
EXAMPLE 6 During the next year, a county government is planning to repair x
hundred miles of public roads and bridges and to improve yhundred acres of parks and
recreation areas. The county must decide how to allocate its resources (funds, equip-
ment, labor, etc.) between these two projects. If it is more cost effective to work si-
multaneously on both projects rather than on only one, then xandymight satisfy a
constraint such as
4x2C9y236
See Figure 3. Each point .x; y/ in the shaded feasible set represents a possible public
works schedule for the year. The points on the constraint curve, 4x2C9y2D36, use
the maximum amounts of resources available.
y
Parks and
recreation
4x2 + 9y2 = 36
Feasible
set2
3
Road and bridge repairx
FIGURE 3 Public works schedules.
In choosing its public works schedule, the county wants to consider the opinions of
the county residents. To measure the value, or utility , that the residents would assign to
the various work schedules .x; y/ , economists sometimes use a function such as
q.x; y/ Dxy
The set of points .x; y/ at which q.x; y/ is a constant is called an indifference curve .
Three such curves are shown in Figure 4. Points along an indifference curve correspond
to alternatives that county residents as a group would Ô¨Ånd equally valuable.2Find the
public works schedule that maximizes the utility function q.
SOLUTION The constraint equation 4x2C9y2D36does not describe a set of unit
vectors, but a change of variable can Ô¨Åx that problem. Rewrite the constraint in the form
x
32
Cy
22
D1
2Indifference curves are discussed in Michael D. Intriligator, Ronald G. Bodkin, and Cheng Hsiao,
Econometric Models, Techniques, and Applications (Upper Saddle River, NJ: Prentice-Hall, 1996).
SECOND REVISED PAGES


--- Page 432 ---
7.3Constrained Optimization 415
y
Parks and
recreation
1.44x2 + 9y2 = 36
(indifference curves)
q(x, y) = 4
q(x, y) = 3
q(x, y) = 2
Road and brid ge repair2.1x
FIGURE 4 The optimum public works schedule
is.2:1; 1:4/ .
and deÔ¨Åne
x1Dx
3; x 2Dy
2; that is ; x D3x1and yD2x2
Then the constraint equation becomes
x2
1Cx2
2D1
and the utility function becomes q.3x 1; 2x 2/D.3x1/.2x 2/D6x1x2. Let xDx1
x2
.
Then the problem is to maximize Q.x/D6x1x2subject to xTxD1. Note that Q.x/D
xTAx, where
AD0 3
3 0
The eigenvalues of Aare3, with eigenvectors"
1=p
2
1=p
2#
forD3and"
 1=p
2
1=p
2#
for
D  3. Thus the maximum value of Q.x/Dq.x1; x2/is 3, attained when x1D1=p
2
andx2D1=p
2.
In terms of the original variables, the optimum public works schedule is xD3x1D
3=p
22:1hundred miles of roads and bridges and yD2x2Dp
21:4hundred
acres of parks and recreational areas. The optimum public works schedule is the point
where the constraint curve and the indifference curve q.x; y/ D3just meet. Points
.x; y/ with a higher utility lie on indifference curves that do not touch the constraint
curve. See Figure 4.
PRACTICE PROBLEMS
1.LetQ.x/D3x2
1C3x2
2C2x1x2. Find a change of variable that transforms Qinto a
quadratic form with no cross-product term, and give the new quadratic form.
2.With Qas in Problem 1, Ô¨Ånd the maximum value of Q.x/subject to the constraint
xTxD1, and Ô¨Ånd a unit vector at which the maximum is attained.
7.3 EXERCISES
In Exercises 1 and 2, Ô¨Ånd the change of variable xDPythat
transforms the quadratic form xTAxintoyTDyas shown.
1.5x2
1C6x2
2C7x2
3C4x1x2 4x2x3D9y2
1C6y2
2C3y2
3
2.3x2
1C3x2
2C5x2
3C6x1x2C2x1x3C2x2x3D7y2
1C4y2
2
Hint: xandymust have the same number of coordinates, so the
quadratic form shown here must have a coefÔ¨Åcient of zero for y2
3.In Exercises 3‚Äì6, Ô¨Ånd (a) the maximum value of Q.x/subject to
the constraint xTxD1, (b) a unit vector uwhere this maximum is
attained, and (c) the maximum of Q.x/subject to the constraints
xTxD1andxTuD0.
3.Q.x/D5x2
1C6x2
2C7x2
3C4x1x2 4x2x3
(See Exercise 1.)
SECOND REVISED PAGES


--- Page 433 ---
416 CHAPTER 7 Symmetric Matrices and Quadratic Forms
4.Q.x/D3x2
1C3x2
2C5x2
3C6x1x2C2x1x3C2x2x3(See Exer-
cise 2.)
5.Q.x/Dx2
1Cx2
2 10x 1x2
6.Q.x/D3x2
1C9x2
2C8x1x2
7.LetQ.x/D  2x2
1 x2
2C4x1x2C4x2x3. Find a unit vector
xinR3at which Q.x/is maximized, subject to xTxD1.
[Hint: The eigenvalues of the matrix of the quadratic form
Qare 2,  1, and 4.]
8.LetQ.x/D7x2
1Cx2
2C7x2
3 8x1x2 4x1x3 8x2x3.
Find a unit vector xinR3at which Q.x/is maximized,
subject to xTxD1. [Hint: The eigenvalues of the matrix of
the quadratic form Qare 9 and  3.]
9.Find the maximum value of Q.x/D7x2
1C3x2
2 2x1x2,
subject to the constraint x2
1Cx2
2D1. (Do not go on to Ô¨Ånd
a vector where the maximum is attained.)
10. Find the maximum value of Q.x/D  3x2
1C5x2
2 2x1x2,
subject to the constraint x2
1Cx2
2D1. (Do not go on to Ô¨Ånd
a vector where the maximum is attained.)
11.Suppose xis a unit eigenvector of a matrix Acorresponding
to an eigenvalue 3. What is the value of xTAx?12. Letbe any eigenvalue of a symmetric matrix A. Justify
the statement made in this section that mM, where
mandMare deÔ¨Åned as in (2). [ Hint: Find an xsuch that
DxTAx.]
13. LetAbe an nnsymmetric matrix, let Mandmdenote the
maximum and minimum values of the quadratic form xTAx,
where xTxD1;and denote corresponding unit eigenvectors
byu1andun. The following calculations show that given any
number tbetween Mandm, there is a unit vector xsuch that
tDxTAx. Verify that tD.1 /mCMfor some number
between 0 and 1. Then let xDp
1 unCpu1, and
show that xTxD1andxTAxDt.
[M] In Exercises 14‚Äì17, follow the instructions given for Exer-
cises 3‚Äì6.
14.3x1x2C5x1x3C7x1x4C7x2x3C5x2x4C3x3x4
15.4x2
1 6x1x2 10x 1x3 10x 1x4 6x2x3 6x2x4 2x3x4
16. 6x2
1 10x2
2 13x2
3 13x2
4 4x1x2 4x1x3 4x1x4C6x3x4
17.x1x2C3x1x3C30x 1x4C30x 2x3C3x2x4Cx3x4
SOLUTIONS TO PRACTICE PROBLEMS
1.The matrix of the quadratic form is AD3 1
1 3
. It is easy to Ô¨Ånd the eigenvalues,
11 x2
x14
z
The maximum value of Q.x/
subject to xTxD1is 4.4 and 2, and corresponding unit eigenvectors,"
1=p
2
1=p
2#
and"
 1=p
2
1=p
2#
. So the
desired change of variable is xDPy, where PD"
1=p
2 1=p
2
1=p
2 1=p
2#
. (A common
error here is to forget to normalize the eigenvectors.) The new quadratic form is
yTDyD4y2
1C2y2
2.
2.The maximum of Q.x/, for a unit vector x, is 4 and the maximum is attained at
the unit eigenvector1=p
2
1=p
2
. [A common incorrect answer is1
0
. This vector
maximizes the quadratic form yTDyinstead of Q.x/.]
7.4 THE SINGULAR VALUE DECOMPOSITION
The diagonalization theorems in Sections 5.3 and 7.1 play a part in many interesting ap-
plications. Unfortunately, as we know, not all matrices can be factored as ADPDP 1
with Ddiagonal. However, a factorization ADQDP 1ispossible for any mn
matrix A! A special factorization of this type, called the singular value decomposition ,
is one of the most useful matrix factorizations in applied linear algebra.
The singular value decomposition is based on the following property of the ordinary
diagonalization that can be imitated for rectangular matrices: The absolute values of the
eigenvalues of a symmetric matrix Ameasure the amounts that Astretches or shrinks
SECOND REVISED PAGES


--- Page 434 ---
7.4The Singular Value Decomposition 417
certain vectors (the eigenvectors). If AxDxandkxk D1, then
kAxk D k xk D j j kxk D j j (1)
If1is the eigenvalue with the greatest magnitude, then a corresponding unit eigenvec-
torv1identiÔ¨Åes a direction in which the stretching effect of Ais greatest. That is, the
length of Axis maximized when xDv1, andkAv1k D j 1j, by (1). This description of
v1andj1jhas an analogue for rectangular matrices that will lead to the singular value
decomposition.
EXAMPLE 1 IfAD4 11 14
8 7  2
, then the linear transformation x7!Axmaps
the unit sphere fxW kxk D1ginR3onto an ellipse in R2, shown in Figure 1. Find a unit
vector xat which the length kAxkis maximized, and compute this maximum length.
x1x2Multiplication
by A
(3, /H110029)(18, 6)
x3
x2
x1
FIGURE 1 A transformation from R3toR2.
SOLUTION The quantity kAxk2is maximized at the same xthat maximizes kAxk, and
kAxk2is easier to study. Observe that
kAxk2D.Ax/T.Ax/DxTATAxDxT.ATA/x
Also, ATAis a symmetric matrix, since .ATA/TDATAT TDATA. So the problem now
is to maximize the quadratic form xT.ATA/xsubject to the constraint kxk D1. By
Theorem 6 in Section 7.3, the maximum value is the greatest eigenvalue 1ofATA.
Also, the maximum value is attained at a unit eigenvector of ATAcorresponding to 1.
For the matrix Ain this example,
ATAD2
44 8
11 7
14 23
54 11 14
8 7  2
D2
480 100 40
100 170 140
40 140 2003
5
The eigenvalues of ATAare1D360,2D90, and 3D0. Corresponding unit eigen-
vectors are, respectively,
v1D2
41=3
2=3
2=33
5;v2D2
4 2=3
 1=3
2=33
5;v3D2
42=3
 2=3
1=33
5
The maximum value of kAxk2is 360, attained when xis the unit vector v1. The vector
Av1is a point on the ellipse in Figure 1 farthest from the origin, namely,
Av1D4 11 14
8 7  22
41=3
2=3
2=33
5D18
6
Forkxk D1, the maximum value of kAxkiskAv1k Dp
360D6p
10.
SECOND REVISED PAGES


--- Page 435 ---
418 CHAPTER 7 Symmetric Matrices and Quadratic Forms
Example 1 suggests that the effect of Aon the unit sphere in R3is related to the
quadratic form xT.ATA/x. In fact, the entire geometric behavior of the transformation
x7!Axis captured by this quadratic form, as we shall see.
The Singular Values of an mnMatrix
LetAbe an mnmatrix. Then ATAis symmetric and can be orthogonally diagonalized.
Letfv1; : : : ; vngbe an orthonormal basis for Rnconsisting of eigenvectors of ATA, and
let1; : : : ;  nbe the associated eigenvalues of ATA. Then, for 1in,
kAvik2D.Avi/TAviDvT
iATAvi
DvT
i.ivi/ Since viis an eigenvector of ATA
Di Since viis a unit vector (2)
So the eigenvalues of ATAare all nonnegative. By renumbering, if necessary, we may
assume that the eigenvalues are arranged so that
12     n0
Thesingular values ofAare the square roots of the eigenvalues of ATA, denoted by
1; : : : ;  n, and they are arranged in decreasing order. That is, iDpifor1in.
By equation (2), the singular values of A are the lengths of the vectors Av1; : : : ; A vn.
EXAMPLE 2 LetAbe the matrix in Example 1. Since the eigenvalues of ATAare
360, 90, and 0, the singular values of Aare
1Dp
360D6p
10;  2Dp
90D3p
10;  3D0
From Example 1, the Ô¨Årst singular value of Ais the maximum of kAxkover all unit
vectors, and the maximum is attained at the unit eigenvector v1. Theorem 7 in Section 7.3
shows that the second singular value of Ais the maximum of kAxkover all unit vectors
that are orthogonal to v1, and this maximum is attained at the second unit eigenvector,
v2(Exercise 22). For the v2in Example 1,
Av2D4 11 14
8 7  22
4 2=3
 1=3
2=33
5D3
 9
This point is on the minor axis of the ellipse in Figure 1, just as Av1is on the major
axis. (See Figure 2.) The Ô¨Årst two singular values of Aare the lengths of the major and
minor semiaxes of the ellipse.
Av1
Av2x2
x1 FIGURE 2
The fact that Av1andAv2are orthogonal in Figure 2 is no accident, as the next
theorem shows.
T H E O R E M 9 Suppose fv1; : : : ; vngis an orthonormal basis of Rnconsisting of eigenvectors of
ATA, arranged so that the corresponding eigenvalues of ATAsatisfy 1     n,
and suppose Ahasrnonzero singular values. Then fAv1; : : : ; A vrgis an orthog-
onal basis for Col A, and rank ADr.
PROOF Because viandjvjare orthogonal for i¬§j,
.Avi/T.Avj/DvT
iATAvjDvT
i.jvj/D0
SECOND REVISED PAGES


--- Page 436 ---
7.4The Singular Value Decomposition 419
Thus fAv1; : : : ; A vngis an orthogonal set. Furthermore, since the lengths of the vec-
torsAv1; : : : ; A vnare the singular values of A, and since there are rnonzero singular
values, Avi¬§0if and only if 1ir. SoAv1; : : : ; A vrare linearly independent
vectors, and they are in Col A. Finally, for any yin Col A‚Äîsay, yDAx‚Äîwe can write
xDc1v1C    C cnvn, and
yDAxDc1Av1C    C crAvrCcrC1AvrC1C    C cnAvn
Dc1Av1C    C crAvrC0C    C 0
Thus yis in Span fAv1; : : : ; A vrg, which shows that fAv1; : : : ; A vrgis an (orthogonal)
basis for Col A. Hence rank ADdim Col ADr.
N U M E R I C A L N O T E
In some cases, the rank of Amay be very sensitive to small changes in the entries
ofA. The obvious method of counting the number of pivot columns in Adoes
not work well if Ais row reduced by a computer. Roundoff error often creates an
echelon form with full rank.
In practice, the most reliable way to estimate the rank of a large matrix A
is to count the number of nonzero singular values. In this case, extremely small
nonzero singular values are assumed to be zero for all practical purposes, and the
effective rank of the matrix is the number obtained by counting the remaining
nonzero singular values.1
The Singular Value Decomposition
The decomposition of Ainvolves an mn‚Äúdiagonal‚Äù matrix ¬Üof the form
¬ÜDD 0
0 0
m rrows(3)
6n rcolumns
where Dis an rrdiagonal matrix for some rnot exceeding the smaller of mandn.
(Ifrequals mornor both, some or all of the zero matrices do not appear.)
T H E O R E M 1 0 The Singular Value Decomposition
LetAbe an mnmatrix with rank r. Then there exists an mnmatrix ¬Üas
in (3) for which the diagonal entries in Dare the Ô¨Årst rsingular values of A,
12     r> 0, and there exist an mmorthogonal matrix Uand an
nnorthogonal matrix Vsuch that
ADU ¬ÜVT
Any factorization ADU ¬ÜVT, with UandVorthogonal, ¬Üas in (3), and positive
diagonal entries in D, is called a singular value decomposition (orSVD ) ofA. The
matrices UandVare not uniquely determined by A, but the diagonal entries of ¬Ü
are necessarily the singular values of A. See Exercise 19. The columns of Uin such a
decomposition are called left singular vectors ofA, and the columns of Vare called
right singular vectors ofA.
1In general, rank estimation is not a simple problem. For a discussion of the subtle issues involved, see
Philip E. Gill, Walter Murray, and Margaret H. Wright, Numerical Linear Algebra and Optimization , vol. 1
(Redwood City, CA: Addison-Wesley, 1991), Sec. 5.8.
SECOND REVISED PAGES


--- Page 437 ---
420 CHAPTER 7 Symmetric Matrices and Quadratic Forms
PROOF Letiandvibe as in Theorem 9, so that fAv1; : : : ; A vrgis an orthogonal basis
for Col A. Normalize each Avito obtain an orthonormal basis fu1; : : : ; urg, where
uiD1
kAvikAviD1
iAvi
and
AviDiui .1ir/ (4)
Now extend fu1; : : : ; urgto an orthonormal basis fu1; : : : ; umgofRm, and let
UD¬åu1u2   um¬çand VD¬åv1v2   vn¬ç
By construction, UandVare orthogonal matrices. Also, from (4),
AVD¬åAv1  Avr0   0¬çD¬å1u1  rur0   0¬ç
LetDbe the diagonal matrix with diagonal entries 1; : : : ;  r, and let ¬Übe as in
(3) above. Then
U ¬ÜD¬åu1u2   um¬ç2
6666641 0
2 0
:::
0 r
0 03
777775
D¬å1u1  rur0   0¬ç
DAV
Since Vis an orthogonal matrix, U ¬ÜVTDAV VTDA.
The next two examples focus attention on the internal structure of a singular value
decomposition. An efÔ¨Åcient and numerically stable algorithm for this decomposition
would use a different approach. See the Numerical Note at the end of the section.
EXAMPLE 3 Use the results of Examples 1 and 2 to construct a singular value
decomposition of AD4 11 14
8 7  2
.
SOLUTION A construction can be divided into three steps.
Step 1 .Find an orthogonal diagonalization of ATA.That is, Ô¨Ånd the eigenvalues of
SG Computing an SVD
7‚Äì10 ATAand a corresponding orthonormal set of eigenvectors. If Ahad only two columns,
the calculations could be done by hand. Larger matrices usually require a matrix pro-
gram.2However, for the matrix Ahere, the eigendata for ATAare provided in Example 1.
Step 2 .Set up V and ¬Ü.Arrange the eigenvalues of ATAin decreasing order. In Ex-
ample 1, the eigenvalues are already listed in decreasing order: 360, 90, and 0. The
corresponding unit eigenvectors, v1,v2, and v3, are the right singular vectors of A. Using
Example 1, construct
VD¬åv1v2v3¬çD2
41=3 2=3 2=3
2=3  1=3 2=3
2=3 2=3 1=33
5
2See the Study Guide for software and graphing calculator commands. MATLAB, for instance, can produce
both the eigenvalues and the eigenvectors with one command, eig.
SECOND REVISED PAGES


--- Page 438 ---
7.4The Singular Value Decomposition 421
The square roots of the eigenvalues are the singular values:
1D6p
10;  2D3p
10;  3D0
The nonzero singular values are the diagonal entries of D. The matrix ¬Üis the same
size as A, with Din its upper left corner and with 0‚Äôs elsewhere.
DD"
6p
10 0
0 3p
10#
; ¬Ü D¬åD 0 ¬çD"
6p
10 0 0
0 3p
10 0#
Step 3 .Construct U. When Ahas rank r, the Ô¨Årst rcolumns of Uare the normalized
vectors obtained from Av1; : : : ; A vr. In this example, Ahas two nonzero singular val-
ues, so rank AD2. Recall from equation (2) and the paragraph before Example 2 that
kAv1k D1andkAv2k D2. Thus
u1D1
1Av1D1
6p
1018
6
D"
3=p
10
1=p
10#
u2D1
2Av2D1
3p
103
 9
D"
1=p
10
 3=p
10#
Note that fu1;u2gis already a basis for R2. Thus no additional vectors are needed for
U, and UD¬åu1u2¬ç. The singular value decomposition of Ais
AD"
3=p
10 1=p
10
1=p
10 3=p
10# "
6p
10 0 0
0 3p
10 0#2
41=3 2=3 2=3
 2=3  1=3 2=3
2=3 2=3 1=33
5
" " "
U ¬Ü VT
EXAMPLE 4 Find a singular value decomposition of AD2
41 1
 2 2
2 23
5.
SOLUTION First, compute ATAD9 9
 9 9
. The eigenvalues of ATAare 18 and 0,
with corresponding unit eigenvectors
v1D"
1=p
2
 1=p
2#
;v2D"
1=p
2
1=p
2#
These unit vectors form the columns of V:
VD¬åv1v2¬çD"
1=p
2 1=p
2
 1=p
2 1=p
2#
The singular values are 1Dp
18D3p
2and2D0. Since there is only one nonzero
singular value, the ‚Äúmatrix‚Äù Dmay be written as a single number. That is, DD3p
2.
The matrix ¬Üis the same size as A, with Din its upper left corner:
¬ÜD2
4D 0
0 0
0 03
5D2
43p
2 0
0 0
0 03
5
To construct U, Ô¨Årst construct Av1andAv2:
Av1D2
642=p
2
 4=p
2
4=p
23
75; A v2D2
40
0
03
5
SECOND REVISED PAGES


--- Page 439 ---
422 CHAPTER 7 Symmetric Matrices and Quadratic Forms
As a check on the calculations, verify that kAv1k D1D3p
2. Of course, Av2D0
u1v1
v1Av11x1x2
x2
x1x3
u3
u21
FIGURE 3because kAv2k D2D0. The only column found for Uso far is
u1D1
3p
2Av1D2
41=3
 2=3
2=33
5
The other columns of Uare found by extending the set fu1gto an orthonormal basis for
R3. In this case, we need two orthogonal unit vectors u2andu3that are orthogonal to u1.
(See Figure 3.) Each vector must satisfy uT
1xD0, which is equivalent to the equation
x1 2x2C2x3D0. A basis for the solution set of this equation is
w1D2
42
1
03
5;w2D2
4 2
0
13
5
(Check that w1andw2are each orthogonal to u1.) Apply the Gram‚ÄìSchmidt process
(with normalizations) to fw1;w2g, and obtain
u2D2
42=p
5
1=p
5
03
5;u3D2
64 2=p
45
4=p
45
5=p
453
75
Finally, set UD¬åu1u2u3¬ç, take ¬ÜandVTfrom above, and write
AD2
41 1
 2 2
2 23
5D2
641=3 2=p
5 2=p
45
 2=3 1=p
5 4=p
45
2=3 0 5=p
453
752
43p
2 0
0 0
0 03
5"
1=p
2 1=p
2
1=p
2 1=p
2#
Applications of the Singular Value Decomposition
The SVD is often used to estimate the rank of a matrix, as noted above. Several other nu-
merical applications are described brieÔ¨Çy below, and an application to image processing
is presented in Section 7.5.
EXAMPLE 5 (The Condition Number) Most numerical calculations involving an
equation AxDbare as reliable as possible when the SVD of Ais used. The two
orthogonal matrices UandVdo not affect lengths of vectors or angles between vectors
(Theorem 7 in Section 6.2). Any possible instabilities in numerical calculations are
identiÔ¨Åed in ¬Ü. If the singular values of Aare extremely large or small, roundoff errors
are almost inevitable, but an error analysis is aided by knowing the entries in ¬ÜandV.
IfAis an invertible nnmatrix, then the ratio 1=nof the largest and smallest
singular values gives the condition number ofA. Exercises 41‚Äì43 in Section 2.3
showed how the condition number affects the sensitivity of a solution of AxDbto
changes (or errors) in the entries of A. (Actually, a ‚Äúcondition number‚Äù of Acan be
computed in several ways, but the deÔ¨Ånition given here is widely used for studying
AxDb.)
EXAMPLE 6 (Bases for Fundamental Subspaces) Given an SVD for an mn
matrix A, letu1; : : : ; umbe the left singular vectors, v1; : : : ; vnthe right singular vectors,
and1; : : : ;  nthe singular values, and let rbe the rank of A. By Theorem 9,
fu1; : : : ; urg (5)
is an orthonormal basis for Col A.
SECOND REVISED PAGES


--- Page 440 ---
7.4The Singular Value Decomposition 423
Recall from Theorem 3 in Section 6.1 that .ColA/?DNulAT. Hence
furC1; : : : ; umg (6)
is an orthonormal basis for Nul AT.
Since kAvik Difor1in, and iis 0 if and only if i > r , the vectors
vrC1; : : : ; vnspan a subspace of Nul Aof dimension n r. By the Rank Theorem,
dim Nul ADn rankA. It follows that
fvrC1; : : : ; vng (7)
is an orthonormal basis for Nul A, by the Basis Theorem (in Section 4.5).
From (5) and (6), the orthogonal complement of Nul ATis Col A. Interchanging A
andAT, note that .NulA/?DColATDRowA. Hence, from (7),
fv1; : : : ; vrg (8)
is an orthonormal basis for Row A.
Figure 4 summarizes (5)‚Äì(8), but shows the orthogonal basis f1u1; : : : ;  rurgfor
ColAinstead of the normalized basis, to remind you that AviDiuifor1ir.
Explicit orthonormal bases for the four fundamental subspaces determined by Aare
useful in some calculations, particularly in constrained optimization problems.
Col 
Au1v1
Av1x1x2
x1x3
u3
u2
Nul A 
Row 
A 
Col A‚ä•
x2
The fundamental subspaces in
Example 4.
vr + 1Col A = Row ATRow AMultiplication
by A
Nul AT
Nul A vn ‚Äì 1
vnumvr
00
ur + 1œÉrur œÉ2u2 œÉ1u1 v1 
v2......
......
...
FIGURE 4 The four fundamental subspaces and the
action of A.
The four fundamental subspaces and the concept of singular values provide the Ô¨Ånal
statements of the Invertible Matrix Theorem. (Recall that statements about AThave been
omitted from the theorem, to avoid nearly doubling the number of statements.) The other
statements were given in Sections 2.3, 2.9, 3.2, 4.6, and 5.2.
T H E O R E M The Invertible Matrix Theorem (concluded)
LetAbe an nnmatrix. Then the following statements are each equivalent to
the statement that Ais an invertible matrix.
u..ColA/?D f0g.
v..NulA/?DRn.
w.RowADRn.
x.Ahasnnonzero singular values.
SECOND REVISED PAGES


--- Page 441 ---
424 CHAPTER 7 Symmetric Matrices and Quadratic Forms
EXAMPLE 7 (Reduced SVD and the Pseudoinverse of A)When ¬Ücontains rows or
columns of zeros, a more compact decomposition of Ais possible. Using the notation
established above, let rDrankA, and partition UandVinto submatrices whose Ô¨Årst
blocks contain rcolumns:
UD¬åUrUm r¬ç;where UrD¬åu1   ur¬ç
VD¬åVrVn r¬ç; where VrD¬åv1   vr¬ç
Then UrismrandVrisnr. (To simplify notation, we consider Um rorVn r
even though one of them may have no columns.) Then partitioned matrix multiplication
shows that
AD¬åUrUm r¬ç"
D 0
0 0#"
VT
r
VT
n r#
DUrDVT
r (9)
This factorization of Ais called a reduced singular value decomposition ofA. Since
the diagonal entries in Dare nonzero, Dis invertible. The following matrix is called
thepseudoinverse (also, the Moore‚ÄìPenrose inverse ) ofA:
ACDVrD 1UT
r (10)
Supplementary Exercises 12‚Äì14 at the end of the chapter explore some of the properties
of the reduced singular value decomposition and the pseudoinverse.
EXAMPLE 8 (Least-Squares Solution) Given the equation AxDb, use the pseu-
doinverse of Ain (10) to deÔ¨Åne
OxDACbDVrD 1UT
rb
Then, from the SVD in (9),
AOxD.UrDVT
r/.VrD 1UT
rb/
DUrDD 1UT
rb Because VT
rVrDIr
DUrUT
rb
It follows from (5) that UrUT
rbis the orthogonal projection Obofbonto Col A. (See
Theorem 10 in Section 6.3.) Thus Oxis a least-squares solution of AxDb. In fact, this Ox
has the smallest length among all least-squares solutions of AxDb. See Supplementary
Exercise 14.
N U M E R I C A L N O T E
Examples 1‚Äì4 and the exercises illustrate the concept of singular values and
suggest how to perform calculations by hand. In practice, the computation of ATA
should be avoided, since any errors in the entries of Aare squared in the entries
ofATA. There exist fast iterative methods that produce the singular values and
singular vectors of Aaccurately to many decimal places.
Further Reading
Horn, Roger A., and Charles R. Johnson, Matrix Analysis (Cambridge: Cambridge
University Press, 1990).
Long, Cliff, ‚ÄúVisualization of Matrix Singular Value Decomposition.‚Äù Mathematics
Magazine 56(1983), pp. 161‚Äì167.
SECOND REVISED PAGES


--- Page 442 ---
7.4The Singular Value Decomposition 425
Moler, C. B., and D. Morrison, ‚ÄúSingular Value Analysis of Cryptograms.‚Äù Amer. Math.
Monthly 90(1983), pp. 78‚Äì87.
Strang, Gilbert, Linear Algebra and Its Applications, 4th ed. (Belmont, CA: Brooks/
Cole, 2005).
Watkins, David S., Fundamentals of Matrix Computations (New York: Wiley, 1991),
pp. 390‚Äì398, 409‚Äì421.
PRACTICE PROBLEMS
1.Given a singular value decomposition, ADU ¬ÜVT, Ô¨Ånd an SVD of AT. How are
WEB
the singular values of AandATrelated?
2.For any nnmatrix A, use the SVD to show that there is an nnorthogonal matrix
Qsuch that ATADQT.ATA/Q .
Remark: Practice Problem 2 establishes that for any nnmatrix A, the matrices AAT
andATAareorthogonally similar .
7.4 EXERCISES
Find the singular values of the matrices in Exercises 1‚Äì4.
1.1 0
0 3
2. 3 0
0 0
3.2 3
0 2
4.3 0
8 3
Find an SVD of each matrix in Exercises 5‚Äì12. [ Hint: In Exer-
cise 11, one choice for Uis2
4 1=3 2=3 2=3
2=3  1=3 2=3
2=3 2=3  1=33
5. In Exer-
cise 12, one column of Ucan be2
641=p
6
 2=p
6
1=p
63
75.]
5. 2 0
0 0
6. 3 0
0 2
7.2 1
2 2
8.4 6
0 4
9.2
43 3
0 0
1 13
5 10.2
47 1
5 5
0 03
5
11.2
4 3 1
6 2
6 23
5 12.2
41 1
0 1
 1 13
5
13. Find the SVD of AD3 2 2
2 3  2
[Hint: Work with AT.]
14. In Exercise 7, Ô¨Ånd a unit vector xat which Axhas maximum
length.
15. Suppose the factorization below is an SVD of a matrix A,
with the entries in UandVrounded to two decimal places.AD2
4:40 :78 :47
:37 :33 :87
 :84 :52 :163
52
47:10 0 0
0 3:10 0
0 0 03
5
2
4:30 :51 :81
:76 :64  :12
:58 :58 :583
5
a.What is the rank of A?
b.Use this decomposition of A, with no calculations, to
write a basis for Col Aand a basis for Nul A. [Hint: First
write the columns of V.]
16. Repeat Exercise 15 for the following SVD of a 34
matrix A:
AD2
4 :86 :11 :50
:31 :68  :67
:41 :73 :553
52
412:48 0 0 0
0 6:34 0 0
0 0 0 03
5
2
664:66 :03 :35 :66
 :13 :90 :39 :13
:65 :08  :16 :73
 :34 :42  :84 :083
775
In Exercises 17‚Äì24, Ais an mnmatrix with a singular value
decomposition ADU ¬ÜVT, where Uis an mmorthogonal
matrix, ¬Üis an mn‚Äúdiagonal‚Äù matrix with rpositive entries
and no negative entries, and Vis an nnorthogonal matrix.
Justify each answer.
17. Show that if Ais square, then jdetAjis the product of the
singular values of A.
18. Suppose Ais square and invertible. Find a singular value
decomposition of A 1.
19. Show that the columns of Vare eigenvectors of ATA, the
columns of Uare eigenvectors of AAT, and the diagonal
SECOND REVISED PAGES


--- Page 443 ---
426 CHAPTER 7 Symmetric Matrices and Quadratic Forms
entries of ¬Üare the singular values of A. [Hint: Use the SVD
to compute ATAandAAT.]
20. Show that if Pis an orthogonal mmmatrix, then PAhas
the same singular values as A.
21. Justify the statement in Example 2 that the second singular
value of a matrix Ais the maximum of kAxkasxvaries
over all unit vectors orthogonal to v1, with v1a right singular
vector corresponding to the Ô¨Årst singular value of A. [Hint:
Use Theorem 7 in Section 7.3.]
22. Show that if Ais an nnpositive deÔ¨Ånite matrix, then an
orthogonal diagonalization ADPDPTis a singular value
decomposition of A.
23. LetUD¬åu1   um¬çandVD¬åv1   vn¬ç, where the
uiandviare as in Theorem 10. Show that
AD1u1vT
1C2u2vT
2C    C rurvT
r:
24. Using the notation of Exercise 23, show that ATujDjvj
for1jrDrankA.
25. LetTWRn!Rmbe a linear transformation. Describe how
to Ô¨Ånd a basis BforRnand a basis CforRmsuch that thematrix for Trelative to BandCis an mn‚Äúdiagonal‚Äù
matrix.
[M] Compute an SVD of each matrix in Exercises 26 and 27.
Report the Ô¨Ånal matrix entries accurate to two decimal places. Use
the method of Examples 3 and 4.
26.AD2
664 18 13  4 4
2 19  4 12
 14 11  12 8
 2 21 4 83
775
27.AD2
6646 8 4 5  4
2 7  5 6 4
0 1 8 2 2
 1 2 4 4  83
775
28. [M] Compute the singular values of the 44matrix in
Exercise 9 in Section 2.3, and compute the condition number
1=4.
29. [M] Compute the singular values of the 55matrix in
Exercise 10 in Section 2.3, and compute the condition num-
ber1=5.
SOLUTIONS TO PRACTICE PROBLEMS
1.IfADU ¬ÜVT, where ¬Üismn, then ATD.VT/T¬ÜTUTDV ¬ÜTUT. This is an
SVD of ATbecause VandUare orthogonal matrices and ¬ÜTis annm‚Äúdiagonal‚Äù
matrix. Since ¬Üand¬ÜThave the same nonzero diagonal entries, AandAThave the
same nonzero singular values. [ Note: IfAis2n, then AATis only 22and its
eigenvalues may be easier to compute (by hand) than the eigenvalues of ATA.]
2.Use the SVD to write ADU ¬ÜVT, where UandVarennorthogonal matrices
and¬Üis an nndiagonal matrix. Notice that UTUDIDVTVand¬ÜTD¬Ü,
since UandVare orthogonal matrices and ¬Üis a diagonal matrix. Substituting the
SVD for AintoAATandATAresults in
AATDU ¬ÜVT.U ¬ÜVT/TDU ¬ÜVTV ¬ÜTUTDU ¬Ü¬ÜTUTDU ¬Ü2UT;
and
ATAD.U ¬ÜVT/TU ¬ÜVTDV ¬ÜTUTU ¬ÜVTDV ¬ÜT¬ÜVTDV ¬Ü2VT:
LetQDV UT. Then
QT.ATA/QD.V UT/T.V ¬Ü2VT/.V UT/DU VTV ¬Ü2VTV UTDU ¬Ü2UTDAAT:
7.5 APPLICATIONS TO IMAGE PROCESSING AND STATISTICS
The satellite photographs in this chapter‚Äôs introduction provide an example of multidi-
mensional, or multivariate , data‚Äîinformation organized so that each datum in the data
set is identiÔ¨Åed with a point (vector) in Rn. The main goal of this section is to explain a
technique, called principal component analysis , used to analyze such multivariate data.
The calculations will illustrate the use of orthogonal diagonalization and the singular
value decomposition.
SECOND REVISED PAGES


--- Page 444 ---
7.5Applications to Image Processing and Statistics 427
Principal component analysis can be applied to any data that consist of lists of
measurements made on a collection of objects or individuals. For instance, consider a
chemical process that produces a plastic material. To monitor the process, 300 samples
are taken of the material produced, and each sample is subjected to a battery of eight
tests, such as melting point, density, ductility, tensile strength, and so on. The laboratory
report for each sample is a vector in R8, and the set of such vectors forms an 8300
matrix, called the matrix of observations .
Loosely speaking, we can say that the process control data are eight-dimensional.
The next two examples describe data that can be visualized graphically.
EXAMPLE 1 An example of two-dimensional data is given by a set of weights and
heights of Ncollege students. Let Xjdenote the observation vector inR2that lists the
weight and height of the jth student. If wdenotes weight and hheight, then the matrix
of observations has the formw1w2   wN
h1h2   hN
66 6
X1 X2 XN
The set of observation vectors can be visualized as a two-dimensional scatter plot . See
Figure 1.
h
w
FIGURE 1 A scatter plot of observation
vectors X1; : : : ; XN.
EXAMPLE 2 The Ô¨Årst three photographs of Railroad Valley, Nevada, shown in the
chapter introduction can be viewed as oneimage of the region, with three spectral
components , because simultaneous measurements of the region were made at three
separate wavelengths. Each photograph gives different information about the same
physical region. For instance, the Ô¨Årst pixel in the upper-left corner of each photograph
corresponds to the same place on the ground (about 30 meters by 30 meters). To each
pixel there corresponds an observation vector in R3that lists the signal intensities for
that pixel in the three spectral bands.
Typically, the image is 20002000 pixels, so there are 4 million pixels in the
image. The data for the image form a matrix with 3 rows and 4 million columns
(with columns arranged in any convenient order). In this case, the ‚Äúmultidimensional‚Äù
character of the data refers to the three spectral dimensions rather than the two spatial
dimensions that naturally belong to any photograph. The data can be visualized as a
cluster of 4 million points in R3, perhaps as in Figure 2.
x2x1x3FIGURE 2
A scatter plot of spectral data for a
satellite image.
Mean and Covariance
To prepare for principal component analysis, let ¬åX1   XN¬çbe apNmatrix of
observations, such as described above. The sample mean ,M, of the observation vectors
SECOND REVISED PAGES


--- Page 445 ---
428 CHAPTER 7 Symmetric Matrices and Quadratic Forms
X1; : : : ; XNis given by
MD1
N.X1C    C XN/
For the data in Figure 1, the sample mean is the point in the ‚Äúcenter‚Äù of the scatter plot.
ForkD1; : : : ; N , let
OXkDXk M
The columns of the pNmatrix
BD¬åOX1OX2  OXN¬ç
have a zero sample mean, and Bis said to be in mean-deviation form . When the sample
mean is subtracted from the data in Figure 1, the resulting scatter plot has the form in
ÀÜh
ÀÜwFIGURE 3
Weight‚Äìheight data in
mean-deviation form. Figure 3.
The ( sample )covariance matrix is the ppmatrix SdeÔ¨Åned by
SD1
N 1BBT
Since any matrix of the form BBTis positive semideÔ¨Ånite, so is S. (See Exercise 25 in
Section 7.2 with BandBTinterchanged.)
EXAMPLE 3 Three measurements are made on each of four individuals in a random
sample from a population. The observation vectors are
X1D2
41
2
13
5;X2D2
44
2
133
5;X3D2
47
8
13
5;X4D2
48
4
53
5
Compute the sample mean and the covariance matrix.
SOLUTION The sample mean is
MD1
40
@2
41
2
13
5C2
44
2
133
5C2
47
8
13
5C2
48
4
53
51
AD1
42
420
16
203
5D2
45
4
53
5
Subtract the sample mean from X1; : : : ; X4to obtain
OX1D2
4 4
 2
 43
5;OX2D2
4 1
 2
83
5;OX3D2
42
4
 43
5;OX4D2
43
0
03
5
and
BD2
4 4 1 2 3
 2 2 4 0
 4 8  4 03
5
The sample covariance matrix is
SD1
32
4 4 1 2 3
 2 2 4 0
 4 8  4 03
52
664 4 2 4
 1 2 8
2 4  4
3 0 03
775
D1
32
430 18 0
18 24  24
0 24 963
5D2
410 6 0
6 8  8
0 8 323
5
SECOND REVISED PAGES


--- Page 446 ---
7.5Applications to Image Processing and Statistics 429
To discuss the entries in SD¬åsij¬ç, let Xrepresent a vector that varies over the
set of observation vectors and denote the coordinates of Xbyx1; : : : ; x p. Then x1,
for example, is a scalar that varies over the set of Ô¨Årst coordinates of X1; : : : ; XN. For
jD1; : : : ; p , the diagonal entry sjjinSis called the variance ofxj.
The variance of xjmeasures the spread of the values of xj. (See Exercise 13.) In
Example 3, the variance of x1is 10 and the variance of x3is 32. The fact that 32 is more
than 10 indicates that the set of third entries in the response vectors contains a wider
spread of values than the set of Ô¨Årst entries.
Thetotal variance of the data is the sum of the variances on the diagonal of S. In
general, the sum of the diagonal entries of a square matrix Sis called the trace of the
matrix, written tr .S/. Thus
ftotal variance g Dtr.S/
The entry sijinSfori¬§jis called the covariance ofxiandxj. Observe that
in Example 3, the covariance between x1andx3is 0 because the .1; 3/ -entry in Sis 0.
Statisticians say that x1andx3areuncorrelated . Analysis of the multivariate data in
X1; : : : ; XNis greatly simpliÔ¨Åed when most or all of the variables x1; : : : ; x pare uncor-
related, that is, when the covariance matrix of X1; : : : ; XNis diagonal or nearly diagonal.
Principal Component Analysis
For simplicity, assume that the matrix ¬åX1   XN¬çis already in mean-deviation
form. The goal of principal component analysis is to Ô¨Ånd an orthogonal ppmatrix
PD¬åu1   up¬çthat determines a change of variable, XDPY, or
2
6664x1
x2
:::
xp3
7775Du1u2   up2
6664y1
y2
:::
yp3
7775
with the property that the new variables y1; : : : ; y pare uncorrelated and are arranged in
order of decreasing variance.
The orthogonal change of variable XDPYmeans that each observation vector Xk
receives a ‚Äúnew name,‚Äù Yk, such that XkDPYk. Notice that Ykis the coordinate vector
ofXkwith respect to the columns of P, and YkDP 1XkDPTXkforkD1; : : : ; N .
It is not difÔ¨Åcult to verify that for any orthogonal P, the covariance matrix of
Y1; : : : ; YNisPTSP(Exercise 11). So the desired orthogonal matrix Pis one that
makes PTSPdiagonal. Let Dbe a diagonal matrix with the eigenvalues 1; : : : ;  p
ofSon the diagonal, arranged so that 12     p0, and let Pbe an
orthogonal matrix whose columns are the corresponding unit eigenvectors u1; : : : ; up.
Then SDPDPTandPTSPDD.
The unit eigenvectors u1; : : : ; upof the covariance matrix Sare called the principal
components of the data (in the matrix of observations). The Ô¨Årst principal component
is the eigenvector corresponding to the largest eigenvalue of S, the second principal
component is the eigenvector corresponding to the second largest eigenvalue, and so on.
The Ô¨Årst principal component u1determines the new variable y1in the following
way. Let c1; : : : ; c pbe the entries in u1. Since uT
1is the Ô¨Årst row of PT, the equation
YDPTXshows that
y1DuT
1XDc1x1Cc2x2C    C cpxp
Thus y1is a linear combination of the original variables x1; : : : ; x p, using the entries in
the eigenvector u1as weights. In a similar fashion, u2determines the variable y2, and
so on.
SECOND REVISED PAGES


--- Page 447 ---
430 CHAPTER 7 Symmetric Matrices and Quadratic Forms
EXAMPLE 4 The initial data for the multispectral image of Railroad Valley
(Example 2) consisted of 4 million vectors in R3. The associated covariance matrix is1
SD2
42382:78 2611:84 2136:20
2611:84 3106:47 2553:90
2136:20 2553:90 2650:713
5
Find the principal components of the data, and list the new variable determined by the
Ô¨Årst principal component.
SOLUTION The eigenvalues of Sand the associated principal components (the unit
eigenvectors) are
1D7614:23  2D427:63 3D98:10
u1D2
4:5417
:6295
:55703
5u2D2
4 :4894
 :3026
:81793
5u3D2
4:6834
 :7157
:14413
5
Using two decimal places for simplicity, the variable for the Ô¨Årst principal component is
y1D:54x 1C:63x 2C:56x 3
This equation was used to create photograph (d) in the chapter introduction. The
variables x1,x2, and x3are the signal intensities in the three spectral bands. The values
ofx1, converted to a gray scale between black and white, produced photograph (a).
Similarly, the values of x2andx3produced photographs (b) and (c), respectively. At
each pixel in photograph (d), the gray scale value is computed from y1, a weighted
linear combination of x1; x2;andx3. In this sense, photograph (d) ‚Äúdisplays‚Äù the Ô¨Årst
principal component of the data.
In Example 4, the covariance matrix for the transformed data, using variables y1,
y2, and y3, is
DD2
47614:23 0 0
0 427:63 0
0 0 98:103
5
Although Dis obviously simpler than the original covariance matrix S, the merit
of constructing the new variables is not yet apparent. However, the variances of the
variables y1,y2, and y3appear on the diagonal of D, and obviously the Ô¨Årst variance
inDis much larger than the other two. As we shall see, this fact will permit us to view
the data as essentially one-dimensional rather than three-dimensional.
Reducing the Dimension of Multivariate Data
Principal component analysis is potentially valuable for applications in which most of
the variation, or dynamic range, in the data is due to variations in only a few of the new
variables, y1; : : : ; y p.
It can be shown that an orthogonal change of variables, XDPY, does not change
the total variance of the data. (Roughly speaking, this is true because left-multiplication
byPdoes not change the lengths of vectors or the angles between them. See Exercise
12.) This means that if SDPDPT, thentotal variance
ofx1; : : : ; x p
Dtotal variance
ofy1; : : : ; y p
Dtr.D/D1C    C p
The variance of yjisj, and the quotient j=tr.S/measures the fraction of the total
variance that is ‚Äúexplained‚Äù or ‚Äúcaptured‚Äù by yj.
1Data for Example 4 and Exercises 5 and 6 were provided by Earth Satellite Corporation, Rockville,
Maryland.
SECOND REVISED PAGES


--- Page 448 ---
7.5Applications to Image Processing and Statistics 431
EXAMPLE 5 Compute the various percentages of variance of the Railroad Valley
multispectral data that are displayed in the principal component photographs, (d)‚Äì(f),
shown in the chapter introduction.
SOLUTION The total variance of the data is
tr.D/D7614:23 C427:63 C98:10 D8139:96
[Verify that this number also equals tr .S/.] The percentages of the total variance
explained by the principal components are
First component Second component Third component
7614:23
8139:96D93:5%427:63
8139:96D5:3%98:10
8139:96D1:2%
In a sense, 93.5% of the information collected by Landsat for the Railroad Valley region
is displayed in photograph (d), with 5.3% in (e) and only 1.2% remaining for (f).
The calculations in Example 5 show that the data have practically no variance in
the third (new) coordinate. The values of y3are all close to zero. Geometrically, the
data points lie nearly in the plane y3D0, and their locations can be determined fairly
accurately by knowing only the values of y1andy2. In fact, y2also has relatively small
variance, which means that the points lie approximately along a line, and the data are
essentially one-dimensional. See Figure 2, in which the data resemble a popsicle stick.
Characterizations of Principal Component Variables
Ify1; : : : ; y parise from a principal component analysis of a pNmatrix of obser-
vations, then the variance of y1is as large as possible in the following sense: If uis
any unit vector and if yDuTX, then the variance of the values of yasXvaries over
the original data X1; : : : ; XNturns out to be uTSu. By Theorem 8 in Section 7.3, the
maximum value of uTSu, over all unit vectors u, is the largest eigenvalue 1ofS, and
this variance is attained when uis the corresponding eigenvector u1. In the same way,
Theorem 8 shows that y2has maximum possible variance among all variables yDuTX
that are uncorrelated withy1. Likewise, y3has maximum possible variance among all
variables uncorrelated with both y1andy2, and so on.
N U M E R I C A L N O T E
The singular value decomposition is the main tool for performing principal com-
ponent analysis in practical applications. If Bis apNmatrix of observations
in mean-deviation form, and if AD 
1=p
N 1
BT, then ATAis the covariance
matrix, S. The squares of the singular values of Aare the peigenvalues of S,
and the right singular vectors of Aare the principal components of the data.
As mentioned in Section 7.4, iterative calculation of the SVD of Ais faster
and more accurate than an eigenvalue decomposition of S. This is particularly
true, for instance, in the hyperspectral image processing (with pD224) men-
tioned in the chapter introduction. Principal component analysis is completed in
seconds on specialized workstations.
Further Reading
Lillesand, Thomas M., and Ralph W. Kiefer, Remote Sensing and Image Interpretation ,
4th ed. (New York: John Wiley, 2000).
SECOND REVISED PAGES


--- Page 449 ---
432 CHAPTER 7 Symmetric Matrices and Quadratic Forms
PRACTICE PROBLEMS
The following table lists the weights and heights of Ô¨Åve boys:
Boy #1 #2 #3 #4 #5
Weight (lb) 120 125 125 135 145
Height (in.) 61 60 64 68 72
1.Find the covariance matrix for the data.
2.Make a principal component analysis of the data to Ô¨Ånd a single size index that
explains most of the variation in the data.
7.5 EXERCISES
In Exercises 1 and 2, convert the matrix of observations to mean-
deviation form, and construct the sample covariance matrix.
1.19 22 6 3 2 20
12 6 9 15 13 5
2.1 5 2 6 7 3
3 11 6 8 15 11
3.Find the principal components of the data for Exercise 1.
4.Find the principal components of the data for Exercise 2.
5.[M] A Landsat image with three spectral components was
made of Homestead Air Force Base in Florida (after the
base was hit by Hurricane Andrew in 1992). The covariance
matrix of the data is shown below. Find the Ô¨Årst principal
component of the data, and compute the percentage of the
total variance that is contained in this component.
SD2
4164:12 32:73 81:04
32:73 539:44 249:13
81:04 249:13 189:113
5
6.[M] The covariance matrix below was obtained from a Land-
sat image of the Columbia River in Washington, using data
from three spectral bands. Let x1,x2,x3denote the spectral
components of each pixel in the image. Find a new variable of
the form y1Dc1x1Cc2x2Cc3x3that has maximum possi-
ble variance, subject to the constraint that c2
1Cc2
2Cc2
3D1.
What percentage of the total variance in the data is explained
byy1?
SD2
429:64 18:38 5:00
18:38 20:82 14:06
5:00 14:06 29:213
5
7.Letx1; x2denote the variables for the two-dimensional
data in Exercise 1. Find a new variable y1of the form
y1Dc1x1Cc2x2, with c2
1Cc2
2D1, such that y1has maxi-
mum possible variance over the given data. How much of the
variance in the data is explained by y1?
8.Repeat Exercise 7 for the data in Exercise 2.9.Suppose three tests are administered to a random sample
of college students. Let X1; : : : ; XNbe observation vectors
inR3that list the three scores of each student, and for
jD1; 2; 3 , letxjdenote a student‚Äôs score on the jth exam.
Suppose the covariance matrix of the data is
SD2
45 2 0
2 6 2
0 2 73
5
Letybe an ‚Äúindex‚Äù of student performance, with yD
c1x1Cc2x2Cc3x3andc2
1Cc2
2Cc2
3D1. Choose c1; c2; c3
so that the variance of yover the data set is as large as
possible. [ Hint: The eigenvalues of the sample covariance
matrix are D3; 6, and 9.]
10. [M] Repeat Exercise 9 with SD2
45 4 2
4 11 4
2 4 53
5.
11.Given multivariate data X1; : : : ; XN(inRp/in mean-
deviation form, let Pbe a ppmatrix, and deÔ¨Åne
YkDPTXkforkD1; : : : ; N .
a.Show that Y1; : : : ; YNare in mean-deviation form. [ Hint:
Letwbe the vector in RNwith a 1 in each entry. Then
¬åX1   XN¬çwD0(the zero vector in Rp/.]
b.Show that if the covariance matrix of X1; : : : ; XNisS,
then the covariance matrix of Y1; : : : ; YNisPTSP.
12. LetXdenote a vector that varies over the columns of a pN
matrix of observations, and let Pbe a pporthogonal
matrix. Show that the change of variable XDPYdoes not
change the total variance of the data. [ Hint: By Exercise 11,
it sufÔ¨Åces to show that tr .PTSP /Dtr.S/. Use a property
of the trace mentioned in Exercise 25 in Section 5.4.]
13. The sample covariance matrix is a generalization of a formula
for the variance of a sample of Nscalar measurements, say,
t1; : : : ; t N. Ifmis the average of t1; : : : ; t N, then the sample
variance is given by
1
N 1nX
kD1.tk m/2.1/
SECOND REVISED PAGES


--- Page 450 ---
7.5Applications to Image Processing and Statistics 433
Show how the sample covariance matrix, S, deÔ¨Åned prior to
Example 3, may be written in a form similar to (1). [ Hint: Use
partitioned matrix multiplication to write Sas1=.N  1/times the sum of Nmatrices of size pp. For 1kN,
write Xk Min place of OXk.]
SOLUTIONS TO PRACTICE PROBLEMS
1.First arrange the data in mean-deviation form. The sample mean vector is easily
seen to be MD130
65
. Subtract Mfrom the observation vectors (the columns in
the table) and obtain
BD 10 5 5 5 15
 4 5 1 3 7
Then the sample covariance matrix is
SD1
5 1 10 5 5 5 15
 4 5 1 3 72
66664 10 4
 5 5
 5 1
5 3
15 73
77775
D1
4400 190
190 100
D100:0 47:5
47:5 25:0
2.The eigenvalues of Sare (to two decimal places)
1D123:02 and 2D1:98
The unit eigenvector corresponding to 1isuD:900
:436
. (Since Sis22, the
computations can be done by hand if a matrix program is not available.) For the size
index , set
yD:900OwC:436Oh
where OwandOhare weight and height, respectively, in mean-deviation form. The
variance of this index over the data set is 123.02. Because the total variance is
tr.S/D100C25D125, the size index accounts for practically all (98.4%) of the
variance of the data.
The original data for Practice Problem 1 and the line determined by the Ô¨Årst
principal component uare shown in Figure 4. (In parametric vector form, the line
isxDMCtu.) It can be shown that the line is the best approximation to the data,
h
1205560657075
130 140 150w
PoundsInches
FIGURE 4 An orthogonal regression line determined by the
Ô¨Årst principal component of the data.
SECOND REVISED PAGES


--- Page 451 ---
434 CHAPTER 7 Symmetric Matrices and Quadratic Forms
in the sense that the sum of the squares of the orthogonal distances to the line is
minimized. In fact, principal component analysis is equivalent to what is termed
orthogonal regression , but that is a story for another day.
CHAPTER 7 SUPPLEMENTARY EXERCISES
1.Mark each statement True or False. Justify each answer. In
each part, Arepresents an nnmatrix.
a.IfAis orthogonally diagonalizable, then Ais symmetric.
b.IfAis an orthogonal matrix, then Ais symmetric.
c.IfAis an orthogonal matrix, then kAxk D k xkfor all x
inRn.
d.The principal axes of a quadratic form xTAxcan be the
columns of any matrix Pthat diagonalizes A.
e.IfPis an nnmatrix with orthogonal columns, then
PTDP 1.
f.If every coefÔ¨Åcient in a quadratic form is positive, then
the quadratic form is positive deÔ¨Ånite.
g.IfxTAx> 0for some x, then the quadratic form xTAxis
positive deÔ¨Ånite.
h.By a suitable change of variable, any quadratic form can
be changed into one with no cross-product term.
i.The largest value of a quadratic form xTAx, forkxk D1,
is the largest entry on the diagonal of A.
j.The maximum value of a positive deÔ¨Ånite quadratic form
xTAxis the greatest eigenvalue of A.
k.A positive deÔ¨Ånite quadratic form can be changed into
a negative deÔ¨Ånite form by a suitable change of variable
xDPu, for some orthogonal matrix P.
l.An indeÔ¨Ånite quadratic form is one whose eigenvalues
are not deÔ¨Ånite.
m.IfPis an nnorthogonal matrix, then the change of
variable xDPutransforms xTAxinto a quadratic form
whose matrix is P 1AP.
n.IfUismnwith orthogonal columns, then U UTxis the
orthogonal projection of xonto Col U.
o.IfBismnandxis a unit vector in Rn, then kBxk 1,
where 1is the Ô¨Årst singular value of B.
p.A singular value decomposition of an mnmatrix B
can be written as BDP ¬ÜQ , where Pis an mm
orthogonal matrix, Qis an nnorthogonal matrix, and
¬Üis an mn‚Äúdiagonal‚Äù matrix.
q.IfAisnn, then AandATAhave the same singular
values.
2.Letfu1; : : : ; ungbe an orthonormal basis for Rn, and let
1; : : : ;  nbe any real scalars. DeÔ¨Åne
AD1u1uT
1C    C nunuT
n
a.Show that Ais symmetric.b.Show that 1; : : : ;  nare the eigenvalues of A.
3.LetAbe an nnsymmetric matrix of rank r. Explain why
the spectral decomposition of Arepresents Aas the sum of
rrank1matrices.
4.LetAbe an nnsymmetric matrix.
a.Show that .ColA/?DNulA. [Hint: See Section 6.1.]
b.Show that each yinRncan be written in the form yD
OyCz, with Oyin Col Aandzin Nul A.
5.Show that if vis an eigenvector of an nnmatrix Aandv
corresponds to a nonzero eigenvalue of A, then vis in Col A.
[Hint: Use the deÔ¨Ånition of an eigenvector.]
6.LetAbe an nnsymmetric matrix. Use Exercise 5 and
an eigenvector basis for Rnto give a second proof of the
decomposition in Exercise 4(b).
7.Prove that an nnmatrix Ais positive deÔ¨Ånite if and only
ifAadmits a Cholesky factorization , namely, ADRTRfor
some invertible upper triangular matrix Rwhose diagonal
entries are all positive. [ Hint: Use a QR factorization and
Exercise 26 in Section 7.2.]
8.Use Exercise 7 to show that if Ais positive deÔ¨Ånite, then
Ahas an LU factorization, ADLU, where Uhas positive
pivots on its diagonal. (The converse is true, too.)
IfAismn, then the matrix GDATAis called the Gram matrix
ofA. In this case, the entries of Gare the inner products of the
columns of A. (See Exercises 9 and 10.)
9.Show that the Gram matrix of any matrix Ais positive
semideÔ¨Ånite, with the same rank as A. (See the Exercises in
Section 6.5.)
10. Show that if an nnmatrix Gis positive semideÔ¨Ånite and
has rank r, then Gis the Gram matrix of some rnmatrix
A. This is called a rank-revealing factorization ofG. [Hint:
Consider the spectral decomposition of G, and Ô¨Årst write G
asBBTfor an nrmatrix B.]
11.Prove that any nnmatrix Aadmits a polar decomposition
of the form ADPQ, where Pis annnpositive semideÔ¨Å-
nite matrix with the same rank as Aand where Qis annn
orthogonal matrix. [ Hint: Use a singular value decomposi-
tion, ADU ¬ÜVT, and observe that AD.U ¬ÜUT/.UVT/.]
This decomposition is used, for instance, in mechanical en-
gineering to model the deformation of a material. The matrix
Pdescribes the stretching or compression of the material (in
the directions of the eigenvectors of P), and Qdescribes the
rotation of the material in space.
SECOND REVISED PAGES


--- Page 452 ---
Chapter 7 Supplementary Exercises 435
Exercises 12‚Äì14 concern an mnmatrix Awith a reduced sin-
gular value decomposition, ADUrDVT
r, and the pseudoinverse
ACDVrD 1UT
r.
12. Verify the properties of AC:
a.For each yinRm,AACyis the orthogonal projection of
yonto Col A.
b.For each xinRn,ACAxis the orthogonal projection of x
onto Row A.
c.AACADAandACAACDAC.
13. Suppose the equation AxDbis consistent, and let
xCDACb. By Exercise 23 in Section 6.3, there is exactly
one vector pin Row Asuch that ApDb. The following steps
prove that xCDpandxCis the minimum length solution of
AxDb.
a.Show that xCis in Row A. [Hint: Write basAxfor some
x, and use Exercise 12.]
b.Show that xCis a solution of AxDb.
c.Show that if uis any solution of AxDb, then
kxCk  k uk, with equality only if uDxC.14. Given any binRm, adapt Exercise 13 to show that ACbis the
least-squares solution of minimum length . [Hint: Consider
the equation AxDOb, where Obis the orthogonal projection
ofbonto Col A.]
[M] In Exercises 15 and 16, construct the pseudoinverse of A. Be-
gin by using a matrix program to produce the SVD of A, or, if that
is not available, begin with an orthogonal diagonalization of ATA.
Use the pseudoinverse to solve AxDb, for bD.6; 1; 4; 6/,
and let Oxbe the solution. Make a calculation to verify that Ox
is in Row A. Find a nonzero vector uin Nul A, and verify that
kOxk<kOxCuk, which must be true by Exercise 13(c).
15.AD2
664 3 3 6 6 1
 1 1 1 1  2
0 0  1 1  1
0 0  1 1  13
775
16.AD2
6644 0  1 2 0
 5 0 3 5 0
2 0  1 2 0
6 0  3 6 03
775
SECOND REVISED PAGES


--- Page 453 ---
SECOND REVISED PAGES


--- Page 454 ---
8The Geometry of
Vector Spaces
INTRODUCTORY EXAMPLE
The Platonic Solids
In the city of Athens in 387 B.C., the Greek philosopher
Plato founded an Academy, sometimes referred to as the
world‚Äôs Ô¨Årst university. While the curriculum included
astronomy, biology, political theory, and philosophy, the
subject closest to his heart was geometry. Indeed, inscribed
over the doors of his academy were these words: ‚ÄúLet no
one destitute of geometry enter my doors.‚Äù
The Greeks were greatly impressed by geometric
patterns such as the regular solids. A polyhedron is called
regular if its faces are congruent regular polygons and all
the angles at the vertices are equal. As early as 100 years
before Plato, the Pythagoreans knew at least three of the
regular solids: the tetrahedron (4 triangular faces), the cube
(6 square faces), and the octahedron (8 triangular faces).
(See Figure 1.) These shapes occur naturally as crystals of
common minerals. There are only Ô¨Åve such regular solids,
the remaining two being the dodecahedron (12 pentagonal
faces) and the icosahedron (20 triangular faces).
Plato discussed the basic theory of these Ô¨Åve solids in
the dialogue Timaeus , and since then they have carried his
name: the Platonic solids.
For centuries there was no need to envision geometric
objects in more than three dimensions. But nowadays
mathematicians regularly deal with objects in vector spaceshaving four, Ô¨Åve, or even hundreds of dimensions. It is not
necessarily clear what geometrical properties one might
ascribe to these objects in higher dimensions.
For example, what properties do lines have in
2-space and planes have in 3-space that would be useful
in higher dimensions? How can one characterize such
objects? Sections 8.1 and 8.4 provide some answers.
The hyperplanes of Section 8.4 will be important for
understanding the multidimensional nature of the linear
programming problems in Chapter 9.
What would the analogue of a polyhedron ‚Äúlook
like‚Äù in more than three dimensions? A partial answer
is provided by two-dimensional projections of the four-
dimensional object, created in a manner analogous to two-
dimensional projections of a three-dimensional object.
Section 8.5 illustrates this idea for the four-dimensional
‚Äúcube‚Äù and the four-dimensional ‚Äúsimplex.‚Äù
The study of geometry in higher dimensions not
only provides new ways of visualizing abstract algebraic
concepts, but also creates tools that may be applied in R3.
For instance, Sections 8.2 and 8.6 include applications to
computer graphics, and Section 8.5 outlines a proof (in
Exercise 22) that there are only Ô¨Åve regular polyhedra in
R3.
SECOND REVISED PAGES
437

--- Page 455 ---
438 CHAPTER 8 The Geometry of Vector Spaces
FIGURE 1 The Ô¨Åve Platonic solids.
Most applications in earlier chapters involved algebraic calculations with subspaces and
linear combinations of vectors. This chapter studies sets of vectors that can be visualized
as geometric objects such as line segments, polygons, and solid objects. Individual
vectors are viewed as points. The concepts introduced here are used in computer graph-
ics, linear programming (in Chapter 9), and other areas of mathematics.1
Throughout the chapter, sets of vectors are described by linear combinations, but
with various restrictions on the weights used in the combinations. For instance, in Sec-
tion 8.1, the sum of the weights is 1, while in Section 8.2, the weights are positive and
sum to 1. The visualizations are in R2orR3, of course, but the concepts also apply to
Rnand other vector spaces.
8.1 AFFINE COMBINATIONS
An afÔ¨Åne combination of vectors is a special kind of linear combination. Given vec-
tors (or ‚Äúpoints‚Äù) v1;v2; : : : ; vpinRnand scalars c1; : : : ; c p, anafÔ¨Åne combination of
v1;v2; : : : ; vpis a linear combination
c1v1C  C cpvp
such that the weights satisfy c1C  C cpD1.
1See Foley, van Dam, Feiner, and Hughes, Computer Graphics‚ÄîPrinciples and Practice , 2nd edition
(Boston: Addison-Wesley, 1996), pp. 1083‚Äì1112. That material also discusses coordinate-free ‚ÄúafÔ¨Åne
spaces.‚Äù
SECOND REVISED PAGES


--- Page 456 ---
8.1 Affine Combinations 439
D E F I N I T I O N The set of all afÔ¨Åne combinations of points in a set Sis called the afÔ¨Åne hull (or
afÔ¨Åne span ) ofS, denoted by aff S.
The afÔ¨Åne hull of a single point v1is just the set fv1g, since it has the form c1v1where
c1D1. The afÔ¨Åne hull of two distinct points is often written in a special way. Suppose
yDc1v1Cc2v2withc1Cc2D1. Write tin place of c2, so that c1D1 c2D1 t.
Then the afÔ¨Åne hull of fv1;v2gis the set
yD.1 t/v1Ctv2;withtinR (1)
This set of points includes v1(when tD0) and v2(when tD1). Ifv2Dv1, then (1)
again describes just one point. Otherwise, (1) describes the linethrough v1andv2. To
see this, rewrite (1) in the form
yDv1Ct.v2 v1/DpCtu;withtinR
where pisv1anduisv2 v1. The set of all multiples of uis Span fug, the line through
uand the origin. Adding pto each point on this line translates Span fuginto the line
through pparallel to the line through uand the origin. See Figure 1. (Compare this
Ô¨Ågure with Figure 5 in Section 1.5.)
tup + tu
p
u
FIGURE 1
Figure 2 uses the original points v1andv2, and displays aff fv1;v2gas the line
through v1andv2.
v2
t(v2 ‚Äì v1)aff{v1, v2}y = v1 + t(v2 ‚Äì v1)
v1
v2 ‚Äì v1
FIGURE 2
Notice that while the point yin Figure 2 is an afÔ¨Åne combination of v1andv2, the
point y v1equals t.v2 v1/, which is a linear combination (in fact, a multiple) of
v2 v1. This relation between yandy v1holds for any afÔ¨Åne combination of points,
as the following theorem shows.
T H E O R E M 1 A point yinRnis an afÔ¨Åne combination of v1; : : : ; vpinRnif and only if y v1
is a linear combination of the translated points v2 v1; : : : ; vp v1:
SECOND REVISED PAGES


--- Page 457 ---
440 CHAPTER 8 The Geometry of Vector Spaces
PROOF Ify v1is a linear combination of v2 v1; : : : ; vp v1;there exist weights
c2; : : : ; c psuch that
y v1Dc2.v2 v1/C  C cp.vp v1/ (2)
Then
yD.1 c2     cp/v1Cc2v2C  C cpvp (3)
and the weights in this linear combination sum to 1. So yis an afÔ¨Åne combination of
v1; : : : ; vp. Conversely, suppose
yDc1v1Cc2v2C  C cpvp (4)
where c1C  C cpD1. Since c1D1 c2     cp, equation (4) may be written
as in (3), and this leads to (2), which shows that y v1is a linear combination of
v2 v1; : : : ; vp v1:
In the statement of Theorem 1, the point v1could be replaced by any of the other
points in the list v1; : : : ; vp:Only the notation in the proof would change.
EXAMPLE 1 Letv1D1
2
,v2D2
5
,v3D1
3
,v4D 2
2
, and yD4
1
.
If possible, write yas an afÔ¨Åne combination of v1;v2;v3, and v4.
SOLUTION Compute the translated points
v2 v1D1
3
;v3 v1D0
1
;v4 v1D 3
0
;y v1D3
 1
To Ô¨Ånd scalars c2,c3, and c4such that
c2.v2 v1/Cc3.v3 v1/Cc4.v4 v1/Dy v1 (5)
row reduce the augmented matrix having these points as columns:
1 0  3 3
3 1 0  1
1 0  3 3
0 1 9  10
This shows that equation (5) is consistent, and the general solution is c2D3c4C3,
c3D  9c4 10, with c4free. When c4D0,
y v1D3.v2 v1/ 10.v3 v1/C0.v4 v1/
and
yD8v1C3v2 10v3
As another example, take c4D1. Then c2D6andc3D  19, so
y v1D6.v2 v1/ 19.v3 v1/C1.v4 v1/
and
yD13v1C6v2 19v3Cv4
While the procedure in Example 1 works for arbitrary points v1;v2; : : : ; vpinRn,
the question can be answered more directly if the chosen points viare a basis for Rn.
For example, let BD fb1; : : : ; bngbe such a basis. Then any yinRnis a unique linear
combination of b1; : : : ; bn. This combination is an afÔ¨Åne combination of the b‚Äôs if and
only if the weights sum to 1. (These weights are just the B-coordinates of y, as in
Section 4.4.)
SECOND REVISED PAGES


--- Page 458 ---
8.1 Affine Combinations 441
EXAMPLE 2 Letb1D2
44
0
33
5,b2D2
40
4
23
5,b3D2
45
2
43
5,p1D2
42
0
03
5, and p2D2
41
2
23
5.
The set BD fb1;b2;b3gis a basis for R3. Determine whether the points p1andp2are
afÔ¨Åne combinations of the points in B.
SOLUTION Find the B-coordinates of p1andp2. These two calculations can be com-
bined by row reducing the matrix ¬åb1b2b3p1p2¬ç, with two augmented columns:
2
44 0 5 2 1
0 4 2 0 2
3 2 4 0 23
52
66641 0 0  22
3
0 1 0  12
3
0 0 1 2  1
33
7775
Read column 4 to build p1, and read column 5 to build p2:
p1D  2b1 b2C2b3and p2D2
3b1C2
3b2 1
3b3
The sum of the weights in the linear combination for p1is 1, not 1, sop1isnotan
afÔ¨Åne combination of the b‚Äôs. However, p2isan afÔ¨Åne combination of the b‚Äôs, because
the sum of the weights for p2is1.
D E F I N I T I O N A set SisafÔ¨Åne ifp;q2Simplies that .1 t/pCtq2Sfor each real number t.
Geometrically, a set is afÔ¨Åne if whenever two points are in the set, the entire line
through these points is in the set. (If Scontains only one point, p, then the line through
pandpis just a point, a ‚Äúdegenerate‚Äù line.) Algebraically, for a set Sto be afÔ¨Åne,
the deÔ¨Ånition requires that every afÔ¨Åne combination of two points of Sbelong to S.
Remarkably, this is equivalent to requiring that Scontain every afÔ¨Åne combination of
an arbitrary number of points of S.
T H E O R E M 2 A set Sis afÔ¨Åne if and only if every afÔ¨Åne combination of points of Slies in S.
That is, Sis afÔ¨Åne if and only if SDaffS.
Remark: See the remark prior to Theorem 5 in Chapter 3 regarding mathematical indu-
ction.
PROOF Suppose that Sis afÔ¨Åne and use induction on the number mof points of S
occurring in an afÔ¨Åne combination. When mis 1 or 2, an afÔ¨Åne combination of mpoints
ofSlies in S, by the deÔ¨Ånition of an afÔ¨Åne set. Now, assume that every afÔ¨Åne combina-
tion of kor fewer points of Syields a point in S, and consider a combination of kC1
points. Take viinSforiD1; : : : ; k C1, and let yDc1v1C  C ckvkCckC1vkC1,
where c1C  C ckC1D1. Since the ci‚Äôs sum to 1, at least one of them must not be
equal to 1. By reindexing the viandci, if necessary, we may assume that ckC1¬§1. Let
tDc1C  C ck. Then tD1 ckC1¬§0, and
yD.1 ckC1/c1
tv1C  Cck
tvk
CckC1vkC1 (6)
By the induction hypothesis, the point zD.c1=t/v1C  C .ck=t/vkis inS, since the
coefÔ¨Åcients sum to 1. Thus (6) displays yas an afÔ¨Åne combination of two points in S,
and so y2S. By the principle of induction, every afÔ¨Åne combination of such points
lies in S. That is, aff SS. But the reverse inclusion, SaffS, always applies. Thus,
when Sis afÔ¨Åne, SDaffS. Conversely, if SDaffS, then afÔ¨Åne combinations of two
(or more) points of Slie in S, soSis afÔ¨Åne.
SECOND REVISED PAGES


--- Page 459 ---
442 CHAPTER 8 The Geometry of Vector Spaces
The next deÔ¨Ånition provides terminology for afÔ¨Åne sets that emphasizes their close
connection with subspaces of Rn.
D E F I N I T I O N A translate of a set SinRnby a vector pis the set SCpD fsCpWs2Sg.2AÔ¨Çat
inRnis a translate of a subspace of Rn. Two Ô¨Çats are parallel if one is a translate of
the other. The dimension of a Ô¨Çat is the dimension of the corresponding parallel
subspace. The dimension of a set S, written as dim S, is the dimension of the
smallest Ô¨Çat containing S. AlineinRnis a Ô¨Çat of dimension 1. A hyperplane in
Rnis a Ô¨Çat of dimension n 1.
InR3, the proper subspaces3consist of the origin 0, the set of all lines through 0, and
the set of all planes through 0. Thus the proper Ô¨Çats in R3are points (zero-dimensional),
lines (one-dimensional), and planes (two-dimensional), which may or may not pass
through the origin.
The next theorem shows that these geometric descriptions of lines and planes in R3
(as translates of subspaces) actually coincide with their earlier algebraic descriptions as
sets of all afÔ¨Åne combinations of two or three points, respectively.
T H E O R E M 3 A nonempty set Sis afÔ¨Åne if and only if it is a Ô¨Çat.
Remark: Notice the key role that deÔ¨Ånitions play in this proof. For example, the Ô¨Årst
part assumes that Sis afÔ¨Åne and seeks to show that Sis a Ô¨Çat. By deÔ¨Ånition, a Ô¨Çat is a
translate of a subspace. By choosing pinSand deÔ¨Åning WDSC. p/, the set Sis
translated to the origin and SDWCp. It remains to show that Wis a subspace, for
thenSwill be a translate of a subspace and hence a Ô¨Çat.
PROOF Suppose that Sis afÔ¨Åne. Let pbe any Ô¨Åxed point in Sand let WDSC. p/,
so that SDWCp. To show that Sis a Ô¨Çat, it sufÔ¨Åces to show that Wis a subspace of
Rn. Since pis inS, the zero vector is in W. To show that Wis closed under sums and
scalar multiples, it sufÔ¨Åces to show that if u1andu2are elements of W, then u1Ctu2
is inWfor every real t. Since u1andu2are in W, there exist s1ands2inSsuch that
u1Ds1 pandu2Ds2 p. So, for each real t,
u1Ctu2D.s1 p/Ct.s2 p/
D.1 t/s1Ct.s1Cs2 p/ p
LetyDs1Cs2 p. Then yis an afÔ¨Åne combination of points in S. Since Sis afÔ¨Åne, yis
inS(by Theorem 2). But then .1 t/s1Ctyis also in S. Sou1Ctu2is in pCSDW.
This shows that Wis a subspace of Rn. Thus Sis a Ô¨Çat, because SDWCp.
Conversely, suppose Sis a Ô¨Çat. That is, SDWCpfor some p2Rnand some
subspace W. To show that Sis afÔ¨Åne, it sufÔ¨Åces to show that for any pair s1ands2of
points in S, the line through s1ands2lies in S. By deÔ¨Ånition of W, there exist u1and
u2inWsuch that s1Du1Cpands2Du2Cp. So, for each real t,
.1 t/s1Cts2D.1 t/.u1Cp/Ct.u2Cp/
D.1 t/u1Ctu2Cp
Since Wis a subspace, .1 t/u1Ctu22Wand so .1 t/s1Cts22WCpDS.
Thus Sis afÔ¨Åne.
2IfpD0, then the translate is just Sitself. See Figure 4 in Section 1.5.
3A subset Aof a set Bis called a proper subset of BifA6DB. The same condition applies to proper
subspaces and proper Ô¨Çats in Rn: they are not equal to Rn.
SECOND REVISED PAGES


--- Page 460 ---
8.1 Affine Combinations 443
Theorem 3 provides a geometric way to view the afÔ¨Åne hull of a set: it is the Ô¨Çat that
x3
x1x2p1p25
55b3b2b1
FIGURE 3consists of all the afÔ¨Åne combinations of points in the set. For instance, Figure 3 shows
the points studied in Example 2. Although the set of all linear combinations of b1,b2,
andb3is all of R3, the set of all afÔ¨Åne combinations is only the plane through b1,b2,
andb3. Note that p2(from Example 2) is in the plane through b1,b2, and b3, while p1
is not in that plane. Also, see Exercise 14.
The next example takes a fresh look at a familiar set‚Äîthe set of all solutions of a
system AxDb.
EXAMPLE 3 Suppose that the solutions of an equation AxDbare all of the form
xDx3uCp, where uD2
42
 3
13
5andpD2
44
0
 33
5. Recall from Section 1.5 that this set
is parallel to the solution set of AxD0, which consists of all points of the form x3u.
Find points v1andv2such that the solution set of AxDbis afffv1;v2g.
SOLUTION The solution set is a line through pin the direction of u, as in Figure 1. Since
afffv1;v2gis a line through v1andv2, identify two points on the line xDx3uCp. Two
simple choices appear when x3D0andx3D1. That is, take v1Dpandv2DuCp,
so that
v2DuCpD2
42
 3
13
5C2
44
0
 33
5D2
46
 3
 23
5:
In this case, the solution set is described as the set of all afÔ¨Åne combinations of the form
xD.1 x3/2
44
0
 33
5Cx32
46
 3
 23
5:
Earlier, Theorem 1 displayed an important connection between afÔ¨Åne combinations
and linear combinations. The next theorem provides another view of afÔ¨Åne combina-
tions, which for R2andR3is closely connected to applications in computer graphics,
discussed in the next section (and in Section 2.7).
D E F I N I T I O N ForvinRn, the standard homogeneous form ofvis the point QvDv
1
inRnC1.
T H E O R E M 4 A point yinRnis an afÔ¨Åne combination of v1; : : : ; vpinRnif and only if the
homogeneous form of yis in Span fQv1; : : : ;Qvpg. In fact, yDc1v1C  C cpvp,
withc1C  C cpD1, if and only if QyDc1Qv1C  C cpQvp.
PROOF A point yis in aff fv1; : : : ; vpgif and only if there exist weights c1; : : : ; c psuch
that y
1
Dc1v1
1
Cc2v2
1
C  C cpvp
1
This happens if and only if Qyis in Span fQv1;Qv2; : : : ;Qvpg.
EXAMPLE 4 Letv1D2
43
1
13
5,v2D2
41
2
23
5,v3D2
41
7
13
5, and pD2
44
3
03
5. Use Theo-
rem 4 to write pas an afÔ¨Åne combination of v1,v2, and v3, if possible.
SECOND REVISED PAGES


--- Page 461 ---
444 CHAPTER 8 The Geometry of Vector Spaces
SOLUTION Row reduce the augmented matrix for the equation
x1Qv1Cx2Qv2Cx3Qv3DQp
To simplify the arithmetic, move the fourth row of 1‚Äôs to the top (equivalent to three
row interchanges). After this, the number of arithmetic operations here is basically the
same as the number needed for the method using Theorem 1.
¬åQv1Qv2Qv3Qp¬ç2
6641 1 1 1
3 1 1 4
1 2 7 3
1 2 1 03
7752
6641 1 1 1
0 2 2 1
0 1 6 2
0 1 0  13
775
  2
6641 0 0 1:5
0 1 0  1
0 0 1 :5
0 0 0 03
775
By Theorem 4, 1:5v1 v2C:5v3Dp. See Figure 4, which shows the plane that con-
tains v1,v2,v3, and p(together with points on the coordinate axes).
15 53x3
v2
v1 v3
p
x1x2
FIGURE 4
PRACTICE PROBLEM
Plot the points v1D1
0
,v2D 1
2
,v3D3
1
, and pD4
3
on graph paper, and
explain why pmust be an afÔ¨Åne combination of v1,v2, and v3. Then Ô¨Ånd the afÔ¨Åne
combination for p. [Hint: What is the dimension of aff fv1,v2,v3g¬ã¬ç
8.1 EXERCISES
In Exercises 1‚Äì4, write yas an afÔ¨Åne combination of the other
points listed, if possible.
1.v1D1
2
,v2D 2
2
,v3D0
4
,v4D3
7
,yD5
3
2.v1D1
1
,v2D 1
2
,v3D3
2
,yD5
73.v1D2
4 3
1
13
5,v2D2
40
4
 23
5,v3D2
44
 2
63
5,yD2
417
1
53
5
4.v1D2
41
2
03
5,v2D2
42
 6
73
5,v3D2
44
3
13
5,yD2
4 3
4
 43
5
SECOND REVISED PAGES


--- Page 462 ---
8.1 Affine Combinations 445
In Exercises 5 and 6, let b1D2
42
1
13
5,b2D2
41
0
 23
5,b3D2
42
 5
13
5,
andSD fb1;b2;b3g. Note that Sis an orthogonal basis for R3.
Write each of the given points as an afÔ¨Åne combination of the
points in the set S, if possible. [ Hint: Use Theorem 5 in Section
6.2 instead of row reduction to Ô¨Ånd the weights.]
5.a.p1D2
43
8
43
5 b.p2D2
46
 3
33
5 c.p3D2
40
 1
 53
5
6.a.p1D2
40
 19
 53
5 b.p2D2
41:5
 1:3
 :53
5c.p3D2
45
 4
03
5
7.Let
v1D2
6641
0
3
03
775; v2D2
6642
 1
0
43
775;v3D2
664 1
2
1
13
775;
p1D2
6645
 3
5
33
775;p2D2
664 9
10
9
 133
775;p3D2
6644
2
8
53
775;
andSD fv1;v2;v3g. It can be shown that Sis linearly
independent.
a.Isp1in Span S? Isp1in aff S?
b.Isp2in Span S? Isp2in aff S?
c.Isp3in Span S? Isp3in aff S?
8.Repeat Exercise 7 when
v1D2
6641
0
3
 23
775;v2D2
6642
1
6
 53
775;v3D2
6643
0
12
 63
775;
p1D2
6644
 1
15
 73
775;p2D2
664 5
3
 8
63
775;and p3D2
6641
6
 6
 83
775:
9.Suppose that the solutions of an equation AxDbare all of
the form xDx3uCp, where uD4
 2
andpD 3
0
.
Find points v1andv2such that the solution set of AxDbis
afffv1;v2g.
10. Suppose that the solutions of an equation AxDbare all of
the form xDx3uCp, where uD2
45
1
 23
5andpD2
41
 3
43
5.
Find points v1andv2such that the solution set of AxDbis
afffv1;v2g.
In Exercises 11 and 12, mark each statement True or False. Justify
each answer.11.a.The set of all afÔ¨Åne combinations of points in a set Sis
called the afÔ¨Åne hull of S.
b.Iffb1; : : : ; bkgis a linearly independent subset of Rnand
ifpis a linear combination of b1; : : : ; bk, then pis an
afÔ¨Åne combination of b1; : : : ; bk.
c.The afÔ¨Åne hull of two distinct points is called a line.
d.A Ô¨Çat is a subspace.
e.A plane in R3is a hyperplane.
12. a.IfSD fxg, then aff Sis the empty set.
b.A set is afÔ¨Åne if and only if it contains its afÔ¨Åne hull.
c.A Ô¨Çat of dimension 1 is called a line.
d.A Ô¨Çat of dimension 2 is called a hyperplane.
e.A Ô¨Çat through the origin is a subspace.
13. Suppose fv1;v2;v3gis a basis for R3. Show that
Spanfv2 v1;v3 v1gis a plane in R3. [Hint: What can
you say about uandvwhen Span fu;vgis a plane?]
14. Show that if fv1;v2;v3gis a basis for R3, then aff fv1;v2;v3g
is the plane through v1,v2, and v3.
15. LetAbe an mnmatrix and, given binRm, show that the
setSof all solutions of AxDbis an afÔ¨Åne subset of Rn.
16. Letv2Rnand let k2R. Prove that SD fx2RnWxvDkg
is an afÔ¨Åne subset of Rn.
17. Choose a set Sof three points such that aff Sis the plane in
R3whose equation is x3D5. Justify your work.
18. Choose a set Sof four distinct points in R3such that aff Sis
the plane 2x1Cx2 3x3D12. Justify your work.
19. LetSbe an afÔ¨Åne subset of Rn, suppose fWRn!Rmis a
linear transformation, and let f .S/ denote the set of images
ff .x/Wx2Sg. Prove that f .S/ is an afÔ¨Åne subset of Rm.
20. LetfWRn!Rmbe a linear transformation, let Tbe an
afÔ¨Åne subset of Rm, and let SD fx2RnWf .x/2Tg. Show
thatSis an afÔ¨Åne subset of Rn.
In Exercises 21‚Äì26, prove the given statement about subsets A
andBofRn, or provide the required example in R2. A proof
for an exercise may use results from earlier exercises (as well as
theorems already available in the text).
21. IfABandBis afÔ¨Åne, then aff AB.
22. IfAB, then aff AaffB.
23.¬å.affA/[.affB/¬çaff.A[B/. [Hint: To show that
D[EF, show that DFandEF.]
24. Find an example in R2to show that equality need not hold in
the statement of Exercise 23. [ Hint: Consider sets AandB,
each of which contains only one or two points.]
25. aff.A\B/.affA\affB/.
26. Find an example in R2to show that equality need not hold in
the statement of Exercise 25.
SECOND REVISED PAGES


--- Page 463 ---
446 CHAPTER 8 The Geometry of Vector Spaces
SOLUTION TO PRACTICE PROBLEM
Since the points v1,v2, and v3are not collinear (that is, not on a single line),
x1x2
p
v3
v1v2
afffv1;v2;v3gcannot be one-dimensional. Thus, aff fv1;v2;v3gmust equal R2. To Ô¨Ånd
the actual weights used to express pas an afÔ¨Åne combination of v1,v2, and v3, Ô¨Årst
compute
v2 v1D 2
2
;v3 v1D2
1
;and p v1D3
3
To write p v1as a linear combination of v2 v1andv3 v1, row reduce the matrix
having these points as columns:
 2 2 3
2 1 3
"
1 01
2
0 1 2#
Thus p v1D1
2.v2 v1/C2.v3 v1/, which shows that
pD 
1 1
2 2
v1C1
2v2C2v3D  3
2v1C1
2v2C2v3
This expresses pas an afÔ¨Åne combination of v1,v2, and v3, because the coefÔ¨Åcients sum
to 1.
Alternatively, use the method of Example 4 and row reduce:
v1 v2 v3 p
1 1 1 1
2
41 1 1 1
1 1 3 4
0 2 1 33
52
641 0 0  3
2
0 1 01
2
0 0 1 23
75
This shows that pD  3
2v1C1
2v2C2v3.
8.2 AFFINE INDEPENDENCE
This section continues to explore the relation between linear concepts and afÔ¨Åne con-
cepts. Consider Ô¨Årst a set of three vectors in R3, say SD fv1;v2;v3g. IfSis linearly
dependent, then one of the vectors is a linear combination of the other two vectors. What
happens when one of the vectors is an afÔ¨Åne combination of the others? For instance,
suppose that
v3D.1 t/v1Ctv2;for some tinR.
Then
.1 t/v1Ctv2 v3D0:
This is a linear dependence relation because not all the weights are zero. But more is
true‚Äîthe weights in the dependence relation sum to 0:
.1 t/CtC. 1/D0:
This is the additional property needed to deÔ¨Åne afÔ¨Åne dependence .
D E F I N I T I O N An indexed set of points fv1; : : : ; vpginRnisafÔ¨Ånely dependent if there exist
real numbers c1; : : : ; c p, not all zero, such that
c1C  C cpD0and c1v1C  C cpvpD0 (1)
Otherwise, the set is afÔ¨Ånely independent .
SECOND REVISED PAGES


--- Page 464 ---
8.2 Affine Independence 447
An afÔ¨Åne combination is a special type of linear combination, and afÔ¨Åne depen-
dence is a restricted type of linear dependence. Thus, each afÔ¨Ånely dependent set is
automatically linearly dependent.
A setfv1gof only one point (even the zero vector) must be afÔ¨Ånely independent
because the required properties of the coefÔ¨Åcients cicannot be satisÔ¨Åed when there is
only one coefÔ¨Åcient. For fv1g, the Ô¨Årst equation in (1) is just c1D0, and yet at least one
(the only one) coefÔ¨Åcient must be nonzero.
Exercise 13 asks you to show that an indexed set fv1;v2gis afÔ¨Ånely dependent if
and only if v1Dv2. The following theorem handles the general case and shows how
the concept of afÔ¨Åne dependence is analogous to that of linear dependence. Parts (c) and
(d) give useful methods for determining whether a set is afÔ¨Ånely dependent. Recall from
Section 8.1 that if vis inRn, then the vector QvinRnC1denotes the homogeneous form
ofv.
T H E O R E M 5 Given an indexed set SD fv1; : : : ; vpginRn, with p2, the following state-
ments are logically equivalent. That is, either they are all true statements or they
are all false.
a.Sis afÔ¨Ånely dependent.
b.One of the points in Sis an afÔ¨Åne combination of the other points in S.
c.The set fv2 v1; : : : ; vp v1ginRnis linearly dependent.
d. The set fQv1; : : : ;Qvpgof homogeneous forms in RnC1is linearly dependent.
PROOF Suppose statement (a) is true, and let c1; : : : ; c psatisfy (1). By renaming the
points if necessary, one may assume that c1¬§0and divide both equations in (1) by c1,
so that 1C.c2=c1/C  C .cp=c1/D0and
v1D. c2=c1/v2C  C . cp=c1/vp (2)
Note that the coefÔ¨Åcients on the right side of (2) sum to 1. Thus (a) implies (b). Now,
suppose that (b) is true. By renaming the points if necessary, one may assume that
v1Dc2v2C  C cpvp, where c2C  C cpD1. Then
.c2C  C cp/v1Dc2v2C  C cpvp (3)
and
c2.v2 v1/C  C cp.vp v1/D0 (4)
Not all of c2; : : : ; c pcan be zero because they sum to 1. So (b) implies (c).
Next, if (c) is true, then there exist weights c2; : : : ; c p, not all zero, such that (4)
holds. Rewrite (4) as (3) and set c1D  .c2C  C cp/. Then c1C  C cpD0. Thus
(3) shows that (1) is true. So (c) implies (a), which proves that (a), (b), and (c) are
logically equivalent. Finally, (d) is equivalent to (a) because the two equations in (1)
are equivalent to the following equation involving the homogeneous forms of the points
inS:
c1v1
1
C  C cpvp
1
D0
0
In statement (c) of Theorem 5, v1could be replaced by any of the other points in
the list v1; : : : ; vp. Only the notation in the proof would change. So, to test whether a
set is afÔ¨Ånely dependent, subtract one point in the set from the other points, and check
whether the translated set of p 1points is linearly dependent.
SECOND REVISED PAGES


--- Page 465 ---
448 CHAPTER 8 The Geometry of Vector Spaces
EXAMPLE 1 The afÔ¨Åne hull of two distinct points pandqis a line. If a third point
ris on the line, then fp;q;rgis an afÔ¨Ånely dependent set. If a point sis not on the line
through pandq, then these three points are not collinear and fp;q;sgis an afÔ¨Ånely
independent set. See Figure 1.
paff{p, q}
q
r s
FIGURE 1 fp;q;rgis afÔ¨Ånely dependent.
EXAMPLE 2 Letv1D2
41
3
73
5,v2D2
42
7
6:53
5,v3D2
40
4
73
5, and SD fv1;v2;v3g.
Determine whether Sis afÔ¨Ånely independent.
SOLUTION Compute v2 v1D2
41
4
 :53
5andv3 v1D2
4 1
1
03
5. These two points are
not multiples and hence form a linearly independent set, S0. So all statements in Theorem
5 are false, and Sis afÔ¨Ånely independent. Figure 2 shows Sand the translated set S0.
Notice that Span S0is a plane through the origin and aff Sis a parallel plane through v1,
v2, and v3. (Only a portion of each plane is shown here, of course.)
x3
v1
v2v3
x1
x2v2 /H11002 v1aff{v1, v2, v3}
Span{ v2 /H11002 v1, v3 /H11002 v1}v3 /H11002 v1
FIGURE 2 An afÔ¨Ånely independent set
fv1;v2;v3g.
EXAMPLE 3 Letv1D2
41
3
73
5,v2D2
42
7
6:53
5,v3D2
40
4
73
5, and v4D2
40
14
63
5, and let
SD fv1; : : : ; v4g. IsSafÔ¨Ånely dependent?
SOLUTION Compute v2 v1D2
41
4
 :53
5,v3 v1D2
4 1
1
03
5, and v4 v1D2
4 1
11
 13
5,
and row reduce the matrix:2
41 1 1
4 1 11
 :5 0  13
52
41 1 1
0 5 15
0 :5 1:53
52
41 1 1
0 5 15
0 0 03
5
Recall from Section 4.6 (or Section 2.8) that the columns are linearly dependent be-
cause not every column is a pivot column; so v2 v1;v3 v1, and v4 v1are linearly
SECOND REVISED PAGES


--- Page 466 ---
8.2 Affine Independence 449
dependent. By statement (c) in Theorem 5, fv1;v2;v3;v4gis afÔ¨Ånely dependent. This
dependence can also be established using (d) in Theorem 5 instead of (c).
The calculations in Example 3 show that v4 v1is a linear combination of v2 v1
andv3 v1, which means that v4 v1is in Span fv2 v1;v3 v1g. By Theorem 1 in
Section 8.1, v4is in aff fv1;v2;v3g. In fact, complete row reduction of the matrix in
Example 3 would show that
v4 v1D2.v2 v1/C3.v3 v1/ (5)
v4D  4v1C2v2C3v3 (6)
See Figure 3.
v3 /H11002 v1
v4 /H11002 v1v1v3x3
v1
v2v3
x1v2 /H11002 v1aff{v1, v2, v3}v4
x2
FIGURE 3 v4is in the plane aff fv1;v2;v3g.
Figure 3 shows grids on both Span fv2 v1;v3 v1gand aff fv1;v2;v3g. The grid
on afffv1;v2;v3gis based on (5). Another ‚Äúcoordinate system‚Äù can be based on (6), in
which the coefÔ¨Åcients  4, 2, and 3 are called afÔ¨Åne orbarycentric coordinates of v4.
Barycentric Coordinates
The deÔ¨Ånition of barycentric coordinates depends on the following afÔ¨Åne version of the
Unique Representation Theorem in Section 4.4. See Exercise 17 in this section for the
proof.
T H E O R E M 6 LetSD fv1; : : : ; vkgbe an afÔ¨Ånely independent set in Rn. Then each pin aff S
has a unique representation as an afÔ¨Åne combination of v1; : : : ; vk. That is, for
each pthere exists a unique set of scalars c1; : : : ; c ksuch that
pDc1v1C  C ckvkand c1C  C ckD1 (7)
D E F I N I T I O N LetSD fv1; : : : ; vkgbe an afÔ¨Ånely independent set. Then for each point pin
affS, the coefÔ¨Åcients c1; : : : ; c kin the unique representation (7) of pare called
thebarycentric (or, sometimes, afÔ¨Åne )coordinates ofp.
Observe that (7) is equivalent to the single equationp
1
Dc1v1
1
C  C ckvk
1
(8)
involving the homogeneous forms of the points. Row reduction of the augmented matrixQv1QvkQp
for (8) produces the barycentric coordinates of p.
SECOND REVISED PAGES


--- Page 467 ---
450 CHAPTER 8 The Geometry of Vector Spaces
EXAMPLE 4 LetaD1
7
,bD3
0
,cD9
3
, and pD5
3
. Find the barycen-
tric coordinates of pdetermined by the afÔ¨Ånely independent set fa;b;cg.
SOLUTION Row reduce the augmented matrix of points in homogeneous form, moving
the last row of ones to the top to simplify the arithmetic:

QaQbQcQp
D2
41 3 9 5
7 0 3 3
1 1 1 13
52
41 1 1 1
1 3 9 5
7 0 3 33
5
2
6641 0 01
4
0 1 01
3
0 0 15
123
775
The coordinates are1
4,1
3, and5
12, sopD1
4aC1
3bC5
12c.
Barycentric coordinates have both physical and geometric interpretations. They
were originally deÔ¨Åned by A. F. Moebius in 1827 for a point pinside a triangular
region with vertices a,b, and c. He wrote that the barycentric coordinates of pare
three nonnegative numbers ma; mb, and mcsuch that pis the center of mass of a system
consisting of the triangle (with no mass) and masses ma,mb, andmcat the corresponding
vertices. The masses are uniquely determined by requiring that their sum be 1. This view
is still useful in physics today.1
Figure 4 gives a geometric interpretation to the barycentric coordinates in Example
4, showing the triangle ¬Åabcand three small triangles ¬Åpbc,¬Åapc, and ¬Åabp. The
areas of the small triangles are proportional to the barycentric coordinates of p. In fact,
area.¬Åpbc/D1
4area.¬Åabc/
area.¬Åapc/D1
3area.¬Åabc/
area.¬Åabp/D5
12area.¬Åabc/(9)
a
bcp area /H11005 t ¬∑ area( Œîabc)area /H11005 s ¬∑ area( Œîabc)
area /H11005 r ¬∑ area( Œîabc)
FIGURE 4 pDraCsbCtc. Here, rD1
4,
sD1
3,tD5
12.
The formulas in Figure 4 are veriÔ¨Åed in Exercises 21‚Äì23. Analogous equalities for
volumes of tetrahedrons hold for the case when pis a point inside a tetrahedron in R3,
with vertices a,b,c, and d.
1See Exercise 29 in Section 1.3. In astronomy, however, ‚Äúbarycentric coordinates‚Äù usually refer to ordinary
R3coordinates of points in what is now called the International Celestial Reference System , a Cartesian
coordinate system for outer space, with the origin at the center of mass (the barycenter) of the solar system.
SECOND REVISED PAGES


--- Page 468 ---
8.2 Affine Independence 451
When a point is not inside the triangle (or tetrahedron), some of the barycentric
coordinates will be negative. The case of a triangle is illustrated in Figure 5, for vertices
a,b,c, and coordinate values r; s; t , as above. The points on the line through bandc, for
instance, have rD0because they are afÔ¨Åne combinations of only bandc. The parallel
line through aidentiÔ¨Åes points with rD1.
a
bc pr = 1
r = 0
s = 1s = 0
FIGURE 5 Barycentric coordinates
for points in aff fa;b;cg.
Barycentric Coordinates in Computer Graphics
When working with geometric objects in a computer graphics program, a designer may
use a ‚Äúwire-frame‚Äù approximation to an object at certain key points in the process
of creating a realistic Ô¨Ånal image.2For instance, if the surface of part of an object
consists of small Ô¨Çat triangular surfaces, then a graphics program can easily add color,
lighting, and shading to each small surface when that information is known only at the
vertices. Barycentric coordinates provide the tool for smoothly interpolating the vertex
information over the interior of a triangle. The interpolation at a point is simply the
linear combination of the vertex values using the barycentric coordinates as weights.
Colors on a computer screen are often described by RGB coordinates. A triple
.r; g; b/ indicates the amount of each color‚Äîred, green, and blue‚Äîwith the parameters
varying from 0to1. For example, pure red is .1; 0; 0/ , white is .1; 1; 1/ , and black is
.0; 0; 0/ .
EXAMPLE 5 Letv1D2
43
1
53
5,v2D2
44
3
43
5,v3D2
41
5
13
5, and pD2
43
3
3:53
5. The col-
ors at the vertices v1,v2, and v3of a triangle are magenta .1; 0; 1/ , light magenta .1; :4; 1/ ,
and purple .:6; 0; 1/ , respectively. Find the interpolated color at p. See Figure 6.
v2v3v1
FIGURE 6 Interpolated colors.
2The Introductory Example for Chapter 2 shows a wire-frame model of a Boeing 777 airplane, used to
visualize the Ô¨Çow of air over the surface of the plane.
SECOND REVISED PAGES


--- Page 469 ---
452 CHAPTER 8 The Geometry of Vector Spaces
SOLUTION First, Ô¨Ånd the barycentric coordinates of p. Here is the calculation using
homogeneous forms of the points, with the Ô¨Årst step moving row 4 to row 1:
Qv1Qv2Qv3Qp
2
6641 1 1 1
3 4 1 3
1 3 5 3
5 4 1 3:53
7752
6641 0 0 :25
0 1 0 :50
0 0 1 :25
0 0 0 03
775
SopD:25v1C:5v2C:25v3. Use the barycentric coordinates of pto make a linear
combination of the color data. The RGB values for pare
:252
41
0
13
5C:502
41
:4
13
5C:252
4:6
0
13
5D2
4:9
:2
13
5red
green
blue
One of the last steps in preparing a graphics scene for display on a computer screen
is to remove ‚Äúhidden surfaces‚Äù that should not be visible on the screen. Imagine the
viewing screen as consisting of, say, a million pixels, and consider a ray or ‚Äúline of sight‚Äù
from the viewer‚Äôs eye through a pixel and into the collection of objects that make up the
3D display. The color and other information displayed in the pixel on the screen should
come from the object that the ray Ô¨Årst intersects. See Figure 7. When the objects in
the graphics scene are approximated by wire frames with triangular patches, the hidden
surface problem can be solved using barycentric coordinates.
FIGURE 7 A ray from the eye through the screen to the
nearest object.
The mathematics for Ô¨Ånding the ray-triangle intersections can also be used to per-
form extremely realistic shading of objects. Currently, this ray-tracing method is too
slow for real-time rendering, but recent advances in hardware implementation may
change that in the future.3
EXAMPLE 6 Let
v1D2
41
1
 63
5;v2D2
48
1
 43
5;v3D2
45
11
 23
5;aD2
40
0
103
5;bD2
4:7
:4
 33
5;
andx.t/DaCtbfort0. Find the point where the ray x.t/intersects the plane that
contains the triangle with vertices v1,v2, and v3. Is this point inside the triangle?
3See Joshua Fender and Jonathan Rose, ‚ÄúA High-Speed Ray Tracing Engine Built on a Field-Programmable
System,‚Äù in Proc. Int. Conf on Field-Programmable Technology , IEEE (2003). (A single processor can
calculate 600 million ray-triangle intersections per second.)
SECOND REVISED PAGES


--- Page 470 ---
8.2 Affine Independence 453
SOLUTION The plane is aff fv1;v2;v3g. A typical point in this plane may be written
as.1 c2 c3/v1Cc2v2Cc3v3for some c2andc3. (The weights in this combination
sum to 1.) The ray x.t/intersects the plane when c2,c3, and tsatisfy
.1 c2 c3/v1Cc2v2Cc3v3DaCtb
Rearrange this as c2.v2 v1/Cc3.v3 v1/Ct. b/Da v1. In matrix form,
v2 v1 v3 v1 b2
4c2
c3
t3
5Da v1
For the speciÔ¨Åc points given here,
v2 v1D2
47
0
23
5;v3 v1D2
44
10
43
5;a v1D2
4 1
 1
163
5
Row reduction of the augmented matrix above produces
2
47 4  :7 1
0 10  :4 1
2 4 3 163
52
41 0 0 :3
0 1 0 :1
0 0 1 53
5
Thus c2D:3,c3D:1, and tD5. Therefore, the intersection point is
x.5/DaC5bD2
40
0
103
5C52
4:7
:4
 33
5D2
43:5
2:0
 5:03
5
Also,
x.5/D.1 :3 :1/v1C:3v2C:1v3
D:62
41
1
 63
5C:32
48
1
 43
5C:12
45
11
 23
5D2
43:5
2:0
 5:03
5
The intersection point is inside the triangle because the barycentric weights for x.5/are
all positive.
PRACTICE PROBLEMS
1.Describe a fast way to determine when three points are collinear.
2.The points v1D4
1
,v2D1
0
,v3D5
4
, and v4D1
2
form an afÔ¨Ånely de-
pendent set. Find weights c1; : : : ; c 4that produce an afÔ¨Åne dependence relation
c1v1C  C c4v4D0, where c1C  C c4D0and not all ciare zero. [ Hint: See
the end of the proof of Theorem 5.]
SECOND REVISED PAGES


--- Page 471 ---
454 CHAPTER 8 The Geometry of Vector Spaces
8.2 EXERCISES
In Exercises 1‚Äì6, determine if the set of points is afÔ¨Ånely depen-
dent. (See Practice Problem 2.) If so, construct an afÔ¨Åne depen-
dence relation for the points.
1.3
 3
,0
6
,2
0
2.2
1
,5
4
, 3
 2
3.2
41
2
 13
5,2
4 2
 4
83
5,2
42
 1
113
5,2
40
15
 93
5
4.2
4 2
5
33
5,2
40
 3
73
5,2
41
 2
 63
5,2
4 2
7
 33
5
5.2
41
0
 23
5,2
40
1
13
5,2
4 1
5
13
5,2
40
5
 33
5
6.2
41
3
13
5,2
40
 1
 23
5,2
42
5
23
5,2
43
5
03
5
In Exercises 7 and 8, Ô¨Ånd the barycentric coordinates of pwith
respect to the afÔ¨Ånely independent set of points that precedes it.
7.2
6641
 1
2
13
775,2
6642
1
0
13
775,2
6641
2
 2
03
775,p=2
6645
4
 2
23
775
8.2
6640
1
 2
13
775,2
6641
1
0
23
775,2
6641
4
 6
53
775,p=2
664 1
1
 4
03
775
In Exercises 9 and 10, mark each statement True or False. Justify
each answer.
9.a.Ifv1; : : : ; vpare inRnand if the set fv1 v2;v3 v2; : : : ;
vp v2gis linearly dependent, then fv1; : : : ; vpgis
afÔ¨Ånely dependent. (Read this carefully.)
b.Ifv1; : : : ; vpare inRnand if the set of homogeneous
forms fQv1; : : : ;QvpginRnC1is linearly independent, then
fv1; : : : ; vpgis afÔ¨Ånely dependent.
c.A Ô¨Ånite set of points fv1; : : : ; vkgis afÔ¨Ånely dependent if
there exist real numbers c1; : : : ; c k, not all zero, such that
c1C  C ckD1andc1v1C  C ckvkD0.
d.IfSD fv1; : : : ; vpgis an afÔ¨Ånely independent set in Rn
and if pinRnhas a negative barycentric coordinate
determined by S, then pis not in aff S.
e.Ifv1;v2;v3;a, and bare inR3and if a ray aCtbfor
t0intersects the triangle with vertices v1,v2, and v3,
then the barycentric coordinates of the intersection point
are all nonnegative.
10. a.Iffv1; : : : ; vpgis an afÔ¨Ånely dependent set in Rn, then the
setfQv1; : : : ;QvpginRnC1of homogeneous forms may be
linearly independent.b.Ifv1,v2,v3, and v4are in R3and if the set
fv2 v1;v3 v1;v4 v1gis linearly independent, then
fv1; : : : ; v4gis afÔ¨Ånely independent.
c.Given SD fb1; : : : ; bkginRn, each pin aff Shas
a unique representation as an afÔ¨Åne combination of
b1; : : : ; bk.
d.When color information is speciÔ¨Åed at each vertex v1,v2,
v3of a triangle in R3, then the color may be interpolated
at a point pin afffv1;v2;v3gusing the barycentric coor-
dinates of p.
e.IfTis a triangle in R2and if a point pis on an edge of
the triangle, then the barycentric coordinates of p(for this
triangle) are not all positive.
11.Explain why any set of Ô¨Åve or more points in R3must be
afÔ¨Ånely dependent.
12. Show that a set fv1; : : : ; vpginRnis afÔ¨Ånely dependent when
pnC2.
13. Use only the deÔ¨Ånition of afÔ¨Åne dependence to show that an
indexed set fv1;v2ginRnis afÔ¨Ånely dependent if and only if
v1Dv2.
14. The conditions for afÔ¨Åne dependence are stronger than those
for linear dependence, so an afÔ¨Ånely dependent set is auto-
matically linearly dependent. Also, a linearly independent set
cannot be afÔ¨Ånely dependent and therefore must be afÔ¨Ånely
independent. Construct two linearly dependent indexed sets
S1andS2inR2such that S1is afÔ¨Ånely dependent and S2
is afÔ¨Ånely independent. In each case, the set should contain
either one, two, or three nonzero points.
15. Let v1D 1
2
,v2D0
4
,v3D2
0
, and let SD
fv1;v2;v3g.
a.Show that the set Sis afÔ¨Ånely independent.
b.Find the barycentric coordinates of p1D2
3
,
p2D1
2
,p3D 2
1
,p4D1
 1
, and p5D1
1
,
with respect to S.
c.LetTbe the triangle with vertices v1,v2, and v3. When
the sides of Tare extended, the lines divide R2into seven
regions. See Figure 8. Note the signs of the barycentric
coordinates of the points in each region. For example, p5
is inside the triangle Tand all its barycentric coordinates
are positive. Point p1has coordinates . ;C;C/. Its third
coordinate is positive because p1is on the v3side of the
line through v1andv2. Its Ô¨Årst coordinate is negative
because p1is opposite the v1side of the line through v2
andv3. Point p2is on the v2v3edge of T. Its coordinates
are.0;C;C/. Without calculating the actual values, de-
termine the signs of the barycentric coordinates of points
p6,p7, and p8as shown in Figure 8.
SECOND REVISED PAGES


--- Page 472 ---
8.2 Affine Independence 455
y
xp1
v1p2v2
p3
p4p5
p6p7
p8
v3
FIGURE 8
16. Let v1D0
1
,v2D1
5
,v3D4
3
,p1D3
5
,
p2D5
1
, p3D2
3
, p4D 1
0
, p5D0
4
,
p6D1
2
,p7D6
4
, and SD fv1;v2;v3g.
a.Show that the set Sis afÔ¨Ånely independent.
b.Find the barycentric coordinates of p1,p2, and p3with
respect to S.
c.On graph paper, sketch the triangle Twith vertices v1,
v2, and v3, extend the sides as in Figure 8, and plot the
points p4,p5,p6, and p7. Without calculating the actual
values, determine the signs of the barycentric coordinates
of points p4,p5,p6, and p7.
17. Prove Theorem 6 for an afÔ¨Ånely independent set
SD fv1; : : : ; vkginRn. [Hint: One method is to mimic the
proof of Theorem 7 in Section 4.4.]
18. LetTbe a tetrahedron in ‚Äústandard‚Äù position, with three
edges along the three positive coordinate axes in R3,
and suppose the vertices are ae1,be2,ce3, and 0, where
¬åe1e2e3¬çDI3. Find formulas for the barycentric coor-
dinates of an arbitrary point pinR3.
19. Letfp1;p2;p3gbe an afÔ¨Ånely dependent set of points in Rn
and let fWRn!Rmbe a linear transformation. Show that
ff .p1/; f . p2/; f . p3/gis afÔ¨Ånely dependent in Rm.
20. Suppose that fp1;p2;p3gis an afÔ¨Ånely independent set in Rn
andqis an arbitrary point in Rn. Show that the translated set
fp1Cq;p2Cq;p3Cqgis also afÔ¨Ånely independent.In Exercises 21‚Äì24, a,b, and care noncollinear points in R2and
pis any other point in R2. Let¬Åabcdenote the closed triangular
region determined by a;b, and c, and let ¬Åpbc be the region
determined by p,b, and c. For convenience, assume that a,b, and
care arranged so that det ¬åQaQbQc¬çis positive, where Qa,Qb, and
Qcare the standard homogeneous forms for the points.
21. Show that the area of ¬Åabcis det ¬åQaQbQc¬ç=2. [Hint: Con-
sult Sections 3.2 and 3.3, including the Exercises.]
22. Letpbe a point on the line through aandb. Show that
det¬åQaQbQp¬çD0.
23. Letpbe any point in the interior of ¬Åabc, with barycentric
coordinates .r; s; t/ , so that

QaQbQc2
4r
s
t3
5DQp
Use Exercise 21 and a fact about determinants (Chapter 3) to
show that
rD(area of ¬Åpbc/=(area of ¬Åabc/
sD(area of ¬Åapc/=(area of ¬Åabc/
tD(area of ¬Åabp/=(area of ¬Åabc/
24. Take qon the line segment from btocand consider the line
through qanda, which may be written as pD.1 x/qCxa
for all real x. Show that, for each x, det ¬åQpQbQc¬çD
xdet¬åQaQbQc¬ç. From this and earlier work, conclude that
the parameter xis the Ô¨Årst barycentric coordinate of p. How-
ever, by construction, the parameter xalso determines the
relative distance between pandqalong the segment from
qtoa. (When xD1,pDa.) When this fact is applied to
Example 5, it shows that the colors at vertex aand the point q
are smoothly interpolated as pmoves along the line between
aandq.
25. Letv1D2
41
3
 63
5,v2D2
47
3
 53
5,v3D2
43
9
 23
5;aD2
40
0
93
5;
bD2
41:4
1:5
 3:13
5, and x.t/DaCtbfort0. Find the point
where the ray x.t/intersects the plane that contains the
triangle with vertices v1,v2, and v3. Is this point inside the
triangle?
26. Repeat Exercise 25 with v1D2
41
2
 43
5,v2D2
48
2
 53
5,
v3D2
43
10
 23
5,aD2
40
0
83
5;andbD2
4:9
2:0
 3:73
5.
SECOND REVISED PAGES


--- Page 473 ---
456 CHAPTER 8 The Geometry of Vector Spaces
SOLUTIONS TO PRACTICE PROBLEMS
1.From Example 1, the problem is to determine if the points are afÔ¨Ånely dependent.
Use the method of Example 2 and subtract one point from the other two. If one of
these two new points is a multiple of the other, the original three points lie on a line.
2.The proof of Theorem 5 essentially points out that an afÔ¨Åne dependence relation
among points corresponds to a linear dependence relation among the homogeneous
forms of the points, using the same weights. So, row reduce:
Qv1Qv2Qv3Qv4
D2
44 1 5 1
1 0 4 2
1 1 1 13
52
41 1 1 1
4 1 5 1
1 0 4 23
5
2
41 0 0  1
0 1 0 1:25
0 0 1 :753
5
View this matrix as the coefÔ¨Åcient matrix for AxD0with four variables. Then x4
is free, x1Dx4,x2D  1:25x 4, and x3D  :75x 4. One solution is x1Dx4D4,
x2D  5, and x3D  3. A linear dependence among the homogeneous forms is
4Qv1 5Qv2 3Qv3C4Qv4D0. So4v1 5v2 3v3C4v4D0.
Another solution method is to translate the problem to the origin by subtracting
v1from the other points, Ô¨Ånd a linear dependence relation among the translated
points, and then rearrange the terms. The amount of arithmetic involved is about
the same as in the approach shown above.
8.3 CONVEX COMBINATIONS
Section 8.1 considered special linear combinations of the form
c1v1Cc2v2C  C ckvk;where c1Cc2C  C ckD1
This section further restricts the weights to be nonnegative.
D E F I N I T I O N Aconvex combination of points v1;v2; : : : ; vkinRnis a linear combination of
the form
c1v1Cc2v2C  C ckvk
such that c1Cc2C  C ckD1andci0for all i. The set of all convex
combinations of points in a set Sis called the convex hull ofS, denoted by conv S.
The convex hull of a single point v1is just the set fv1g, the same as the afÔ¨Åne hull.
In other cases, the convex hull is properly contained in the afÔ¨Åne hull. Recall that the
afÔ¨Åne hull of distinct points v1andv2is the line
yD.1 t/v1Ctv2;withtinR
Because the weights in a convex combination are nonnegative, the points in conv fv1;v2g
may be written as
yD.1 t/v1Ctv2;with0t1
which is the line segment between v1andv2, hereafter denoted by v1v2.
If a set Sis afÔ¨Ånely independent and if p2affS, then p2conv Sif and only if
the barycentric coordinates of pare nonnegative. Example 1 shows a special situation
in which Sis much more than just afÔ¨Ånely independent.
SECOND REVISED PAGES


--- Page 474 ---
8.3 Convex Combinations 457
EXAMPLE 1 Let
v1D2
6643
0
6
 33
775;v2D2
664 6
3
3
03
775;v3D2
6643
6
0
33
775;p1D2
6640
3
3
03
775;p2D2
664 10
5
11
 43
775;
andSD fv1;v2;v3g. Note that Sis an orthogonal set. Determine whether p1is in
Span S, affS, and conv S. Then do the same for p2.
SOLUTION Ifp1is at least a linear combination of the points in S, then the weights are
easily found, because Sis an orthogonal set. Let Wbe the subspace spanned by S. A cal-
culation as in Section 6.3 shows that the orthogonal projection of p1ontoWisp1itself:
projWp1Dp1v1
v1v1v1Cp1v2
v2v2v2Cp1v3
v3v3v3
D18
54v1C18
54v2C18
54v3
D1
32
6643
0
6
 33
775C1
32
664 6
3
3
03
775C1
32
6643
6
0
33
775D2
6640
3
3
03
775Dp1
This shows that p1isinSpan S. Also, since the coefÔ¨Åcients sum to 1, p1is in aff S. In
fact, p1is in conv S, because the coefÔ¨Åcients are also nonnegative.
Forp2, a similar calculation shows that projWp2¬§p2. Since projWp2is the closest
point in Span Stop2, the point p2is not in Span S. In particular, p2cannot be in aff S
or conv S.
Recall that a set Sis afÔ¨Åne if it contains all lines determined by pairs of points in S.
When attention is restricted to convex combinations, the appropriate condition involves
line segments rather than lines.
D E F I N I T I O N A set Sisconvex if for each p;q2S, the line segment pqis contained in S.
Intuitively, a set Sis convex if every two points in the set can ‚Äúsee‚Äù each other
without the line of sight leaving the set. Figure 1 illustrates this idea.
Convex Convex Not convex
FIGURE 1
The next result is analogous to Theorem 2 for afÔ¨Åne sets.
T H E O R E M 7 A set Sis convex if and only if every convex combination of points of Slies in
S. That is, Sis convex if and only if SDconv S.
PROOF The argument is similar to the proof of Theorem 2. The only difference is
in the induction step. When taking a convex combination of kC1points, consider
yDc1v1C  C ckvkCckC1vkC1, where c1C  C ckC1D1and0ci1for
SECOND REVISED PAGES


--- Page 475 ---
458 CHAPTER 8 The Geometry of Vector Spaces
alli. IfckC1D1, then yDvkC1, which belongs to S, and there is nothing further to
prove. If ckC1< 1, lettDc1C  C ck. Then tD1 ckC1> 0and
yD.1 ckC1/c1
tv1C  Cck
tvk
CckC1vkC1 (1)
By the induction hypothesis, the point zD.c1=t/v1C  C .ck=t/vkis inS, since the
nonnegative coefÔ¨Åcients sum to 1. Thus equation (1) displays yas a convex combination
of two points in S. By the principle of induction, every convex combination of such
points lies in S.
Theorem 9 below provides a more geometric characterization of the convex hull
of a set. It requires a preliminary result on intersections of sets. Recall from Section
4.1 (Exercise 32) that the intersection of two subspaces is itself a subspace. In fact, the
intersection of any collection of subspaces is itself a subspace. A similar result holds for
afÔ¨Åne sets and convex sets.
T H E O R E M 8 LetfSW2Agbe any collection of convex sets. Then \2ASis convex. If
fTW2Bgis any collection of afÔ¨Åne sets, then \2BTis afÔ¨Åne.
PROOF Ifpandqare in\S, then pandqare in each S. Since each Sis convex,
the line segment between pandqis inSfor all and hence that segment is contained
in\S. The proof of the afÔ¨Åne case is similar.
T H E O R E M 9 For any set S, the convex hull of Sis the intersection of all the convex sets that
contain S.
PROOF LetTdenote the intersection of all the convex sets containing S. Since conv S
is a convex set containing S, it follows that Tconv S. On the other hand, let Cbe
any convex set containing S. Then Ccontains every convex combination of points of
C(Theorem 7), and hence also contains every convex combination of points of the
subset S. That is, conv SC. Since this is true for every convex set Ccontaining S,
it is also true for the intersection of them all. That is, conv ST.
Theorem 9 shows that conv Sis in a natural sense the ‚Äúsmallest‚Äù convex set con-
taining S. For example, consider a set Sthat lies inside some large rectangle in R2, and
imagine stretching a rubber band around the outside of S. As the rubber band contracts
around S, it outlines the boundary of the convex hull of S. Or to use another analogy,
the convex hull of SÔ¨Ållsinall the holes in the inside of Sand Ô¨Ålls outall the dents in
the boundary of S.
EXAMPLE 2
a.The convex hulls of sets SandTinR2are shown below.
S conv ST conv T
SECOND REVISED PAGES


--- Page 476 ---
8.3 Convex Combinations 459
b.LetSbe the set consisting of the standard basis for R3; SD fe1;e2;e3g. Then conv S
e10e2e3x2x3
x1
FIGURE 2is a triangular surface in R3, with vertices e1,e2, and e3. See Figure 2.
EXAMPLE 3 LetSDx
y
Wx0andyDx2
. Show that the convex hull of
Sis the union of the origin andx
y
Wx > 0 andyx2
. See Figure 3.
SOLUTION Every point in conv Smust lie on a line segment that connects two points
ofS. The dashed line in Figure 3 indicates that, except for the origin, the positive y-
axis is not in conv S, because the origin is the only point of Son the y-axis. It may
seem reasonable that Figure 3 does show conv S, but how can you be sure that the point
.10 2; 104/, for example, is on a line segment from the origin to a point on the curve in
S?
Consider any point pin the shaded region of Figure 3, say
xy
y = x2
FIGURE 3pDa
b
;witha > 0 andba2
The line through 0andphas the equation yD.b=a/t fortreal. That line intersects
Swhere tsatisÔ¨Åes .b=a/t Dt2, that is, when tDb=a. Thus, pis on the line segment
from 0tob=a
b2=a2
, which shows that Figure 3 is correct.
The following theorem is basic in the study of convex sets. It was Ô¨Årst proved by
Constantin Caratheodory in 1907. If pis in the convex hull of S, then, by deÔ¨Ånition, p
must be a convex combination of points of S. But the deÔ¨Ånition makes no stipulation
as to how many points of Sare required to make the combination. Caratheodory‚Äôs
remarkable theorem says that in an n-dimensional space, the number of points of S
in the convex combination never has to be more than nC1.
T H E O R E M 1 0 (Caratheodory) IfSis a nonempty subset of Rn, then every point in conv Scan
be expressed as a convex combination of nC1or fewer points of S.
PROOF Given pin conv S, one may write pDc1v1C  C ckvk, where vi2S;
c1C  C ckD1, and ci0, for some kandiD1; : : : ; k . The goal is to show that
such an expression exists for pwithknC1.
Ifk > n C1, thenfv1; : : : ; vkgis afÔ¨Ånely dependent, by Exercise 12 in Section 8.2.
Thus there exist scalars d1; : : : ; d k, not all zero, such that
kX
iD1diviD0andkX
iD1diD0
Consider the two equations
c1v1Cc2v2C  C ckvkDp
and
d1v1Cd2v2C  C dkvkD0
By subtracting an appropriate multiple of the second equation from the Ô¨Årst, we now
eliminate one of the viterms and obtain a convex combination of fewer than kelements
ofSthat is equal to p.
SECOND REVISED PAGES


--- Page 477 ---
460 CHAPTER 8 The Geometry of Vector Spaces
Since not all of the dicoefÔ¨Åcients are zero, we may assume (by reordering sub-
scripts if necessary) that dk> 0and that ck=dkci=difor all those ifor which di> 0.
ForiD1; : : : ; k , letbiDci .ck=dk/di. Then bkD0and
kX
iD1biDkX
iD1ci ck
dkkX
iD1diD1 0D1
Furthermore, each bi0. Indeed, if di0, then bici0. Ifdi> 0, then biD
di.ci=di ck=dk/0. By construction,
k 1X
iD1biviDkX
iD1biviDkX
iD1
ci ck
dkdi
vi
DkX
iD1civi ck
dkkX
iD1diviDkX
iD1civiDp
Thus pis now a convex combination of k 1of the points v1; : : : ; vk. This process may
be repeated until pis expressed as a convex combination of at most nC1of the points
ofS.
The following example illustrates the calculations in the proof above.
EXAMPLE 4 Let
v1D1
0
;v2D2
3
;v3D5
4
;v4D3
0
;pD"
10
3
5
2#
;
andSD fv1;v2;v3;v4g. Then
1
4v1C1
6v2C1
2v3C1
12v4Dp (2)
Use the procedure in the proof of Caratheodory‚Äôs Theorem to express pas a convex
combination of three points of S.
SOLUTION The set Sis afÔ¨Ånely dependent. Use the technique of Section 8.2 to obtain
an afÔ¨Åne dependence relation
 5v1C4v2 3v3C4v4D0 (3)
Next, choose the points v2andv4in (3), whose coefÔ¨Åcients are positive. For each
point, compute the ratio of the coefÔ¨Åcients in equations (2) and (3). The ratio for v2
is1
64D1
24, and that for v4is1
124D1
48. The ratio for v4is smaller, so subtract1
48
times equation (3) from equation (2) to eliminate v4:
 1
4C5
48
v1C 1
6 4
48
v2C 1
2C3
48
v3C 1
12 4
48
v4Dp
17
48v1C4
48v2C27
48v3Dp
This result cannot, in general, be improved by decreasing the required number of
points. Indeed, given any three non-collinear points in R2, the centroid of the triangle
formed by them is in the convex hull of all three, but is not in the convex hull of any two.
SECOND REVISED PAGES


--- Page 478 ---
8.3 Convex Combinations 461
PRACTICE PROBLEMS
1.Letv1D2
46
2
23
5,v2D2
47
1
53
5,v3D2
4 2
4
 13
5,p1D2
41
3
13
5, and p2D2
43
2
13
5, and let
SD fv1;v2;v3g. Determine whether p1andp2are in conv S.
2.LetSbe the set of points on the curve yD1=x forx > 0 . Explain geometrically
why conv Sconsists of all points on and above the curve S.
8.3 EXERCISES
1.InR2, let SD0
y
W0y < 1S2
0
. Describe
(or sketch) the convex hull of S.
2.Describe the convex hull of the set Sof pointsx
y
inR2
that satisfy the given conditions. Justify your answers. (Show
that an arbitrary point pinSbelongs to conv S.)
a.yD1=xandx1=2
b.yDsinx
c.yDx1=2andx0
3.Consider the points in Exercise 5 in Section 8.1. Which of
p1,p2, and p3are in conv S?
4.Consider the points in Exercise 6 in Section 8.1. Which of
p1,p2, and p3are in conv S?
5.Let
v1D2
4 1
 3
43
5;v2D2
40
 3
13
5;v3D2
41
 1
43
5;v4D2
41
1
 23
5;
p1D2
41
 1
23
5;p2D2
40
 2
23
5;
andSD fv1;v2;v3;v4g. Determine whether p1andp2are in
conv S.
6.Letv1D2
6642
0
 1
23
775,v2D2
6640
 2
2
13
775,v3D2
664 2
1
0
23
775,p1D2
6664 1
2
 3
2
5
23
7775,
p2D2
6664 1
2
0
1
4
7
43
7775,p3D2
66646
 4
1
 13
7775, and p4D2
664 1
 2
0
43
775, and let Sbe
the orthogonal set fv1;v2;v3g. Determine whether each piis
in Span S, affS, or conv S.
a.p1 b.p2 c.p3 d.p4Exercises 7‚Äì10 use the terminology from Section 8.2.
7.a.LetTD 1
0
;2
3
;4
1
, and let
p1D2
1
;p2D3
2
;p3D2
0
;andp4D0
2
:
Find the barycentric coordinates of p1,p2,p3, and p4with
respect to T.
b.Use your answers in part (a) to determine whether each
ofp1; : : : ; p4in part (a) is inside, outside, or on the edge
of conv T, a triangular region.
8.Repeat Exercise 7 for TD2
0
;0
5
; 1
1
and
p1D2
1
;p2D1
1
;p3D"
1
1
3#
;andp4D1
0
:
9.LetSD fv1;v2;v3;v4gbe an afÔ¨Ånely independent set. Con-
sider the points p1; : : : ; p5whose barycentric coordinates
with respect to Sare given by .2; 0; 0;  1/, 
0;1
2;1
4;1
4
, 1
2; 0;3
2; 1
, 1
3;1
4;1
4;1
6
, and 1
3; 0;2
3; 0
, respectively. De-
termine whether each of p1; : : : ; p5is inside, outside, or on
the surface of conv S, a tetrahedron. Are any of these points
on an edge of conv S?
10. Repeat Exercise 9 for the points q1; : : : ; q5whose barycen-
tric coordinates with respect to Sare given by 1
8;1
4;1
8;1
2
, 3
4; 1
4; 0;1
2
, 
0;3
4;1
4; 0
,.0; 2; 0; 3/ , and 1
3;1
3;1
3; 0
,
respectively.
In Exercises 11 and 12, mark each statement True or False. Justify
each answer.
11.a.IfyDc1v1Cc2v2Cc3v3andc1Cc2Cc3D1, then y
is a convex combination of v1;v2, and v3.
b.IfSis a nonempty set, then conv Scontains some points
that are not in S.
c.IfSandTare convex sets, then S[Tis also convex.
12. a.A set is convex if x,y2Simplies that the line segment
between xandyis contained in S.
b.IfSandTare convex sets, then S\Tis also convex.
SECOND REVISED PAGES


--- Page 479 ---
462 CHAPTER 8 The Geometry of Vector Spaces
c.IfSis a nonempty subset of R5andy2conv S, then there
exist distinct points v1; : : : ; v6inSsuch that yis a convex
combination of v1; : : : ; v6.
13. LetSbe a convex subset of Rnand suppose that
fWRn!Rmis a linear transformation. Prove that the set
f .S/D ff .x/Wx2Sgis a convex subset of Rm.
14. LetfWRn!Rmbe a linear transformation and let
Tbe a convex subset of Rm. Prove that the set
SD fx2RnWf .x/2Tgis a convex subset of Rn.
15. Let v1D1
0
,v2D1
2
,v3D4
2
,v4D4
0
, and
pD2
1
. ConÔ¨Årm that
pD1
3v1C1
3v2C1
6v3C1
6v4and v1 v2Cv3 v4D0:
Use the procedure in the proof of Caratheodory‚Äôs Theorem
to express pas a convex combination of three of the vi‚Äôs. Do
this in twoways.
16. Repeat Exercise 15 for points v1D 1
0
,v2D0
3
,
v3D3
1
,v4D1
 1
, and pD1
2
, given that
pD1
121v1C72
121v2C37
121v3C1
11v4
and
10v1 6v2C7v3 11v4D0:
In Exercises 17‚Äì20, prove the given statement about subsets A
andBofRn. A proof for an exercise may use results of earlier
exercises.
17. IfABandBis convex, then conv AB.
18. IfAB, then conv Aconv B.
19. a.¬å.conv A/[.conv B/¬çconv .A[B/b.Find an example in R2to show that equality need not hold
in part (a).
20. a.conv .A\B/¬å.conv A/\.conv B/¬ç
b.Find an example in R2to show that equality need not hold
in part (a).
21. Let p0,p1, and p2be points in Rn, and deÔ¨Åne
f0.t/D.1 t/p0Ctp1,f1.t/D.1 t/p1Ctp2, and
g.t/D.1 t/f0.t/Ctf1.t/for0t1. For the points
as shown below, draw a picture that shows f0 1
2
,f1 1
2
, and
g 1
2
.
p1p2
p0
22. Repeat Exercise 21 for f0 3
4
,f1 3
4
, and g 3
4
.
23. Letg.t/be deÔ¨Åned as in Exercise 21. Its graph is called
aquadratic B√©zier curve , and it is used in some computer
graphics designs. The points p0,p1, and p2are called the
control points for the curve. Compute a formula for g.t/
that involves only p0,p1, and p2. Then show that g.t/is in
convfp0;p1;p2gfor0t1.
24. Given control points p0,p1,p2, and p3inRn, let g1.t/
for0t1be the quadratic B√©zier curve from Exer-
cise 23 determined by p0,p1, and p2, and let g2.t/be
deÔ¨Åned similarly for p1,p2, and p3. For 0t1, deÔ¨Åne
h.t/D.1 t/g1.t/Ctg2.t/. Show that the graph of h.t/
lies in the convex hull of the four control points. This curve
is called a cubic B√©zier curve , and its deÔ¨Ånition here is one
step in an algorithm for constructing B√©zier curves (discussed
later in Section 8.6). A B√©zier curve of degree kis determined
bykC1control points, and its graph lies in the convex hull
of these control points.
SOLUTIONS TO PRACTICE PROBLEMS
1.The points v1,v2, and v3are not orthogonal, so compute
v2 v1D2
41
 1
33
5;v3 v1D2
4 8
2
 33
5;p1 v1D2
4 5
1
 13
5;andp2 v1D2
4 3
0
 13
5
Augment the matrix ¬åv2 v1v3 v1¬çwith both p1 v1andp2 v1, and row
reduce:
2
41 8 5 3
 1 2 1 0
3 3 1 13
52
6641 01
31
0 12
31
2
0 0 0  5
23
775
The third column shows that p1 v1D1
3.v2 v1/C2
3.v3 v1/, which leads to
p1D0v1C1
3v2C2
3v3. Thus p1is in conv S. In fact, p1is in conv fv2;v3g.
SECOND REVISED PAGES


--- Page 480 ---
8.4 Hyperplanes 463
The last column of the matrix shows that p2 v1is not a linear combination of
v2 v1andv3 v1. Thus p2is not an afÔ¨Åne combination of v1,v2, and v3, sop2
cannot possibly be in conv S.
An alternative method of solution is to row reduce the augmented matrix of
homogeneous forms:
Qv1Qv2Qv3Qp1Qp2
2
666641 0 0 0 0
0 1 01
30
0 0 12
30
0 0 0 0 13
77775
2.Ifpis a point above S, then the line through pwith slope  1will intersect Sat two
points before it reaches the positive x- and y-axes.
8.4 HYPERPLANES
Hyperplanes play a special role in the geometry of Rnbecause they divide the space into
two disjoint pieces, just as a plane separates R3into two parts and a line cuts through
R2. The key to working with hyperplanes is to use simple implicit descriptions, rather
than the explicit or parametric representations of lines and planes used in the earlier
work with afÔ¨Åne sets.1
An implicit equation of a line in R2has the form axCbyDd. An implicit equa-
tion of a plane in R3has the form axCbyCc¬¥Dd. Both equations describe the
line or plane as the set of all points at which a linear expression (also called a linear
functional ) has a Ô¨Åxed value, d.
D E F I N I T I O N Alinear functional onRnis a linear transformation ffromRnintoR. For each
scalar dinR, the symbol ¬åf:d¬çdenotes the set of all xinRnat which the value
offisd. That is,
¬åf:d¬ç is the set fx2RnWf .x/Ddg
Thezero functional is the transformation such that f .x/D0for all xinRn. All
other linear functionals on Rnare said to be nonzero .
EXAMPLE 1 InR2, the line x 4yD13is a hyperplane in R2, and it is the set
of points at which the linear functional f .x; y/ Dx 4yhas the value 13. That is, the
line is the set ¬åf:13¬ç.
EXAMPLE 2 InR3, the plane 5x 2yC3¬¥D21is a hyperplane, the set of points
at which the linear functional g.x; y; ¬¥/ D5x 2yC3¬¥has the value 21. This hyper-
plane is the set ¬åg:21¬ç.
Iffis a linear functional on Rn, then the standard matrix of this linear transforma-
tionfis a1nmatrix A, sayAD¬åa1a2an¬ç. So
¬åf:0¬çis the same as fx2RnWAxD0g DNulA (1)
1Parametric representations were introduced in Section 1.5.
SECOND REVISED PAGES


--- Page 481 ---
464 CHAPTER 8 The Geometry of Vector Spaces
Iffis a nonzero functional, then rank AD1, and dim Nul ADn 1, by the Rank
Theorem.2Thus, the subspace ¬åfW0¬çhas dimension n 1and so is a hyperplane. Also,
ifdis any number in R, then
¬åf:d¬ç is the same as fx2RnWAxDdg (2)
Recall from Theorem 6 in Section 1.5 that the set of solutions of AxDbis obtained
by translating the solution set of AxD0, using any particular solution pofAxDb.
When Ais the standard matrix of the transformation f, this theorem says that
¬åf:d¬çD¬åfW0¬çCpfor any pin¬åfWd¬ç (3)
Thus the sets ¬åfWd¬çare hyperplanes parallel to ¬åfW0¬ç. See Figure 1.
p
[f: d]
[f: 0]
FIGURE 1 Parallel hyperplanes,
withf .p/Dd.
When Ais a1nmatrix, the equation AxDdmay be written with an inner
product nx, using ninRnwith the same entries as A. Thus, from (2),
¬åf:d¬ç is the same as fx2RnWnxDdg (4)
Then ¬åf:0¬çD fx2RnWnxD0g, which shows that ¬åf:0¬çis the orthogonal comple-
ment of the subspace spanned by n. In the terminology of calculus and geometry for R3,
nis called a normal vector to ¬åf:0¬ç. (A ‚Äúnormal‚Äù vector in this sense need not have unit
length.) Also, nis said to be normal to each parallel hyperplane ¬åf:d¬ç, even though
nxis not zero when d¬§0.
Another name for ¬åf:d¬çis alevel set off, and nis sometimes called the gradient
offwhen f .x/Dnxfor each x.
EXAMPLE 3 LetnD3
4
andvD1
 6
, and let HD fxWnxD12g, soHD
¬åf:12¬ç, where f .x; y/ D3xC4y. Thus His the line 3xC4yD12. Find an implicit
description of the parallel hyperplane (line) H1DHCv.
SOLUTION First, Ô¨Ånd a point pinH1. To do this, Ô¨Ånd a point in Hand add vto it.
For instance,0
3
is inH, sopD1
 6
C0
3
D1
 3
is inH1. Now, compute
npD  9. This shows that H1D¬åf: 9¬ç. See Figure 2, which also shows the sub-
space H0D fxWnxD0g.
The next three examples show connections between implicit and explicit descrip-
tions of hyperplanes. Example 4 begins with an implicit form.
2See Theorem 14 in Section 2.9 or Theorem 14 in Section 4.6.
SECOND REVISED PAGES


--- Page 482 ---
8.4 Hyperplanes 465
ny
44
‚Äì4‚Äì4x
vv
vH = [f : 12]
H0 = [f : 0]
H1 = [f : ‚Äì9]
FIGURE 2
EXAMPLE 4 InR2, give an explicit description of the line x 4yD13in para-
metric vector form.
SOLUTION This amounts to solving a nonhomogeneous equation AxDb, where AD
¬å1 4¬çandbis the number 13 in R. Write xD13C4y, where yis a free variable.
In parametric form, the solution is
xDx
y
D13C4y
y
D13
0
Cy4
1
DpCyq; y2R
Converting an explicit description of a line into implicit form is more involved. The
basic idea is to construct ¬åf:0¬çand then Ô¨Ånd dfor¬åf:d¬ç.
EXAMPLE 5 Letv1D1
2
andv2D6
0
, and let L1be the line through v1and
v2. Find a linear functional fand a constant dsuch that L1D¬åf:d¬ç.
SOLUTION The line L1is parallel to the translated line L0through v2 v1and the
origin. The deÔ¨Åning equation for L0has the form
¬åa b ¬çx
y
D0ornxD0; where nDa
b
(5)
Since nis orthogonal to the subspace L0, which contains v2 v1, compute
v2 v1D6
0
 1
2
D5
 2
and solvea b5
 2
D0
By inspection, a solution is ¬åa b ¬çD¬å2 5 ¬ç. Let f .x; y/ D2xC5y. From (5),
L0D¬åf:0¬ç, and L1D¬åf:d¬çfor some d. Since v1is on line L1,dDf .v1/D
2.1/C5.2/D12. Thus, the equation for L1is2xC5yD12. As a check, note that
f .v2/Df .6; 0/ D2.6/C5.0/D12, sov2is on L1, too.
EXAMPLE 6 Letv1D2
41
1
13
5,v2D2
42
 1
43
5, and v3D2
43
1
23
5. Find an implicit de-
scription ¬åf:d¬çof the plane H1that passes through v1,v2, and v3.
SECOND REVISED PAGES


--- Page 483 ---
466 CHAPTER 8 The Geometry of Vector Spaces
SOLUTION H1is parallel to a plane H0through the origin that contains the translated
points
v2 v1D2
41
 2
33
5and v3 v1D2
42
0
13
5
Since these two points are linearly independent, H0DSpanfv2 v1;v3 v1g. Let
nD2
4a
b
c3
5be the normal to H0. Then v2 v1andv3 v1are each orthogonal to n. That
is,.v2 v1/nD0and.v3 v1/nD0. These two equations form a system whose
augmented matrix can be row reduced:
1 2 32
4a
b
c3
5D0;2 0 12
4a
b
c3
5D0;1 2 3 0
2 0 1 0
Row operations yield aD. 2
4/c,bD.5
4/c, with cfree. Set cD4, for instance. Then
nD2
4 2
5
43
5andH0D¬åf:0¬ç, where f .x/D  2x1C5x2C4x3.
The parallel hyperplane H1is¬åf:d¬ç. To Ô¨Ånd d, use the fact that v1is in H1,
and compute dDf .v1/Df .1; 1; 1/ D  2.1/C5.1/C4.1/D7. As a check, com-
putef .v2/Df .2; 1; 4/D  2.2/C5. 1/C4.4/D16 9D7. Observe f .v3/D
7also.
The procedure in Example 6 generalizes to higher dimensions. However, for the
special case of R3, one can also use the cross-product formula to compute n, using a
symbolic determinant as a mnemonic device:
nD.v2 v1/.v3 v1/
D1 2 i
 2 0 j
3 1 kD 2 0
3 1i 1 2
3 1jC1 2
 2 0k
D  2iC5jC4kD2
4 2
5
43
5
If only the formula for fis needed, the cross-product calculation may be written
as an ordinary determinant:
f .x 1; x2; x3/D1 2 x 1
 2 0 x 2
3 1 x 3D 2 0
3 1x1 1 2
3 1x2C1 2
 2 0x3
D  2x1C5x2C4x3
So far, every hyperplane examined has been described as ¬åf:d¬çfor some linear
functional fand some dinR, or equivalently as fx2RnWnxDdgfor some ninRn.
The following theorem shows that every hyperplane has these equivalent descriptions.
T H E O R E M 1 1 A subset HofRnis a hyperplane if and only if HD¬åf:d¬çfor some nonzero
linear functional fand some scalar dinR. Thus, if His a hyperplane, there
exist a nonzero vector nand a real number dsuch that HD fxWnxDdg.
SECOND REVISED PAGES


--- Page 484 ---
8.4 Hyperplanes 467
PROOF Suppose that His a hyperplane, take p2H, and let H0DH p. Then H0
is an .n 1/-dimensional subspace. Next, take any point ythat is not in H0. By the
Orthogonal Decomposition Theorem in Section 6.3,
yDy1Cn
where y1is a vector in H0andnis orthogonal to every vector in H0. The function f
deÔ¨Åned by
f .x/Dnxforx2Rn
is a linear functional, by properties of the inner product. Now, ¬åf:0¬çis a hyperplane that
contains H0, by construction of n. It follows that
H0D¬åf:0¬ç
[Argument: H0contains a basis Sofn 1vectors, and since Sis in the .n 1/-
dimensional subspace ¬åf:0¬ç,Smust also be a basis for ¬åf:0¬ç, by the Basis Theorem.]
Finally, let dDf .p/Dnp. Then, as in (3) shown earlier,
¬åf:d¬çD¬åf:0¬çCpDH0CpDH
The converse statement that ¬åf:d¬çis a hyperplane follows from (1) and (3) above.
Many important applications of hyperplanes depend on the possibility of ‚Äúseparat-
ing‚Äù two sets by a hyperplane. Intuitively, this means that one of the sets is on one side
of the hyperplane and the other set is on the other side. The following terminology and
notation will help to make this idea more precise.
TOPOLOGY IN Rn: TERMS AND FACTS
For any point pinRnand any real  > 0 , the open ball B.p; /with center pand
radius is given by
B.p; /D fxW kx pk< g
Given a set SinRn, a point pis an interior point ofSif there exists a  > 0
such that B.p; /S. If every open ball centered at pintersects both Sand the
complement of S, then pis called a boundary point ofS. A set is open if it
contains none of its boundary points. (This is equivalent to saying that all of its
points are interior points.) A set is closed if it contains all of its boundary points.
(IfScontains some but not all of its boundary points, then Sis neither open nor
closed.) A set Sisbounded if there exists a  > 0 such that SB.0; /. A set
inRniscompact if it is closed and bounded.
Theorem: The convex hull of an open set is open, and the convex hull of a
compact set is compact. (The convex hull of a closed set need not be closed. See
Exercise 27.)
EXAMPLE 7 Let
SDconv 2
2
; 2
 2
;2
 2
;2
2
;p1D 1
0
;and p2D2
1
;
as shown in Figure 3. Then p1is an interior point since B 
p;3
4
S. The point p2
is a boundary point since every open ball centered at p2intersects both Sand the
complement of S. The set Sis closed since it contains all its boundary points. The
setSis bounded since SB.0; 3/. Thus Sis also compact.
xS
B(0, 3)y
p1p2FIGURE 3
The set Sis closed and bounded.
SECOND REVISED PAGES


--- Page 485 ---
468 CHAPTER 8 The Geometry of Vector Spaces
Notation: Iffis a linear functional, then f .A/dmeans f .x/dfor each x2A.
Corresponding notations will be used when the inequalities are reversed or when they
are strict.
D E F I N I T I O N The hyperplane HD¬åf:d¬çseparates two sets AandBif one of the following
holds:
(i)f .A/dandf .B/d, or
(ii)f .A/dandf .B/d.
If in the conditions above all the weak inequalities are replaced by strict inequal-
ities, then His said to strictly separate AandB.
Notice that strict separation requires that the two sets be disjoint, while mere sep-
aration does not. Indeed, if two circles in the plane are externally tangent, then their
common tangent line separates them (but does not separate them strictly).
Although it is necessary that two sets be disjoint in order to strictly separate them,
this condition is not sufÔ¨Åcient, even for closed convex sets. For example, let
ADx
y
Wx1
2and1
xy2
and BDx
y
Wx0andyD0
Then AandBare disjoint closed convex sets, but they cannot be strictly separated
by a hyperplane (line in R2). See Figure 4. Thus the problem of separating (or strictly
separating) two sets by a hyperplane is more complex than it might at Ô¨Årst appear.
2
24y
xA
B
FIGURE 4 Disjoint closed convex sets.
There are many interesting conditions on the sets AandBthat imply the existence
of a separating hyperplane, but the following two theorems are sufÔ¨Åcient for this section.
The proof of the Ô¨Årst theorem requires quite a bit of preliminary material,3but the second
theorem follows easily from the Ô¨Årst.
T H E O R E M 1 2 Suppose AandBare nonempty convex sets such that Ais compact and Bis
closed. Then there exists a hyperplane Hthat strictly separates AandBif and
only if A\BD¬ø.
T H E O R E M 1 3 Suppose AandBare nonempty compact sets. Then there exists a hyperplane that
strictly separates AandBif and only if .conv A/\.conv B/D¬ø.
3A proof of Theorem 12 is given in Steven R. Lay, Convex Sets and Their Applications (New York: John
Wiley & Sons, 1982; Mineola, NY: Dover Publications, 2007), pp. 34‚Äì39.
SECOND REVISED PAGES


--- Page 486 ---
8.4 Hyperplanes 469
PROOF Suppose that .conv A/\.conv B/D¬ø. Since the convex hull of a compact
set is compact, Theorem 12 ensures that there is a hyperplane Hthat strictly separates
conv Aand conv B. Clearly, Halso strictly separates the smaller sets AandB.
Conversely, suppose the hyperplane HD¬åf:d¬çstrictly separates AandB. With-
out loss of generality, assume that f .A/ < d andf .B/ > d . Let xDc1x1C  C ckxk
be any convex combination of elements of A. Then
f .x/Dc1f .x1/C  C ckf .xk/ < c 1dC  C ckdDd
since c1C  C ckD1. Thus f .conv A/ < d . Likewise, f .conv B/ > d , soHD
¬åf:d¬çstrictly separates conv Aand conv B. By Theorem 12, conv Aand conv Bmust
be disjoint.
EXAMPLE 8 Let
a1D2
42
1
13
5;a2D2
4 3
2
13
5;a3D2
43
4
03
5;b1D2
41
0
23
5;and b2D2
42
 1
53
5;
and let AD fa1;a2;a3gandBD fb1;b2g. Show that the hyperplane HD¬åf:5¬ç, where
f .x 1; x2; x3/D2x1 3x2Cx3, does not separate AandB. Is there a hyperplane par-
allel to Hthat does separate AandB? Do the convex hulls of AandBintersect?
SOLUTION Evaluate the linear functional fat each of the points in AandB:
f .a1/D2; f . a2/D  11; f . a3/D  6; f . b1/D4; and f .b2/D12
Since f .b1/D4is less than 5 and f .b2/D12is greater than 5, points of Blie on both
sides of HD¬åf:5¬çand so Hdoes not separate AandB.
Since f .A/ < 3 andf .B/ > 3 , the parallel hyperplane ¬åf:3¬çstrictly separates A
andB. By Theorem 13, .conv A/\.conv B/D¬ø.
Caution: If there were no hyperplane parallel to Hthat strictly separated AandB,
this would notnecessarily imply that their convex hulls intersect. It might be that some
other hyperplane not parallel to Hwould strictly separate them.
PRACTICE PROBLEM
Letp1D2
41
0
23
5,p2D2
4 1
2
13
5,n1D2
41
1
 23
5, and n2D2
4 2
1
33
5; letH1be the hyper-
plane (plane) in R3passing through the point p1and having normal vector n1; and let
H2be the hyperplane passing through the point p2and having normal vector n2. Give
an explicit description of H1\H2by a formula that shows how to generate all points
inH1\H2.
8.4 EXERCISES
1.LetLbe the line in R2through the points 1
4
and3
1
.
Find a linear functional fand a real number dsuch that
LD¬åf:d¬ç.2.LetLbe the line in R2through the points1
4
and 2
 1
.
Find a linear functional fand a real number dsuch that
LD¬åf:d¬ç.
SECOND REVISED PAGES


--- Page 487 ---
470 CHAPTER 8 The Geometry of Vector Spaces
In Exercises 3 and 4, determine whether each set is open or closed
or neither open nor closed.
3.a.f.x; y/ Wy > 0g
b.f.x; y/ WxD2and1y3g
c.f.x; y/ WxD2and1 < y < 3 g
d.f.x; y/ WxyD1andx > 0g
e.f.x; y/ Wxy1andx > 0g
4.a.f.x; y/ Wx2Cy2D1g
b.f.x; y/ Wx2Cy2> 1g
c.f.x; y/ Wx2Cy21andy > 0g
d.f.x; y/ Wyx2g
e.f.x; y/ Wy < x2g
In Exercises 5 and 6, determine whether or not each set is compact
and whether or not it is convex.
5.Use the sets from Exercise 3.
6.Use the sets from Exercise 4.
In Exercises 7‚Äì10, let Hbe the hyperplane through the listed
points. (a) Find a vector nthat is normal to the hyperplane. (b) Find
a linear functional fand a real number dsuch that HD¬åf:d¬ç.
7.2
41
1
33
5,2
42
4
13
5,2
4 1
 2
53
5 8.2
41
 2
13
5,2
44
 2
33
5,2
47
 4
43
5
9.2
6641
0
1
03
775,2
6642
3
1
03
775,2
6641
2
2
03
775,2
6641
1
1
13
775
10.2
6641
2
0
03
775,2
6642
2
 1
 33
775,2
6641
3
2
73
775,2
6643
2
 1
 13
775
11.LetpD2
6641
 3
1
23
775,nD2
6642
1
5
 13
775,v1D2
6640
1
1
13
775,v2D2
664 2
0
1
33
775,
andv3D2
6641
4
0
43
775, and let Hbe the hyperplane in R4with
normal nand passing through p. Which of the points v1,v2,
andv3are on the same side of Has the origin, and which are
not?
12. Leta1D2
42
 1
53
5,a2D2
43
1
33
5,a3D2
4 1
6
03
5,b1D2
40
5
 13
5,
b2D2
41
 3
 23
5,b3D2
42
2
13
5, and nD2
43
1
 23
5, and let
AD fa1;a2;a3gandBD fb1;b2;b3g. Find a hyperplane Hwith normal nthat separates AandB. Is there a hyperplane
parallel to Hthat strictly separates AandB?
13. Let p1D2
6642
 3
1
23
775,p2D2
6641
2
 1
33
775,n1D2
6641
2
4
23
775, and
n2D2
6642
3
1
53
775; letH1be the hyperplane in R4through p1with
normal n1; and let H2be the hyperplane through p2with
normal n2. Give an explicit description of H1\H2. [Hint:
Find a point pinH1\H2and two linearly independent
vectors v1and v2that span a subspace parallel to the 2-
dimensional Ô¨Çat H1\H2.]
14. LetF1andF2be 4-dimensional Ô¨Çats in R6, and suppose that
F1\F2¬§¬ø. What are the possible dimensions of F1\F2?
In Exercises 15‚Äì20, write a formula for a linear functional fand
specify a number d, so that ¬åf:d¬çis the hyperplane Hdescribed
in the exercise.
15. LetAbe the 14matrix1 3 4  2
and let bD5. Let
HD fxinR4WAxDbg.
16. LetAbe the 15matrix2 5  3 0 6
. Note that
NulAis inR5. LetHDNulA.
17. LetHbe the plane in R3spanned by the rows of BD1 3 5
0 2 4
. That is, HDRowB. [Hint: How is H
related to Nul B? See Section 6.1.]
18. LetHbe the plane in R3spanned by the rows of BD1 4  5
0 2 8
. That is, HDRowB.
19. LetHbe the column space of the matrix BD2
41 0
4 2
 7 63
5.
That is, HDColB. [Hint: How is Col Brelated to Nul BT?
See Section 6.1.]
20. LetHbe the column space of the matrix BD2
41 0
5 2
 4 43
5.
That is, HDColB.
In Exercises 21 and 22, mark each statement True or False. Justify
each answer.
21. a.A linear transformation from RtoRnis called a linear
functional.
b.Iffis a linear functional deÔ¨Åned on Rn, then there exists
a real number ksuch that f .x/Dkxfor all xinRn.
c.If a hyperplane strictly separates sets AandB, then
A\BD¬ø.
d.IfAandBare closed convex sets and A\BD¬ø, then
there exists a hyperplane that strictly separates AandB.
SECOND REVISED PAGES


--- Page 488 ---
8.5 Polytopes 471
22. a.Ifdis a real number and fis a nonzero linear functional
deÔ¨Åned on Rn, then ¬åf:d¬çis a hyperplane in Rn.
b.Given any vector nand any real number d, the set
fxWnxDdgis a hyperplane.
c.IfAandBare nonempty disjoint sets such that Ais
compact and Bis closed, then there exists a hyperplane
that strictly separates AandB.
d.If there exists a hyperplane Hsuch that Hdoes not
strictly separate two sets AandB, then .conv A/\
.conv B/¬§¬ø.
23. Letv1D1
1
,v2D3
0
,v3D5
3
, and pD4
1
. Find
a hyperplane ¬åf:d¬ç(in this case, a line) that strictly separates
pfrom conv fv1;v2;v3g.
24. Repeat Exercise 23 for v1D1
2
,v2D5
1
,v3D4
4
,
andpD2
3
.25. LetpD4
1
. Find a hyperplane ¬åf:d¬çthat strictly sepa-
rates B.0; 3/andB.p; 1/. [Hint: After Ô¨Ånding f, show that
the point vD.1 :75/0C:75pis neither in B.0; 3/nor in
B.p; 1/:¬ç
26. LetqD2
3
andpD6
1
. Find a hyperplane ¬åf:d¬çthat
strictly separates B.q; 3/andB.p; 1/.
27. Give an example of a closed subset SofR2such that conv S
is not closed.
28. Give an example of a compact set Aand a closed set BinR2
such that .conv A/\.conv B/D¬øbutAandBcannot be
strictly separated by a hyperplane.
29. Prove that the open ball B.p; /D fxW kx pk< gis a
convex set. [ Hint: Use the Triangle Inequality.]
30. Prove that the convex hull of a bounded set is bounded.
SOLUTION TO PRACTICE PROBLEM
First, compute n1p1D  3andn2p2D7. The hyperplane H1is the solution set of the
equation x1Cx2 2x3D  3, and H2is the solution set of the equation  2x1Cx2C
3x3D7. Then
H1\H2D fxWx1Cx2 2x3D  3and 2x1Cx2C3x3D7g
This is an implicit description of H1\H2. To Ô¨Ånd an explicit description, solve the
system of equations by row reduction:
1 1  2 3
 2 1 3 7
"
1 0  5
3 10
3
0 1  1
31
3#
Thus x1D  10
3C5
3x3,x2D1
3C1
3x3,x3Dx3. Let pD2
664 10
3
1
3
03
775andvD2
6645
3
1
3
13
775. The
general solution can be written as xDpCx3v. Thus H1\H2is the line through pin
the direction of v. Note that vis orthogonal to both n1andn2.
8.5 POLYTOPES
This section studies geometric properties of an important class of compact convex sets
called polytopes. These sets arise in all sorts of applications, including game theory
(Section 9.1), linear programming (Sections 9.2 to 9.4), and more general optimization
problems, such as the design of feedback controls for engineering systems.
SECOND REVISED PAGES


--- Page 489 ---
472 CHAPTER 8 The Geometry of Vector Spaces
Apolytope inRnis the convex hull of a Ô¨Ånite set of points. In R2, a polytope
is simply a polygon. In R3, a polytope is called a polyhedron. Important features of
a polyhedron are its faces, edges, and vertices. For example, the cube has 6 square
faces, 12 edges, and 8 vertices. The following deÔ¨Ånitions provide terminology for higher
dimensions as well as R2andR3. Recall that the dimension of a set in Rnis the dimen-
sion of the smallest Ô¨Çat that contains it. Also, note that a polytope is a special type of
compact convex set, because a Ô¨Ånite set in Rnis compact and the convex hull of this set
is compact, by the theorem in the topology terms and facts box in Section 8.4.
D E F I N I T I O N LetSbe a compact convex subset of Rn. A nonempty subset FofSis called
a (proper) face ofSifF¬§Sand there exists a hyperplane HD¬åf:d¬çsuch
thatFDS\Hand either f .S/dorf .S/d. The hyperplane His called
asupporting hyperplane toS. If the dimension of Fisk, then Fis called a
k-face ofS.
IfPis a polytope of dimension k, then Pis called a k-polytope . A 0-face of P
is called a vertex (plural: vertices ), a 1-face is an edge , and a .k 1/-dimensional
face is a facet ofS.
EXAMPLE 1 Suppose Sis a cube in R3. When a plane His translated through
R3until it just touches (supports) the cube but does not cut through the interior of the
cube, there are three possibilities for H\S, depending on the orientation of H. (See
Figure 1.)
H\Smay be a 2-dimensional square face (facet) of the cube.
H\Smay be a 1-dimensional edge of the cube.
H\Smay be a 0-dimensional vertex of the cube.
H /H20669 S is 2-dimensional. H /H20669 S is 1-dimensional. H /H20669 S is 0-dimensional.SS S
HHH
FIGURE 1
Most applications of polytopes involve the vertices in some way, because they have
a special property that is identiÔ¨Åed in the following deÔ¨Ånition.
D E F I N I T I O N LetSbe a convex set. A point pinSis called an extreme point ofSifpis
not in the interior of any line segment that lies in S. More precisely, if x;y2S
andp2xy, then pDxorpDy. The set of all extreme points of Sis called the
proÔ¨Åle ofS.
SECOND REVISED PAGES


--- Page 490 ---
8.5 Polytopes 473
A vertex of any compact convex set Sis automatically an extreme point of S. This
fact is proved during the proof of Theorem 14, below. In working with a polytope, say
PDconvfv1; : : : ; vkgforv1; : : : ; vkinRn, it is usually helpful to know that v1; : : : ; vk
are the extreme points of P. However, such a list might contain extraneous points. For
example, some vector vicould be the midpoint of an edge of the polytope. Of course,
in this case viis not really needed to generate the convex hull. The following deÔ¨Ånition
describes the property of the vertices that will make them all extreme points.
D E F I N I T I O N The set fv1; : : : ; vkgis a minimal representation of the polytope PifPD
convfv1; : : : ; vkgand for each iD1; : : : ; k; vi62convfvjWj¬§ig.
Every polytope has a minimal representation. For if PDconvfv1; : : : ; vkgand if
some viis a convex combination of the other points, then vimay be deleted from the
set of points without changing the convex hull. This process may be repeated until the
minimal representation is left. It can be shown that the minimal representation is unique.
T H E O R E M 1 4 Suppose MD fv1; : : : ; vkgis the minimal representation of the polytope P. Then
the following three statements are equivalent:
a.p2M.
b.pis a vertex of P.
c.pis an extreme point of P.
PROOF (a))(b) Suppose p2Mand let QDconvfvWv2Mandv¬§pg. It fol-
HH '
p Q
FIGURE 2lows from the deÔ¨Ånition of Mthatp62Q, and since Qis compact, Theorem 13 implies
the existence of a hyperplane H0that strictly separates fpgandQ. LetHbe the hyper-
plane through pparallel to H0. See Figure 2.
Then Qlies in one of the closed half-spaces HCbounded by Hand so PHC.
Thus Hsupports Patp. Furthermore, pis the only point of Pthat can lie on H, so
H\PD fpgandpis a vertex of P.
(b))(c) Let pbe a vertex of P. Then there exists a hyperplane HD¬åf:d¬çsuch
thatH\PD fpgandf .P / d. Ifpwere not an extreme point, then there would
exist points xandyinPsuch that pD.1 c/xCcywith0 < c < 1 . That is,
cyDp .1 c/xand yD1
c
.p/ 1
c 1
.x/
It follows that f .y/D1
cf .p/ 1
c 1
f .x/. But f .p/Ddandf .x/d, so
f .y/1
c
.d/ 1
c 1
.d/Dd
On the other hand, y2P, sof .y/d. It follows that f .y/Ddand that y2H\P.
This contradicts the fact that pis a vertex. So pmust be an extreme point. (Note that
this part of the proof does not depend on Pbeing a polytope. It holds for any compact
convex set.)
(c))(a) It is clear that any extreme point of Pmust be a member of M.
SECOND REVISED PAGES


--- Page 491 ---
474 CHAPTER 8 The Geometry of Vector Spaces
EXAMPLE 2 Recall that the proÔ¨Åle of a set Sis the set of extreme points of S.
Theorem 14 shows that the proÔ¨Åle of a polygon in R2is the set of vertices. (See Figure 3.)
The proÔ¨Åle of a closed ball is its boundary. An open set has no extreme points, so its
proÔ¨Åle is empty. A closed half-space has no extreme points, so its proÔ¨Åle is empty.
FIGURE 3
Exercise 18 asks you to show that a point pin a convex set Sis an extreme point
ofSif and only if, when pis removed from S, the remaining points still form a convex
set. It follows that if Sis any subset of Ssuch that conv Sis equal to S, then Smust
contain the proÔ¨Åle of S. The sets in Example 2 show that in general Smay have to be
larger than the proÔ¨Åle of S. It is true, however, that when Sis compact, we may actually
takeSto be the proÔ¨Åle of S, as Theorem 15 will show. Thus every nonempty compact
convex set Shas an extreme point, and the set of all extreme points is the smallest subset
ofSwhose convex hull is equal to S.
T H E O R E M 1 5 LetSbe a nonempty compact convex set. Then Sis the convex hull of its proÔ¨Åle
(the set of extreme points of S).
PROOF The proof is by induction on the dimension of the set S.1
One important application of Theorem 15 is the following theorem. It is one of the
key theoretical results in the development of linear programming. Linear functionals
are continuous, and continuous functions always attain their maximum and minimum
on a compact set. The signiÔ¨Åcance of Theorem 16 is that for compact convex sets, the
maximum (and minimum) is actually attained at an extreme point of S.
T H E O R E M 1 6 Letfbe a linear functional deÔ¨Åned on a nonempty compact convex set S. Then
there exist extreme points OvandOwofSsuch that
f .Ov/Dmaxv2Sf .v/and f .Ow/Dminv2Sf .v/
PROOF Assume that fattains its maximum monSat some point v0inS. That is,
f .v0/Dm. We wish to show that there exists an extreme point in Swith the same
property. By Theorem 15, v0is a convex combination of the extreme points of S. That
is, there exist extreme points v1; : : : ; vkofSand nonnegative c1; : : : ; c ksuch that
v0Dc1v1C  C ckvkwithc1C  C ckD1
If none of the extreme points of SsatisÔ¨Åes f .v/Dm, then
f .vi/ < m foriD1; : : : ; k
1The details may be found in Steven R. Lay, Convex Sets and Their Applications (New York: John Wiley &
Sons, 1982; Mineola, NY: Dover Publications, 2007), p. 43.
SECOND REVISED PAGES


--- Page 492 ---
8.5 Polytopes 475
since mis the maximum of fonS. But then, because fis linear,
mDf .v0/Df .c 1v1C  C ckvk/
Dc1f .v1/C  C ckf .vk/
< c 1mC  C ckmDm.c 1C  C ck/Dm
This contradiction implies that some extreme point OvofSmust satisfy f .Ov/Dm.
The proof for Owis similar.
EXAMPLE 3 Given points p1D 1
0
,p2D3
1
, and p3D1
2
inR2, letSD
convfp1;p2;p3g. For each linear functional f, Ô¨Ånd the maximum value moffon the
setS, and Ô¨Ånd all points xinSat which f .x/Dm.
a.f1.x1; x2/Dx1Cx2b.f2.x1; x2/D  3x1Cx2c.f3.x1; x2/Dx1C2x2
SOLUTION By Theorem 16, the maximum value is attained at one of the extreme points
ofS. So to Ô¨Ånd m, evaluate fat each extreme point and select the largest value.
a.f1.p1/D  1,f1.p2/D4, and f1.p3/D3, som1D4. Graph the line f1.x1; x2/D
m1, that is, x1Cx2D4, and note that xDp2is the only point in Sat which f1.x/D
4. See Figure 4(a).
b.f2.p1/D3,f2.p2/D  8, andf2.p3/D  1, som2D3. Graph the line f2.x1; x2/D
m2, that is,  3x1Cx2D3, and note that xDp1is the only point in Sat which
f2.x/D3. See Figure 4(b).
c.f3.p1/D  1,f3.p2/D5, and f3.p3/D5, som3D5. Graph the line f3.x1; x2/D
m3, that is, x1C2x2D5. Here, f3attains its maximum value at p2, atp3, and at
every point in the convex hull of p2andp3. See Figure 4(c).
(a) x1 + x2 = 4 (b) ‚Äì3 x1 + x2 = 3 (c) x1 + 2x2 = 5p3
p1p2
2 ‚Äì2 424
x1x2
2 ‚Äì2 424
x1x2
2 ‚Äì2 424
x1x2
Sp3
p1p2 Sp3
p1p2 S
FIGURE 4
The situation illustrated in Example 3 for R2also applies in higher dimensions. The
maximum value of a linear functional fon a polytope Poccurs at the intersection of
a supporting hyperplane and P. This intersection is either a single extreme point of P,
or the convex hull of 2 or more extreme points of P. In either case, the intersection is a
polytope, and its extreme points form a subset of the extreme points of P.
By deÔ¨Ånition, a polytope is the convex hull of a Ô¨Ånite set of points. This is an explicit
representation of the polytope since it identiÔ¨Åes points in the set. A polytope may also
be represented implicitly as the intersection of a Ô¨Ånite number of closed half-spaces.
Example 4 illustrates this in R2.
SECOND REVISED PAGES


--- Page 493 ---
476 CHAPTER 8 The Geometry of Vector Spaces
EXAMPLE 4 Let
p1D0
1
;p2D1
0
;and p3D3
2
inR2, and let SDconvfp1;p2;p3g. Simple algebra shows that the line through p1and
p2is given by x1Cx2D1, and Sis on the side of this line where
x1Cx21or, equivalently,  x1 x2  1:
Similarly, the line through p2andp3isx1 x2D1, and Sis on the side where
x1 x21
Also, the line through p3andp1is x1C3x2D3, and Sis on the side where
 x1C3x23:
See Figure 5. It follows that Scan be described as the solution set of the system of linear
inequalities
 x1 x2  1
x1 x21
 x1C3x23
This system may be written as Axb, where
AD2
4 1 1
1 1
 1 33
5;xDx1
x2
;and bD2
4 1
1
33
5:
Note that an inequality between two vectors, such as Axandb, applies to each of the
corresponding coordinates in those vectors.
‚Äìx1 + 3x2 = 3
x1 + x2 = 1x1 ‚Äì x2 = 1
p3
p2p1S
4 2 ‚Äì2x2
x14
2
FIGURE 5
In Chapter 9, it will be necessary to replace an implicit description of a polytope by
a minimal representation of the polytope, listing all the extreme points of the polytope.
In simple cases, a graphical solution is feasible. The following example shows how to
handle the situation when several points of interest are too close to identify easily on a
graph.
EXAMPLE 5 LetPbe the set of points in R2that satisfy Axb, where
AD2
41 3
1 1
3 23
5and bD2
418
8
213
5
andx0. Find the minimal representation of P.
SECOND REVISED PAGES


--- Page 494 ---
8.5 Polytopes 477
SOLUTION The condition x0places Pin the Ô¨Årst quadrant of R2, a typical con-
dition in linear programming problems. The three inequalities in Axbinvolve three
boundary lines:
.1/ x 1C3x2D18 .2/ x 1Cx2D8 .3/ 3x 1C2x2D21
All three lines have negative slopes, so a general idea of the shape of Pis easy to
visualize. Even a rough sketch of the graphs of these lines will reveal that .0; 0/ ,.7; 0/ ,
and.0; 6/ are vertices of the polytope P.
What about the intersections of the lines (1), (2), and (3)? Sometimes it is clear
from the graph which intersections to include. But if not, then the following algebraic
procedure will work well:
When an intersection point is found that corresponds to two inequalities, test it
in the other inequalities to see whether the point is in the polytope.
The intersection of (1) and (2) is p12D.3; 5/ . Both coordinates are nonnegative,
sop12satisÔ¨Åes all inequalities except possibly the third inequality. Test this:
3.3/C2.5/D19 < 21
This intersection point satisÔ¨Åes the inequality for (3), so p12is in the polytope.
The intersection of (2) and (3) is p23D.5; 3/ . This satisÔ¨Åes all inequalities except
possibly the inequality for (1). Test this:
1.5/C3.3/D14 < 18
This shows that p23is in the polytope.
Finally, the intersection of (1) and (3) is p13D 27
7;33
7
. Test this in the inequality
for (2):
1 27
7
C1 33
7
D60
78:6 > 8
Thus p13does notsatisfy the second inequality, which shows that p13isnotinP. In
conclusion, the minimal representation of the polytope Pis
0
0
;7
0
;3
5
;5
3
;0
6
:
The remainder of this section discusses the construction of two basic polytopes
inR3(and higher dimensions). The Ô¨Årst appears in linear programming problems, the
subject of Chapter 9. Both polytopes provide opportunities to visualize R4in a remark-
able way.
Simplex
Asimplex is the convex hull of an afÔ¨Ånely independent Ô¨Ånite set of vectors. To construct
ak-dimensional simplex (or k-simplex), proceed as follows:
0-simplex S0: a single point fv1g
1-simplex S1: conv .S0[ fv2g/, with v2not in aff S0
2-simplex S2: conv .S1[ fv3g/, with v3not in aff S1
:::
k-simplex Sk: conv .Sk 1[ fvkC1g/;with vkC1not in aff Sk 1
The simplex S1is a line segment. The triangle S2comes from choosing a point
v3that is not in the line containing S1and then forming the convex hull with S1.
SECOND REVISED PAGES


--- Page 495 ---
478 CHAPTER 8 The Geometry of Vector Spaces
S0v1v1 v1 v1 v4
v2 v2 v3 v2 v3
S1S2S3
FIGURE 6
(See Figure 6.) The tetrahedron S3is produced by choosing a point v4not in the plane
ofS2and then forming the convex hull with S2.
Before continuing, consider some of the patterns that are appearing. The triangle S2
has three edges. Each of these edges is a line segment like S1. Where do these three line
segments come from? One of them is S1. One of them comes by joining the endpoint
v2to the new point v3. The third comes from joining the other endpoint v1tov3. You
might say that each endpoint in S1is stretched out into a line segment in S2.
The tetrahedron S3in Figure 6 has four triangular faces. One of these is the original
triangle S2, and the other three come from stretching the edges of S2out to the new
point v4. Notice too that the vertices of S2get stretched out into edges in S3. The other
edges in S3come from the edges in S2. This suggests how to ‚Äúvisualize‚Äù the four-
dimensional S4.
The construction of S4, called a pentatope, involves forming the convex hull of S3
with a point v5not in the 3-space of S3. A complete picture is impossible, of course,
but Figure 7 is suggestive: S4has Ô¨Åve vertices, and any four of the vertices determine
a facet in the shape of a tetrahedron. For example, the Ô¨Ågure emphasizes the facet with
vertices v1,v2,v4, and v5and the facet with vertices v2,v3,v4, and v5. There are Ô¨Åve
v4
v3v5
v2v1
v4
v3v5
v2v1v4
v3v5
v2v1
FIGURE 7 The4-dimensional simplex S4projected onto R2, with two
tetrahedral facets emphasized.
SECOND REVISED PAGES


--- Page 496 ---
8.5 Polytopes 479
such facets. Figure 7 identiÔ¨Åes all ten edges of S4, and these can be used to visualize
the ten triangular faces.
Figure 8 shows another representation of the 4-dimensional simplex S4. This time
the Ô¨Åfth vertex appears ‚Äúinside‚Äù the tetrahedron S3. The highlighted tetrahedral facets
also appear to be ‚Äúinside‚Äù S3.
v1 v3v2v5v4
v1 v3v2v5v4v1 v3v2v4
v1 v3v2v5v4
FIGURE 8 The Ô¨Åfth vertex of S4is ‚Äúinside‚Äù S3.
Hypercube
LetIiD0eibe the line segment from the origin 0to the standard basis vector eiinRn.
Then for ksuch that 1kn, the vector sum2
CkDI1CI2C  C Ik
is called a k-dimensional hypercube .
To visualize the construction of Ck, start with the simple cases. The hypercube C1
is the line segment I1. IfC1is translated by e2, the convex hull of its initial and Ô¨Ånal
positions describes a square C2. (See Figure 9.) Translating C2bye3creates the cube
C3. A similar translation of C3by the vector e4yields the 4-dimensional hypercube C4.
Again, this is hard to visualize, but Figure 10 shows a 2-dimensional projection
ofC4. Each of the edges of C3is stretched into a square face of C4. And each of the
square faces of C3is stretched into a cubic face of C4. Figure 11 shows three facets
ofC4. Part (a) highlights the cube that comes from the left square face of C3. Part (b)
shows the cube that comes from the front square face of C3. And part (c) emphasizes
the cube that comes from the top square face of C3.
2The vector sum of two sets AandBis deÔ¨Åned by ACBD fcWcDaCbfor some a2Aandb2Bg.
SECOND REVISED PAGES


--- Page 497 ---
480 CHAPTER 8 The Geometry of Vector Spaces
C1C2C3
FIGURE 9 Constructing the cube C3.
FIGURE 10 C4projected onto R2.
(a) (b) (c)
FIGURE 11 Three of the cubic facets of C4.
Figure 12 shows another representation of C4in which the translated cube is placed
‚Äúinside‚Äù C3. This makes it easier to visualize the cubic facets of C4, since there is less
distortion.
FIGURE 12 The translated image of
C3is placed ‚Äúinside‚Äù C3to obtain C4.
Altogether, the 4-dimensional cube C4has eight cubic faces. Two come from the
original and translated images of C3, and six come from the square faces of C3that are
stretched into cubes. The square 2-dimensional faces of C4come from the square faces
SECOND REVISED PAGES


--- Page 498 ---
8.5 Polytopes 481
ofC3and its translate, and the edges of C3that are stretched into squares. Thus there
are26C12D24square faces. To count the edges, take 2 times the number of edges
inC3and add the number of vertices in C3. This makes 212C8D32edges in C4.
The vertices in C4all come from C3and its translate, so there are 28D16vertices.
One of the truly remarkable results in the study of polytopes is the following for-
mula, Ô¨Årst proved by Leonard Euler (1707‚Äì1783). It establishes a simple relationship
between the number of faces of different dimensions in a polytope. To simplify the
statement of the formula, let fk.P / denote the number of k-dimensional faces of an
n-dimensional polytope P.3
Euler‚Äôs formula:n 1X
kD0. 1/kfk.P /D1C. 1/n 1
In particular, when nD3; v eCfD2, where v,e, and fdenote the number of
vertices, edges, and facets (respectively) of P.
PRACTICE PROBLEM
Find the minimal representation of the polytope PdeÔ¨Åned by the inequalities Axb
andx0, when AD2
41 3
1 2
2 13
5andbD2
412
9
123
5.
8.5 EXERCISES
1.Given points p1D1
0
,p2D2
3
, and p3D 1
2
inR2,
letSDconvfp1;p2;p3g. For each linear functional f, Ô¨Ånd
the maximum value moffon the set S, and Ô¨Ånd all points
xinSat which f .x/Dm.
a.f .x 1; x2/Dx1 x2 b.f .x 1; x2/Dx1Cx2
c.f .x 1; x2/D  3x1Cx2
2.Given points p1D0
 1
,p2D2
1
, and p3D1
2
inR2,
letSDconvfp1;p2;p3g. For each linear functional f, Ô¨Ånd
the maximum value moffon the set S, and Ô¨Ånd all points
xinSat which f .x/Dm.
a.f .x 1; x2/Dx1Cx2 b.f .x 1; x2/Dx1 x2
c.f .x 1; x2/D  2x1Cx2
3.Repeat Exercise 1 where mis the minimum value of fonS
instead of the maximum value.
4.Repeat Exercise 2 where mis the minimum value of fonS
instead of the maximum value.
In Exercises 5‚Äì8, Ô¨Ånd the minimal representation of the polytope
deÔ¨Åned by the inequalities Axbandx0.
5.AD1 2
3 1
,bD10
156.AD2 3
4 1
,bD18
16
7.AD2
41 3
1 1
4 13
5,bD2
418
10
283
5
8.AD2
42 1
1 1
1 23
5,bD2
48
6
73
5
9.LetSD f.x; y/ Wx2C.y 1/21g [ f.3; 0/g. Is the ori-
gin an extreme point of conv S? Is the origin a vertex of
conv S?
10. Find an example of a closed convex set SinR2such that its
proÔ¨Åle Pis nonempty but conv P¬§S.
11.Find an example of a bounded convex set SinR2such that
its proÔ¨Åle Pis nonempty but conv P¬§S.
12. a.Determine the number of k-faces of the 5-dimensional
simplex S5forkD0; 1; : : : ; 4 . Verify that your answer
satisÔ¨Åes Euler‚Äôs formula.
b.Make a chart of the values of fk.Sn/fornD1; : : : ; 5 and
kD0; 1; : : : ; 4 . Can you see a pattern? Guess a general
formula for fk.Sn/.
3A proof when nD3is presented in Steven R. Lay, Convex Sets and Their Applications (New York:
John Wiley & Sons, 1982; Mineola, NY: Dover Publications, 2007), p. 131.
SECOND REVISED PAGES


--- Page 499 ---
482 CHAPTER 8 The Geometry of Vector Spaces
13. a.Determine the number of k-faces of the 5-dimensional
hypercube C5forkD0; 1; : : : ; 4 . Verify that your an-
swer satisÔ¨Åes Euler‚Äôs formula.
b.Make a chart of the values of fk.Cn/fornD1; : : : ; 5 and
kD0; 1; : : : ; 4 . Can you see a pattern? Guess a general
formula for fk.Cn/.
14. Suppose v1; : : : ; vkare linearly independent vectors in
Rn.1kn/. Then the set XkDconvfv1; : : : ;vkgis
called a k-crosspolytope .
a.Sketch X1andX2.
b.Determine the number of k-faces of the 3-dimensional
crosspolytope X3forkD0; 1; 2 . What is another name
forX3?
c.Determine the number of k-faces of the 4-dimensional
crosspolytope X4forkD0; 1; 2; 3 . Verify that your an-
swer satisÔ¨Åes Euler‚Äôs formula.
d.Find a formula for fk.Xn/, the number of k-faces of Xn,
for0kn 1.
15. Ak-pyramid Pkis the convex hull of a .k 1/-polytope
Qand a point x62affQ. Find a formula for each of the
following in terms of fj.Q/; j D0; : : : ; n  1.
a.The number of vertices of Pn:f0.Pn/.
b.The number of k-faces of Pn:fk.Pn/, for1kn 2.
c.The number of .n 1/-dimensional facets of Pn:
fn 1.Pn/.
In Exercises 16 and 17, mark each statement True or False. Justify
each answer.
16. a.A polytope is the convex hull of a Ô¨Ånite set of points.
b.Letpbe an extreme point of a convex set S. Ifu;v2S,
p2uv, and p¬§u, then pDv.
c.IfSis a nonempty convex subset of Rn, then Sis the
convex hull of its proÔ¨Åle.
d.The 4-dimensional simplex S4has exactly Ô¨Åve facets,
each of which is a 3-dimensional tetrahedron.17. a.A cube in R3has exactly Ô¨Åve facets.
b.A point pis an extreme point of a polytope Pif and only
ifpis a vertex of P.
c.IfSis a nonempty compact convex set and a linear
functional attains its maximum at a point p, then pis an
extreme point of S.
d.A 2-dimensional polytope always has the same number
of vertices and edges.
18. Letvbe an element of the convex set S. Prove that vis an
extreme point of Sif and only if the set fx2SWx¬§vgis
convex.
19. Ifc2RandSis a set, deÔ¨Åne cSD fcxWx2Sg. Let S
be a convex set and suppose c > 0 andd > 0 . Prove that
cSCdSD.cCd/S.
20. Find an example to show that the convexity of Sis necessary
in Exercise 19.
21. IfAandBare convex sets, prove that ACBis convex.
22. A polyhedron (3-polytope) is called regular if all its facets
are congruent regular polygons and all the angles at the
vertices are equal. Supply the details in the following proof
that there are only Ô¨Åve regular polyhedra.
a.Suppose that a regular polyhedron has rfacets, each of
which is a k-sided regular polygon, and that sedges
meet at each vertex. Letting vandedenote the numbers
of vertices and edges in the polyhedron, explain why
krD2eandsvD2e.
b.Use Euler‚Äôs formula to show that1
sC1
kD1
2C1
e.
c.Find all the integral solutions of the equation in part
(b) that satisfy the geometric constraints of the problem.
(How small can kandsbe?)
For your information, the Ô¨Åve regular polyhedra are the
tetrahedron (4, 6, 4), the cube (8, 12, 6), the octahedron (6, 12,
8), the dodecahedron (20, 30, 12), and the icosahedron (12,
30, 20). (The numbers in parentheses indicate the numbers of
vertices, edges, and faces, respectively.)
SOLUTION TO PRACTICE PROBLEM
The matrix inequality Axbyields the following system of inequalities:
(a)x1C3x212
(b)x1C2x29
(c)2x1Cx212
The condition x0, places the polytope in the Ô¨Årst quadrant of the plane. One vertex
is.0; 0/ . The x1-intercepts of the three lines (when x2D0) are 12, 9, and 6, so .6; 0/ is
a vertex. The x2-intercepts of the three lines (when x1D0) are 4, 4.5, and 12, so .0; 4/
is a vertex.
SECOND REVISED PAGES


--- Page 500 ---
8.6 Curves and Surfaces 483
How do the three boundary lines intersect for positive values of x1andx2? The
intersection of (a) and (b) is at pabD.3; 3/ . Testing pabin (c) gives 2.3/C1.3/D
9 < 12 , sopabis in P. The intersection of (b) and (c) is at pbcD.5; 2/ . Testing pbc
in (a) gives 1.5/C3.2/D11 < 12 , sopbcis inP. The intersection of (a) and (c) is at
pacD.4:8; 2:4/ . Testing pacin (b) gives 1.4:8/ C2.2:4/ D9:6 > 9 . Sopacis not in P.
Finally, the Ô¨Åve vertices (extreme points) of the polytope are .0; 0/ ,.6; 0/ ,.5; 2/
.3; 3/ , and .0; 4/ . These points form the minimal representation of P. This is displayed
graphically in Figure 13.
(a)(c)
(b)P
48 1 24812x2
x1
FIGURE 13
8.6 CURVES AND SURFACES
For thousands of years, builders used long thin strips of wood to create the hull of a boat.
In more recent times, designers used long, Ô¨Çexible metal strips to lay out the surfaces of
cars and airplanes. Weights and pegs shaped the strips into smooth curves called natural
cubic splines . The curve between two successive control points (pegs or weights) has
a parametric representation using cubic polynomials. Unfortunately, such curves have
the property that moving one control point affects the shape of the entire curve, because
of physical forces that the pegs and weights exert on the strip. Design engineers had
long wanted local control of the curve‚Äîin which movement of one control point would
affect only a small portion of the curve. In 1962, a French automotive engineer, Pierre
B√©zier, solved this problem by adding extra control points and using a class of curves
now called by his name.
B√©zier Curves
The curves described below play an important role in computer graphics as well as
engineering. For example, they are used in Adobe Illustrator and Macromedia Freehand,
and in application programming languages such as OpenGL. These curves permit a
program to store exact information about curved segments and surfaces in a relatively
small number of control points. All graphics commands for the segments and surfaces
have only to be computed for the control points. The special structure of these curves
also speeds up other calculations in the ‚Äúgraphics pipeline‚Äù that creates the Ô¨Ånal display
on the viewing screen.
Exercises in Section 8.3 introduced quadratic B√©zier curves and showed one method
for constructing B√©zier curves of higher degree. The discussion here focuses on quadratic
and cubic B√©zier curves, which are determined by three or four control points, denoted
SECOND REVISED PAGES


--- Page 501 ---
484 CHAPTER 8 The Geometry of Vector Spaces
byp0,p1,p2, and p3. These points can be in R2orR3, or they can be represented by
homogeneous forms in R3orR4. The standard parametric descriptions of these curves,
for0t1, are
w.t/D.1 t/2p0C2t.1 t/p1Ct2p2 (1)
x.t/D.1 t/3p0C3t.1 t/2p1C3t2.1 t/p2Ct3p3 (2)
Figure 1 shows two typical curves. Usually, the curves pass through only the initial and
terminal control points, but a B√©zier curve is always in the convex hull of its control
points. (See Exercises 21‚Äì24 in Section 8.3.)
p1p2
p0p1p2
p0p3
FIGURE 1 Quadratic and cubic B√©zier curves.
B√©zier curves are useful in computer graphics because their essential properties are
preserved under the action of linear transformations and translations. For instance, if
Ais a matrix of appropriate size, then from the linearity of matrix multiplication, for
0t1,
Ax.t/DA¬å.1 t/3p0C3t.1 t/2p1C3t2.1 t/p2Ct3p3¬ç
D.1 t/3Ap0C3t.1 t/2Ap1C3t2.1 t/Ap2Ct3Ap3
The new control points are Ap0; : : : ; A p3. Translations of B√©zier curves are considered
in Exercise 1.
The curves in Figure 1 suggest that the control points determine the tangent lines
to the curves at the initial and terminal control points. Recall from calculus that for any
parametric curve, say y.t/, the direction of the tangent line to the curve at a point y.t/
is given by the derivative y0.t/, called the tangent vector of the curve. (This derivative
is computed entry by entry.)
EXAMPLE 1 Determine how the tangent vector of the quadratic B√©zier curve w.t/
is related to the control points of the curve, at tD0andtD1.
SOLUTION Write the weights in equation (1) as simple polynomials
w.t/D.1 2tCt2/p0C.2t 2t2/p1Ct2p2
Then, because differentiation is a linear transformation on functions,
w0.t/D. 2C2t/p0C.2 4t/p1C2tp2
So
w0.0/D  2p0C2p1D2.p1 p0/
w0.1/D  2p1C2p2D2.p2 p1/
The tangent vector at p0, for instance, points from p0top1, but it is twice as long as the
segment from p0top1. Notice that w0.0/D0when p1Dp0. In this case,
w.t/D.1 t2/p1Ct2p2, and the graph of w.t/is the line segment from p1
top2.
SECOND REVISED PAGES


--- Page 502 ---
8.6 Curves and Surfaces 485
Connecting Two B√©zier Curves
Two basic B√©zier curves can be joined end to end, with the terminal point of the Ô¨Årst
curve x.t/being the initial point p2of the second curve y.t/. The combined curve is
said to have G0geometric continuity (atp2) because the two segments join at p2. If the
tangent line to curve 1atp2has a different direction than the tangent line to curve 2,
then a ‚Äúcorner,‚Äù or abrupt change of direction, may be apparent at p2. See Figure 2.
p2p3p4
p1
p0
FIGURE 2 G0continuity at p2.
To avoid a sharp bend, it usually sufÔ¨Åces to adjust the curves to have what is called
G1geometric continuity , where both tangent vectors at p2point in the same direction.
That is, the derivatives x0.1/andy0.0/point in the same direction, even though their
magnitudes may be different. When the tangent vectors are actually equal at p2, the
tangent vector is continuous at p2, and the combined curve is said to have C1continuity,
orC1parametric continuity . Figure 3 shows G1continuity in (a) and C1continuity
in (b).
2
0
2 0 4 6 8 10 12 14
(a) (b)p1
p0p2 p3
p4p1
p0p2 p3
p4
FIGURE 3 (a)G1continuity and (b) C1continuity.
EXAMPLE 2 Letx.t/andy.t/determine two quadratic B√©zier curves, with control
points fp0;p1;p2gandfp2;p3;p4g, respectively. The curves are joined at p2Dx.1/D
y.0/.
a.Suppose the combined curve has G1continuity (at p2). What algebraic restriction
does this condition impose on the control points? Express this restriction in geomet-
ric language.
b.Repeat part (a) for C1continuity.
SOLUTION
a.From Example 1, x0.1/D2.p2 p1/. Also, using the control points for y.t/in place
ofw.t/, Example 1 shows that y0.0/D2.p3 p2/.G1continuity means that
y0.0/Dkx0.1/for some positive constant k. Equivalently,
p3 p2Dk.p2 p1/;withk > 0 (3)
SECOND REVISED PAGES


--- Page 503 ---
486 CHAPTER 8 The Geometry of Vector Spaces
Geometrically, (3) implies that p2lies on the line segment from p1top3. To
prove this, let tD.kC1/ 1, and note that 0 < t < 1 . Solve for kto obtain
kD.1 t/=t. When this expression is used for kin (3), a rearrangement shows
thatp2D.1 t/p1Ctp3, which veriÔ¨Åes the assertion about p2.
b.C1continuity means that y0.0/Dx0.1/. Thus 2.p3 p2/D2.p2 p1/, so
p3 p2Dp2 p1, and p2D.p1Cp3/=2. Geometrically, p2is the midpoint of the
line segment from p1top3. See Figure 3.
Figure 4 shows C1continuity for two cubic B√©zier curves. Notice how the point
joining the two segments lies in the middle of the line segment between the adjacent
control points.
p1p0
x(t)
y(t)p2p3p4
p5
p6
FIGURE 4 Two cubic B√©zier curves.
Two curves have C2(parametric) continuity when they have C1continuity and the
second derivatives x00.1/andy00.0/are equal. This is possible for cubic B√©zier curves,
but it severely limits the positions of the control points. Another class of cubic curves,
called B-splines , always have C2continuity because each pair of curves share three
control points rather than one. Graphics Ô¨Ågures using B-splines have more control points
and consequently require more computations. Some exercises for this section examine
these curves.
Surprisingly, if x.t/andy.t/join at p3, the apparent smoothness of the curve at
p3is usually the same for both G1continuity and C1continuity. This is because the
magnitude of x0.t/is not related to the physical shape of the curve. The magnitude
reÔ¨Çects only the mathematical parameterization of the curve. For instance, if a new
vector function z.t/equals x.2t/, then the point z.t/traverses the curve from p0to
p3twice as fast as the original version, because 2treaches 1when tis:5. But, by the
chain rule of calculus, z0.t/D2x0.2t/, so the tangent vector to z.t/atp3is twice the
tangent vector to x.t/atp3.
In practice, many simple B√©zier curves are joined to create graphics objects. Type-
setting programs provide one important application, because many letters in a type font
involve curved segments. Each letter in a PostScript¬Æfont, for example, is stored as a
set of control points, along with information on how to construct the ‚Äúoutline‚Äù of the
letter using line segments and B√©zier curves. Enlarging such a letter basically requires
multiplying the coordinates of each control point by one constant scale factor. Once the
outline of the letter has been computed, the appropriate solid parts of the letter are Ô¨Ålled
in. Figure 5 illustrates this for a character in a PostScript font. Note the control points.
SECOND REVISED PAGES


--- Page 504 ---
8.6 Curves and Surfaces 487
Q
FIGURE 5 A PostScript character.
Matrix Equations for B√©zier Curves
Since a B√©zier curve is a linear combination of control points using polynomials as
weights, the formula for x.t/may be written as
x.t/Dp0p1p2p32
664.1 t/3
3t.1 t/2
3t2.1 t/
t33
775
Dp0p1p2p32
6641 3tC3t2 t3
3t 6t2C3t3
3t2 3t3
t33
775
Dp0p1p2p32
6641 3 3  1
0 3  6 3
0 0 3  3
0 0 0 13
7752
6641
t
t2
t33
775
The matrix whose columns are the four control points is called a geometry matrix ,G.
The44matrix of polynomial coefÔ¨Åcients is the B√©zier basis matrix ,MB. Ifu.t/is
the column vector of powers of t, then the B√©zier curve is given by
x.t/DGM Bu.t/ (4)
Other parametric cubic curves in computer graphics are written in this form, too. For
instance, if the entries in the matrix MBare changed appropriately, the resulting curves
are B-splines. They are ‚Äúsmoother‚Äù than B√©zier curves, but they do not pass through any
of the control points. A Hermite cubic curve arises when the matrix MBis replaced by
a Hermite basis matrix. In this case, the columns of the geometry matrix consist of the
starting and ending points of the curves and the tangent vectors to the curves at those
points.1
The B√©zier curve in equation (4) can also be ‚Äúfactored‚Äù in another way, to be used
in the discussion of B√©zier surfaces. For convenience later, the parameter tis replaced
1The term basis matrix comes from the rows of the matrix that list the coefÔ¨Åcients of the blending poly-
nomials used to deÔ¨Åne the curve. For a cubic B√©zier curve, the four polynomials are .1 t/3,3t.1 t/2,
3t2.1 t/, and t3. They form a basis for the space P3of polynomials of degree 3or less. Each entry in the
vector x.t/is a linear combination of these polynomials. The weights come from the rows of the geometry
matrix Gin (4).
SECOND REVISED PAGES


--- Page 505 ---
488 CHAPTER 8 The Geometry of Vector Spaces
by a parameter s:
x.s/Du.s/TMT
B2
664p0
p1
p2
p33
775D1 s s2s32
6641 0 0 0
 3 3 0 0
3 6 3 0
 1 3  3 13
7752
664p0
p1
p2
p33
775
D.1 s/33s.1 s/23s2.1 s/ s32
664p0
p1
p2
p33
775(5)
This formula is not quite the same as the transpose of the product on the right of
(4), because x.s/and the control points appear in (5) without transpose symbols. The
matrix of control points in (5) is called a geometry vector . This should be viewed as a
41block (partitioned) matrix whose entries are column vectors. The matrix to the left
of the geometry vector, in the second part of (5), can be viewed as a block matrix, too,
with a scalar in each block. The partitioned matrix multiplication makes sense, because
each (vector) entry in the geometry vector can be left-multiplied by a scalar as well as
by a matrix. Thus, the column vector x.s/is represented by (5).
B√©zier Surfaces
A 3D bicubic surface patch can be constructed from a set of four B√©zier curves. Consider
the four geometry matrices
p11p12p13p14
p21p22p23p24
p31p32p33p34
p41p42p43p44
and recall from equation (4) that a B√©zier curve is produced when any one of these
matrices is multiplied on the right by the following vector of weights:
MBu.t/D2
664.1 t/3
3t.1 t/2
3t2.1 t/
t33
775
LetGbe the block (partitioned) 44matrix whose entries are the control points pij
displayed above. Then the following product is a block 41matrix, and each entry is
a B√©zier curve:
GM Bu.t/D2
664p11 p12 p13 p14
p21 p22 p23 p24
p31 p32 p33 p34
p41 p42 p43 p443
7752
664.1 t/3
3t.1 t/2
3t2.1 t/
t33
775
In fact,
GM Bu.t/D2
664.1 t/3p11C3t.1 t/2p12C3t2.1 t/p13Ct3p14
.1 t/3p21C3t.1 t/2p22C3t2.1 t/p23Ct3p24
.1 t/3p31C3t.1 t/2p32C3t2.1 t/p33Ct3p34
.1 t/3p41C3t.1 t/2p42C3t2.1 t/p43Ct3p443
775
SECOND REVISED PAGES


--- Page 506 ---
8.6 Curves and Surfaces 489
Now Ô¨Åx t. Then GM Bu.t/is a column vector that can be used as a geometry vector
in equation (5) for a B√©zier curve in another variable s. This observation produces the
B√©zier bicubic surface :
x.s; t/Du.s/TMT
BGM Bu.t/; where 0s; t1 (6)
The formula for x.s; t/ is a linear combination of the sixteen control points. If one
imagines that these control points are arranged in a fairly uniform rectangular array, as
in Figure 6, then the B√©zier surface is controlled by a web of eight B√©zier curves, four
in the ‚Äú s-direction‚Äù and four in the ‚Äú t-direction.‚Äù The surface actually passes through
the four control points at its ‚Äúcorners.‚Äù When it is in the middle of a larger surface, the
sixteen-point surface shares its twelve boundary control points with its neighbors.
p41p31p21p11p12
p13
p14 p24
p34
p44p43p42p32
p33p23p22
FIGURE 6 Sixteen control points for a B√©zier
bicubic surface patch.
Approximations to Curves and Surfaces
In CAD programs and in programs used to create realistic computer games, the designer
often works at a graphics workstation to compose a ‚Äúscene‚Äù involving various geometric
structures. This process requires interaction between the designer and the geometric ob-
jects. Each slight repositioning of an object requires new mathematical computations by
the graphics program. B√©zier curves and surfaces can be useful in this process because
they involve fewer control points than objects approximated by many polygons. This
dramatically reduces the computation time and speeds up the designer‚Äôs work.
After the scene composition, however, the Ô¨Ånal image preparation has different
computational demands that are more easily met by objects consisting of Ô¨Çat surfaces
and straight edges, such as polyhedra. The designer needs to render the scene, by in-
troducing light sources, adding color and texture to surfaces, and simulating reÔ¨Çections
from the surfaces.
Computing the direction of a reÔ¨Çected light at a point pon a surface, for instance,
requires knowing the directions of both the incoming light and the surface normal ‚Äî
the vector perpendicular to the tangent plane at p. Computing such normal vectors is
much easier on a surface composed of, say, tiny Ô¨Çat polygons than on a curved surface
whose normal vector changes continuously as pmoves. If p1,p2, and p3are adjacent
vertices of a Ô¨Çat polygon, then the surface normal is just plus or minus the cross product
.p2 p1/.p2 p3/. When the polygon is small, only one normal vector is needed for
rendering the entire polygon. Also, two widely used shading routines, Gouraud shading
and Phong shading, both require a surface to be deÔ¨Åned by polygons.
As a result of these needs for Ô¨Çat surfaces, the B√©zier curves and surfaces from the
scene composition stage now are usually approximated by straight line segments and
SECOND REVISED PAGES


--- Page 507 ---
490 CHAPTER 8 The Geometry of Vector Spaces
polyhedral surfaces. The basic idea for approximating a B√©zier curve or surface is to
divide the curve or surface into smaller pieces, with more and more control points.
Recursive Subdivision of B√©zier Curves and Surfaces
Figure 7 shows the four control points p0; : : : ; p3for a B√©zier curve, along with control
points for two new curves, each coinciding with half of the original curve. The ‚Äúleft‚Äù
curve begins at q0Dp0and ends at q3, at the midpoint of the original curve. The ‚Äúright‚Äù
curve begins at r0Dq3and ends at r3Dp3.
p0 = q0q3 = r0
p3 = r3p1p2
r1
r2q1q2
FIGURE 7 Subdivision of a B√©zier curve.
Figure 8 shows how the new control points enclose regions that are ‚Äúthinner‚Äù than
the region enclosed by the original control points. As the distances between the control
points decrease, the control points of each curve segment also move closer to a line
segment. This variation-diminishing property of B√©zier curves depends on the fact that
a B√©zier curve always lies in the convex hull of the control points.
p0 = q0q3 = r0
p3 = r3p1p2
r1
r2q1q2
FIGURE 8 Convex hulls of the control points.
The new control points are related to the original control points by simple formulas.
Of course, q0Dp0andr3Dp3. The midpoint of the original curve x.t/occurs at x.:5/
when x.t/has the standard parameterization,
x.t/D.1 3tC3t2 t3/p0C.3t 6t2C3t3/p1C.3t2 3t3/p2Ct3p3 (7)
for0t1. Thus, the new control points q3andr0are given by
q3Dr0Dx.:5/D1
8.p0C3p1C3p2Cp3/ (8)
The formulas for the remaining ‚Äúinterior‚Äù control points are also simple, but the deriva-
tion of the formulas requires some work involving the tangent vectors of the curves. By
deÔ¨Ånition, the tangent vector to a parameterized curve x.t/is the derivative x0.t/. This
vector shows the direction of the line tangent to the curve at x.t/. For the B√©zier curve
in (7),
x0.t/D. 3C6t 3t2/p0C.3 12tC9t2/p1C.6t 9t2/p2C3t2p3
for0t1. In particular,
x0.0/D3.p1 p0/and x0.1/D3.p3 p2/ (9)
SECOND REVISED PAGES


--- Page 508 ---
8.6 Curves and Surfaces 491
Geometrically, p1is on the line tangent to the curve at p0, and p2is on the line tangent
to the curve at p3. See Figure 8. Also, from x0.t/, compute
x0.:5/D3
4. p0 p1Cp2Cp3/ (10)
Lety.t/be the B√©zier curve determined by q0; : : : ; q3, and let z.t/be the B√©zier curve
determined by r0; : : : ; r3. Since y.t/traverses the same path as x.t/but only gets to
x.:5/astgoes from 0to1,y.t/Dx.:5t/ for0t1. Similarly, since z.t/starts at
x.:5/when tD0,z.t/Dx.:5C:5t/for0t1. By the chain rule for derivatives,
y0.t/D:5x0.:5t/ and z0.t/D:5x0.:5C:5t/ for0t1 (11)
From (9) with y0.0/in place of x0.0/, from (11) with tD0, and from (9), the control
points for y.t/satisfy
3.q1 q0/Dy0.0/D:5x0.0/D3
2.p1 p0/ (12)
From (9) with y0.1/in place of x0.1/, from (11) with tD1, and from (10),
3.q3 q2/Dy0.1/D:5x0.:5/D3
8. p0 p1Cp2Cp3/ (13)
Equations (8), (9), (10), (12), and (13) can be solved to produce the formulas for q0; : : : ;
q3shown in Exercise 13. Geometrically, the formulas are displayed in Figure 9. The
interior control points q1andr2are the midpoints, respectively, of the segment from p0
top1and the segment from p2top3. When the midpoint of the segment from p1top2
is connected to q1, the resulting line segment has q2in the middle!
q0 = p0q3 = r0
p3 = r3p1p2
r1
r2q1q2(p1 + p2)1
2
FIGURE 9 Geometric structure of new control points.
This completes one step of the subdivision process. The ‚Äúrecursion‚Äù begins, and
both new curves are subdivided. The recursion continues to a depth at which all curves
are sufÔ¨Åciently straight. Alternatively, at each step the recursion can be ‚Äúadaptive‚Äù and
not subdivide one of the two new curves if that curve is sufÔ¨Åciently straight. Once the
subdivision completely stops, the endpoints of each curve are joined by line segments,
and the scene is ready for the next step in the Ô¨Ånal image preparation.
A B√©zier bicubic surface has the same variation-diminishing property as the B√©zier
curves that make up each cross-section of the surface, so the process described above
can be applied in each cross-section. With the details omitted, here is the basic strategy.
Consider the four ‚Äúparallel‚Äù B√©zier curves whose parameter is s, and apply the subdi-
vision process to each of them. This produces four sets of eight control points; each set
determines a curve as svaries from 0 to 1. As tvaries, however, there are eight curves,
each with four control points. Apply the subdivision process to each of these sets of
four points, creating a total of 64 control points. Adaptive recursion is possible in this
setting, too, but there are some subtleties involved.2
2See Foley, van Dam, Feiner, and Hughes, Computer Graphics‚ÄîPrinciples and Practice , 2nd Ed. (Boston:
Addison-Wesley, 1996), pp. 527‚Äì528.
SECOND REVISED PAGES


--- Page 509 ---
492 CHAPTER 8 The Geometry of Vector Spaces
PRACTICE PROBLEMS
Aspline usually refers to a curve that passes through speciÔ¨Åed points. A B-spline,
however, usually does not pass through its control points. A single segment has the
parametric form
x.t/D1
6
.1 t/3p0C.3t3 6t2C4/p1
C. 3t3C3t2C3tC1/p2Ct3p3 (14)
for0t1, where p0,p1,p2, and p3are the control points. When tvaries from 0 to 1,
x.t/creates a short curve that lies close to p1p2. Basic algebra shows that the B-spline
formula can also be written as
x.t/D1
6
.1 t/3p0C.3t.1 t/2 3tC4/p1
C.3t2.1 t/C3tC1/p2Ct3p3 (15)
This shows the similarity with the B√©zier curve. Except for the 1=6factor at the front,
thep0andp3terms are the same. The p1component has been increased by  3tC4
and the p2component has been increased by 3tC1. These components move the curve
closer to p1p2than the B√©zier curve. The 1=6factor is necessary to keep the sum of the
coefÔ¨Åcients equal to 1. Figure 10 compares a B-spline with a B√©zier curve that has the
same control points.
FIGURE 10 A B-spline segment and a B√©zier curve.
1.Show that the B-spline does not begin at p0, but x.0/is in conv fp0;p1;p2g. Assum-
ing that p0,p1, and p2are afÔ¨Ånely independent, Ô¨Ånd the afÔ¨Åne coordinates of x.0/
with respect to fp0;p1;p2g.
2.Show that the B-spline does not end at p3, but x.1/is in conv fp1;p2;p3g. Assuming
thatp1,p2, and p3are afÔ¨Ånely independent, Ô¨Ånd the afÔ¨Åne coordinates of x.1/with
respect to fp1;p2;p3g.
8.6 EXERCISES
1.Suppose a B√©zier curve is translated to x.t/Cb. That is, for
0t1, the new curve is
x.t/D.1 t/3p0C3t.1 t/2p1
C3t2.1 t/p2Ct3p3Cb
Show that this new curve is again a B√©zier curve. [ Hint:
Where are the new control points?]
2.The parametric vector form of a B-spline curve was deÔ¨Åned
in the Practice Problems as
x.t/D1
6
.1 t/3p0C.3t.1 t/2 3tC4/p1
C.3t2.1 t/C3tC1/p2Ct3p3
for0t1,
where p0,p1,p2, and p3are the control points.a.Show that for 0t1,x.t/is in the convex hull of the
control points.
b.Suppose that a B-spline curve x.t/is translated to
x.t/Cb(as in Exercise 1). Show that this new curve is
again a B-spline.
3.Letx.t/be a cubic B√©zier curve determined by points p0,p1,
p2, and p3.
a.Compute the tangent vector x0.t/. Determine how x0.0/
andx0.1/are related to the control points, and give ge-
ometric descriptions of the directions of these tangent
vectors. Is it possible to have x0.1/D0?
b.Compute the second derivative x00.t/and determine how
x00.0/andx00.1/are related to the control points. Draw a
SECOND REVISED PAGES


--- Page 510 ---
8.6 Curves and Surfaces 493
Ô¨Ågure based on Figure 10, and construct a line segment
that points in the direction of x00.0/. [Hint: Usep1as the
origin of the coordinate system.]
4.Letx.t/be the B-spline in Exercise 2, with control points p0,
p1,p2, and p3.
a.Compute the tangent vector x0.t/and determine how
the derivatives x0.0/andx0.1/are related to the control
points. Give geometric descriptions of the directions of
these tangent vectors. Explore what happens when both
x0.0/andx0.1/equal 0. Justify your assertions.
b.Compute the second derivative x00.t/and determine how
x00.0/andx00.1/are related to the control points. Draw a
Ô¨Ågure based on Figure 10, and construct a line segment
that points in the direction of x00.1/. [Hint: Usep2as the
origin of the coordinate system.]
5.Letx.t/andy.t/be cubic B√©zier curves with control points
fp0;p1;p2;p3gandfp3;p4;p5;p6g, respectively, so that x.t/
andy.t/are joined at p3. The following questions refer to
the curve consisting of x.t/followed by y.t/. For simplicity,
assume that the curve is in R2.
a.What condition on the control points will guarantee that
the curve has C1continuity at p3? Justify your answer.
b.What happens when x0.1/andy0.0/are both the zero
vector?
6.A B-spline is built out of B-spline segments, described in
Exercise 2. Let p0; : : : ; p4be control points. For 0t1,
letx.t/andy.t/be determined by the geometry matrices
¬åp0p1p2p3¬çand¬åp1p2p3p4¬ç, respectively.
Notice how the two segments share three control points.
The two segments do not overlap, however‚Äîthey join at a
common endpoint, close to p2.
a.Show that the combined curve has G0continuity‚Äîthat is,
x.1/Dy.0/.
b.Show that the curve has C1continuity at the join point,
x.1/. That is, show that x0.1/Dy0.0/.
7.Letx.t/andy.t/be B√©zier curves from Exercise 5, and sup-
pose the combined curve has C2continuity (which includes
C1continuity) at p3. Set x00.1/Dy00.0/and show that p5is
completely determined by p1,p2, and p3. Thus, the points
p0; : : : ; p3and the C2condition determine all but one of the
control points for y.t/.
8.Let x.t/and y.t/be segments of a B-spline as in
Exercise 6. Show that the curve has C2continuity (as well
asC1continuity) at x.1/. That is, show that x00.1/Dy00.0/.
This higher-order continuity is desirable in CAD applica-
tions such as automotive body design, since the curves and
surfaces appear much smoother. However, B-splines require
three times the computation of B√©zier curves, for curves
of comparable length. For surfaces, B-splines require nine
times the computation of B√©zier surfaces. Programmers often
choose B√©zier surfaces for applications (such as an airplane
cockpit simulator) that require real-time rendering.9.A quartic B√©zier curve is determined by Ô¨Åve control points,
p0,p1,p2;p3, and p4:
x.t/D.1 t/4p0C4t.1 t/3p1C6t2.1 t/2p2
C4t3.1 t/p3Ct4p4for0t1
Construct the quartic basis matrix MBforx.t/.
10. The ‚ÄúB‚Äù in B-spline refers to the fact that a segment x.t/may
be written in terms of a basis matrix, MS, in a form similar
to a B√©zier curve. That is,
x.t/DGM Su.t/ for0t1
where Gis the geometry matrix ¬åp0p1p2p3¬çandu.t/
is the column vector .1; t; t2; t3/. In a uniform B-spline, each
segment uses the same basis matrix, but the geometry matrix
changes. Construct the basis matrix MSforx.t/.
In Exercises 11 and 12, mark each statement True or False. Justify
each answer.
11.a.The cubic B√©zier curve is based on four control points.
b.Given a quadratic B√©zier curve x.t/with control points
p0,p1, and p2, the directed line segment p1 p0(from
p0top1) is the tangent vector to the curve at p0.
c.When two quadratic B√©zier curves with control points
fp0;p1;p2gandfp2;p3;p4gare joined at p2, the combined
B√©zier curve will have C1continuity at p2ifp2is the
midpoint of the line segment between p1andp3.
12. a.The essential properties of B√©zier curves are preserved
under the action of linear transformations, but not
translations.
b.When two B√©zier curves x.t/andy.t/are joined at the
point where x.1/Dy.0/, the combined curve has G0
continuity at that point.
c.The B√©zier basis matrix is a matrix whose columns are
the control points of the curve.
Exercises 13‚Äì15 concern the subdivision of a B√©zier curve shown
in Figure 7. Let x.t/be the B√©zier curve, with control points
p0; : : : ; p3, and let y.t/andz.t/be the subdividing B√©zier curves
as in the text, with control points q0; : : : ; q3and r0; : : : ; r3,
respectively.
13. a.Use equation (12) to show that q1is the midpoint of the
segment from p0top1.
b.Use equation (13) to show that
8q2D8q3Cp0Cp1 p2 p3:
c.Use part (b), equation (8), and part (a) to show that q2is
the midpoint of the segment from q1to the midpoint of the
segment from p1top2. That is, q2D1
2¬åq1C1
2.p1Cp2/¬ç.
14. a.Justify each equal sign:
3.r3 r2/Dz0.1/D:5x0.1/D3
2.p3 p2/:
SECOND REVISED PAGES


--- Page 511 ---
494 CHAPTER 8 The Geometry of Vector Spaces
b.Show that r2is the midpoint of the segment from p2top3.
c.Justify each equal sign: 3.r1 r0/Dz0.0/D:5x0.:5/.
d.Use part (c) to show that 8r1D  p0 p1Cp2Cp3C
8r0.
e.Use part (d), equation (8), and part (a) to show that r1is
the midpoint of the segment from r2to the midpoint of the
segment from p1top2. That is, r1D1
2¬år2C1
2.p1Cp2/¬ç.
15. Sometimes only one half of a B√©zier curve needs further
subdividing. For example, subdivision of the ‚Äúleft‚Äù side is
accomplished with parts (a) and (c) of Exercise 13 and
equation (8). When both halves of the curve x.t/are divided,
it is possible to organize calculations efÔ¨Åciently to calculate
both left and right control points concurrently, without using
equation (8) directly.
a.Show that the tangent vectors y0.1/andz0.0/are equal.
b.Use part (a) to show that q3(which equals r0/is the
midpoint of the segment from q2tor1.
c.Using part (b) and the results of Exercises 13 and 14, write
an algorithm that computes the control points for both
y.t/andz.t/in an efÔ¨Åcient manner. The only operations
needed are sums and division by 2.
16. Explain why a cubic B√©zier curve is completely determined
byx.0/,x0.0/,x.1/, and x0.1/.17. TrueType¬Æfonts, created by Apple Computer and Adobe
Systems, use quadratic B√©zier curves, while PostScript¬Æ
fonts, created by Microsoft, use cubic B√©zier curves. The
cubic curves provide more Ô¨Çexibility for typeface design,
but it is important to Microsoft that every typeface using
quadratic curves can be transformed into one that uses cubic
curves. Suppose that w.t/is a quadratic curve, with control
points p0,p1, and p2.
a.Find control points r0,r1,r2, and r3such that the cubic
B√©zier curve x.t/with these control points has the prop-
erty that x.t/andw.t/have the same initial and terminal
points and the same tangent vectors at tD0andtD1.
(See Exercise 16.)
b.Show that if x.t/is constructed as in part (a), then
x.t/Dw.t/for0t1.
18. Use partitioned matrix multiplication to compute the follow-
ing matrix product, which appears in the alternative formula
(5) for a B√©zier curve:
2
6641 0 0 0
 3 3 0 0
3 6 3 0
 1 3  3 13
7752
664p0
p1
p2
p33
775
SOLUTIONS TO PRACTICE PROBLEMS
1.From equation (14) with tD0,x.0/6Dp0because
x.0/D1
6¬åp0C4p1Cp2¬çD1
6p0C2
3p1C1
6p2:
The coefÔ¨Åcients are nonnegative and sum to 1, so x.0/is in conv fp0;p1;p2g, and
the afÔ¨Åne coordinates with respect to fp0;p1;p2gare 1
6;2
3;1
6
.
2.From equation (14) with tD1,x.1/6Dp3because
x.1/D1
6¬åp1C4p2Cp3¬çD1
6p1C2
3p2C1
6p3:
The coefÔ¨Åcients are nonnegative and sum to 1, so x.1/is in conv fp1;p2;p3g, and
the afÔ¨Åne coordinates with respect to fp1;p2;p3gare 1
6;2
3;1
6
.
SECOND REVISED PAGES


--- Page 512 ---
APPENDIX
A
Uniqueness of the Reduced
Echelon Form
T H E O R E M Uniqueness of the Reduced Echelon Form
Eachmnmatrix Ais row equivalent to a unique reduced echelon matrix U.
PROOF The proof uses the idea from Section 4.3 that the columns of row-equivalent
matrices have exactly the same linear dependence relations.
The row reduction algorithm shows that there exists at least one such matrix U.
Suppose that Ais row equivalent to matrices UandVin reduced echelon form. The
leftmost nonzero entry in a row of Uis a ‚Äúleading l.‚Äù Call the location of such a leading
1 a pivot position, and call the column that contains it a pivot column. (This deÔ¨Ånition
uses only the echelon nature of UandVand does not assume the uniqueness of the
reduced echelon form.)
The pivot columns of UandVare precisely the nonzero columns that are not
linearly dependent on the columns to their left. (This condition is satisÔ¨Åed automatically
by a Ô¨Årstcolumn if it is nonzero.) Since UandVare row equivalent (both being row
equivalent to A), their columns have the same linear dependence relations. Hence, the
pivot columns of UandVappear in the same locations. If there are rsuch columns,
then since UandVare in reduced echelon form, their pivot columns are the Ô¨Årst r
columns of the mmidentity matrix. Thus, corresponding pivot columns of UandV
are equal .
Finally, consider any nonpivot column of U, say column j. This column is either
zero or a linear combination of the pivot columns to its left (because those pivot columns
are a basis for the space spanned by the columns to the left of column j). Either case
can be expressed by writing UxD0for some xwhose jth entry is 1. Then VxD0,
too, which says that column jofVis either zero or the same linear combination of the
pivot columns of Vtoitsleft. Since corresponding pivot columns of UandVare equal,
columns jofUandVare also equal. This holds for all nonpivot columns, so VDU,
which proves that Uis unique.
CONFIRMING PAGES
A1

--- Page 513 ---
CONFIRMING PAGES
APPENDIX
B
Complex Numbers
Acomplex number is a number written in the form
¬¥DaCbi
where aandbare real numbers and iis a formal symbol satisfying the relation i2D  1.
The number ais the real part of¬¥, denoted by Re ¬¥, and bis the imaginary part of
¬¥, denoted by Im ¬¥. Two complex numbers are considered equal if and only if their
real and imaginary parts are equal. For example, if ¬¥D5C. 2/i, then Re ¬¥D5and
Im¬¥D  2. For simplicity, we write ¬¥D5 2i.
A real number ais considered as a special type of complex number, by identifying
awith aC0i. Furthermore, arithmetic operations on real numbers can be extended to
the set of complex numbers.
Thecomplex number system , denoted by C, is the set of all complex numbers,
together with the following operations of addition and multiplication:
.aCbi/C.cCdi/D.aCc/C.bCd/i (1)
.aCbi/.cCdi/D.ac bd/C.adCbc/i (2)
These rules reduce to ordinary addition and multiplication of real numbers when
banddare zero in (1) and (2). It is readily checked that the usual laws of arithmetic
forRalso hold for C. For this reason, multiplication is usually computed by algebraic
expansion, as in the following example.
EXAMPLE 1 .5 2i/.3C4i/D15C20i 6i 8i2
D15C14i 8. 1/
D23C14i
That is, multiply each term of 5 2iby each term of 3C4i, use i2D  1, and write
the result in the form aCbi.
Subtraction of complex numbers ¬¥1and¬¥2is deÔ¨Åned by
¬¥1 ¬¥2D¬¥1C. 1/¬¥ 2
In particular, we write  ¬¥in place of . 1/¬¥.
A2

--- Page 514 ---
APPENDIX B Complex Numbers A3
Theconjugate of¬¥DaCbiis the complex number ¬¥(read as ‚Äú ¬¥bar‚Äù), deÔ¨Åned
by
¬¥Da bi
Obtain ¬¥from ¬¥by reversing the sign of the imaginary part.
EXAMPLE 2 The conjugate of  3C4iis 3 4i; write  3C4iD  3 4i.
Observe that if ¬¥DaCbi, then
¬¥¬¥D.aCbi/.a bi/Da2 abiCbai b2i2Da2Cb2(3)
Since ¬¥¬¥is real and nonnegative, it has a square root. The absolute value (ormodulus )
of¬¥is the real number j¬¥jdeÔ¨Åned by
j¬¥j Dp
¬¥¬¥Dp
a2Cb2
If¬¥is a real number, then ¬¥DaC0i, andj¬¥j Dp
a2, which equals the ordinary
absolute value of a.
Some useful properties of conjugates and absolute value are listed below; wand¬¥
denote complex numbers.
1.¬¥D¬¥if and only if ¬¥is a real number.
2.wC¬¥DwC¬¥.
3.w¬¥Dw¬¥; in particular, r¬¥Dr¬¥ifris a real number.
4.¬¥¬¥D j¬¥j20.
5.jw¬¥j D j wjj¬¥j.
6.jwC¬¥j  j wj C j ¬¥j.
If¬¥¬§0, then j¬¥j> 0and¬¥has a multiplicative inverse, denoted by 1=¬¥or¬¥ 1
and given by
1
¬¥D¬¥ 1D¬¥
j¬¥j2
Of course, a quotient w=¬¥ simply means w.1=¬¥/ .
EXAMPLE 3 LetwD3C4iand¬¥D5 2i. Compute ¬¥¬¥,j¬¥j, and w=¬¥.
SOLUTION From equation (3),
¬¥¬¥D52C. 2/2D25C4D29
For the absolute value, j¬¥j Dp
¬¥¬¥Dp
29. To compute w=¬¥, Ô¨Årst multiply both the
numerator and the denominator by ¬¥, the conjugate of the denominator. Because of (3),
CONFIRMING PAGES


--- Page 515 ---
A4 APPENDIX B Complex Numbers
this eliminates the iin the denominator:
w
¬¥D3C4i
5 2i
D3C4i
5 2i5C2i
5C2i
D15C6iC20i 8
52C. 2/2
D7C26i
29
D7
29C26
29i
Geometric Interpretation
Each complex number ¬¥DaCbicorresponds to a point .a; b/ in the plane R2, as
in Figure 1. The horizontal axis is called the real axis because the points ( a; 0) on it
correspond to the real numbers. The vertical axis is the imaginary axis because the
points .0; b/ on it correspond to the pure imaginary numbers of the form 0Cbi, or
simply bi. The conjugate of ¬¥is the mirror image of ¬¥in the real axis. The absolute
value of ¬¥is the distance from .a; b/ to the origin.
Imaginary
axis
z = a + bi
z = a ‚Äì biab
Real axis
FIGURE 1 The complex conjugate is a mirror image.
Addition of complex numbers ¬¥DaCbiandwDcCdicorresponds to vector
addition of .a; b/ and.c; d/ inR2, as in Figure 2.
zw
Re zIm z
w + z
FIGURE 2 Addition of complex numbers.
CONFIRMING PAGES


--- Page 516 ---
APPENDIX B Complex Numbers A5
To give a graphical representation of complex multiplication, we use polar coordi-
nates inR2. Given a nonzero complex number ¬¥DaCbi, let'be the angle between
the positive real axis and the point .a; b/ , as in Figure 3 where   < ' . The angle
'is called the argument of¬¥; we write 'Darg¬¥. From trigonometry,
aD j¬¥jcos'; b D j¬¥jsin'
and so
¬¥DaCbiD j¬¥j.cos'Cisin'/
œï
|z| cos œï|z| sin œï|z|z
Re zIm z
FIGURE 3 Polar coordinates of ¬¥.
Ifwis another nonzero complex number, say,
wD jwj.cos#Cisin#/
then, using standard trigonometric identities for the sine and cosine of the sum of two
angles, one can verify that
w¬¥D jwjj¬¥j¬åcos.#C'/Cisin.#C'/¬ç (4)
See Figure 4. A similar formula may be written for quotients in polar form. The formulas
for products and quotients can be stated in words as follows.
œï/H9277|z|z wwz
Re zIm z
/H9277 + œï
FIGURE 4 Multiplication with polar
coordinates.
The product of two nonzero complex numbers is given in polar form by the
product of their absolute values and the sum of their arguments. The quotient
of two nonzero complex numbers is given by the quotient of their absolute values
and the difference of their arguments.
EXAMPLE 4
a.Ifwhas absolute value 1, then wDcos#Cisin#, where #is the argument of w.
Multiplication of any nonzero number ¬¥bywsimply rotates ¬¥through the angle #.
b.The argument of iitself is =2 radians, so multiplication of ¬¥byirotates ¬¥through
an angle of =2 radians. For example, 3Ciis rotated into .3Ci/iD  1C3i.
œïœï /H11001
œÄ
2z = 3 + iiz
i
Re zIm z
œÄ
2 Multiplication by i.
CONFIRMING PAGES


--- Page 517 ---
A6 APPENDIX B Complex Numbers
Powers of a Complex Number
Formula (4) applies when ¬¥DwDr.cos'Cisin'/. In this case
¬¥2Dr2.cos2'Cisin2'/
and
¬¥3D¬¥¬¥2
Dr.cos'Cisin'/r2.cos2'Cisin2'/
Dr3.cos3'Cisin3'/
In general, for any positive integer k,
¬¥kDrk.cosk'Cisink'/
This fact is known as De Moivre‚Äô s Theorem .
Complex Numbers and R2
Although the elements of R2andCare in one-to-one correspondence, and the operations
of addition are essentially the same, there is a logical distinction between R2andC. In
R2we can only multiply a vector by a real scalar, whereas in Cwe can multiply any
two complex numbers to obtain a third complex number. (The dot product in R2doesn‚Äôt
count, because it produces a scalar, not an element of R2:/We use scalar notation for
elements in Cto emphasize this distinction.
(2, 4)
(‚Äì1, 2)
(‚Äì3, ‚Äì1)
(3, ‚Äì2)(4, 0)2 + 4 i
‚Äì1 + 2 i
‚Äì3 ‚Äì i
3 ‚Äì 2 i4 + 0 ix2
x1Re zIm z
The real plane R2. The complex plane C.
CONFIRMING PAGES


--- Page 518 ---
Glossary
A
adjugate (orclassical adjoint ): The matrix adj Aformed from
a square matrix Aby replacing the .i; j / -entry of Aby
the.i; j / -cofactor, for all iandj, and then transposing the
resulting matrix.
afÔ¨Åne combination : A linear combination of vectors (points in
Rn) in which the sum of the weights involved is 1.
afÔ¨Åne dependence relation : An equation of the form c1v1C
   C cpvpD0, where the weights c1; : : : ; c pare not all
zero, and c1C    C cpD0.
afÔ¨Åne hull (orafÔ¨Åne span ) of a set S: The set of all afÔ¨Åne
combinations of points in S, denoted by aff S.
afÔ¨Ånely dependent set : A set fv1; : : : ; vpginRnsuch that there
are real numbers c1; : : : ; c p, not all zero, such that c1C    C
cpD0andc1v1C    C cpvpD0.
afÔ¨Ånely independent set : A set fv1; : : : ; vpginRnthat is not
afÔ¨Ånely dependent.
afÔ¨Åne set (orafÔ¨Åne subset ): A set Sof points such that if pand
qare in S, then .1 t/pCtq2Sfor each real number t.
afÔ¨Åne transformation : A mapping TWRn!Rmof the form
T .x/DAxCb, with Aanmnmatrix and binRm.
algebraic multiplicity : The multiplicity of an eigenvalue as a
root of the characteristic equation.
angle (between nonzero vectors uandvinR2orR3/: The angle
#between the two directed line segments from the origin to
the points uandv. Related to the scalar product by
uvD kuk kvkcos#
associative law of multiplication :A.BC/D.AB/C , for all A,
B,C.
attractor (of a dynamical system in R2/: The origin when all
trajectories tend toward 0.
augmented matrix : A matrix made up of a coefÔ¨Åcient matrix
for a linear system and one or more columns to the right.
Each extra column contains the constants from the right side
of a system with the given coefÔ¨Åcient matrix.
auxiliary equation : A polynomial equation in a variable r,
created from the coefÔ¨Åcients of a homogeneous difference
equation.
B
back-substitution (with matrix notation): The backward phase
of row reduction of an augmented matrix that transforms an
echelon matrix into a reduced echelon matrix; used to Ô¨Ånd
the solution(s) of a system of linear equations.backward phase (of row reduction): The last part of the al-
gorithm that reduces a matrix in echelon form to a reduced
echelon form.
band matrix : A matrix whose nonzero entries lie within a band
along the main diagonal.
barycentric coordinates (of a point pwith respect to an afÔ¨Ånely
independent set SD fv1; : : : ; vkg): The (unique) set of
weights c1; : : : ; c ksuch that pDc1v1C    C ckvkandc1C
   C ckD1. (Sometimes also called the afÔ¨Åne coordinates
ofpwith respect to S.)
basic variable : A variable in a linear system that corresponds
to a pivot column in the coefÔ¨Åcient matrix.
basis (for a nontrivial subspace Hof a vector space V /: An
indexed set BD fv1; : : : ; vpginVsuch that: (i) Bis a
linearly independent set and (ii) the subspace spanned by B
coincides with H, that is, HDSpanfv1; : : : ; vpg.
B-coordinates of x :See coordinates of xrelative to the
basis B.
best approximation : The closest point in a given subspace to a
given vector.
bidiagonal matrix : A matrix whose nonzero entries lie on the
main diagonal and on one diagonal adjacent to the main
diagonal.
block diagonal (matrix): A partitioned matrix AD¬åAij¬çsuch
that each block Aijis a zero matrix for i¬§j.
block matrix :Seepartitioned matrix.
block matrix multiplication : The row‚Äìcolumn multiplication
of partitioned matrices as if the block entries were scalars.
block upper triangular (matrix): A partitioned matrix
AD¬åAij¬çsuch that each block Aijis a zero matrix for
i > j .
boundary point of a set SinRn: A point psuch that every open
ball in Rncentered at pintersects both Sand the complement
ofS.
bounded set inRn: A set that is contained in an open ball
B.0; /for some  > 0 .
B-matrix (forT): A matrix ¬åT ¬çBfor a linear transformation
TWV!Vrelative to a basis BforV, with the property that
¬åT .x/¬çBD¬åT ¬çB¬åx¬çBfor all xinV.
C
Cauchy‚ÄìSchwarz inequality :jhu;vij  k ukkvkfor all u,v.
change of basis :Seechange-of-coordinates matrix.
CONFIRMING PAGES
A7

--- Page 519 ---
A8 Glossary
change-of-coordinates matrix (from a basis Bto a basis C): A
matrix P
C Bthat transforms B-coordinate vectors into C-
coordinate vectors: ¬åx¬çCDP
C B¬åx¬çB. IfCis the standard
basis for Rn, then P
C Bis sometimes written as PB.
characteristic equation (ofA): det .A I/D0.
characteristic polynomial (ofA): det .A I/ or, in some
texts, det .I A/.
Cholesky factorization : A factorization ADRTR, where Ris
an invertible upper triangular matrix whose diagonal entries
are all positive.
closed ball (inRn): A set fxW kx pk< ginRn, where pis
inRnand > 0 .
closed set (inRn): A set that contains all of its boundary points.
codomain (of a transformation TWRn!Rm/: The set Rmthat
contains the range of T. In general, if Tmaps a vector space
Vinto a vector space W, then Wis called the codomain
ofT.
coefÔ¨Åcient matrix : A matrix whose entries are the coefÔ¨Åcients
of a system of linear equations.
cofactor : A number CijD. 1/iCjdetAij, called the .i; j / -
cofactor of A, where Aijis the submatrix formed by deleting
theith row and the jth column of A.
cofactor expansion : A formula for det Ausing cofactors asso-
ciated with one row or one column, such as for row 1:
detADa11C11C    C a1nC1n
column‚Äìrow expansion : The expression of a product AB
as a sum of outer products: col 1.A/row 1.B/C    C
coln.A/row n.B/, where nis the number of columns of A.
column space (of an mnmatrix A): The set Col Aof all
linear combinations of the columns of A. IfAD¬åa1  an¬ç,
then Col ADSpanfa1; : : : ; ang. Equivalently,
ColAD fyWyDAxfor some xinRng
column sum : The sum of the entries in a column of a matrix.
column vector : A matrix with only one column, or a single
column of a matrix that has several columns.
commuting matrices : Two matrices AandBsuch that
ABDBA.
compact set (inRn): A set in Rnthat is both closed and
bounded.
companion matrix : A special form of matrix whose charac-
teristic polynomial is . 1/np./ when p./ is a speciÔ¨Åed
polynomial whose leading term is n.
complex eigenvalue : A nonreal root of the characteristic equa-
tion of an nnmatrix.
complex eigenvector : A nonzero vector xinCnsuch that
AxDx, where Ais an nnmatrix and is a complex
eigenvalue.
component of y orthogonal to u (for u¬§0): The vector
y yu
uuu.composition of linear transformations : A mapping produced
by applying two or more linear transformations in succes-
sion. If the transformations are matrix transformations, say
left-multiplication by Bfollowed by left-multiplication by
A, then the composition is the mapping x7!A.B x/.
condition number (ofA): The quotient 1=n, where 1is the
largest singular value of Aandnis the smallest singular
value. The condition number is C1 when nis zero.
conformable for block multiplication : Two partitioned matri-
cesAandBsuch that the block product ABis deÔ¨Åned: The
column partition of Amust match the row partition of B.
consistent linear system : A linear system with at least one
solution.
constrained optimization : The problem of maximizing a quan-
tity such as xTAxorkAxkwhen xis subject to one or more
constraints, such as xTxD1orxTvD0.
consumption matrix : A matrix in the Leontief input‚Äìoutput
model whose columns are the unit consumption vectors for
the various sectors of an economy.
contraction : A mapping x7!rxfor some scalar r, with
0r1.
controllable (pair of matrices): A matrix pair .A; B/ where A
isnn,Bhasnrows, and
rank¬åB AB A2B  An 1B¬çDn
Related to a state-space model of a control system and the
difference equation xkC1DAxkCBuk.kD0; 1; : : :/ .
convergent (sequence of vectors): A sequence fxkgsuch that
the entries in xkcan be made as close as desired to the entries
in some Ô¨Åxed vector for all ksufÔ¨Åciently large.
convex combination (of points v1; : : : ; vkinRn): A linear
combination of vectors (points) in which the weights in the
combination are nonnegative and the sum of the weights
is 1.
convex hull (of a set S): The set of all convex combinations of
points in S, denoted by: conv S.
convex set : A set Swith the property that for each pandqin
S, the line segment pqis contained in S.
coordinate mapping (determined by an ordered basis Bin a
vector space V): A mapping that associates to each xin
Vits coordinate vector ¬åx¬çB.
coordinates of x relative to the basis BD fb1; : : : ; bng: The
weights c1; : : : ; c nin the equation xDc1b1C    C cnbn.
coordinate vector of x relative to B: The vector ¬åx¬çBwhose
entries are the coordinates of xrelative to the basis B.
covariance (of variables xiandxj, fori¬§j): The entry sijin
the covariance matrix Sfor a matrix of observations, where
xiandxjvary over the ith and jth coordinates, respectively,
of the observation vectors.
covariance matrix (orsample covariance matrix ): The pp
matrix SdeÔ¨Åned by SD.N 1/ 1BBT, where Bis a
pNmatrix of observations in mean-deviation form.
CONFIRMING PAGES


--- Page 520 ---
Glossary A9
Cramer‚Äôs rule : A formula for each entry in the solution xof
the equation AxDbwhen Ais an invertible matrix.
cross-product term : A term cxixjin a quadratic form, with
i¬§j.
cube : A three-dimensional solid object bounded by six square
faces, with three faces meeting at each vertex.
D
decoupled system : A difference equation ykC1DAyk, or a
differential equation y0.t/DAy.t/, in which Ais a diagonal
matrix. The discrete evolution of each entry in yk(as a
function of k), or the continuous evolution of each entry
in the vector-valued function y.t/, is unaffected by what
happens to the other entries as k! 1 ort! 1 .
design matrix : The matrix Xin the linear model yDXC,
where the columns of Xare determined in some way by the
observed values of some independent variables.
determinant (of a square matrix A): The number det AdeÔ¨Åned
inductively by a cofactor expansion along the Ô¨Årst row of A.
Also, . 1/rtimes the product of the diagonal entries in any
echelon form Uobtained from Aby row replacements and
rrow interchanges (but no scaling operations).
diagonal entries (in a matrix): Entries having equal row and
column indices.
diagonalizable (matrix): A matrix that can be written in fac-
tored form as PDP 1, where Dis a diagonal matrix and P
is an invertible matrix.
diagonal matrix : A square matrix whose entries noton the
main diagonal are all zero.
difference equation (orlinear recurrence relation ): An equa-
tion of the form xkC1DAxk(kD0; 1; 2; : : : ) whose solu-
tion is a sequence of vectors, x0;x1; : : : :
dilation : A mapping x7!rxfor some scalar r, with 1 < r .
dimension :
of a Ô¨Çat S: The dimension of the corresponding parallel
subspace.
of a set S: The dimension of the smallest Ô¨Çat containing S.
of a subspace S: The number of vectors in a basis for S,
written as dim S.
of a vector space V: The number of vectors in a basis for V,
written as dim V. The dimension of the zero space is 0.
discrete linear dynamical system : A difference equation of the
form xkC1DAxkthat describes the changes in a system
(usually a physical system) as time passes. The physical
system is measured at discrete times, when kD0; 1; 2; : : : ;
and the state of the system at time kis a vector xkwhose
entries provide certain facts of interest about the system.
distance between u and v : The length of the vector u v,
denoted by dist .u;v/.
distance to a subspace : The distance from a given point (vec-
tor)vto the nearest point in the subspace.
distributive laws : (left) A.BCC/DABCAC, and (right)
.BCC/ADBACCA, for all A,B,C.domain (of a transformation T): The set of all vectors xfor
which T .x/is deÔ¨Åned.
dot product :Seeinner product.
dynamical system :Seediscrete linear dynamical system.
E
echelon form (orrow echelon form , of a matrix): An echelon
matrix that is row equivalent to the given matrix.
echelon matrix (orrow echelon matrix ): A rectangular matrix
that has three properties: (1) All nonzero rows are above
any row of all zeros. (2) Each leading entry of a row is in
a column to the right of the leading entry of the row above
it. (3) All entries in a column below a leading entry are zero.
eigenfunctions (of a differential equation x0.t/DAx.t/): A
function x.t/Dvet, where vis an eigenvector of Aand
is the corresponding eigenvalue.
eigenspace (ofAcorresponding to ): The set of allsolutions
ofAxDx, where is an eigenvalue of A. Consists of the
zero vector and all eigenvectors corresponding to .
eigenvalue (ofA): A scalar such that the equation AxDx
has a solution for some nonzero vector x.
eigenvector (ofA): A nonzero vector xsuch that AxDxfor
some scalar .
eigenvector basis : A basis consisting entirely of eigenvectors
of a given matrix.
eigenvector decomposition (ofx): An equation, xDc1v1C
   C cnvn, expressing xas a linear combination of eigen-
vectors of a matrix.
elementary matrix : An invertible matrix that results by per-
forming one elementary row operation on an identity matrix.
elementary row operations : (1) (Replacement) Replace one
row by the sum of itself and a multiple of another row.
(2) Interchange two rows. (3) (Scaling) Multiply all entries
in a row by a nonzero constant.
equal vectors : Vectors in Rnwhose corresponding entries are
the same.
equilibrium prices : A set of prices for the total output of the
various sectors in an economy, such that the income of each
sector exactly balances its expenses.
equilibrium vector :Seesteady-state vector.
equivalent (linear) systems : Linear systems with the same
solution set.
exchange model :SeeLeontief exchange model.
existence question : Asks, ‚ÄúDoes a solution to the system ex-
ist?‚Äù That is, ‚ÄúIs the system consistent?‚Äù Also, ‚ÄúDoes a
solution of AxDbexist for allpossible b?‚Äù
expansion by cofactors :Seecofactor expansion.
explicit description (of a subspace WofRn): A parametric
representation of Was the set of all linear combinations of
a set of speciÔ¨Åed vectors.
extreme point (of a convex set S): A point pinSsuch that pis
not in the interior of any line segment that lies in S. (That is,
CONFIRMING PAGES


--- Page 521 ---
A10 Glossary
ifx,yare in Sandpis on the line segment xy, then pDx
orpDy.)
F
factorization (ofA): An equation that expresses Aas a product
of two or more matrices.
Ô¨Ånal demand vector (orbill of Ô¨Ånal demands) : The vector d
in the Leontief input‚Äìoutput model that lists the dollar values
of the goods and services demanded from the various sectors
by the nonproductive part of the economy. The vector d
can represent consumer demand, government consumption,
surplus production, exports, or other external demand.
Ô¨Ånite-dimensional (vector space): A vector space that is
spanned by a Ô¨Ånite set of vectors.
Ô¨Çat(inRn): A translate of a subspace of Rn.
Ô¨Çexibility matrix : A matrix whose jth column gives the de-
Ô¨Çections of an elastic beam at speciÔ¨Åed points when a unit
force is applied at the jth point on the beam.
Ô¨Çoating point arithmetic : Arithmetic with numbers repre-
sented as decimals :d1  dp10r, where ris an integer
and the number pof digits to the right of the decimal point
is usually between 8 and 16.
Ô¨Çop: One arithmetic operation .C; ;; =/on two real Ô¨Çoating
point numbers.
forward phase (of row reduction): The Ô¨Årst part of the algo-
rithm that reduces a matrix to echelon form.
Fourier approximation (of order n): The closest point in the
subspace of nth-order trigonometric polynomials to a given
function in C¬å0; 2¬ç .
Fourier coefÔ¨Åcients : The weights used to make a trigonometric
polynomial as a Fourier approximation to a function.
Fourier series : An inÔ¨Ånite series that converges to a function
in the inner product space C¬å0; 2¬ç , with the inner product
given by a deÔ¨Ånite integral.
free variable : Any variable in a linear system that is not a basic
variable.
full rank (matrix): An mnmatrix whose rank is the smaller
ofmandn.
fundamental set of solutions : A basis for the set of all solutions
of a homogeneous linear difference or differential equation.
fundamental subspaces (determined by A): The null space and
column space of A, and the null space and column space of
AT, with Col ATcommonly called the row space of A.
G
Gaussian elimination :Seerow reduction algorithm.
general least-squares problem : Given an mnmatrix
Aand a vector binRm, Ô¨Ånd OxinRnsuch that
kb AOxk  k b Axkfor all xinRn.
general solution (of a linear system): A parametric description
of a solution set that expresses the basic variables in terms ofthe free variables (the parameters), if any. After Section 1.5,
the parametric description is written in vector form.
Givens rotation : A linear transformation from RntoRnused in
computer programs to create zero entries in a vector (usually
a column of a matrix).
Gram matrix (ofA): The matrix ATA.
Gram‚ÄìSchmidt process : An algorithm for producing an or-
thogonal or orthonormal basis for a subspace that is spanned
by a given set of vectors.
H
homogeneous coordinates : In R3, the representation of
.x; y; ¬¥/ as.X; Y; Z; H/ for any H¬§0, where xDX=H ,
yDY=H , and ¬¥DZ=H . InR2,His usually taken as 1,
and the homogeneous coordinates of .x; y/ are written as
.x; y; 1/ .
homogeneous equation : An equation of the form AxD0, pos-
sibly written as a vector equation or as a system of linear
equations.
homogeneous form of (a vector) vinRn: The point QvDv
1
inRnC1.
Householder reÔ¨Çection : A transformation x7!Qx, where
QDI 2uuTanduis a unit vector .uTuD1/.
hyperplane (inRn): A Ô¨Çat in Rnof dimension n 1. Also: a
translate of a subspace of dimension n 1.
I
identity matrix (denoted by IorIn): A square matrix with ones
on the diagonal and zeros elsewhere.
ill-conditioned matrix : A square matrix with a large (or pos-
sibly inÔ¨Ånite) condition number; a matrix that is singular or
can become singular if some of its entries are changed ever
so slightly.
image (of a vector xunder a transformation T): The vector T .x/
assigned to xbyT.
implicit description (of a subspace WofRn): A set of one
or more homogeneous equations that characterize the points
ofW.
Im x : The vector in Rnformed from the imaginary parts of the
entries of a vector xinCn.
inconsistent linear system : A linear system with no solution.
indeÔ¨Ånite matrix : A symmetric matrix Asuch that xTAxas-
sumes both positive and negative values.
indeÔ¨Ånite quadratic form : A quadratic form Qsuch that Q.x/
assumes both positive and negative values.
inÔ¨Ånite-dimensional (vector space): A nonzero vector space V
that has no Ô¨Ånite basis.
inner product : The scalar uTv, usually written as uv, where u
andvare vectors in Rnviewed as n1matrices. Also called
thedot product ofuandv. In general, a function on a vector
CONFIRMING PAGES


--- Page 522 ---
Glossary A11
space that assigns to each pair of vectors uandva number
hu;vi, subject to certain axioms. See Section 6.7.
inner product space : A vector space on which is deÔ¨Åned an
inner product.
input‚Äìoutput matrix :Seeconsumption matrix.
input‚Äìoutput model :SeeLeontief input‚Äìoutput model.
interior point (of a set SinRn): A point pinSsuch that for
some  > 0 , the open ball B.p; /centered at pis contained
inS.
intermediate demands : Demands for goods or services that
will be consumed in the process of producing other goods
and services for consumers. If xis the production level and
Cis the consumption matrix, then Cxlists the intermediate
demands.
interpolating polynomial : A polynomial whose graph passes
through every point in a set of data points in R2.
invariant subspace (forA): A subspace Hsuch that Axis in
Hwhenever xis inH.
inverse (of an nnmatrix A): An nnmatrix A 1such that
AA 1DA 1ADIn.
inverse power method : An algorithm for estimating an eigen-
value of a square matrix, when a good initial estimate of 
is available.
invertible linear transformation : A linear transformation
TWRn!Rnsuch that there exists a function SWRn!Rn
satisfying both T .S. x//DxandS.T . x//Dxfor all x
inRn.
invertible matrix : A square matrix that possesses an inverse.
isomorphic vector spaces : Two vector spaces VandWfor
which there is a one-to-one linear transformation Tthat maps
VontoW.
isomorphism : A one-to-one linear mapping from one vector
space onto another.
K
kernel (of a linear transformation TWV!W /: The set of xin
Vsuch that T .x/D0.
Kirchhoff‚Äôs laws : (1) ( voltage law ) The algebraic sum of the
RIvoltage drops in one direction around a loop equals the
algebraic sum of the voltage sources in the same direction
around the loop. (2) ( current law) The current in a branch is
the algebraic sum of the loop currents Ô¨Çowing through that
branch.
L
ladder network : An electrical network assembled by connect-
ing in series two or more electrical circuits.
leading entry : The leftmost nonzero entry in a row of a matrix.
least-squares error : The distance kb AOxkfrom btoAOx,
when Oxis a least-squares solution of AxDb.
least-squares line : The line yDO0CO1xthat minimizes the
least-squares error in the equation yDXC.least-squares solution (ofAxDb): A vector Oxsuch that
kb AOxk  k b Axkfor all xinRn.
left inverse (ofA): Any rectangular matrix Csuch that
CADI.
left-multiplication (byA): Multiplication of a vector or matrix
on the left by A.
left singular vectors (ofA): The columns of Uin the singular
value decomposition ADU ¬ÜVT.
length (ornorm , ofv): The scalar kvk DpvvDp
hv;vi.
Leontief exchange (orclosed )model : A model of an economy
where inputs and outputs are Ô¨Åxed, and where a set of prices
for the outputs of the sectors is sought such that the income
of each sector equals its expenditures. This ‚Äúequilibrium‚Äù
condition is expressed as a system of linear equations, with
the prices as the unknowns.
Leontief input‚Äìoutput model (orLeontief production equa-
tion): The equation xDCxCd, where xis production, d
is Ô¨Ånal demand, and Cis the consumption (or input‚Äìoutput)
matrix. The jth column of Clists the inputs that sector j
consumes per unit of output.
level set (orgradient ) of a linear functional fonRn: A set
¬åf:d¬çD fx2RnWf .x/Ddg
linear combination : A sum of scalar multiples of vectors. The
scalars are called the weights .
linear dependence relation : A homogeneous vector equation
where the weights are all speciÔ¨Åed and at least one weight is
nonzero.
linear equation (in the variables x1; : : : ; x n/: An equation that
can be written in the form a1x1Ca2x2C    C anxnDb,
where band the coefÔ¨Åcients a1; : : : ; a nare real or complex
numbers.
linear Ô¨Ålter : A linear difference equation used to transform
discrete-time signals.
linear functional (onRn): A linear transformation ffromRn
intoR.
linearly dependent (vectors): An indexed set fv1; : : : ; vpgwith
the property that there exist weights c1; : : : ; c p, not all zero,
such that c1v1C    C cpvpD0. That is, the vector equation
c1v1Cc2v2C    C cpvpD0has a nontrivial solution.
linearly independent (vectors): An indexed set fv1; : : : ; vpg
with the property that the vector equation c1v1C
c2v2C    C cpvpD0has only the trivial solution,
c1D    D cpD0.
linear model (in statistics): Any equation of the form
yDXC, where Xandyare known and is to be chosen
to minimize the length of the residual vector ,.
linear system : A collection of one or more linear equations
involving the same variables, say, x1; : : : ; x n.
linear transformation T(from a vector space Vinto a vec-
tor space W): A rule Tthat assigns to each vector
xinVa unique vector T .x/inW, such that (i)
T .uCv/DT .u/CT .v/for all u;vinV, and (ii)
T .cu/DcT .u/for all uinVand all scalars c. Notation:
CONFIRMING PAGES


--- Page 523 ---
A12 Glossary
TWV!W; also, x7!Axwhen TWRn!RmandAis the
standard matrix for T.
line through p parallel to v : The set fpCtvWtinRg.
loop current : The amount of electric current Ô¨Çowing through a
loop that makes the algebraic sum of the RIvoltage drops
around the loop equal to the algebraic sum of the voltage
sources in the loop.
lower triangular matrix : A matrix with zeros above the main
diagonal.
lower triangular part (ofA): A lower triangular matrix whose
entries on the main diagonal and below agree with those
inA.
LU factorization : The representation of a matrix Ain the form
ADLU where Lis a square lower triangular matrix with
ones on the diagonal (a unit lower triangular matrix) and U
is an echelon form of A.
M
magnitude (of a vector): Seenorm.
main diagonal (of a matrix): The entries with equal row and
column indices.
mapping :Seetransformation.
Markov chain : A sequence of probability vectors x0,x1,
x2; : : : ; together with a stochastic matrix Psuch that
xkC1DPxkforkD0; 1; 2; : : : :
matrix : A rectangular array of numbers.
matrix equation : An equation that involves at least one matrix;
for instance, AxDb.
matrix for Trelative to bases BandC: A matrix Mfor
a linear transformation TWV!Wwith the property that
¬åT .x/¬çCDM¬åx¬çBfor all xinV, where Bis a basis for Vand
Cis a basis for W. When WDVandCDB, the matrix M
is called the B-matrix for Tand is denoted by ¬åT ¬çB.
matrix of observations : A pNmatrix whose columns are
observation vectors, each column listing pmeasurements
made on an individual or object in a speciÔ¨Åed population
or set.
matrix transformation : A mapping x7!Ax, where Ais an
mnmatrix and xrepresents any vector in Rn.
maximal linearly independent set (inV): A linearly indepen-
dent set BinVsuch that if a vector vinVbut not in Bis
added to B, then the new set is linearly dependent.
mean-deviation form (of a matrix of observations): A matrix
whose row vectors are in mean-deviation form. For each
row, the entries sum to zero.
mean-deviation form (of a vector): A vector whose entries sum
to zero.
mean square error : The error of an approximation in an inner
product space, where the inner product is deÔ¨Åned by a deÔ¨Å-
nite integral.migration matrix : A matrix that gives the percentage move-
ment between different locations, from one period to the
next.
minimal spanning set (for a subspace H): A set Bthat spans
Hand has the property that if one of the elements of Bis
removed from B, then the new set does not span H.
mnmatrix: A matrix with mrows and ncolumns.
Moore‚ÄìPenrose inverse :Seepseudoinverse.
multiple regression : A linear model involving several indepen-
dent variables and one dependent variable.
N
nearly singular matrix : An ill-conditioned matrix.
negative deÔ¨Ånite matrix : A symmetric matrix Asuch that
xTAx< 0for all x¬§0.
negative deÔ¨Ånite quadratic form : A quadratic form Qsuch
thatQ.x/ < 0 for all x¬§0.
negative semideÔ¨Ånite matrix : A symmetric matrix Asuch that
xTAx0for all x.
negative semideÔ¨Ånite quadratic form : A quadratic form Q
such that Q.x/0for all x.
nonhomogeneous equation : An equation of the form AxDb
with b¬§0, possibly written as a vector equation or as a
system of linear equations.
nonsingular (matrix): An invertible matrix.
nontrivial solution : A nonzero solution of a homogeneous
equation or system of homogeneous equations.
nonzero (matrix or vector): A matrix (with possibly only one
row or column) that contains at least one nonzero entry.
norm (orlength , ofv): The scalar kvk DpvvDp
hv;vi.
normal equations : The system of equations represented by
ATAxDATb, whose solution yields all least-squares so-
lutions of AxDb. In statistics, a common notation is
XTXDXTy.
normalizing (a nonzero vector v): The process of creating a unit
vector uthat is a positive multiple of v.
normal vector (to a subspace VofRn): A vector ninRnsuch
thatnxD0for all xinV.
null space (of an mnmatrix A): The set Nul Aof all solutions
to the homogeneous equation AxD0. Nul AD fxWxis in
RnandAxD0g.
O
observation vector : The vector yin the linear model
yDXC, where the entries in yare the observed values
of a dependent variable.
one-to-one (mapping): A mapping TWRn!Rmsuch that
each binRmis the image of at most onexinRn.
onto (mapping): A mapping TWRn!Rmsuch that each bin
Rmis the image of at least onexinRn.
CONFIRMING PAGES


--- Page 524 ---
Glossary A13
open ball B .p; /inRn: The set fxW kx pk< ginRn, where
 > 0 .
open set SinRn: A set that contains none of its boundary
points. (Equivalently, Sis open if every point of Sis an
interior point.)
origin : The zero vector.
orthogonal basis : A basis that is also an orthogonal set.
orthogonal complement (ofW /: The set W?of all vectors
orthogonal to W.
orthogonal decomposition : The representation of a vector y
as the sum of two vectors, one in a speciÔ¨Åed subspace
Wand the other in W?. In general, a decomposition
yDc1u1C    C cpup, where fu1; : : : ; upgis an orthogonal
basis for a subspace that contains y.
orthogonally diagonalizable (matrix): A matrix Athat admits
a factorization, ADPDP 1, with Pan orthogonal matrix
.P 1DPT/andDdiagonal.
orthogonal matrix : A square invertible matrix Usuch that
U 1DUT.
orthogonal projection of y onto u (or onto the line through uand
the origin, for u¬§0): The vector OydeÔ¨Åned by OyDyu
uuu.
orthogonal projection of y onto W: The unique vector OyinW
such that y Oyis orthogonal to W. Notation: OyDprojWy.
orthogonal set : A set Sof vectors such that uvD0for each
distinct pair u;vinS.
orthogonal to W: Orthogonal to every vector in W.
orthonormal basis : A basis that is an orthogonal set of unit
vectors.
orthonormal set : An orthogonal set of unit vectors.
outer product : A matrix product uvTwhere uandvare vectors
inRnviewed as n1matrices. (The transpose symbol is on
the ‚Äúoutside‚Äù of the symbols uandv.)
overdetermined system : A system of equations with more
equations than unknowns.
P
parallel Ô¨Çats : Two or more Ô¨Çats such that each Ô¨Çat is a translate
of the other Ô¨Çats.
parallelogram rule for addition : A geometric interpretation of
the sum of two vectors u,vas the diagonal of the parallelo-
gram determined by u,v, and 0.
parameter vector : The unknown vector in the linear model
yDXC.
parametric equation of a line : An equation of the form
xDpCtv(tinR).
parametric equation of a plane : An equation of the form
xDpCsuCtv(s,tinR), with uand vlinearly
independent.
partitioned matrix (orblock matrix ): A matrix whose entries
are themselves matrices of appropriate sizes.permuted lower triangular matrix : A matrix such that a per-
mutation of its rows will form a lower triangular matrix.
permuted LU factorization : The representation of a matrix A
in the form ADLU where Lis a square matrix such that
a permutation of its rows will form a unit lower triangular
matrix, and Uis an echelon form of A.
pivot : A nonzero number that either is used in a pivot position
to create zeros through row operations or is changed into a
leading 1, which in turn is used to create zeros.
pivot column : A column that contains a pivot position.
pivot position : A position in a matrix Athat corresponds to a
leading entry in an echelon form of A.
plane through u, v, and the origin : A set whose parametric
equation is xDsuCtv(s,tinR/, with uandvlinearly
independent.
polar decomposition (ofA): A factorization ADPQ, where
Pis annnpositive semideÔ¨Ånite matrix with the same rank
asA, and Qis annnorthogonal matrix.
polygon : A polytope in R2.
polyhedron : A polytope in R3.
polytope : The convex hull of a Ô¨Ånite set of points in Rn(a
special type of compact convex set).
positive combination (of points v1; : : : ; vminRn): A linear
combination c1v1C    C cmvm, where all ci0.
positive deÔ¨Ånite matrix : A symmetric matrix Asuch that
xTAx> 0for all x¬§0.
positive deÔ¨Ånite quadratic form : A quadratic form Qsuch
thatQ.x/ > 0 for all x¬§0.
positive hull (of a set S): The set of all positive combinations
of points in S, denoted by pos S.
positive semideÔ¨Ånite matrix : A symmetric matrix Asuch that
xTAx0for all x.
positive semideÔ¨Ånite quadratic form : A quadratic form Q
such that Q.x/0for all x.
power method : An algorithm for estimating a strictly dominant
eigenvalue of a square matrix.
principal axes (of a quadratic form xTAx): The orthonormal
columns of an orthogonal matrix Psuch that P 1APis
diagonal. (These columns are unit eigenvectors of A.) Usu-
ally the columns of Pare ordered in such a way that the
corresponding eigenvalues of Aare arranged in decreasing
order of magnitude.
principal components (of the data in a matrix Bof
observations): The unit eigenvectors of a sample co-
variance matrix SforB, with the eigenvectors arranged
so that the corresponding eigenvalues of Sdecrease in
magnitude. If Bis in mean-deviation form, then the principal
components are the right singular vectors in a singular value
decomposition of BT.
probability vector : A vector in Rnwhose entries are nonnega-
tive and sum to one.
CONFIRMING PAGES


--- Page 525 ---
A14 Glossary
product Ax: The linear combination of the columns of Ausing
the corresponding entries in xas weights.
production vector : The vector in the Leontief input‚Äìoutput
model that lists the amounts that are to be produced by the
various sectors of an economy.
proÔ¨Åle (of a set SinRn): The set of extreme points of S.
projection matrix (ororthogonal projection matrix ): A sym-
metric matrix Bsuch that B2DB. A simple example is
BDvvT, where vis a unit vector.
proper subset of a set S: A subset of Sthat does not equal S
itself.
proper subspace : Any subspace of a vector space Vother than
Vitself.
pseudoinverse (ofA): The matrix VD 1UT, when UDVTis a
reduced singular value decomposition of A.
Q
QR factorization : A factorization of an mnmatrix Awith
linearly independent columns, ADQR, where Qis an
mnmatrix whose columns form an orthonormal basis for
ColA, and Ris an nnupper triangular invertible matrix
with positive entries on its diagonal.
quadratic B√©zier curve : A curve whose description may be
written in the form g.t/D.1 t/f0.t/Ctf1.t/for0t
1, where f0.t/D.1 t/p0Ctp1andf1.t/D.1 t/p1C
tp2. The points p0,p1,p2are called the control points for
the curve.
quadratic form : A function QdeÔ¨Åned for xinRnbyQ.x/D
xTAx, where Ais an nnsymmetric matrix (called the
matrix of the quadratic form ).
R
range (of a linear transformation T): The set of all vectors of
the form T .x/for some xin the domain of T.
rank (of a matrix A): The dimension of the column space of A,
denoted by rank A.
Rayleigh quotient :R.x/D.xTAx/=.xTx/. An estimate of an
eigenvalue of A(usually a symmetric matrix).
recurrence relation :Seedifference equation.
reduced echelon form (orreduced row echelon form ): A
reduced echelon matrix that is row equivalent to a given
matrix.
reduced echelon matrix : A rectangular matrix in echelon form
that has these additional properties: The leading entry in each
nonzero row is 1, and each leading 1 is the only nonzero entry
in its column.
reduced singular value decomposition : A factorization
ADUDVT, for an mnmatrix Aof rank r, where Uis
mrwith orthonormal columns, Dis an rrdiagonal
matrix with the rnonzero singular values of Aon its
diagonal, and Visnrwith orthonormal columns.regression coefÔ¨Åcients : The coefÔ¨Åcients 0and1in the least-
squares line yD0C1x.
regular solid : One of the Ô¨Åve possible regular polyhedrons
inR3: the tetrahedron (4 equal triangular faces), the cube
(6 square faces), the octahedron (8 equal triangular faces),
the dodecahedron (12 equal pentagonal faces), and the icosa-
hedron (20 equal triangular faces).
regular stochastic matrix : A stochastic matrix Psuch that
some matrix power Pkcontains only strictly positive entries.
relative change orrelative error (inb/: The quantity
k¬Åbk=kbkwhen bis changed to bC¬Åb.
repellor (of a dynamical system in R2/: The origin when all
trajectories except the constant zero sequence or function
tend away from 0.
residual vector : The quantity that appears in the general
linear model: yDXC; that is, Dy X, the differ-
ence between the observed values and the predicted values
(ofy).
Re x : The vector in Rnformed from the real parts of the entries
of a vector xinCn.
right inverse (ofA): Any rectangular matrix Csuch that
ACDI.
right-multiplication (byA): Multiplication of a matrix on the
right by A.
right singular vectors (ofA): The columns of Vin the singular
value decomposition ADU ¬ÜVT.
roundoff error : Error in Ô¨Çoating point arithmetic caused when
the result of a calculation is rounded (or truncated) to the
number of Ô¨Çoating point digits stored. Also, the error that
results when the decimal representation of a number such as
1/3 is approximated by a Ô¨Çoating point number with a Ô¨Ånite
number of digits.
row‚Äìcolumn rule : The rule for computing a product ABin
which the .i; j / -entry of ABis the sum of the products of
corresponding entries from row iofAand column jofB.
row equivalent (matrices): Two matrices for which there exists
a (Ô¨Ånite) sequence of row operations that transforms one
matrix into the other.
row reduction algorithm : A systematic method using elemen-
tary row operations that reduces a matrix to echelon form or
reduced echelon form.
row replacement : An elementary row operation that replaces
one row of a matrix by the sum of the row and a multiple of
another row.
row space (of a matrix A): The set Row Aof all linear combina-
tions of the vectors formed from the rows of A; also denoted
by Col AT.
row sum : The sum of the entries in a row of a matrix.
row vector : A matrix with only one row, or a single row of a
matrix that has several rows.
row‚Äìvector rule for computing Ax: The rule for computing a
product Axin which the ith entry of Axis the sum of the
CONFIRMING PAGES


--- Page 526 ---
Glossary A15
products of corresponding entries from row iofAand from
the vector x.
S
saddle point (of a dynamical system in R2): The origin when
some trajectories are attracted to 0and other trajectories are
repelled from 0.
same direction (as a vector v): A vector that is a positive
multiple of v.
sample mean : The average Mof a set of vectors, X1; : : : ; XN,
given by MD.1=N /. X1C    C XN/.
scalar : A (real) number used to multiply either a vector or a
matrix.
scalar multiple of u by c: The vector cuobtained by multiply-
ing each entry in ubyc.
scale (a vector): Multiply a vector (or a row or column of a
matrix) by a nonzero scalar.
Schur complement : A certain matrix formed from the blocks
of a22partitioned matrix AD¬åAij¬ç. IfA11is invert-
ible, its Schur complement is given by A22 A21A 1
11A12.
IfA22is invertible, its Schur complement is given by
A11 A12A 1
22A21.
Schur factorization (ofA, for real scalars): A factorization
ADURUTof an nnmatrix Ahaving nreal eigenvalues,
where Uis an nnorthogonal matrix and Ris an upper
triangular matrix.
set spanned by fv1; : : : ; vpg: The set Span fv1; : : : ; vpg.
signal (ordiscrete-time signal ): A doubly inÔ¨Ånite sequence of
numbers, fykg; a function deÔ¨Åned on the integers; belongs to
the vector space S.
similar (matrices): Matrices AandBsuch that P 1APDB,
or equivalently, ADPBP 1, for some invertible matrix P.
similarity transformation : A transformation that changes A
intoP 1AP.
simplex : The convex hull of an afÔ¨Ånely independent Ô¨Ånite set
of vectors in Rn.
singular (matrix): A square matrix that has no inverse.
singular value decomposition (of an mnmatrix A):AD
U ¬ÜVT, where Uis an mmorthogonal matrix, Vis an
nnorthogonal matrix, and ¬Üis anmnmatrix with non-
negative entries on the main diagonal (arranged in decreas-
ing order of magnitude) and zeros elsewhere. If rank ADr,
then¬Ühas exactly rpositive entries (the nonzero singular
values of A) on the diagonal.
singular values (ofA): The (positive) square roots of the eigen-
values of ATA, arranged in decreasing order of magnitude.
size (of a matrix): Two numbers, written in the form mn,
that specify the number of rows ( m) and columns ( n) in the
matrix.
solution (of a linear system involving variables x1; : : : ; x n): A
list.s1; s2; : : : ; s n/of numbers that makes each equation inthe system a true statement when the values s1; : : : ; s nare
substituted for x1; : : : ; x n, respectively.
solution set : The set of all possible solutions of a linear sys-
tem. The solution set is empty when the linear system is
inconsistent.
Span fv1; : : : ; vpg: The set of all linear combinations of
v1; : : : ; vp. Also, the subspace spanned (orgenerated ) by
v1; : : : ; vp.
spanning set (for a subspace H/: Any set fv1; : : : ; vpginH
such that HDSpanfv1; : : : ; vpg.
spectral decomposition (ofA): A representation
AD1u1uT
1C    C nunuT
n
where fu1; : : : ; ungis an orthonormal basis of eigenvectors
ofA, and1; : : : ;  nare the corresponding eigenvalues of A.
spiral point (of a dynamical system in R2): The origin when
the trajectories spiral about 0.
stage-matrix model : A difference equation xkC1DAxkwhere
xklists the number of females in a population at time k,
with the females classiÔ¨Åed by various stages of development
(such as juvenile, subadult, and adult).
standard basis : The basis ED fe1; : : : ; engforRnconsisting
of the columns of the nnidentity matrix, or the basis
f1; t; : : : ; tngforPn.
standard matrix (for a linear transformation T /: The matrix A
such that T .x/DAxfor all xin the domain of T.
standard position : The position of the graph of an equation
xTAxDc, when Ais a diagonal matrix.
state vector : A probability vector. In general, a vector that de-
scribes the ‚Äústate‚Äù of a physical system, often in connection
with a difference equation xkC1DAxk.
steady-state vector (for a stochastic matrix P): A probability
vector qsuch that PqDq.
stiffness matrix : The inverse of a Ô¨Çexibility matrix. The jth
column of a stiffness matrix gives the loads that must be
applied at speciÔ¨Åed points on an elastic beam in order to
produce a unit deÔ¨Çection at the jth point on the beam.
stochastic matrix : A square matrix whose columns are proba-
bility vectors.
strictly dominant eigenvalue : An eigenvalue 1of a matrix A
with the property that j1j>jkjfor all other eigenvalues
kofA.
submatrix (ofA): Any matrix obtained by deleting some rows
and/or columns of A; also, Aitself.
subspace : A subset Hof some vector space Vsuch that Hhas
these properties: (1) the zero vector of Vis in H; (2) H
is closed under vector addition; and (3) His closed under
multiplication by scalars.
supporting hyperplane (to a compact convex set SinRn): A
hyperplane HD¬åf:d¬çsuch that H\S6D¬øand either
f .x/dfor all xinSorf .x/dfor all xinS.
symmetric matrix : A matrix Asuch that AT=A.
CONFIRMING PAGES


--- Page 527 ---
A16 Glossary
system of linear equations (or a linear system ): A collection
of one or more linear equations involving the same set of
variables, say, x1; : : : ; x n.
T
tetrahedron : A three-dimensional solid object bounded by four
equal triangular faces, with three faces meeting at each
vertex.
total variance : The trace of the covariance matrix Sof a matrix
of observations.
trace (of a square matrix A): The sum of the diagonal entries in
A, denoted by tr A.
trajectory : The graph of a solution fx0;x1;x2; : : :gof a dynam-
ical system xkC1DAxk, often connected by a thin curve
to make the trajectory easier to see. Also, the graph of x.t/
fort0, when x.t/is a solution of a differential equation
x0.t/DAx.t/.
transfer matrix : A matrix Aassociated with an electrical cir-
cuit having input and output terminals, such that the output
vector is Atimes the input vector.
transformation (orfunction , or mapping )TfromRnto
Rm: A rule that assigns to each vector xinRna
unique vector T .x/inRm. Notation: TWRn!Rm. Also,
TWV!Wdenotes a rule that assigns to each xinVa
unique vector T .x/inW.
translation (by a vector p/: The operation of adding pto a
vector or to each vector in a given set.
transpose (ofA): An nmmatrix ATwhose columns are the
corresponding rows of the mnmatrix A.
trend analysis : The use of orthogonal polynomials to Ô¨Åt data,
with the inner product given by evaluation at a Ô¨Ånite set of
points.
triangle inequality :kuCvk  k uk C k vkfor all u,v.
triangular matrix : A matrix Awith either zeros above or zeros
below the diagonal entries.
trigonometric polynomial : A linear combination of the con-
stant function 1 and sine and cosine functions such as cos nt
and sin nt.
trivial solution : The solution xD0of a homogeneous equation
AxD0.
U
uncorrelated variables : Any two variables xiandxj(with
i¬§j) that range over the ith and jth coordinates of the
observation vectors in an observation matrix, such that the
covariance sijis zero.
underdetermined system : A system of equations with fewer
equations than unknowns.
uniqueness question : Asks, ‚ÄúIf a solution of a system exists, is
it unique‚Äîthat is, is it the only one?‚Äùunit consumption vector : A column vector in the Leontief
input‚Äìoutput model that lists the inputs a sector needs for
each unit of its output; a column of the consumption matrix.
unit lower triangular matrix : A square lower triangular ma-
trix with ones on the main diagonal.
unit vector : A vector vsuch that kvk D1.
upper triangular matrix : A matrix U(not necessarily square)
with zeros below the diagonal entries u11; u22; : : : :
V
Vandermonde matrix : An nnmatrix Vor its transpose,
when Vhas the form
VD2
6666641 x 1x2
1   xn 1
1
1 x 2x2
2   xn 1
2::::::::::::
1 x nx2
n   xn 1
n3
777775
variance (of a variable xj): The diagonal entry sjjin the covari-
ance matrix Sfor a matrix of observations, where xjvaries
over the jth coordinates of the observation vectors.
vector : A list of numbers; a matrix with only one column. In
general, any element of a vector space.
vector addition : Adding vectors by adding corresponding
entries.
vector equation : An equation involving a linear combination
of vectors with undetermined weights.
vector space : A set of objects, called vectors, on which two
operations are deÔ¨Åned, called addition and multiplication by
scalars. Ten axioms must be satisÔ¨Åed. See the Ô¨Årst deÔ¨Ånition
in Section 4.1.
vector subtraction : Computing uC. 1/vand writing the re-
sult as u v.
W
weighted least squares : Least-squares problems with a
weighted inner product such as
hx;yi Dw2
1x1y1C    C w2
nxnyn:
weights : The scalars used in a linear combination.
Z
zero subspace : The subspace f0gconsisting of only the zero
vector.
zero vector : The unique vector, denoted by 0, such that
uC0Dufor all u. InRn,0is the vector whose entries are
all zeros.
CONFIRMING PAGES


--- Page 528 ---
Answers to Odd-Numbered
Exercises
Chapter 1
Section 1.1, page 10
1.The solution is .x1; x2/D. 8; 3/, or simply . 8; 3/.
3..4=7; 9=7/
5.Replace row 2 by its sum with 3 times row 3, and then
replace row 1 by its sum with  5times row 3.
7.The solution set is empty.
9..4; 8; 5; 2/ 11.Inconsistent
13..5; 3; 1/ 15.Consistent
17. The three lines have one point in common.
19.h¬§2 21.Allh
23. Mark a statement True only if the statement is always true.
Giving you the answers here would defeat the purpose of
the true‚Äìfalse questions, which is to help you learn to read
the text carefully. The Study Guide will tell you where to
look for the answers, but you should not consult it until you
have made an honest attempt to Ô¨Ånd the answers yourself.
25.kC2gChD0
27. The row reduction of1 3 f
c d g
to
1 3 f
0 d  3c g  cf
shows that d 3cmust be
nonzero, since fandgare arbitrary. Otherwise, for some
choices of fandgthe second row could correspond to an
equation of the form 0Db, where bis nonzero. Thus
d¬§3c.
29. Swap row 1 and row 2; swap row 1 and row 2.
31. Replace row 3 by row 3 C( 4) row 1; replace row 3 by
row 3 C(4) row 1.
33.4T1 T2  T4D30
 T1C4T2 T3 D60
 T2C4T3 T4D70
 T1  T3C4T4D40
Section 1.2, page 21
1.Reduced echelon form: a and b. Echelon form: d. Not
echelon: c.3.2
41 0 1 2
0 1 2 3
0 0 0 03
5. Pivot cols 1 and 2:
2
41 2 3 4
4 5 6 7
6 7 8 93
5.
5.
0
,
0 0
,0
0 0
7.8
<
:x1D  5 3x2
x2is free
x3D39.8
<
:x1D4C5x3
x2D5C6x3
x3is free
11.8
¬à¬à<
¬à¬à:x1D4
3x2 2
3x3
x2is free
x3is free
13.8
¬à¬à¬à¬à<
¬à¬à¬à¬à:x1D5C3x5
x2D1C4x5
x3is free
x4D4 9x5
x5is free
Note: TheStudy Guide discusses
the common mistake x3D0.
15. a.Consistent, with a unique solution
b.Inconsistent
17.hD7=2
19. a.Inconsistent when hD2andk¬§8
b.A unique solution when h¬§2
c.Many solutions when hD2andkD8
21. Read the text carefully, and write your answers before you
consult the Study Guide . Remember, a statement is true
only if it is true in all cases.
23. Yes. The system is consistent because with three pivots,
there must be a pivot in the third (bottom) row of the
coefÔ¨Åcient matrix. The reduced echelon form cannot
contain a row of the form ¬å0 0 0 0 0 1¬ç .
25. If the coefÔ¨Åcient matrix has a pivot position in every row,
then there is a pivot position in the bottom row, and there is
no room for a pivot in the augmented column. So, the
system is consistent, by Theorem 2.
SECOND REVISED PAGES
A17

--- Page 529 ---
A18 Answers to Odd-Numbered Exercises
27. If a linear system is consistent, then the solution is unique if
and only if every column in the coefÔ¨Åcient matrix is a pivot
column; otherwise, there are inÔ¨Ånitely many solutions.
29. An underdetermined system always has more variables than
equations. There cannot be more basic variables than there
are equations, so there must be at least one free variable.
Such a variable may be assigned inÔ¨Ånitely many different
values. If the system is consistent, each different value of a
free variable will produce a different solution.
31. Yes, a system of linear equations with more equations than
unknowns can be consistent. The following system has a
solution .x1Dx2D1/:
x1Cx2D2
x1 x2D0
3x1C2x2D5
33. [M]p.t/D7C6t t2
Section 1.3, page 32
1. 4
1
,5
4
3.
x2
x1u ‚Äì 2v
‚Äì2vu ‚Äì v
‚Äìv
vu
u + v
5.x12
46
 1
53
5Cx22
4 3
4
03
5D2
41
 7
 53
5,
2
46x1
 x1
5x13
5C2
4 3x2
4x2
03
5D2
41
 7
 53
5,2
46x1 3x2
 x1C4x2
5x13
5D2
41
 7
 53
5
6x1 3x2D1
 x1C4x2D  7
5x1 D  5
Usually the intermediate steps are not displayed.
7.aDu 2v,bD2u 2v,cD2u 3:5v,dD3u 4v
9.x12
40
4
 13
5Cx22
41
6
33
5Cx32
45
 1
 83
5D2
40
0
03
5
11.Yes, bis a linear combination of a1,a2, and a3.
13. No,bisnota linear combination of the columns of A.
15. Noninteger weights are acceptable, of course, but some
simple choices are 0v1C0v2D0, and1v1C0v2D2
47
1
 63
5; 0v1C1v2D2
4 5
3
03
5
1v1C1v2D2
42
4
 63
5; 1v1 1v2D2
412
 2
 63
5
17.hD  17
19. Spanfv1;v2gis the set of points on the line through v1
and0.
21. Hint: Show that2 2 h
 1 1 k
is consistent for all hand
k. Explain what this calculation shows about Span fu;vg.
23. Before you consult your Study Guide , read the entire
section carefully. Pay special attention to deÔ¨Ånitions and
theorem statements, and note any remarks that precede or
follow them.
25. a.No, three b.Yes, inÔ¨Ånitely many
c.a1D1a1C0a2C0a3
27. a.5v1is the output of 5 day‚Äôs operation of mine #1.
b.The total output is x1v1Cx2v2, sox1andx2should
satisfy x1v1Cx2v2D150
2825
.
c.[M] 1.5 days for mine #1 and 4 days for mine #2
29..1:3; :9; 0/
31. a."
10=3
2#
b.Add 3.5 g at .0; 1/ , add .5 g at .8; 1/ , and add 2 g at
.2; 4/ .
33. Review Practice Problem 1 and then write a solution. The
Study Guide has a solution.
Section 1.4, page 40
1.The product is not deÔ¨Åned because the number of columns
(2) in the 32matrix does not match the number of entries
(3) in the vector.
3.AxD2
46 5
 4 3
7 63
52
 3
D22
46
 4
73
5 32
45
 3
63
5
D2
412
 8
143
5C2
4 15
9
 183
5D2
4 3
1
 43
5, and
AxD2
46 5
 4 3
7 63
52
 3
D2
462C5. 3/
. 4/2C. 3/. 3/
72C6. 3/3
5
D2
4 3
1
 43
5. Show your work here and for Exercises 4‚Äì6, but
thereafter perform the calculations mentally.
SECOND REVISED PAGES


--- Page 530 ---
Section 1.5 A19
5.55
 2
 11
 7
C3 8
3
 24
 5
D 8
16
7.2
6644 5 7
 1 3  8
7 5 0
 4 1 23
7752
4x1
x2
x33
5D2
6646
 8
0
 73
775
9.x13
0
Cx21
1
Cx3 5
4
D9
0
and
3 1  5
0 1 42
4x1
x2
x33
5D9
0
11.2
41 2 4  2
0 1 5 2
 2 4 3 93
5,xD2
4x1
x2
x33
5D2
40
 3
13
5
13. Yes. (Justify your answer.)
u
u are here
15. The equation AxDbis not consistent when 3b1Cb2is
nonzero. (Show your work.) The set of bfor which the
equation isconsistent is a line through the origin‚Äîthe set of
all points .b1; b2/satisfying b2D  3b1.
17. Only three rows contain a pivot position. The equation
AxDbdoes nothave a solution for each binR4, by
Theorem 4.
19. The work in Exercise 17 shows that statement (d) in
Theorem 4 is false. So all four statements in Theorem 4 are
false. Thus, not all vectors in R4can be written as a linear
combination of the columns of A. Also, the columns of A
donotspanR4.
21. The matrix ¬åv1v2v3¬çdoes not have a pivot in each row,
so the columns of the matrix do not span R4, by Theorem 4.
That is, fv1;v2;v3gdoes not span R4.
23. Read the text carefully and try to mark each exercise
statement True or False before you consult the Study Guide .
Several parts of Exercises 23 and 24 are implications of the
form
‚ÄúIfhstatement 1 i, then hstatement 2 i‚Äù
or equivalently,
‚Äúhstatement 2 i, ifhstatement 1 i‚Äù
Mark such an implication as True if hstatement 2 iis true in
all cases when hstatement 1 iis true.
25.c1D  3,c2D  1,c3D2
27.QxDv, where QD¬åq1q2q3¬çandxD2
4x1
x2
x33
5
Note: If your answer is the equation AxDb, you must
specify what Aandbare.29. Hint: Start with any 33matrix Bin echelon form that
has three pivot positions.
31. Write your solution before you check the Study Guide .
33. Hint: How many pivot columns does Ahave? Why?
35. Given Ax1Dy1andAx2Dy2, you are asked to show that
the equation AxDwhas a solution, where wDy1Cy2.
Observe that wDAx1CAx2and use Theorem 5(a) with x1
andx2in place of uandv, respectively. That is,
wDAx1CAx2DA.x1Cx2/. So the vector xDx1Cx2
is a solution of wDAx.
37. [M] The columns do not span R4.
39. [M] The columns span R4.
41. [M] Delete column 4 of the matrix in Exercise 39. It is also
possible to delete column 3 instead of column 4.
Section 1.5, page 48
1.The system has a nontrivial solution because there is a free
variable, x3.
3.The system has a nontrivial solution because there is a free
variable, x3.
5.xD2
4x1
x2
x33
5Dx32
45
 2
13
5
7.xD2
664x1
x2
x3
x43
775Dx32
664 9
4
1
03
775Cx42
6648
 5
0
13
775
9.xDx22
43
1
03
5Cx32
4 2
0
13
5
11.Hint: The system derived from the reduced echelon form
is
x1 4x2 C5x6D0
x3  x6D0
x5 4x6D0
0D0
The basic variables are x1,x3, and x5. The remaining
variables are free. The Study Guide discusses two mistakes
that are often made on this type of problem.
13. xD2
45
 2
03
5Cx32
44
 7
13
5DpCx3q:Geometrically, the
solution set is the line through2
45
 2
03
5parallel to2
44
 7
13
5.
SECOND REVISED PAGES


--- Page 531 ---
A20 Answers to Odd-Numbered Exercises
15. xD2
4x1
x2
x33
5D2
4 2
1
03
5Cx32
45
 2
13
5. The solution set is the
line through2
4 2
1
03
5, parallel to the line that is the solution
set of the homogeneous system in Exercise 5.
17. LetuD2
4 9
1
03
5;vD2
44
0
13
5;pD2
4 2
0
03
5. The solution of
the homogeneous equation is xDx2uCx3v, the plane
through the origin spanned by uandv. The solution set of
the nonhomogeneous system is xDpCx2uCx3v, the
plane through pparallel to the solution set of the
homogeneous equation.
19. xDaCtb, where trepresents a parameter, or
xDx1
x2
D 2
0
Ct 5
3
, orx1D  2 5t
x2D3t
21. xDpCt.q p/D2
 5
Ct 5
6
23. It is important to read the text carefully and write your
answers. After that, check the Study Guide , if necessary.
25.AvhDA.w p/DAw ApDb bD0
27. When Ais the 33zero matrix, every xinR3satisÔ¨Åes
AxD0. So the solution set is all vectors in R3.
29. a.When Ais a33matrix with three pivot positions, the
equation AxD0has no free variables and hence has no
nontrivial solution.
b.With three pivot positions, Ahas a pivot position in
each of its three rows. By Theorem 4 in Section 1.4, the
equation AxDbhas a solution for every possible b.
The word ‚Äúpossible‚Äù in the exercise means that the only
vectors considered in this case are those in R3, because
Ahas three rows.
31. a.When Ais a32matrix with two pivot positions, each
column is a pivot column. So the equation AxD0has
no free variables and hence no nontrivial solution.
b.With two pivot positions and three rows, Acannot have
a pivot in every row. So the equation AxDbcannot
have a solution for every possible b(inR3), by
Theorem 4 in Section 1.4.
33. One answer: xD3
 1
35. Your example should have the property that the sum of the
entries in each row is zero. Why?
37. One answer is AD1 4
1 4
. The Study Guide shows how
to analyze the problem in order to construct A. Ifbis any
vector nota multiple of the Ô¨Årst column of A, then the
solution set of AxDbis empty and thus cannot be formed
by translating the solution set of AxDb. This does not
contradict Theorem 6, because that theorem applies when
the equation AxDbhas a nonempty solution set.39. Ifcis a scalar, then A.cu/DcAu, by Theorem 5(b) in
Section 1.4. If usatisÔ¨Åes AxD0, then AuD0,
cAuDc0D0, and so A.cu/D0.
Section 1.6, page 55
1.The general solution is pGoodsD:875p Services ;withpServices
free. One equilibrium solution is pServices D1000 and
pGoodsD875. Using fractions, the general solution could be
written pGoodsD.7=8/p Services , and a natural choice of
prices might be pServices D80andpGoodsD70. Only the
ratio of the prices is important. The economic equilibrium
is unaffected by a proportional change in prices.
3.a.Distribution of
Output From:
C&M F&P Mach.
Output # # # Input Purchased By:
.2 .8 .4 ! C&M
.3 .1 .4 ! F&P
.5 .1 .2 ! Mach.
b.2
4:8 :8 :4 0
 :3 :9  :4 0
 :5 :1 :8 03
5
c.[M]pChemicals D141:7 ,pFuelsD91:7,pMachinery D100.
To two signiÔ¨Åcant Ô¨Ågures, pChemicals D140; p FuelsD92;
pMachinery D100.
5.B2S3C6H2O!2H3BO3C3H2S
7.3NaHCO 3CH3C6H5O7!Na3C6H5O7C3H2OC3CO 2
9.[M] 15PbN 6C44CrMn 2O8!
5Pb 3O4C22Cr2O3C88MnO 2C90NO
11.8
¬à¬à<
¬à¬à:x1D20 x3
x2D60Cx3
x3is free
x4D60The largest value of x3is 20.
13. a.8
¬à¬à¬à¬à¬à¬à<
¬à¬à¬à¬à¬à¬à:x1Dx3 40
x2Dx3C10
x3is free
x4Dx6C50
x5Dx6C60
x6is freeb.8
¬à¬à<
¬à¬à:x2D50
x3D40
x4D50
x5D60
Section 1.7, page 61
Justify your answers to Exercises 1‚Äì22.
1.Lin. indep. 3.Lin. depen.
5.Lin. indep. 7.Lin. depen.
9.a.Noh b.Allh
11.hD6 13.Allh
15. Lin. depen. 17.Lin. depen. 19.Lin. indep.
21. If you consult your Study Guide before you make a good
effort to answer the true-false questions, you will destroy
most of their value.
SECOND REVISED PAGES


--- Page 532 ---
Section 1.8 A21
23.2
4 
0 
0 03
5 25.2
664
0
0 0
0 03
775and2
6640
0 0
0 0
0 03
775
27. All Ô¨Åve columns of the 75matrix Amust be pivot
columns. Otherwise, the equation AxD0would have a
free variable, in which case the columns of Awould be
linearly dependent.
29.A: Any 32matrix with two nonzero columns such that
neither column is a multiple of the other. In this case, the
columns are linearly independent, and so the equation
AxD0has only the trivial solution.
B: Any 32matrix with one column a multiple of the
other.
31. xD2
41
1
 13
5
33. True, by Theorem 7. (The Study Guide adds another
justiÔ¨Åcation.)
35. False. The vector v1could be the zero vector.
37. True. A linear dependence relation among v1,v2,v3may be
extended to a linear dependence relation among v1,v2,v3,
v4by placing a zero weight on v4.
39. You should be able to work this important problem without
help. Write your solution before you consult the Study
Guide .
41. [M]BD2
6648 3 2
 9 4  7
6 2 4
5 1 103
775. Other choices are possible.
43. [M] Each column of Athat is not a column of Bis in the
set spanned by the columns of B.
Section 1.8, page 69
1.2
 6
;2a
2b
3. xD2
43
1
23
5, unique solution
5.xD2
43
1
03
5, not unique 7.aD5; bD6
9.xDx32
6649
4
1
03
775Cx42
664 7
 3
0
13
775
11.Yes, because the system represented by ¬åA b¬çis
consistent.13.
x1x2
uv
T(v)T(u)
A reÔ¨Çection through the origin
15.
x1x2
uv T(v)
T(u)
A projection onto the x2-axis.
17.6
3
; 2
6
;4
9
19.13
7
;2x1 x2
5x1C6x2
21. Read the text carefully and write your answers before you
check the Study Guide . Notice that Exercise 21(e) is a
sentence of the form
‚Äúhstatement 1 iif and only if hstatement 2 i‚Äù
Mark such a sentence as True if hstatement 1 iis true
whenever hstatement 2 iis true andalsohstatement 2 iis true
whenever hstatement 1 iis true.
23.
x1x2
ucu
T(cu)T(u)
T(u)
T(u + v)u + v
x1x2
T(v)vu
25. Hint: Show that the image of a line (that is, the set of
images of all points on a line) can be represented by the
parametric equation of a line.
27. a.The line through pandqis parallel to q p. (See
Exercises 21 and 22 in Section 1.5.) Since pis on the
line, the equation of the line is xDpCt.q p/.
Rewrite this as xDp tpCtqandxD.1 t/pCtq.
b.Consider xD.1 t/pCtqfortsuch that 0t1.
Then, by linearity of T, for0t1
SECOND REVISED PAGES


--- Page 533 ---
A22 Answers to Odd-Numbered Exercises
T .x/DT ..1 t/pCtq/D.1 t/T .p/CtT .q/()
IfT .p/andT .q/are distinct, then (*) is the equation
for the line segment between T .p/andT .q/, as shown
in part (a). Otherwise, the set of images is just the single
point T .p/, because
.1 t/T .p/CtT .q/D.1 t/T .p/CtT .p/DT .p/
29. a.When bD0; f .x/ Dmx. In this case, for all x; y inR
and all scalars candd,
f .cxCdy/Dm.cx Cdy/DmcxCmdy
Dc.mx/ Cd.my/ Dcf .x/Cdf .y/
This shows that fis linear.
b.When f .x/DmxCb, with bnonzero,
f .0/Dm.0/CbDb¬§0.
c.In calculus, fis called a ‚Äúlinear function‚Äù because the
graph of fis a line.
31. Hint: Since fv1;v2;v3gis linearly dependent, you can write
a certain equation and work with it.
33. One possibility is to show that Tdoes not map the zero
vector into the zero vector, something that every linear
transformation does do:T .0; 0/ D.0; 4; 0/ .
35. Take uandvinR3and let canddbe scalars. Then
cuCdvD.cu 1Cdv1; cu 2Cdv2; cu 3Cdv3/
The transformation Tis linear because
T .cuCdv/D.cu 1Cdv1; cu 2Cdv2; .cu 3Cdv3//
D.cu 1Cdv1; cu 2Cdv2; cu3 dv3/
D.cu 1; cu 2; cu3/C.dv 1; dv 2; dv3/
Dc.u 1; u2; u3/Cd.v 1; v2; v3/
DcT .u/CdT .v/
37. [M] All multiples of .7; 9; 0; 2/
39. [M] Yes. One choice for xis.4; 7; 1; 0/ .
Section 1.9, page 79
1.2
6643 5
1 2
3 0
1 03
7753.0 1
 1 0
5.1 0
 2 1
7. 1=p
2 1=p
2
1=p
2 1=p
2
9.0 1
 1 2
11.The described transformation Tmaps e1into e1and maps
e2into e2. A rotation through radians also maps e1into
 e1and maps e2into e2. Since a linear transformation is
completely determined by what it does to the columns of
the identity matrix, the rotation transformation has the same
effect as Ton every vector in R2.13.
x1x2
T(e2)2T(e1)
T(e1)T(2, 1)
15.2
43 0  2
4 0 0
1 1 13
5 17.2
6640 0 0 0
1 1 0 0
0 1 1 0
0 0 1 13
775
19.1 5 4
0 1  6
21. x D7
 4
23. Answer the questions before checking the Study Guide .
Justify your answers to Exercises 25‚Äì28.
25. Not one-to-one and does not map R4ontoR4
27. Not one-to-one but maps R3ontoR2
29.2
664 
0 
0 0
0 0 03
775
31.n. (Explain why, and then check the Study Guide ).
33. Hint: Ifejis the jth column of In, then Bejis the jth
column of B.
35. Hint: Is it possible that m > n ? What about m < n ?
37. [M] No. (Explain why.)
39. [M] No. (Explain why.)
Section 1.10, page 87
1.a.x12
664110
4
20
23
775Cx22
664130
3
18
53
775D2
664295
9
48
83
775, where x1is the
number of servings of Cheerios and x2is the number of
servings of 100% Natural Cereal.
b.2
664110 130
4 3
20 18
2 53
775x1
x2
D2
664295
9
48
83
775. Mix 1.5 servings of
Cheerios together with 1 serving of 100% Natural
Cereal.
3.a.She should mix .99 serving of Mac and Cheese, 1.54
servings of broccoli, and .79 serving of chicken to get
her desired nutritional content.
b.She should mix 1.09 servings of shells and white
cheddar, .88 serving of broccoli, and 1.03 servings of
chicken to get her desired nutritional content. Notice
that this mix contains signiÔ¨Åcantly less broccoli, so she
should like it better.
SECOND REVISED PAGES


--- Page 534 ---
Chapter 1 Supplementary Exercises A23
5.RiDv,2
66411  5 0 0
 5 10  1 0
0 1 9  2
0 0  2 103
7752
664I1
I2
I3
I43
775D2
66450
 40
30
 303
775
[M]:iD2
664I1
I2
I3
I43
775D2
6643:68
 1:90
2:57
 2:493
775
7.RiDv,2
66412  7 0  4
 7 15  6 0
0 6 14  5
 4 0  5 133
7752
664I1
I2
I3
I43
775D2
66440
30
20
 103
775
[M]:iD2
664I1
I2
I3
I43
775D2
66411:43
10:55
8:04
5:843
775
9.xkC1DMxkforkD0; 1; 2; : : : ; where
MD:93 :05
:07 :95
and x0D800;000
500;000
:
The population in 2017 (for kD2) isx2D741;720
558;280
.
11.a.MD:98033 :00179
:01967 :99821
b.[M]x10D35:729
278:18
13. [M]
a.The population of the city decreases. After 7 years, the
populations are about equal, but the city population
continues to decline. After 20 years, there are only
417,000 persons in the city (417,456 rounded off).
However, the changes in population seem to grow
smaller each year.
b.The city population is increasing slowly, and the
suburban population is decreasing. After 20 years, the
city population has grown from 350,000 to about
370,000.
Chapter 1 Supplementary Exercises, page 89
1. a.F b.F c.T d.F e.T
f.T g.F h.F i.T j.F
k.T l.F m.T n.T o.T
p.T q.F r.T s.F t.T
u.F v.F w.T x.T y.T z.F
3.a.Any consistent linear system whose echelon form is
2
4  
0  
0 0 0 03
5or2
4  
0 0 
0 0 0 03
5
or2
40  
0 0 
0 0 0 03
5
b.Any consistent linear system whose reduced echelon
form is I3.c.Any inconsistent linear system of three equations in
three variables.
5.a.The solution set: (i) is empty if hD12andk¬§2; (ii)
contains a unique solution if h¬§12; (iii) contains
inÔ¨Ånitely many solutions if hD12andkD2.
b.The solution set is empty if kC3hD0; otherwise, the
solution set contains a unique solution.
7.a.Setv1D2
42
 5
73
5,v2D2
4 4
1
 53
5,v3D2
4 2
1
 33
5, and
bD2
4b1
b2
b33
5. ‚ÄúDetermine if v1,v2,v3spanR3.‚Äù
Solution: No.
b.SetAD2
42 4 2
 5 1 1
7 5 33
5. ‚ÄúDetermine if the
columns of AspanR3.‚Äù
c.DeÔ¨Åne T .x/DAx. ‚ÄúDetermine if TmapsR3ontoR3.‚Äù
9.5
6
D4
32
1
C7
31
2
or5
6
D
8=3
4=3
C7=3
14=3
10. Hint: Construct a ‚Äúgrid‚Äù on the x1x2-plane determined by
a1anda2.
11.A solution set is a line when the system has one free
variable. If the coefÔ¨Åcient matrix is 23, then two of the
columns should be pivot columns. For instance, take1 2 
0 3 
. Put anything in column 3. The resulting
matrix will be in echelon form. Make one row replacement
operation on the second row to create a matrix notin
echelon form, such as1 2 1
0 3 1
1 2 1
1 5 2
.
12. Hint: How many free variables are in the equation AxD0?
13.ED2
41 0  3
0 1 2
0 0 03
5
15. a.If the three vectors are linearly independent, then a; c;
andfmust all be nonzero.
b.The numbers a; : : : ; f can have any values.
16. Hint: List the columns from right to left as v1; : : : ; v4.
17. Hint: Use Theorem 7.
19. LetMbe the line through the origin that is parallel to the
line through v1,v2, and v3. Then v2 v1andv3 v1are
both on M. So one of these two vectors is a multiple of the
other, say v2 v1Dk.v3 v1). This equation produces a
linear dependence relation: .k 1/v1Cv2 kv3D0.
A second solution: A parametric equation of the line is
xDv1Ct.v2 v1/:Since v3is on the line, there is some t0
such that v3Dv1Ct0.v2 v1/D.1 t0/v1Ct0v2. Sov3
is a linear combination of v1andv2, andfv1,v2,v3gis
linearly dependent.
SECOND REVISED PAGES


--- Page 535 ---
A24 Answers to Odd-Numbered Exercises
21.2
41 0 0
0 1 0
0 0 13
5 23.aD4=5andbD  3=5
25. a.The vector lists the number of three-, two-, and
one-bedroom apartments provided when x1Ô¨Çoors of
plan A are constructed.
b.x12
43
7
83
5Cx22
44
4
83
5Cx32
45
3
93
5
c.[M] Use 2 Ô¨Çoors of plan A and 15 Ô¨Çoors of plan B. Or,
use 6 Ô¨Çoors of plan A, 2 Ô¨Çoors of plan B, and 8 Ô¨Çoors of
plan C. These are the only feasible solutions. There are
other mathematical solutions, but they require a
negative number of Ô¨Çoors of one or two of the plans,
which makes no physical sense.
Chapter 2
Section 2.1, page 102
1. 4 0 2
 8 10  4
,3 5 3
 7 6  7
, not deÔ¨Åned,
1 13
 7 6
3. 1 1
 5 5
,12  3
15  6
5.a.Ab1D2
4 7
7
123
5,Ab2D2
44
 6
 73
5,
ABD2
4 7 4
7 6
12  73
5
b.ABD2
4 13C2. 2/  1. 2/C21
53C4. 2/ 5.  2/C41
23 3. 2/ 2.  2/ 313
5
D2
4 7 4
7 6
12  73
5
7.37 9.kD5
11.ADD2
42 3 5
2 6 15
2 12 253
5,DAD2
42 2 2
3 6 9
5 20 253
5
Right-multiplication (that is, multiplication on the right) by
Dmultiplies each column ofAby the corresponding
diagonal entry of D. Left-multiplication by Dmultiplies
each rowofAby the corresponding diagonal entry of D.
TheStudy Guide tells how to make ABDBA, but you
should try this yourself before looking there.
13. Hint: One of the two matrices is Q.
15. Answer the questions before looking in the Study Guide.
17. b1D7
4
,b2D 8
 519. The third column of ABis the sum of the Ô¨Årst two columns
ofAB. Here‚Äôs why. Write BD¬åb1 b2 b3¬ç. By
deÔ¨Ånition, the third column of ABisAb3. Ifb3Db1Cb2,
thenAb3DA.b1Cb2/DAb1CAb2, by a property of
matrix-vector multiplication.
21. The columns of Aare linearly dependent. Why?
23. Hint: Suppose xsatisÔ¨Åes AxD0, and show that xmust
be0.
25. Hint: Use the results of Exercises 23 and 24, and apply the
associative law of multiplication to the product CAD .
27. uTvDvTuD  2aC3b 4c,
uvTD2
4 2a  2b  2c
3a 3b 3c
 4a  4b  4c3
5;
vuTD2
4 2a 3a  4a
 2b 3b  4b
 2c 3c  4c3
5
29. Hint: For Theorem 2(b), show that the .i; j / -entry of
A.BCC/equals the .i; j / -entry of ABCAC.
31. Hint: Use the deÔ¨Ånition of the product ImAand the fact that
ImxDxforxinRm.
33. Hint: First write the .i; j / -entry of .AB/T, which is the
.j; i/ -entry of AB. Then, to compute the .i; j / -entry in
BTAT, use the facts that the entries in row iofBTare
b1i; : : : ; b ni, because they come from column iofB, and
the entries in column jofATareaj1; : : : ; a jn, because they
come from row jofA.
35. [M] The answer here depends on the choice of matrix
program. For MATLAB, use the help command to read
about zeros, ones, eye, anddiag .
37. [M] Display your results and report your conclusions.
39. [M] The matrix S‚Äúshifts‚Äù the entries in a vector
.a; b; c; d; e/ to yield .b; c; d; e; 0/ .S5is the 55zero
matrix. So is S6.
Section 2.2, page 111
1.2 3
 5=2 4
3. 1
5 5 5
7 8
or
1 1
 7=5  8=5
5.x1D7andx2D  9
7.aandb: 9
4
,11
 5
,6
 2
, and13
 5
9.Write out your answers before checking the Study Guide .
11.The proof can be modeled after the proof of Theorem 5.
13.ABDAC)A 1ABDA 1AC)IBDIC)
BDC. No, in general, BandCcan be different when A
is not invertible. See Exercise 10 in Section 2.1.
15.DDC 1B 1A 1. Show that Dworks.
17.ADBCB 1
SECOND REVISED PAGES


--- Page 536 ---
Section 2.3 A25
19. After you Ô¨Ånd XDCB A, show that Xis a solution.
21. Hint: Consider the equation AxD0.
23. Hint: IfAxD0has only the trivial solution, then there are
no free variables in the equation AxD0, and each column
ofAis a pivot column.
25. Hint: Consider the case aDbD0. Then consider the
vector b
a
, and use the fact that ad bcD0.
27. Hint: For part (a), interchange AandBin the box
following Example 6 in Section 2.1, and then replace Bby
the identity matrix. For parts (b) and (c), begin by writing
AD2
4row 1.A/
row 2.A/
row 3.A/3
5
29. 7 2
4 1
31.2
48 3 1
10 4 1
7=2 3=2 1=23
5
33.A 1DBD2
6666641 0 0    0
 1 1 0 0
0 1 1
:::::::::
0 0      1 13
777775.Hint: For
jD1; : : : ; n , letaj,bj, and ejdenote the jth columns of
A; B , and I, respectively. Use the facts that aj ajC1D
ejandbjDej ejC1forjD1; : : : ; n  1, and
anDbnDen.
35.2
43
 6
43
5. Find this by row reducing ¬åA e3¬ç:
37.CD1 1  1
 1 1 0
39. .27, .30, and .23 inch, respectively
41. [M] 12, 1.5, 21.5, and 12 newtons, respectively
Section 2.3, page 117
The abbreviation IMT (here and in the Study Guide ) denotes the
Invertible Matrix Theorem (Theorem 8).
1.Invertible, by the IMT. Neither column of the matrix is a
multiple of the other column, so they are linearly
independent. Also, the matrix is invertible by Theorem 4 in
Section 2.2 because the determinant is nonzero.
3.Invertible, by the IMT. The matrix row reduces to2
45 0 0
0 7 0
0 0  13
5and has 3 pivot positions.
5.Not invertible, by the IMT. The matrix row reduces to2
41 0 2
03 5
0 0 03
5and is not row equivalent to I3.7.Invertible, by the IMT. The matrix row reduces to2
664 1 3 0 1
0 4 8 0
0 0 3 0
0 0 0 13
775and has four pivot positions.
9.[M] The 44matrix has four pivot positions, so it is
invertible by the IMT.
11.TheStudy Guide will help, but Ô¨Årst try to answer the
questions based on your careful reading of the text.
13. A square upper triangular matrix is invertible if and only if
all the entries on the diagonal are nonzero. Why?
Note: The answers below for Exercises 15‚Äì29 mention the IMT.
In many cases, part or all of an acceptable answer could also be
based on results that were used to establish the IMT.
15. IfAhas two identical columns then its columns are linearly
dependent. Part (e) of the IMT shows that Acannot be
invertible.
17. IfAis invertible, so is A 1, by Theorem 6 in Section 2.2.
By (e) of the IMT applied to A 1, the columns of A 1are
linearly independent.
19. By (e) of the IMT, Dis invertible. Thus the equation
DxDbhas a solution for each binR7, by (g) of the IMT.
Can you say more?
21. The matrix Gcannot be invertible, by Theorem 5 in Section
2.2 or by the paragraph following the IMT. So (g) of the
IMT is false and so is (h). The columns of Gdo not
spanRn.
23. Statement (b) of the IMT is false for K, so statements (e)
and (h) are also false. That is, the columns of Kare linearly
dependent and the columns do notspanRn.
25. Hint: Use the IMT Ô¨Årst.
27. LetWbe the inverse of AB. Then ABW DIand
A.BW / DI. Unfortunately, this equation by itself does
not prove that Ais invertible. Why not? Finish the proof
before you check the Study Guide .
29. Since the transformation x7!Axis not one-to-one,
statement (f) of the IMT is false. Then (i) is also false and
the transformation x7!Axdoes not map RnontoRn. Also,
Ais not invertible, which implies that the transformation
x7!Axis not invertible, by Theorem 9.
31. Hint: If the equation AxDbhas a solution for each b, then
Ahas a pivot in each row (Theorem 4 in Section 1.4).
Could there be free variables in an equation AxDb?
33. Hint: First show that the standard matrix of Tis invertible.
Then use a theorem or theorems to show that
T 1.x/DBx, where BD7 9
4 5
.
35. Hint: To show that Tis one-to-one, suppose that
T .u/DT .v/for some vectors uandvinRn. Deduce
thatuDv. To show that Tis onto, suppose yrepresents
an arbitrary vector in Rnand use the inverse Sto produce
SECOND REVISED PAGES


--- Page 537 ---
A26 Answers to Odd-Numbered Exercises
anxsuch that T .x/Dy. A second proof can be given
using Theorem 9 together with a theorem from
Section 1.9.
37. Hint: Consider the standard matrices of TandU.
39. Given any vinRn, we may write vDT .x) for some x,
because Tis an onto mapping. Then, the assumed
properties of SandUshow that S.v/DS.T . x//D
xandU.v/DU.T . x//Dx. SoS.v/andU.v/are equal
for each v. That is, SandUare the same function from Rn
intoRn.
41. [M]a.The exact solution of (3) is x1D3:94 and
x2D:49. The exact solution of (4) is x1D2:90 and
x2D2:00.
b.When the solution of (4) is used as an approximation for
the solution in (3), the error in using the value of 2.90
forx1is about 26%, and the error in using 2.0 for x2is
about 308%.
c.The condition number of the coefÔ¨Åcient matrix is 3363.
The percentage change in the solution from (3) to (4) is
about 7700 times the percentage change in the right side
of the equation. This is the same order of magnitude as
the condition number. The condition number gives a
rough measure of how sensitive the solution of AxDb
can be to changes in b. Further information about the
condition number is given at the end of Chapter 6 and in
Chapter 7.
43. [M] cond .A/69;000 , which is between 104and105. So
about 4 or 5 digits of accuracy may be lost. Several
experiments with MATLAB should verify that xandx1
agree to 11 or 12 digits.
45. [M] Some versions of MATLAB issue a warning when
asked to invert a Hilbert matrix of order about 12 or larger
using Ô¨Çoating-point arithmetic. The product AA 1should
have several off-diagonal entries that are far from being
zero. If not, try a larger matrix.
Section 2.4, page 123
1.A B
EACC EB CD
3.Y Z
W X
5.YDB 1(explain why), XD  B 1A,ZDC
7.XDA 1(why?), YD  BA 1,ZD0(why?)
9.XD  A21A 1
11,YD  A31A 1
11,B22DA22 A21A 1
11A12
11.You can check your answers in the Study Guide .
13. Hint: Suppose Ais invertible, and let A 1DD E
F G
.
Show that BDDIandCGDI. This implies that Band
Care invertible. (Explain why!) Conversely, suppose B
andCare invertible. To prove that Ais invertible, guess
what A 1must be and check that it works.15.A11 A12
A21 A22
D
I 0
A21A 1
11 IA11 0
0 SI A 1
11A12
0 I
withSDA22 A21A 1
11A12.
17. GkC1DXkxkC1XT
k
xT
kC1
DXkXT
kCxkC1xT
kC1
DGkCxkC1xT
kC1
Only the outer product matrix xkC1xT
kC1needs to be
computed (and then added to Gk).
19.W.s/DIm C.A sIn/ 1B. This is the Schur
complement of A sInin the system matrix.
21. a.A2D1 0
3 11 0
3 1
D1C0 0 C0
3 3 0 C. 1/2
D1 0
0 1
b.M2DA 0
I AA 0
I A
DA2C0 0 C0
A A 0 C. A/2
DI 0
0 I
23. IfA1andB1are.kC1/.kC1/and lower triangular,
then we can write A1Da 0T
v A
and
B1Db 0T
w B
, where AandBarekkand lower
triangular, vandware inRk, and aandbare suitable
scalars. Assume that the product of kklower triangular
matrices is lower triangular, and compute the product A1B1.
What do you conclude?
25. Use Example 5 to Ô¨Ånd the inverse of a matrix of the form
BDB11 0
0 B 22
, where B11ispp,B22isqqand
Bis invertible. Partition the matrix A, and apply your result
twice to Ô¨Ånd that
A 1D2
66664 5 2 0 0 0
3 1 0 0 0
0 0 1=2 0 0
0 0 0 3  4
0 0 0  5=2 7=23
77775
27. a, b. [M] The commands to be used in these exercises
will depend on the matrix program.
c.The algebra needed comes from the block matrix
equation
A11 0
A21 A22x1
x2
Db1
b2
where x1andb1are inR20andx2andb2are inR30.
Then A11x1Db1, which can be solved to produce x1.
The equation A21x1CA22x2Db2yields
SECOND REVISED PAGES


--- Page 538 ---
Section 2.6 A27
A22x2Db2 A21x1, which can be solved for x2by row
reducing the matrix ¬åA22c¬ç, where cDb2 A21x1.
Section 2.5, page 131
1.LyDb)yD2
4 7
 2
63
5,UxDy)xD2
43
4
 63
5
3.yD2
41
3
33
5,xD2
4 1
3
33
5 5. yD2
6641
5
1
 33
775,xD2
664 2
 1
2
 33
775
7.LUD1 0
 3=2 12 5
0 7=2
9.2
41 0 0
 1 1 0
3 2=3 13
52
43 1 2
0 3 12
0 0  83
5
11.2
41 0 0
2 1 0
 1=3 1 13
52
43 6 3
0 5  4
0 0 53
5
13.2
6641 0 0 0
 1 1 0 0
4 5 1 0
 2 1 0 13
7752
6641 3  5 3
0 2 3 1
0 0 0 0
0 0 0 03
775
15.2
41 0 0
3 1 0
 1=2  2 13
52
42 4 4  2
0 3  5 3
0 0 0 53
5
17.U 1D2
41=4 3=8 1=4
0  1=2 1=2
0 0 1=23
5,
L 1D2
41 0 0
1 1 0
 2 0 13
5,
A 1D2
41=8 3=8 1=4
 3=2  1=2 1=2
 1 0 1=23
5
19. Hint: Think about row reducingA I
.
21. Hint: Represent the row operations by a sequence of
elementary matrices.
23. a.Denote the rows of Das transposes of column vectors.
Then partitioned matrix multiplication yields
ADCDDc1   c42
64vT
1
:::
vT
43
75
Dc1vT
1C    C c4vT
4
b.Ahas 40,000 entries. Since Chas 1600 entries and D
has 400 entries, together they occupy only 5% of the
memory needed to store A.25. Explain why U,D, and VTare invertible. Then use a
theorem on the inverse of a product of invertible matrices.
27. a.
i2i1i2i3
v3v2v11/2 ohm9/2
ohms
b.
i2i1i2i3
3/4 ohm
v3v2v16
ohms
29. a.1CR2=R1  R2
 1=R 1 R2=.R 1R3/ 1=R 31CR2=R3
b.AD1 0
 1=6 11 12
0 11 0
 1=36 1
i2i1i2i3i3i4
v3v4v2v136
ohms6
ohms12 ohms
31. [M]
a. LD2
66666666666641 0 0 0 0 0 0 0
 :25 1 0 0 0 0 0 0
 :25 :0667 1 0 0 0 0 0
0 :2667 :2857 1 0 0 0 0
0 0 :2679 :0833 1 0 0 0
0 0 0 :2917 :2921 1 0 0
0 0 0 0 :2697 :0861 1 0
0 0 0 0 0 :2948 :2931 13
7777777777775
UD2
66666666666644 1 1 0 0 0 0 0
0 3:75 :25 1 0 0 0 0
0 0 3:7333 1:0667 1 0 0 0
0 0 0 3:4286 :2857 1 0 0
0 0 0 0 3:7083 1:0833 1 0
0 0 0 0 0 3:3919 :2921 1
0 0 0 0 0 0 3:7052 1:0861
0 0 0 0 0 0 0 3:38683
7777777777775
b. xD.3:9569; 6:5885; 4:2392; 7:3971; 5:6029; 8:7608; 9:4115; 12:0431/
c. A 1D2
6666666666664:2953 :0866 :0945 :0509 :0318 :0227 :0100 :0082
:0866 :2953 :0509 :0945 :0227 :0318 :0082 :0100
:0945 :0509 :3271 :1093 :1045 :0591 :0318 :0227
:0509 :0945 :1093 :3271 :0591 :1045 :0227 :0318
:0318 :0227 :1045 :0591 :3271 :1093 :0945 :0509
:0227 :0318 :0591 :1045 :1093 :3271 :0509 :0945
:0100 :0082 :0318 :0227 :0945 :0509 :2953 :0866
:0082 :0100 :0227 :0318 :0509 :0945 :0866 :29533
7777777777775
Obtain A 1directly and then compute A 1 U 1L 1
to compare the two methods for inverting a matrix.
Section 2.6, page 138
1.CD2
4:10 :60 :60
:30 :20 0
:30 :10 :103
5,intermediate
demand
D2
460
20
103
5
SECOND REVISED PAGES


--- Page 539 ---
A28 Answers to Odd-Numbered Exercises
3.xD2
440
15
153
5 5. xD110
120
7.a.1:6
1:2
b.111:6
121:2
9.xD2
482:8
131:0
110:33
5
11.Hint: Use properties of transposes to obtain
pTDpTCCvT, so that pTxD.pTCCvT/xD
pTCxCvTx. Now compute pTxfrom the production
equation.
13. [M]xD.99576; 97703; 51231; 131570; 49488 ,329554 ,
13835/ . The entries in xsuggest more precision in the
answer than is warranted by the entries in d, which appear
to be accurate only to perhaps the nearest thousand. So a
more realistic answer for xmight be
xD1000.100; 98; 51; 132; 49; 330; 14/ .
15. [M]x.12/is the Ô¨Årst vector whose entries are accurate to the
nearest thousand. The calculation of x.12/takes about
1260 Ô¨Çops, while row reduction of.I C/ d
takes
only about 550 Ô¨Çops. If Cis larger than 2020, then fewer
Ô¨Çops are needed to compute x.12/by iteration than to
compute the equilibrium vector xby row reduction. As the
size of Cgrows, the advantage of the iterative method
increases. Also, because Cbecomes more sparse for larger
models of the economy, fewer iterations are needed for
reasonable accuracy.
Section 2.7, page 146
1.2
41 :25 0
0 1 0
0 0 13
5 3.2
4p
2=2  p
2=2p
2p
2=2p
2=2 2p
2
0 0 13
5
5.2
4p
3=2 1=2 0
1=2  p
3=2 0
0 0 13
5
7.2
41=2  p
3=2 3 C4p
3p
3=2 1=2 4  3p
3
0 0 13
5
See the Practice Problem.
9.A.BD/ requires 1600 multiplications. .AB/D requires 808
multiplications. The Ô¨Årst method uses about twice as many
multiplications. If Dhad 20,000 columns, the counts would
be 160,000 and 80,008, respectively.
11.Use the fact that
sec' tan'sin'D1
cos' sin2'
cos'Dcos'
13.A p
0T1
DI p
0T1A 0
0T1
. First apply the
linear transformation A, and then translate by p.15..12; 6; 3/ 17.2
6641 0 0 0
0 1=2  p
3=2 0
0p
3=2 1=2 0
0 0 0 13
775
19. The triangle with vertices at .7; 2; 0/ ,.7:5; 5; 0/ ,.5; 5; 0/
21. [M]2
42:2586  1:0395  :3473
 1:3495 2:3441 :0696
:0910  :3046 1:27773
52
4X
Y
Z3
5D2
4R
G
B3
5
Section 2.8, page 153
1.The set is closed under sums but not under multiplication
by a negative scalar. (Sketch an example.)
3.The set is not closed under sums or scalar multiples. The
subset consisting of the points on the line x2Dx1is a
subspace, so any ‚Äúcounterexample‚Äù must use at least one
point not on this line.
5.No. The system corresponding to ¬åv1 v2 w¬çis
inconsistent.
7.a.The three vectors v1;v2, and v3
b.InÔ¨Ånitely many vectors
c.Yes, because AxDphas a solution.
9.No, because Ap¬§0.
11.pD4andqD3. NulAis a subspace of R4because
solutions of AxD0must have four entries, to match the
columns of A. ColAis a subspace of R3because each
column vector has three entries.
13. For Nul A, choose .1; 2; 1; 0/ or. 1; 4; 0; 1/ , for
example. For Col A, select any column of A.
15. Yes. Let Abe the matrix whose columns are the vectors
given. Then Ais invertible because its determinant is
nonzero, and so its columns form a basis for R2, by the IMT
(or by Example 5). (Other reasons for the invertibility of A
could be given.)
17. Yes. Let Abe the matrix whose columns are the vectors
given. Row reduction shows three pivots, so Ais invertible.
By the IMT, the columns of Aform a basis for R3.
19. No. Let Abe the 32matrix whose columns are the
vectors given. The columns of Acannot possibly span R3
because Acannot have a pivot in every row. So the columns
are not a basis for R3. (They are a basis for a plane in R3.)
21. Read the section carefully, and write your answers before
checking the Study Guide . This section has terms and key
concepts that you must learn now before going on.
23. Basis for Col A:2
44
6
33
5,2
45
5
43
5
Basis for Nul A:2
6644
 5
1
03
775,2
664 7
6
0
13
775
SECOND REVISED PAGES


--- Page 540 ---
Section 2.9 A29
25. Basis for Col A:2
6641
 1
 2
33
775,2
6644
2
2
63
775,2
664 3
3
5
 53
775
Basis for Nul A:2
666642
 2:5
1
0
03
77775,2
66664 7
:5
0
 4
13
77775
27. Construct a nonzero 33matrix A, and construct bto be
almost any convenient linear combination of the columns
ofA.
29. Hint: You need a nonzero matrix whose columns are
linearly dependent.
31. If Col F¬§R5, then the columns of Fdo not span R5.
Since Fis square, the IMT shows that Fis not invertible
and the equation FxD0has a nontrivial solution. That is,
NulFcontains a nonzero vector. Another way to describe
this is to write Nul F¬§ f0g.
33. If Col QDR4, then the columns of QspanR4. Since Qis
square, the IMT shows that Qis invertible and the equation
QxDbhas a solution for each binR4. Also, each solution
is unique, by Theorem 5 in Section 2.2.
35. If the columns of Bare linearly independent, then the
equation BxD0has only the trivial (zero) solution. That
is, Nul BD f0g.
37. [M] Display the reduced echelon form of A, and select the
pivot columns of Aas a basis for Col A. For Nul A, write
the solution of AxD0in parametric vector form.
Basis for Col AW2
6643
 7
 5
33
775;2
664 5
9
7
 73
775
Basis for Nul AW2
66664 2:5
 1:5
1
0
03
77775;2
666644:5
2:5
0
1
03
77775;2
66664 3:5
 1:5
0
0
13
77775
Section 2.9, page 159
1.xD3b1C2b2D31
1
C22
 1
D7
1
b2
2b2b12b13b1
x
x1x2
3.7
5
5.1=4
 5=47.¬åw¬çBD2
 1
; ¬åx¬çBD1:5
:5
9.Basis for Col A:2
6641
 3
2
 43
775,2
6642
 1
4
23
775,2
664 4
5
 3
73
775; dim Col AD3
Basis for Nul A:2
6643
1
0
03
775; dim Nul AD1
11.Basis for Col A:2
6641
2
 3
33
775,2
6642
5
 9
103
775,2
6640
4
 7
113
775Idim Col
AD3Basis for Nul A:2
666649
 2
1
0
03
77775,2
66664 5
3
0
 2
13
77775; dim Nul AD2
13. Columns 1, 3, and 4 of the original matrix form a basis for
H, so dim HD3.
15. ColADR3, because Ahas a pivot in each row, and so the
columns of AspanR3. NulAcannot equalR2, because
NulAis a subspace of R5. It is true, however, that Nul Ais
two-dimensional. Reason: The equation AxD0has two
free variables, because Ahas Ô¨Åve columns and only three of
them are pivot columns.
17. See the Study Guide after you write your justiÔ¨Åcations.
19. The fact that the solution space of AxD0has a basis of
three vectors means that dim Nul AD3. Since a 57
matrix Ahas seven columns, the Rank Theorem shows that
rankAD7 dim Nul AD4. See the Study Guide for a
justiÔ¨Åcation that does not explicitly mention the Rank
Theorem.
21. A76matrix has six columns. By the Rank Theorem,
dim Nul AD6 rankA. Since the rank is four,
dim Nul AD2. That is, the dimension of the solution space
ofAxD0is two.
23. A34matrix Awith a two-dimensional column space has
two pivot columns. The remaining two columns will
correspond to free variables in the equation AxD0. So the
desired construction is possible. There are six possible
locations for the two pivot columns, one of which is2
4  
0  
0 0 0 03
5. A simple construction is to take
two vectors in R3that are obviously not linearly dependent
and place them in a matrix along with a copy of each vector,
in any order. The resulting matrix will obviously have a
two-dimensional column space. There is no need to worry
about whether Nul Ahas the correct dimension, since this is
guaranteed by the Rank Theorem: dim Nul AD4 rankA.
SECOND REVISED PAGES


--- Page 541 ---
A30 Answers to Odd-Numbered Exercises
25. Thepcolumns of Aspan Col Aby deÔ¨Ånition. If
dim Col ADp, then the spanning set of pcolumns is
automatically a basis for Col A, by the Basis Theorem. In
particular, the columns are linearly independent.
27. a.Hint: The columns of BspanW, and each vector ajis
inW. The vector cjis inRpbecause Bhaspcolumns.
b.Hint: What is the size of C?
c.Hint: How are BandCrelated to A?
29. [M] Your calculations should show that the matrix
¬åv1 v2 x¬çcorresponds to a consistent system. The
B-coordinate vector of xis. 5=3; 8=3/ .
Chapter 2 Supplementary Exercises, page 162
1.a.T b.F c.T d.F
e.F f.F g.T h.T
i.T j.F k.T l.F
m.F n.T o.F p.T
3.I
5.A2D2A I. Multiply by A:A3D2A2 A.
Substitute A2D2A I:A3D2.2A I/ AD
3A 2I.
Multiply by Aagain: A4DA.3A  2I/D3A2 2A.
Substitute the identity A2D2A Iagain:
A4D3.2A I/ 2AD4A 3I.
7.2
410  1
9 10
 5 33
5 9. 3 13
 8 27
11.a.p.x i/Dc0Cc1xiC    C cn 1xn 1
i
Drow i.V /2
64c0
:::
cn 13
75Drow i.Vc/Dyi
b.Suppose x1; : : : ; x nare distinct, and suppose VcD0for
some vector c. Then the entries in care the coefÔ¨Åcients
of a polynomial whose value is zero at the distinct
points x1; : : : ; x n. However, a nonzero polynomial of
degree n 1cannot have nzeros, so the polynomial
must be identically zero. That is, the entries in cmust all
be zero. This shows that the columns of Vare linearly
independent.
c.Hint: When x1; : : : ; x nare distinct, there is a vector c
such that VcDy. Why?
13. a.P2D.uuT/.uuT/Du.uTu/uTDu.1/uTDP
b.PTD.uuT/TDuT TuTDuuTDP
c.Q2D.I 2P /.I  2P /
DI I.2P /  2PIC2P.2P /
DI 4PC4P2DI;because of part (a).
15. Left-multiplication by an elementary matrix produces an
elementary row operation:
BE1BE2E1BE3E2E1BDC
SoBis row equivalent to C. Since row operations are
reversible, Cis row equivalent to B. (Alternatively, showCbeing changed into Bby row operations using the
inverses of the Ei.)
17. Since Bis46(with more columns than rows), its six
columns are linearly dependent and there is a nonzero x
such that BxD0. Thus ABxDA0D0, which shows that
the matrix ABis not invertible, by the Invertible Matrix
Theorem.
19. [M] To four decimal places, as kincreases,
Ak!2
4:2857 :2857 :2857
:4286 :4286 :4286
:2857 :2857 :28573
5 and
Bk!2
4:2022 :2022 :2022
:3708 :3708 :3708
:4270 :4270 :42703
5
or, in rational format,
Ak!2
42=7 2=7 2=7
3=7 3=7 3=7
2=7 2=7 2=73
5 and
Bk!2
418=89 18=89 18=89
33=89 33=89 33=89
38=89 38=89 38=893
5
Chapter 3
Section 3.1, page 169
1.1 3.0 5. 24 7.4
9.15. Start with row 3.
11. 18. Start with column 1 or row 4.
13. 6. Start with row 2 or column 2.
15. 24 17. 10
19.ad bc,cb da. Interchanging two rows changes the
sign of the determinant.
21.ad bc; akd  bkcDk.ad bc/. Scaling a row by a
constant kmultiplies the determinant by k.
23.7a 14bC7c: 7aC14b 7c. Interchanging two rows
changes the sign of the determinant.
25. 1 27.1 29.k
31. 1. The matrix is upper or lower triangular, with only 1‚Äôs on
the diagonal. The determinant is 1, the product of the
diagonal entries.
33. detEADdetaCkc b Ckd
c d
D.aCkc/d .bCkd/c
DadCkcd bc kdcD.C1/.ad  bc/
D.detE/.detA/
35. detEADdetc d
a b
Dcb adD. 1/.ad  bc/
D.detE/.detA/
SECOND REVISED PAGES


--- Page 542 ---
Chapter 3 Supplementary Exercises A31
37.5AD15 5
20 10
; no
39. Hints are in the Study Guide .
41. The area of the parallelogram and the determinant of
u v
both equal 6. If vDx
2
for any x, the area is still
6. In each case the base of the parallelogram is unchanged,
and the altitude remains 2 because the second coordinate of
vis always 2.
43. [M] In general, det A 1D1=detAas long as det Ais
nonzero.
45. [M] You can check your conjectures when you get to
Section 3.2.
Section 3.2, page 177
1.Interchanging two rows reverses the sign of the
determinant.
3.Multiplying a row by 3 multiplies the determinant by 3.
5. 3 7.0 9. 28 11. 48
13. 6 15.21 17.7 19.14
21. Not invertible 23.Invertible
25. Linearly independent 27.See the Study Guide .
29.16
31. Hint: Show that .detA/.detA 1/D1.
33. Hint: Use Theorem 6.
35. Hint: Use Theorem 6 and another theorem.
37. detABDdet6 0
17 4
D24;.detA/.detB/D38D24
39. a. 12 b. 375 c.4 d. 1
3e. 27
41. detAD.aCe/d .bCf /cDadCed bc fc
D.ad bc/C.ed fc/DdetBCdetC
43. Hint: Compute det Aby a cofactor expansion down
column 3.
45. [M] See the Study Guide after you have made a conjecture
about ATAandAAT.
Section 3.3, page 186
1.5=6
 1=6
3.4=5
 3=10
5.2
41=4
11=4
3=83
5
7.s¬§ p
3;x1D5sC4
6.s2 3/,x2D 4s 15
4.s2 3/
9.s¬§0,1;x1D7
3.s 1/,x2D4sC3
6s.s 1/
11.adjAD2
40 1 0
 5 1 5
5 2 103
5,A 1D1
52
40 1 0
 5 1 5
5 2 103
513. adjAD2
4 1 1 5
1 5 1
1 7  53
5,A 1D1
62
4 1 1 5
1 5 1
1 7  53
5
15. adjAD2
4 1 0 0
 1 5 0
 1 15 53
5,A 1D 1
52
4 1 0 0
 1 5 0
 1 15 53
5
17. IfADa b
c d
, then C11Dd,C12D  c,C21D  b,
C22Da. The adjugate matrix is the transpose of cofactors:
adjADd b
 c a
Following Theorem 8, we divide by det A; this produces the
formula from Section 2.2.
19. 8 21.3 23.23
25. A33matrix Ais not invertible if and only if its columns
are linearly dependent (by the Invertible Matrix Theorem).
This happens if and only if one of the columns is in the
plane spanned by the other two columns, which is
equivalent to the condition that the parallelepiped
determined by these columns has zero volume, which in
turn is equivalent to the condition that det AD0.
27.12 29.1
2jdetv1v2
j
31. a.See Example 5. b.4abc=3
33. [M] In MATLAB, the entries in B inv.A/are
approximately 10 15or smaller. See the Study Guide for
suggestions that may save you keystrokes as you work.
35. [M] MATLAB Student Version 4.0 uses 57,771 Ô¨Çops for
inv.A/, and 14,269,045 Ô¨Çops for the inverse formula. The
inv(A) command requires only about 0.4% of the
operations for the inverse formula. The Study Guide shows
how to use the flops command.
Chapter 3 Supplementary Exercises, page 188
1.a.T b.T c.F d.F
e.F f.F g.T h.T
i.F j.F k.T l.F
m.F n.T o.F p.T
The solution for Exercise 3 is based on the fact that if a matrix
contains two rows (or two columns) that are multiples of each
other, then the determinant of the matrix is zero, by Theorem 4,
because the matrix cannot be invertible.
SECOND REVISED PAGES


--- Page 543 ---
A32 Answers to Odd-Numbered Exercises
3.Make two row replacement operations, and then factor out a
common multiple in row 2 and a common multiple in row 3.
1 a b Cc
1 b a Cc
1 c a CbD1 a b Cc
0 b  a a  b
0 c  a a  c
D.b a/.c a/1 a b Cc
0 1  1
0 1  1
D0
5. 12
7.When the determinant is expanded by cofactors of the Ô¨Årst
row, the equation has the form axCbyCcD0, where at
least one of aandbis not zero. This is the equation of a
line. It is clear that .x1; y1/and.x2; y2/are on the line,
because when the coordinates of one of the points are
substituted for xandy, two rows of the matrix are equal
and so the determinant is zero.
9.T2
41 a a2
0 b  a b2 a2
0 c  a c2 a23
5. Thus, by Theorem 3,
detTD.b a/.c a/det2
41 a a2
0 1 b Ca
0 1 c Ca3
5
D.b a/.c a/det2
41 a a2
0 1 b Ca
0 0 c  b3
5
D.b a/.c a/.c b/
11.AreaD12. If one vertex is subtracted from all four vertices,
and if the new vertices are 0,v1,v2, and v3, then the
translated Ô¨Ågure (and hence the original Ô¨Ågure) will be a
parallelogram if and only if one of v1,v2, and v3is the sum
of the other two vectors.
13. By the Inverse Formula, .adjA/1
detAADA 1ADI. By
the Invertible Matrix Theorem, adj Ais invertible and
.adjA/ 1D1
detAA.
15. a.XDCA 1,YDD CA 1B. Now use Exercise
14(c).
b.From part (a), and the multiplicative property of
determinants,
detA B
C D
Ddet¬åA.D  CA 1B/¬ç
Ddet¬åAD ACA 1B¬ç
Ddet¬åAD CAA 1B¬ç
Ddet¬åAD CB¬ç
where the equality ACDCAwas used in the third step.
17. First consider the case nD2, and prove that the result
holds by directly computing the determinants of BandC.
Now assume that the formula holds for all
.k 1/.k 1/matrices, and let A,B, and Cbekkmatrices. Use a cofactor expansion along the Ô¨Årst column
and the inductive hypothesis to Ô¨Ånd det B. Use row
replacement operations on Cto create zeros below the Ô¨Årst
pivot and produce a triangular matrix. Find the determinant
of this matrix and add to det Bto get the result.
19. [M] Compute:
1 1 1
1 2 2
1 2 3D1;1 1 1 1
1 2 2 2
1 2 3 3
1 2 3 4D1;
1 1 1 1 1
1 2 2 2 2
1 2 3 3 3
1 2 3 4 4
1 2 3 4 5D1
Conjecture:
1 1 1 : : : 1
1 2 2 2
1 2 3 3
:::::::::
1 2 3 : : : nD1
To conÔ¨Årm the conjecture, use row replacement operations
to create zeros below the Ô¨Årst pivot, then the second pivot,
and so on. The resulting matrix is
1 1 1 : : : 1
0 1 1 1
0 0 1 1
:::::::::
0 0 0 : : : 1
which is an upper triangular matrix with determinant 1.
Chapter 4
Section 4.1, page 197
1.a.uCvis inVbecause its entries will both be
nonnegative.
b.Example: IfuD2
2
andcD  1, then uis inV, but
cuis not in V.
3.Example: IfuD:5
:5
andcD4, then uis inH, butcuis
not in H.
5.Yes, by Theorem 1, because the set is Span ft2g.
7.No, the set is not closed under multiplication by scalars that
are not integers.
9.HDSpanfvg, where vD2
41
3
23
5. By Theorem 1, His a
subspace of R3.
SECOND REVISED PAGES


--- Page 544 ---
Section 4.2 A33
11.WDSpanfu;vg, where uD2
45
1
03
5,vD2
42
0
13
5. By
Theorem 1, Wis a subspace of R3.
13. a.There are only three vectors in fv1;v2;v3g, and wis not
one of them.
b.There are inÔ¨Ånitely many vectors in Span fv1;v2;v3g.
c.wis in Span fv1;v2;v3g.
15. Not a vector space because the zero vector is not in W
17.SD8
¬à¬à<
¬à¬à:2
6641
0
 1
03
775;2
664 1
1
0
13
775;2
6640
 1
1
03
7759
>>=
>>;
19. Hint: Use Theorem 1.
Warning: Although the Study Guide has complete solutions for
every odd-numbered exercise whose answer here is only a
‚ÄúHint,‚Äù you must really try to work the solution yourself.
Otherwise, you will not beneÔ¨Åt from the exercise.
21. Yes. The conditions for a subspace are obviously satisÔ¨Åed:
The zero matrix is in H, the sum of two upper triangular
matrices is upper triangular, and any scalar multiple of an
upper triangular matrix is again upper triangular.
23. See the Study Guide after you have written your answers.
25. 4 27. a. 8 b.3 c.5 d.4
29. uC. 1/uD1uC. 1/u Axiom 10
D¬å1C. 1/¬çu Axiom 8
D0uD0 Exercise 27
From Exercise 26, it follows that . 1/uD  u.
31. Any subspace Hthat contains uandvmust also contain all
scalar multiples of uandvand hence must contain all sums
of scalar multiples of uandv. Thus Hmust contain
Spanfu;vg.
33. Hint: For part of the solution, consider w1andw2in
HCK, and write w1andw2in the form w1Du1Cv1and
w2Du2Cv2, where u1andu2are in H, and v1andv2are
inK.
35. [M] The reduced echelon form of ¬åv1v2v3w¬çshows
thatwDv1 2v2Cv3.
37. [M] The functions are cos 4tand cos 6t. See Exercise 34 in
Section 4.5.
Section 4.2, page 207
1.2
43 5 3
6 2 0
 8 4 13
52
41
3
 43
5D2
40
0
03
5, sowis in Nul A.
3.2
6647
 4
1
03
775,2
664 6
2
0
13
7755.2
666642
1
0
0
03
77775,2
66664 4
0
9
1
03
777757.Wis not a subspace of R3because the zero vector .0; 0; 0/
is not in W.
9.Wis a subspace of R4because Wis the set of solutions of
the system
a 2b 4c D0
2a  c 3dD0
11.Wis not a subspace because 0is not in W.JustiÔ¨Åcation: If
a typical element .b 2d; 5Cd; bC3d; d/ were zero,
then5CdD0anddD0, which is impossible.
13.WDColAforAD2
41 6
0 1
1 03
5, soWis a vector space
by Theorem 3.
15.2
6640 2 3
1 1  2
4 1 0
3 1 13
775
17. a.2 b.4 19. a. 5 b.2
21.3
1
in Nul A,2
6642
 1
 4
33
775in Col A. Other answers possible.
23. wis in both Nul Aand Col A.
25. See the Study Guide . By now you should know how to use
it properly.
27. LetxD2
43
2
 13
5andAD2
41 3 3
 2 4 2
 1 5 73
5. Then xis in
NulA. Since Nul Ais a subspace of R3,10xis in Nul A.
29. a.A0D0, so the zero vector is in Col A.
b.By a property of matrix multiplication,
AxCAwDA.xCw/, which shows that AxCAwis a
linear combination of the columns of Aand hence is in
ColA.
c.c.Ax/DA.cx/, which shows that c.Ax/is in Col Afor
all scalars c.
31. a.For arbitrary polynomials p,qinP2and any scalar c,
T .pCq/D.pCq/.0/
.pCq/.1/
Dp.0/Cq.0/
p.1/Cq.1/
Dp.0/
p.1/
Cq.0/
q.1/
DT .p/CT .q/
T .cp/Dcp.0/
cp.1/
Dcp.0/
p.1/
DcT .p/
SoTis a linear transformation from P2intoP2.
b.Any quadratic polynomial that vanishes at 0 and 1 must
be a multiple of p.t/Dt.t 1/. The range of TisR2.
SECOND REVISED PAGES


--- Page 545 ---
A34 Answers to Odd-Numbered Exercises
33. a.ForA,BinM22and any scalar c,
T .ACB/D.ACB/C.ACB/T
DACBCATCBTTranspose property
D.ACAT/C.BCBT/DT .A/CT .B/
T .cA/ D.cA/C.cA/TDcACcAT
Dc.ACAT/DcT .A/
SoTis a linear transformation from M22intoM22.
b.IfBis any element in M22with the property that
BTDB, and if AD1
2B, then
T .A/ D1
2BC 1
2BTD1
2BC1
2BDB
c.Part (b) showed that the range of Tcontains all Bsuch
thatBTDB. So it sufÔ¨Åces to show that any Bin the
range of Thas this property. If BDT .A/ , then by
properties of transposes,
BTD.ACAT/TDATCAT TDATCADB
d.The kernel of Tis0 b
 b 0
Wbreal
.
35. Hint: Check the three conditions for a subspace. Typical
elements of T .U / have the form T .u1/andT .u2/, where
u1andu2are in U.
37. [M]wis in Col Abut not in Nul A. (Explain why.)
39. [M] The reduced echelon form of Ais
2
6641 0 1=3 0 10=3
0 1 1=3 0  26=3
0 0 0 1  4
0 0 0 0 03
775
Section 4.3, page 215
1.Yes, the 33matrix AD2
41 1 1
0 1 1
0 0 13
5has 3 pivot
positions. By the Invertible Matrix Theorem, Ais invertible
and its columns form a basis for R3. (See Example 3.)
3.No, the vectors are linearly dependent and do not span R3.
5.No, the set is linearly dependent because the zero vector is
in the set. However,
2
41 2 0 0
 3 9 0  3
0 0 0 53
52
41 2 0 0
0 3 0  3
0 0 0 53
5
The matrix has pivots in each row and hence its columns
spanR3.
7.No, the vectors are linearly independent because they are
not multiples. (More precisely, neither vector is a multiple
of the other.) However, the vectors do not span R3. Thematrix2
4 2 6
3 1
0 53
5can have at most two pivots since it has
only two columns. So there will not be a pivot in each row.
9.2
6643
5
1
03
775,2
664 2
 4
0
13
77511.2
4 2
1
03
5,2
4 1
0
13
5
13. Basis for Nul A:2
664 6
 5=2
1
03
775,2
664 5
 3=2
0
13
775
Basis for Col A:2
4 2
2
 33
5,2
44
 6
83
5
15.fv1;v2;v4g 17.[M]fv1;v2;v3g
19. The three simplest answers are fv1;v2gorfv1;v3gor
fv2;v3g. Other answers are possible.
21. See the Study Guide for hints.
23. Hint: Use the Invertible Matrix Theorem.
25. No. (Why is the set not a basis for H?)
27.fcos!t;sin!tg
29. LetAbe the nkmatrix ¬åv1   vk¬ç. Since Ahas
fewer columns than rows, there cannot be a pivot position
in each row of A. By Theorem 4 in Section 1.4, the columns
ofAdo not span Rnand hence are not a basis for Rn.
31. Hint: Iffv1; : : : ; vpgis linearly dependent, then there exist
c1; : : : ; c p, not all zero, such that c1v1C    C cpvpD0.
Use this equation.
33. Neither polynomial is a multiple of the other polynomial, so
fp1;p2gis a linearly independent set in P3.
35. Letfv1;v3gbe any linearly independent set in the vector
space V, and let v2andv4be linear combinations of v1and
v3. Then fv1;v3gis a basis for Span fv1;v2;v3;v4g.
37. [M] You could be clever and Ô¨Ånd special values of tthat
produce several zeros in (5), and thereby create a system of
equations that can be solved easily by hand. Or, you could
use values of tsuch as tD0; :1; :2; : : : to create a system of
equations that you can solve with a matrix program.
Section 4.4, page 224
1.3
 7
3.2
4 1
 5
93
5 5.8
 5
7.2
4 1
 1
33
5
9.2 1
 9 8
11.6
4
13.2
42
6
 13
5
15. TheStudy Guide has hints.
SECOND REVISED PAGES


--- Page 546 ---
Section 4.6 A35
17.1
1
D5v1 2v2D10v1 3v2Cv3(inÔ¨Ånitely many
answers)
19. Hint: By hypothesis, the zero vector has a unique
representation as a linear combination of elements of S.
21.9 2
4 1
23. Hint: Suppose that ¬åu¬çBD¬åw¬çBfor some uandwinV, and
denote the entries in ¬åu¬çBbyc1; : : : ; c n. Use the deÔ¨Ånition
of¬åu¬çB.
25. One possible approach: First, show that if u1; : : : ; upare
linearly dependent , then ¬åu1¬çB; : : : ; ¬å up¬çBare linearly
dependent. Second, show that if ¬åu1¬çB; : : : ; ¬å up¬çBare
linearly dependent, then u1; : : : ; upare linearly dependent .
Use the two equations displayed in the exercise. A slightly
different proof is given in the Study Guide .
27. Linearly independent. (Justify answers to Exercises 27‚Äì34.)
29. Linearly dependent
31. a.The coordinate vectors2
41
 3
53
5,2
4 3
5
 73
5,2
4 4
5
 63
5,2
41
0
 13
5
do not span R3. Because of the isomorphism between
R3andP2, the corresponding polynomials do not span
P2.
b.The coordinate vectors2
40
5
13
5,2
41
 8
 23
5,2
4 3
4
23
5,2
42
 3
03
5
spanR3. Because of the isomorphism between R3and
P2, the corresponding polynomials span P2.
33. [M] The coordinate vectors2
6643
7
0
03
775,2
6645
1
0
 23
775,2
6640
1
 2
03
775,2
6641
16
 6
23
775
are a linearly dependent subset of R4. Because of the
isomorphism between R4andP3, the corresponding
polynomials form a linearly dependent subset of P3, and
thus cannot be a basis for P3:
35. [M]¬åx¬çBD 5=3
8=3
37.[M]2
41:3
0
0:83
5
Section 4.5, page 231
1.2
41
1
03
5,2
4 2
1
33
5; dim is 2
3.2
6640
1
0
13
775,2
6640
 1
1
23
775,2
6642
0
 3
03
775; dim is 35.2
6641
2
 1
 33
775,2
664 4
5
0
73
775; dim is 2
7.No basis; dim is 0 9.2 11.2 13.2, 3
15. 2, 2 17.0, 3 19.See the Study Guide .
21. Hint: You need only show that the Ô¨Årst four Hermite
polynomials are linearly independent. Why?
23.¬åp¬çBD 
3; 3; 2;3
2
25. Hint: Suppose Sdoes span V, and use the Spanning Set
Theorem. This leads to a contradiction, which shows that
the spanning hypothesis is false.
27. Hint: Use the fact that each Pnis a subspace of P.
29. Justify each answer.
a.True b.True c.True
31. Hint: Since His a nonzero subspace of a Ô¨Ånite-dimensional
space, His Ô¨Ånite-dimensional and has a basis, say,
v1; : : : ; vp. First show that fT .v1/; : : : ; T . vp/gspans T .H/ .
33. [M]a.One basis is fv1;v2;v3;e2;e3g. In fact, any two of
the vectors e2; : : : ; e5will extend fv1;v2;v3gto a basis of
R5.
Section 4.6, page 238
1.rankAD2; dim Nul AD2;
Basis for Col A:2
41
 1
53
5,2
4 4
2
 63
5
Basis for Row A:.1; 0; 1; 5/,.0; 2; 5; 6/
Basis for Nul A:2
6641
5=2
1
03
775,2
664 5
 3
0
13
775
3.rankAD3; dim Nul AD2;
Basis for Col A:2
6642
 2
4
 23
775,2
6646
 3
9
33
775,2
6642
 3
5
 43
775
RowA:.2; 3; 6; 2; 5/ ,.0; 0; 3;  1; 1/,.0; 0; 0; 1; 3/
Basis for Nul A:2
666643=2
1
0
0
03
77775,2
666649=2
0
 4=3
 3
13
77775
5.5, 3, 3
7.Yes; no. Since Col Ais a four-dimensional subspace of R4,
it coincides with R4. The null space cannot be R3, because
the vectors in Nul Ahave 7 entries. Nul Ais a
three-dimensional subspace of R7, by the Rank Theorem.
9.2 11.3
13. 5, 5. In both cases, the number of pivots cannot exceed the
number of columns or the number of rows.
SECOND REVISED PAGES


--- Page 547 ---
A36 Answers to Odd-Numbered Exercises
15. 2 17.See the Study Guide .
19. Yes. Try to write an explanation before you consult the
Study Guide .
21. No. Explain why.
23. Yes. Only six homogeneous linear equations are necessary.
25. No. Explain why.
27. RowAand Nul Aare inRn; ColAand Nul ATare inRm.
There are only four distinct subspaces because
RowATDColAand Col ATDRowA.
29. Recall that dim Col ADmprecisely when Col ADRm, or
equivalently, when the equation AxDbis consistent for all
b. By Exercise 28(b), dim Col ADmprecisely when
dim Nul ATD0, or equivalently, when the equation
ATxD0has only the trivial solution.
31. uvTD2
42a 2b 2c
 3a  3b  3c
5a 5b 5c3
5. The columns are all
multiples of u, so Col uvTis one-dimensional, unless
aDbDcD0:
33. Hint: LetAD¬åu u 2u3¬ç. Ifu¬§0, then uis a basis for
ColA. Why?
35. [M]a. Many answers are possible. Here are the
‚Äúcanonical‚Äù choices, for AD¬åa1a2   a7¬ç:
CD¬åa1a2a4a6¬ç; N D2
666666664 13=2  5 3
 11=2  1=2  2
1 0 0
0 11=2  7
0 1 0
0 0  1
0 0 13
777777775
RD2
6641 0 13=2 0 5 0  3
0 1 11=2 0 1=2 0 2
0 0 0 1  11=2 0 7
0 0 0 0 0 1 13
775
b.MD¬å2 41 0  28 11 ¬çT. The matrix ¬åRTN¬ç
is77because the columns of RTandNare inR7,
and dim Row ACdim Nul AD7. The matrix ¬åC M ¬ç
is55because the columns of CandMare inR5and
dim Col ACdim Nul ATD5, by Exercise 28(b). The
invertibility of these matrices follows from the fact that
their columns are linearly independent, which can be
proved from Theorem 3 in Section 6.1.
37. [M] The CandRgiven for Exercise 35 work here, and
ADCR.
Section 4.7, page 244
1.a.6 9
 2 4
b.0
 2
3.(ii)
5.a.2
44 1 0
 1 1 1
0 1  23
5 b.2
48
2
23
57. P
C BD 3 1
 5 2
,P
B CD 2 1
 5 3
9. P
C BD9 2
 4 1
,P
B CD1 2
4 9
11.See the Study Guide .
13. P
C BD2
41 3 0
 2 5 2
1 4 33
5,¬å 1C2t¬çBD2
45
 2
13
5
15. a.Bis a basis for V.
b.The coordinate mapping is a linear transformation.
c.The product of a matrix and a vector
d.The coordinate vector of vrelative to B
17. a.[M]
P 1D1
322
66666666432 0 16 0 12 0 10
32 0 24 0 20 0
16 0 16 0 15
8 0 10 0
4 0 6
2 0
13
777777775
b.Pis the change-of-coordinates matrix from CtoB. So
P 1is the change-of-coordinates matrix from BtoC,
by equation (5), and the columns of this matrix are the
C-coordinate vectors of the basis vectors in B, by
Theorem 15.
19. [M]Hint: LetCbe the basis fv1;v2;v3g. Then the columns
ofPare¬åu1¬çC; ¬åu2¬çC, and ¬åu3¬çC. Use the deÔ¨Ånition of
C-coordinate vectors and matrix algebra to compute u1,u2
andu3. The solution method is discussed in the Study
Guide . Here are the numerical answers:
a.u1D2
4 6
 5
213
5,u2D2
4 6
 9
323
5,u3D2
4 5
0
33
5
b.w1D2
428
 9
 33
5,w2D2
438
 13
23
5,w3D2
421
 7
33
5
Section 4.8, page 253
1.IfykD2k, then ykC1D2kC1andykC2D2kC2.
Substituting these formulas into the left side of the equation
gives
ykC2C2ykC1 8ykD2kC2C22kC1 82k
D2k.22C22 8/
D2k.0/D0for all k
Since the difference equation holds for all k,2kis a
solution. A similar calculation works for ykD. 4/k.
3.The signals 2kand. 4/kare linearly independent because
neither is a multiple of the other. For instance, there is no
scalar csuch that 2kDc. 4/kfor all k . By Theorem 17,
the solution set Hof the difference equation in Exercise 1
is two-dimensional. By the Basis Theorem in Section 4.5,
SECOND REVISED PAGES


--- Page 548 ---
Section 4.9 A37
the two linearly independent signals 2kand. 4/kform a
basis for H.
5.IfykD. 3/k, then
ykC2C6ykC1C9ykD. 3/kC2C6. 3/kC1C9. 3/k
D. 3/k¬å. 3/2C6. 3/C9¬ç
D. 3/k.0/D0for all k
Similarly, if ykDk. 3/k, then
ykC2C6ykC1C9yk
D.kC2/. 3/kC2C6.kC1/. 3/kC1C9k. 3/k
D. 3/k¬å.kC2/. 3/2C6.kC1/. 3/C9k¬ç
D. 3/k¬å9kC18 18k 18C9k¬ç
D. 3/k.0/ for all k
Thus both . 3/kandk. 3/kare in the solution space Hof
the difference equation. Also, there is no scalar csuch that
k. 3/kDc. 3/kfor all k, because cmust be chosen
independently of k. Likewise, there is no scalar csuch that
. 3/kDck. 3/kfor all k. So the two signals are linearly
independent. Since dim HD2, the signals form a basis for
H, by the Basis Theorem.
7.Yes 9.Yes
11.No, two signals cannot span the three-dimensional solution
space.
13. 1
3k, 2
3k15.5k,. 5/k
17.YkDc1.:8/kCc2.:5/kC10!10 ask! 1
19.ykDc1. 2Cp
3/kCc2. 2 p
3/k
21. 7, 5, 4, 3, 4, 5, 6, 6, 7, 8, 9, 8, 7; see Ô¨Ågure below.
k= original data
= smoothed data
2 0 4 6 8 10 12 14246810
23. a.ykC1 1:01y kD  450,y0D10;000
b.[M] MATLAB code:
pay = 450, y = 10000, m = 0
table = [0 ; y]
while y > 450
y = 1.01*y - pay
m = m + 1
table = [table [m ; y] ]
%append new column
end
m, y
c.[M] At month 26, the last payment is $114.88. The total
paid by the borrower is $11,364.88.
25.k2Cc1. 4/kCc2 27.2 2kCc14kCc22 k29. xkC1DAxk, where
AD2
6640 1 0 0
0 0 1 0
0 0 0 1
9 6 8 63
775;xD2
664yk
ykC1
ykC2
ykC33
775
31. The equation holds for all k, so it holds with kreplaced by
k 1, which transforms the equation into
ykC2C5ykC1C6ykD0for all k
The equation is of order 2.
33. For all k, the Casorati matrix C.k/ is not invertible. In this
case, the Casorati matrix gives no information about the
linear independence/dependence of the set of signals. In
fact, neither signal is a multiple of the other, so they are
linearly independent.
35. Hint: Verify the two properties that deÔ¨Åne a linear
transformation. For fykgandf¬¥kginS, study
T .fykg C f¬¥kg/. Note that if ris any scalar, then the kth
term of rfykgisryk; soT .rfykg/is the sequence fwkg
given by
wkDrykC2Ca.ry kC1/Cb.ry k/
37..TD/.y 0; y1; y2; : : :/DT .D.y 0; y1; y2; : : :// D
T .0; y 0; y1; y2; : : :/D.y0; y1; y2; : : :/DI.y 0; y1; y2; : : :/;
while .DT /.y 0; y1; y2; : : :/DD.T .y 0; y1; y2; : : :// D
D.y 1; y2; y3; : : :/D.0; y 1; y2; y3; : : :/ .
Section 4.9, page 262
1.a. From:
N
:7M
:6
:3 :4To:
News
Musicb.1
0
c.33%
3.a. From:
H
:95I
:45
:05 :55To:
Healthy
Illb.15%, 12.5%
c..925; use x0D1
0
:
5.:4
:6
7.2
41=4
1=2
1=43
5
9.Yes, because P2has all positive entries.
11.a.2=3
1=3
b.2/3
13. a.:9
:1
b..10, no
15. [M] About 17.3% of the United States population
17. a.The entries in a column of Psum to 1. A column in the
matrix P Ihas the same entries as in Pexcept that
SECOND REVISED PAGES


--- Page 549 ---
A38 Answers to Odd-Numbered Exercises
one of the entries is decreased by 1. Hence each column
sum is 0.
b.By (a), the bottom row of P Iis the negative of the
sum of the other rows.
c.By (b) and the Spanning Set Theorem, the bottom row
ofP Ican be removed and the remaining .n 1/
rows will still span the row space. Alternatively, use (a)
and the fact that row operations do not change the row
space. Let Abe the matrix obtained from P Iby
adding to the bottom row all the other rows. By (a), the
row space is spanned by the Ô¨Årst .n 1/rows of A.
d.By the Rank Theorem and (c), the dimension of the
column space of P Iis less than n, and hence the null
space is nontrivial. Instead of the Rank Theorem, you
may use the Invertible Matrix Theorem, since P Iis
a square matrix.
19. a.The product Sxequals the sum of the entries in x. For a
probability vector, this sum must be 1.
b.PD¬åp1p2   pn¬ç, where the piare probability
vectors. By matrix multiplication and part (a),
SPD¬åSp1Sp2  Spn¬çD¬å1 1   1¬çDS
c.By part (b), S.Px/D.SP / xDSxD1. Also, the
entries in Pxare nonnegative (because Pandxhave
nonnegative entries). Hence, by (a), Pxis a probability
vector.
21. [M]
a.To four decimal places,
P4DP5D2
664:2816 :2816 :2816 :2816
:3355 :3355 :3355 :3355
:1819 :1819 :1819 :1819
:2009 :2009 :2009 :20093
775;
qD2
664:2816
:3355
:1819
:20093
775
Note that, due to round-off, the column sums are not 1.
b.To four decimal places,
Q80D2
4:7354 :7348 :7351
:0881 :0887 :0884
:1764 :1766 :17653
5;
Q116DQ117D2
4:7353 :7353 :7353
:0882 :0882 :0882
:1765 :1765 :17653
5;
qD2
4:7353
:0882
:17653
5
c.LetPbe an nnregular stochastic matrix, qthe
steady-state vector of P, and e1the Ô¨Årst column of the
identity matrix. Then Pke1is the Ô¨Årst column of Pk. ByTheorem 18, Pke1!qask! 1 . Replacing e1by the
other columns of the identity matrix, we conclude that
each column of Pkconverges to qask! 1 . Thus
Pk!¬åq q    q¬ç.
Chapter 4 Supplementary Exercises, page 264
1.a.T b.T c.F d.F e.T f.T
g.F h.F i.T j.F k.F l.F
m.T n.F o.T p.T q.F r.T
s.T t.F
3.The set of all .b1; b2; b3/satisfying b1C2b2Cb3D0.
5.The vector p1is not zero and p2is not a multiple of p1, so
keep both of these vectors. Since p3D2p1C2p2, discard
p3. Since p4has a t2term, it cannot be a linear combination
ofp1andp2, so keep p4. Finally, p5Dp1Cp4, so discard
p5. The resulting basis is fp1;p2;p4g.
7.You would have to know that the solution set of the
homogeneous system is spanned by two solutions. In this
case, the null space of the 1820coefÔ¨Åcient matrix Ais at
most two-dimensional. By the Rank Theorem,
dim Col A20 2D18, which means that Col ADR18,
because Ahas 18 rows, and every equation AxDbis
consistent.
9.LetAbe the standard mnmatrix of the transformation T.
a.IfTis one-to-one, then the columns of Aare linearly
independent (Theorem 12 in Section 1.9), so
dim Nul AD0. By the Rank Theorem,
dim Col ADrankADn. Since the range of Tis Col A,
the dimension of the range of Tisn.
b.IfTis onto, then the columns of AspanRm(Theorem
12 in Section 1.9), so dim Col ADm. By the Rank
Theorem, dim Nul ADn dim Col ADn m. Since
the kernel of Tis Nul A, the dimension of the kernel of
Tisn m.
11.IfSis a Ô¨Ånite spanning set for V, then a subset of S‚Äîsay
S0‚Äîis a basis for V. Since S0must span V,S0cannot be a
proper subset of Sbecause of the minimality of S. Thus
S0DS, which proves that Sis a basis for V.
12. a.Hint: Any yin Col ABhas the form yDABxfor some
x.
13. By Exercise 12, rank PArankA, and
rankADrankP 1PArankPA. Thus
rankPADrankA.
15. The equation ABD0shows that each column of Bis in
NulA. Since Nul Ais a subspace, all linear combinations of
the columns of Bare in Nul A, so Col Bis a subspace of
NulA. By Theorem 11 in Section 4.5,
dim Col Bdim Nul A. Applying the Rank Theorem, we
Ô¨Ånd that
nDrankACdim Nul ArankACrankB
SECOND REVISED PAGES


--- Page 550 ---
Section 5.2 A39
17. a.LetA1consist of the rpivot columns in A. The columns
ofA1are linearly independent. So A1is anmrwith
rankr.
b.By the Rank Theorem applied to A1, the dimension of
RowAisr, soA1hasrlinearly independent rows. Use
them to form A2. Then A2isrrwith linearly
independent rows. By the Invertible Matrix Theorem,
A2is invertible.
19.¬åB AB A2B¬çD2
40 1 0
1 :9 :81
1 :5 :253
5
2
41 :9 :81
0 1 0
0 0  :563
5
This matrix has rank 3, so the pair .A; B/ is controllable.
21. [M] rank ¬åB AB A2B A3B¬çD3. The pair .A; B/ is
not controllable.
Chapter 5
Section 5.1, page 273
1.Yes 3.No 5.Yes,D0 7.Yes,2
41
1
 13
5
9.D1:0
1
;D5:2
1
11. 1
3
13.D1:2
40
1
03
5;D2:2
4 1
2
23
5;D3:2
4 1
1
13
5
15.2
4 2
1
03
5,2
4 3
0
13
5 17.0, 2, 1
19. 0. Justify your answer.
21. See the Study Guide , after you have written your answers.
23. Hint: Use Theorem 2.
25. Hint: Use the equation AxDxto Ô¨Ånd an equation
involving A 1.
27. Hint: For any ,.A I/TDAT I. By a theorem
(which one?), AT Iis invertible if and only if A Iis
invertible.
29. Letvbe the vector in Rnwhose entries are all 1‚Äôs. Then
AvDsv.
31. Hint: IfAis the standard matrix of T, look for a nonzero
vector v(a point in the plane) such that AvDv.
33. a.xkC1Dc1kC1uCc2kC1v
b.AxkDA.c 1kuCc2kv/
Dc1kAuCc2kAv Linearity
Dc1kuCc2kv uandvare eigenvectors.
DxkC135.
x1x2
vw
uT(u)T(w)
T(v)
37. [M]D3:2
45
 2
93
5;D13:2
4 2
1
03
5,2
4 1
0
13
5. You can
speed up your calculations with the program nulbasis
discussed in the Study Guide .
39. [M]D  2:2
66664 2
7
 5
5
03
77775,2
666643
7
 5
0
53
77775;
D5:2
666642
 1
1
0
03
77775,2
66664 1
1
0
1
03
77775,2
666642
0
0
0
13
77775
Section 5.2, page 281
1.2 4 45; 9, 5 3.2 2 1;1p
2
5.2 6C9; 3 7.2 9C32; no real eigenvalues
9. 3C42 9 6 11. 3C92 26C24
13. 3C182 95C150 15.4, 3, 3, 1
17. 3, 3, 1, 1, 0
19. Hint: The equation given holds for all .
21. TheStudy Guide has hints.
23. Hint: Find an invertible matrix Pso that RQDP 1AP.
25. a.fv1;v2g, where v2D 1
1
is an eigenvector for D:3
b.x0Dv1 1
14v2
c.x1Dv1 1
14.:3/v2;x2Dv1 1
14.:3/2v2, and
xkDv1 1
14.:3/kv2. Ask! 1 ,.:3/k!0and
xk!v1.
27. a.Av1Dv1,Av2D:5v2,Av3D:2v3. (This also shows
that the eigenvalues of Aare 1, .5, and .2.)
b.fv1;v2;v3gis linearly independent because the eigen-
vectors correspond to distinct eigenvalues (Theorem 2).
Since there are 3 vectors in the set, the set is a basis for
R3. So there exist (unique) constants such that
x0Dc1v1Cc2v2Cc3v3
SECOND REVISED PAGES


--- Page 551 ---
A40 Answers to Odd-Numbered Exercises
Then
wTx0Dc1wTv1Cc2wTv2Cc3wTv3 ()
Since x0andv1are probability vectors and since the
entries in v2and in v3each sum to 0, ( ) shows that
1Dc1.
c.By (b),
x0Dv1Cc2v2Cc3v3
Using (a),
xkDAkx0DAkv1Cc2Akv2Cc3Akv3
Dv1Cc2.:5/kv2Cc3.:2/kv3
!v1ask! 1
29. [M] Report your results and conclusions. You can avoid
tedious calculations if you use the program gauss
discussed in the Study Guide .
Section 5.3, page 288
1.226  525
90  209
3.ak0
3.ak bk/ bk
5.D5:2
41
1
13
5;D1:2
41
0
 13
5,2
42
 1
03
5
When an answer involves a diagonalization, ADPDP 1, the
factors PandDare not unique, so your answer may differ from
that given here.
7.PD1 0
3 1
,DD1 0
0 1
9.Not diagonalizable
11.PD2
41 2 1
3 3 1
4 3 13
5,DD2
43 0 0
0 2 0
0 0 13
5
13.PD2
4 1 2 1
 1 1 0
1 0 13
5,DD2
45 0 0
0 1 0
0 0 13
5
15.PD2
4 1 4 2
1 0  1
0 1 13
5,DD2
43 0 0
0 3 0
0 0 13
5
17. Not diagonalizable
19.PD2
6641 3  1 1
0 2  1 2
0 0 1 0
0 0 0 13
775,DD2
6645 0 0 0
0 3 0 0
0 0 2 0
0 0 0 23
775
21. See the Study Guide . 23.Yes. (Explain why.)
25. No,Amust be diagonalizable. (Explain why.)
27. Hint: Write ADPDP 1. Since Ais invertible, 0 is not an
eigenvalue of A, soDhas nonzero entries on its diagonal.
29. One answer is P1D1 1
 2 1
, whose columns are
eigenvectors corresponding to the eigenvalues in D1.31. Hint: Construct a suitable 22triangular matrix.
33. [M]PD2
6642 2 1 6
1 1 1  3
 1 7 1 0
2 2 0 43
775,
DD2
6645 0 0 0
0 1 0 0
0 0  2 0
0 0 0  23
775
35. [M]PD2
666646 3 2 4 3
 1 1 1 3 1
 3 3 4 2 4
3 0  1 5 0
0 3 4 0 53
77775,
DD2
666645 0 0 0 0
0 5 0 0 0
0 0 3 0 0
0 0 0 1 0
0 0 0 0 13
77775
Section 5.4, page 295
1.3 1 0
 5 6 4
3.a.T .e1/D  b2Cb3,T .e2/D  b1 b3,
T .e3/Db1 b2
b.¬åT .e1/¬çBD2
40
 1
13
5,¬åT .e2/¬çBD2
4 1
0
 13
5,
¬åT .e3/¬çBD2
41
 1
03
5
c.2
40 1 1
 1 0  1
1 1 03
5
5.a.10 3tC4t2Ct3
b.For any p,qinP2and any scalar c,
T ¬åp.t/Cq.t/¬çD.tC5/¬åp.t/Cq.t/¬ç
D.tC5/p.t/C.tC5/q.t/
DT ¬åp.t/¬çCT ¬åq.t/¬ç
T ¬åcp.t/¬çD.tC5/¬åcp.t/¬çDc.tC5/p.t/
DcT ¬åp.t/¬ç
c.2
6645 0 0
1 5 0
0 1 5
0 0 13
775
7.2
43 0 0
5 2 0
0 4 13
5
SECOND REVISED PAGES


--- Page 552 ---
Section 5.6 A41
9.a.2
42
5
83
5
b.Hint: Compute T .pCq/andT .cp/for arbitrary p,q
inP2and an arbitrary scalar c.
c.2
41 1 1
1 0 0
1 1 13
5
11.1 5
0 1
13. b 1D1
1
,b2D1
3
15. b1D 2
1
,b2D1
1
17. a.Ab1D2b1, sob1is an eigenvector of A. However, A
has only one eigenvalue, D2, and the eigenspace is
only one-dimensional, so Ais not diagonalizable.
b.2 1
0 2
19. By deÔ¨Ånition, if Ais similar to B, there exists an invertible
matrix Psuch that P 1APDB. (See Section 5.2.) Then
Bis invertible because it is the product of invertible
matrices. To show that A 1is similar to B 1, use the
equation P 1APDB. See the Study Guide .
21. Hint: Review Practice Problem 2.
23. Hint: Compute B.P 1x/.
25. Hint: Write ADPBP 1D.PB/P 1, and use the trace
property.
27. For each j,I.bj/Dbj. Since the standard coordinate
vector of any vector in Rnis just the vector itself,
¬åI.bj/¬çEDbj. Thus the matrix for Irelative to Band the
standard basis Eis simplyb1b2   bn
. This matrix
is precisely the change-of-coordinates matrix PBdeÔ¨Åned in
Section 4.4.
29. TheB-matrix for the identity transformation is In, because
theB-coordinate vector of the jth basis vector bjis the jth
column of In.
31. [M]2
4 7 2 6
0 4 6
0 0  13
5
Section 5.5, page 302
1.D2Ci, 1Ci
1
;D2 i, 1 i
1
3.D2C3i,1 3i
2
;D2 3i,1C3i
2
5.D2C2i,1
2C2i
;D2 2i,1
2 2i
7.Dp
3i,'D=6 radian, rD2
9.D  p
3=2.1=2/i ,'D  5=6 radians, rD1
11.D:1:1i,'D  =4 radian, rDp
2=10In Exercises 13‚Äì20, other answers are possible. Any Pthat
makes P 1APequal to the given Cor toCTis a satisfactory
answer. First Ô¨Ånd P; then compute P 1AP.
13.PD 1 1
1 0
,CD2 1
1 2
15.PD1 3
2 0
,CD2 3
3 2
17.PD2 1
5 0
,CD :6 :8
:8 :6
19.PD2 1
2 0
,CD:96 :28
:28 :96
21. yD2
 1C2i
D 1C2i
5 2 4i
5
23. (a) Properties of conjugates and the fact that xTDxT;
(b)AxDAxandAis real; (c) because xTAxis a scalar and
hence may be viewed as a 11matrix; (d) properties of
transposes; (e) ATDA, deÔ¨Ånition of q
25. Hint: First write xDRexCi.Imx/.
27. [M]PD2
6641 1 2 0
 4 0 0 2
0 0  3 1
2 0 4 03
775,
CD2
664:2 :5 0 0
:5 :2 0 0
0 0 :3  :1
0 0 :1 :33
775
Other choices are possible, but Cmust equal P 1AP.
Section 5.6, page 311
1.a.Hint: Findc1,c2such that x0Dc1v1Cc2v2. Use this
representation and the fact that v1andv2are
eigenvectors of Ato compute x1D49=3
41=3
.
b.In general, xkD5.3/kv1 4.1
3/kv2fork0.
3.When pD:2, the eigenvalues of Aare .9 and .7, and
xkDc1.:9/k1
1
Cc2.:7/k2
1
!0ask! 1
The higher predation rate cuts down the owls‚Äô food supply,
and eventually both predator and prey populations perish.
5.IfpD:325, the eigenvalues are 1.05 and .55. Since
1:05 > 1 , both populations will grow at 5% per year. An
eigenvector for 1.05 is .6; 13/ , so eventually there will be
approximately 6 spotted owls to every 13 (thousand) Ô¨Çying
squirrels.
7.a.The origin is a saddle point because Ahas one
eigenvalue larger than 1 and one smaller than 1 (in
absolute value).
SECOND REVISED PAGES


--- Page 553 ---
A42 Answers to Odd-Numbered Exercises
b.The direction of greatest attraction is given by the
eigenvector corresponding to the eigenvalue 1=3,
namely, v2. All vectors that are multiples of v2are
attracted to the origin. The direction of greatest
repulsion is given by the eigenvector v1. All multiples
ofv1are repelled.
c.See the Study Guide .
9.Saddle point; eigenvalues: 2, .5; direction of greatest
repulsion: the line through .0; 0/ and. 1; 1/; direction of
greatest attraction: the line through .0; 0/ and.1; 4/
11.Attractor; eigenvalues: .9, .8; greatest attraction: line
through .0; 0/ and.5; 4/
13. Repellor; eigenvalues: 1.2, 1.1; greatest repulsion: line
through .0; 0/ and.3; 4/
15. xkDv1C:1.:5/k2
42
 3
13
5C:3.:2/k2
4 1
0
13
5!v1as
k! 1
17. a.AD0 1:6
:3 :8
b.The population is growing because the largest
eigenvalue of Ais 1.2, which is larger than 1 in
magnitude. The eventual growth rate is 1.2, which is
20% per year. The eigenvector .4; 3/ for1D1:2
shows that there will be 4 juveniles for every
3 adults.
c.[M] The juvenile‚Äìadult ratio seems to stabilize after
about 5 or 6 years. The Study Guide describes how to
construct a matrix program to generate a data matrix
whose columns list the numbers of juveniles and adults
each year. Graphing the data is also discussed.
Section 5.7, page 319
1.x.t/D5
2 3
1
e4t 3
2 1
1
e2t
3. 5
2 3
1
etC9
2 1
1
e t. The origin is a saddle point.
The direction of greatest attraction is the line through
. 1; 1/ and the origin. The direction of greatest repulsion is
the line through . 3; 1/ and the origin.
5. 1
21
3
e4tC7
21
1
e6t. The origin is a repellor. The
direction of greatest repulsion is the line through .1; 1/ and
the origin.
7.SetPD1 1
3 1
andDD4 0
0 6
. Then
ADPDP 1. Substituting xDPyintox0DAx, we have
d
dt.Py/DA.P y/
Py0DPDP 1.Py/DPDy
Left-multiplying by P 1givesy0DDy;ory0
1.t/
y0
2.t/
D4 0
0 6y1.t/
y2.t/
9.(complex solution):
c11 i
1
e. 2Ci/tCc21Ci
1
e. 2 i/t
(real solution):
c1costCsint
cost
e 2tCc2sint cost
sint
e 2t
The trajectories spiral in toward the origin.
11.(complex): c1 3C3i
2
e3itCc2 3 3i
2
e 3it
(real):
c1 3cos3t 3sin3t
2cos3t
Cc2 3sin3tC3cos3t
2sin3t
The trajectories are ellipses about the origin.
13. (complex): c11Ci
2
e.1C3i/tCc21 i
2
e.1 3i/t
(real): c1cos3t sin3t
2cos3t
etCc2sin3tCcos3t
2sin3t
et
The trajectories spiral out, away from the origin.
15. [M]x.t/Dc12
4 1
0
13
5e 2tCc22
4 6
1
53
5e tCc32
4 4
1
43
5et
The origin is a saddle point. A solution with c3D0is
attracted to the origin. A solution with c1Dc2D0is
repelled.
17. [M] (complex):
c12
4 3
1
13
5etCc22
423 34i
 9C14i
33
5e.5C2i/tC
c32
423C34i
 9 14i
33
5e.5 2i/t
(real): c12
4 3
1
13
5etCc22
423cos2tC34sin2t
 9cos2t 14sin2t
3cos2t3
5e5tC
c32
423sin2t 34cos2t
 9sin2tC14cos2t
3sin2t3
5e5t
The origin is a repellor. The trajectories spiral outward,
away from the origin.
19. [M]AD 2 3=4
1 1
,
v1.t/
v2.t/
D5
21
2
e :5t 1
2 3
2
e 2:5t
21. [M]AD 1 8
5 5
,
iL.t/
vC.t/
D 20sin6t
15cos6t 5sin6t
e 3t
SECOND REVISED PAGES


--- Page 554 ---
Chapter 5 Supplementary Exercises A43
Section 5.8, page 326
1.Eigenvector: x4D1
:3326
, orAx4D4:9978
1:6652
;
4:9978
3.Eigenvector: x4D:5188
1
, orAx4D:4594
:9075
;
:9075
5.xD :7999
1
,AxD4:0015
 5:0020
;
estimated D  5:0020
7.[M]xkW:75
1
;1
:9565
;:9932
1
;1
:9990
;:9998
1
kW11:5; 12:78; 12:96; 12:9948; 12:9990
9.[M]5D8:4233 ,6D8:4246 ; actual value: 8.42443
(accurate to 5 places)
11. kW5:8000; 5:9655; 5:9942; 5:9990 .k D1; 2; 3; 4/ I
R.xk/W5:9655; 5:9990; 5:99997; 5:9999993
13. Yes, but the sequences may converge very slowly.
15. Hint: Write Ax xD.A I/x, and use the fact that
.A I/is invertible when isnotan eigenvalue of A.
17. [M]0D3:3384 ,1D3:32119 (accurate to 4 places with
rounding), 2D3:3212209 . Actual value: 3.3212201
(accurate to 7 places)
19. a.6D30:2887 D7to four decimal places. To six
places, the largest eigenvalue is 30.288685, with
eigenvector .:957629; :688937; 1; :943782/ .
b.The inverse power method (with D0/produces
 1
1D:010141 , 1
2D:010150 . To seven places, the
smallest eigenvalue is .0101500, with eigenvector
. :603972; 1;  :251135; :148953/ . The reason for the
rapid convergence is that the next-to-smallest
eigenvalue is near .85.
21. a.If the eigenvalues of Aare all less than 1 in magnitude,
and if x¬§0, then Akxis approximately an eigenvector
for large k.
b.If the strictly dominant eigenvalue is 1, and if xhas a
component in the direction of the corresponding
eigenvector, then fAkxgwill converge to a multiple of
that eigenvector.
c.If the eigenvalues of Aare all greater than 1 in
magnitude, and if xis not an eigenvector, then the
distance from Akxto the nearest eigenvector will
increase ask! 1 .
Chapter 5 Supplementary Exercises, page 328
1.a.T b.F c.T d.F e.T
f.T g.F h.T i.F j.T
k.F l.F m.F n.T o.F
p.T q.F r.T s.F t.T
u.T v.T w.F x.T3.a.Suppose AxDx, with x¬§0. Then
.5I A/xD5x AxD5x xD.5 /x. The
eigenvalue is 5 .
b..5I 3ACA2/xD5x 3AxCA.Ax/D5x 3xC
2xD.5 3C2/x. The eigenvalue is 5 3C2.
5.Suppose AxDx, with x¬§0. Then
p.A/ xD.c0ICc1ACc2A2C    C cnAn/x
Dc0xCc1AxCc2A2xC    C cnAnx
Dc0xCc1xCc22xC    C cnnxDp./ x
Sop./ is an eigenvalue of the matrix p.A/ .
7.IfADPDP 1, then p.A/DPp.D/P 1, as shown in
Exercise 6. If the .j; j / entry in Dis, then the .j; j /
entry in Dkisk, and so the .j; j / entry in p.D/ isp./ . If
pis the characteristic polynomial of A, then p./D0for
each diagonal entry of D, because these entries in Dare the
eigenvalues of A. Thus p.D/ is the zero matrix. Thus
p.A/DP0P 1D0.
9.IfI Awere not invertible, then the equation
.I A/xD0would have a nontrivial solution x. Then
x AxD0andAxD1x, which shows that Awould
have 1 as an eigenvalue. This cannot happen if all the
eigenvalues are less than 1 in magnitude. So I Amust be
invertible.
11.a.Take xinH. Then xDcufor some scalar c. So
AxDA.cu/Dc.Au/Dc.u/D.c/u, which shows
thatAxis inH.
b.Letxbe a nonzero vector in K. Since Kis
one-dimensional, Kmust be the set of all scalar
multiples of x. IfKis invariant under A, then Axis in
Kand hence Axis a multiple of x. Thus xis an
eigenvector of A.
13. 1, 3, 7
15. Replace abya in the determinant formula from
Exercise 16 in Chapter 3 Supplementary Exercises:
det.A I/D.a b /n 1¬åa C.n 1/b¬ç
This determinant is zero only if a b D0or
a C.n 1/bD0. Thus is an eigenvalue of Aif and
only if Da borDaC.n 1/. From the formula
for det .A I/above, the algebraic multiplicity is n 1
fora band 1 for aC.n 1/b.
17. det.A I/D.a11 /.a 22 / a12a21D
2 .a11Ca22/C.a11a22 a12a21/D2 .trA/CdetA.
Use the quadratic formula to solve the characteristic
equation:
DtrAp
.trA/2 4detA
2
The eigenvalues are both real if and only if the discriminant
is nonnegative, that is, .trA/2 4detA0. This inequality
simpliÔ¨Åes to .trA/24detAandtrA
22
detA:
SECOND REVISED PAGES


--- Page 555 ---
A44 Answers to Odd-Numbered Exercises
19.CpD0 1
 6 5
; det.Cp I/D6 5C2Dp./
21. Ifpis a polynomial of order 2, then a calculation such as in
Exercise 19 shows that the characteristic polynomial of Cp
isp./D. 1/2p./ , so the result is true for nD2.
Suppose the result is true for nDkfor some k2, and
consider a polynomial pof degree kC1. Then expanding
det.Cp I/by cofactors down the Ô¨Årst column, the
determinant of Cp Iequals
. /det2
6664  1    0
::::::
0 1
 a1 a2     ak 3
7775C. 1/kC1a0
Thekkmatrix shown is Cq I, where
q.t/Da1Ca2tC    C aktk 1Ctk. By the induction
assumption, the determinant of Cq Iis. 1/kq./ . Thus
det.Cp I/D. 1/kC1a0C. /. 1/kq./
D. 1/kC1¬åa0C.a 1C    C akk 1Ck/¬ç
D. 1/kC1p./
So the formula holds for nDkC1when it holds for
nDk. By the principle of induction, the formula for
det.Cp I/is true for all n2.
23. From Exercise 22, the columns of the Vandermonde matrix
Vare eigenvectors of Cp, corresponding to the eigenvalues
1,2,3(the roots of the polynomial p). Since these
eigenvalues are distinct, the eigenvectors form a linearly
independent set, by Theorem 2 in Section 5.1. Thus Vhas
linearly independent columns and hence is invertible, by the
Invertible Matrix Theorem. Finally, since the columns of V
are eigenvectors of Cp, the Diagonalization Theorem
(Theorem 5 in Section 5.3) shows that V 1CpVis diagonal.
25. [M] If your matrix program computes eigenvalues and
eigenvectors by iterative methods rather than symbolic
calculations, you may have some difÔ¨Åculties. You should
Ô¨Ånd that AP PDhas extremely small entries and
PDP 1is close to A. (This was true just a few years ago,
but the situation could change as matrix programs continue
to improve.) If you constructed Pfrom the program‚Äôs
eigenvectors, check the condition number of P. This may
indicate that you do not really have three linearly
independent eigenvectors.
Chapter 6
Section 6.1, page 338
1.5, 8,8
53.2
43=35
 1=35
 1=73
5 5.8=13
12=13
7.p
35 9. :6
:8
11.2
47=p
69
2=p
69
4=p
693
513.5p
5 15.Not orthogonal 17.Orthogonal
19. Refer to the Study Guide after you have written your
answers.
21. Hint: Use Theorems 3 and 2 from Section 2.1.
23. uvD0,kuk2D30,kvk2D101,
kuCvk2D. 5/2C. 9/2C52D131D30C101
25. The set of all multiples of b
a
(when v¬§0/
27. Hint: Use the deÔ¨Ånition of orthogonality.
29. Hint: Consider a typical vector wDc1v1C    C cpvp
inW.
31. Hint: Ifxis inW?, then xis orthogonal to every vector
inW.
33. [M] State your conjecture and verify it algebraically.
Section 6.2, page 346
1.Not orthogonal 3.Not orthogonal 5.Orthogonal
7.Show u1u2D0, mention Theorem 4, and observe that two
linearly independent vectors in R2form a basis. Then obtain
xD39
132
 3
C26
526
4
D32
 3
C1
26
4
9.Show u1u2D0,u1u3D0, and u2u3D0. Mention
Theorem 4, and observe that three linearly independent
vectors in R3form a basis. Then obtain
xD5
2u1 27
18u2C18
9u3D5
2u1 3
2u2C2u3
11. 2
1
13. y D 4=5
7=5
C14=5
8=5
15. y OyD:6
 :8
, distance is 1
17.2
41=p
3
1=p
3
1=p
33
5,2
4 1=p
2
0
1=p
23
5
19. Orthonormal 21.Orthonormal
23. See the Study Guide .
25. Hint:kUxk2D.Ux/T.Ux/. Also, parts (a) and (c) follow
from (b).
27. Hint: You need two theorems, one of which applies only to
square matrices.
29. Hint: If you have a candidate for an inverse, you can check
to see whether the candidate works.
31. Suppose OyDyu
uuu. Replace ubycuwithc¬§0; then
y.cu/
.cu/.cu/.cu/Dc.yu/
c2uu.c/uDOy
SECOND REVISED PAGES


--- Page 556 ---
Section 6.5 A45
33. LetLDSpanfug, where uis nonzero, and let
T .x/DprojLx. By deÔ¨Ånition,
T .x/Dxu
uuuD.xu/.uu/ 1u
ForxandyinRnand any scalars candd, properties of the
inner product (Theorem 1) show that
T .cxCdy/D¬å.cxCdy/u¬ç.uu/ 1u
D¬åc.xu/Cd.yu/¬ç.uu/ 1u
Dc.xu/.uu/ 1uCd.yu/.uu/ 1u
DcT .x/CdT .y/
Thus Tis linear.
Section 6.3, page 354
1.xD  8
9u1 2
9u2C2
3u3C2u4;xD2
6640
 2
4
 23
775C2
66410
 6
 2
23
775
3.2
4 1
4
03
5 5.2
4 1
2
63
5Dy
7.yD2
410=3
2=3
8=33
5C2
4 7=3
7=3
7=33
5 9. yD2
6642
4
0
03
775C2
6642
 1
3
 13
775
11.2
6643
 1
1
 13
77513.2
664 1
 3
 2
33
77515.p
40
17. a.UTUD1 0
0 1
,
U UTD2
48=9  2=9 2=9
 2=9 5=9 4=9
2=9 4=9 5=93
5
b.projWyD6u1C3u2D2
42
4
53
5, and .U UT/yD2
42
4
53
5
19. Any multiple of2
40
2=5
1=53
5, such as2
40
2
13
5
21. Write your answers before checking the Study Guide .
23. Hint: Use Theorem 3 and the Orthogonal Decomposition
Theorem. For the uniqueness, suppose ApDband
Ap1Db, and consider the equations pDp1C.p p1/
andpDpC0.
25. [M]Uhas orthonormal columns, by Theorem 6 in
Section 6.2, because UTUDI4. The closest point to yin
ColUis the orthogonal projection Oyofyonto Col U. From
Theorem 10,
OyDU UTyD.1:2; :4; 1:2; 1:2; :4; 1:2; :4; :4/Section 6.4, page 360
1.2
43
0
 13
5,2
4 1
5
 33
5 3.2
42
 5
13
5,2
43
3=2
3=23
5
5.2
6641
 4
0
13
775,2
6645
1
 4
 13
7757.2
42=p
30
 5=p
30
1=p
303
5,2
42=p
6
1=p
6
1=p
63
5
9.2
6643
1
 1
33
775,2
6641
3
3
 13
775,2
664 3
1
1
33
77511.2
666641
 1
 1
1
13
77775,2
666643
0
3
 3
33
77775,2
666642
0
2
2
 23
77775
13.RD6 12
0 6
15.QD2
666641=p
5 1=2 1=2
 1=p
5 0 0
 1=p
5 1=2 1=2
1=p
5 1=2 1=2
1=p
5 1=2  1=23
77775,
RD2
4p
5 p
5 4p
5
0 6  2
0 0 43
5
17. See the Study Guide .
19. Suppose xsatisÔ¨Åes RxD0; then QRxDQ0D0, and
AxD0. Since the columns of Aare linearly independent, x
must be zero. This fact, in turn, shows that the columns of
Rare linearly independent. Since Ris square, it is
invertible, by the Invertible Matrix Theorem.
21. Denote the columns of Qbyq1; : : : ; qn. Note that nm,
because Aismnand has linearly independent columns.
Use the fact that the columns of Qcan be extended to an
orthonormal basis for Rm, say,fq1; : : : ; qmg. (The Study
Guide describes one method.) Let Q0DqnC1   qm
andQ1DQ Q 0
. Then, using partitioned matrix
multiplication, Q1R
0
DQRDA.
23. Hint: Partition Ras a22block matrix.
25. [M] The diagonal entries of Rare 20, 6, 10.3923, and
7.0711, to four decimal places.
Section 6.5, page 368
1.a.6 11
 11 22x1
x2
D 4
11
b.OxD3
2
3.a.6 6
6 42x1
x2
D6
 6
b.OxD4=3
 1=3
5.OxD2
45
 3
03
5Cx32
4 1
1
13
5 7.2p
5
SECOND REVISED PAGES


--- Page 557 ---
A46 Answers to Odd-Numbered Exercises
9.a.ObD2
41
1
03
5 b.OxD2=7
1=7
11.a.ObD2
6643
1
4
 13
775b.OxD2
42=3
0
1=33
5
13.AuD2
411
 11
113
5; AvD2
47
 12
73
5,
b AuD2
40
2
 63
5;b AvD2
44
3
 23
5. No, ucould not
possibly be a least-squares solution of AxDb. Why?
15.OxD4
 1
17.See the Study Guide .
19. a.IfAxD0, then ATAxDAT0D0. This shows that
NulAis contained in Nul ATA.
b.IfATAxD0, then xTATAxDxT0D0. So
(Ax/T.Ax/D0(which means that kAxk2D0/, and
hence AxD0. This shows that Nul ATAis contained
in Nul A.
21. Hint: For part (a), use an important theorem from Chapter 2.
23. By Theorem 14, ObDAOxDA.ATA/ 1ATb. The matrix
A.ATA/ 1AToccurs frequently in statistics, where it is
sometimes called the hat-matrix .
25. The normal equations are2 2
2 2x
y
D6
6
, whose
solution is the set of .x; y/ such that xCyD3. The
solutions correspond to points on the line midway between
the lines xCyD2andxCyD4.
Section 6.6, page 376
1.yD:9C:4x 3.yD1:1C1:3x
5.If two data points have different x-coordinates, then the two
columns of the design matrix Xcannot be multiples of each
other and hence are linearly independent. By Theorem 14 in
Section 6.5, the normal equations have a unique solution.
7.a.yDXC, where yD2
666641:8
2:7
3:4
3:8
3:93
77775,XD2
666641 1
2 4
3 9
4 16
5 253
77775,
D1
2
,D2
666641
2
3
4
53
77775
b.[M]yD1:76x  :20x29.yDXC, where yD2
47:9
5:4
 :93
5,XD2
4cos1 sin1
cos2 sin2
cos3 sin33
5,
DA
B
,D2
41
2
33
5
11.[M]D1:45 andeD:811; the orbit is an ellipse. The
equation rD=.1 ecos#/produces rD1:33 when
#D4:6.
13. [M]a.yD  :8558 C4:7025t C5:5554t2 :0274t3
b.The velocity function is
v.t/D4:7025 C11:1108t  :0822t2, and
v.4:5/ D53:0 ft=sec.
15. Hint: Write Xandyas in equation (1), and compute XTX
andXTy.
17. a.The mean of the x-data is NxD5:5. The data in
mean-deviation form are . 3:5; 1/ ,. :5; 2/ ,.1:5; 3/ ,
and.2:5; 3/ . The columns of Xare orthogonal because
the entries in the second column sum to 0.
b.4 0
0 210
1
D9
7:5
,
yD9
4C5
14xD9
4C5
14.x 5:5/
19. Hint: The equation has a nice geometric interpretation.
Section 6.7, page 384
1.a.3,p
105, 225 b.All multiples of1
4
3.28 5.5p
2,3p
3 7.56
25C14
25t
9.a.Constant polynomial, p.t/D5
b.t2 5is orthogonal to p0andp1; values:
.4; 4; 4; 4/; answer: q.t/D1
4.t2 5/
11.17
5t
13. Verify each of the four axioms. For instance:
1:hu;vi D.Au/.Av/ DeÔ¨Ånition
D.Av/.Au/ Property of the dot product
D hv;ui DeÔ¨Ånition
15.hu; cvi D hcv;ui Axiom 1
Dchv;ui Axiom 3
Dchu;vi Axiom 1
17. Hint: Compute 4 times the right-hand side.
19.hu;vi Dpap
bCp
bpaD2p
ab,
kuk2D.pa/2C.p
b/2DaCb. Since aandbare
nonnegative, kuk Dp
aCb. Similarly, kvk Dp
bCa.
By Cauchy‚ÄìSchwarz, 2p
abp
aCbp
bCaDaCb.
Hence,p
abaCb
2.
21. 0 23.2=p
5 25.1,t,3t2 1
SECOND REVISED PAGES


--- Page 558 ---
Chapter 6 Supplementary Exercises A47
27. [M] The new orthogonal polynomials are multiples of
 17tC5t3and72 155t2C35t4. Scale these
polynomials so their values at  2, 1, 0, 1, and 2 are small
integers.
Section 6.8, page 391
1.yD2C3
2t
3.p.t/D4p0 :1p1 :5p2C:2p3
D4 :1t :5.t2 2/C:2 5
6t3 17
6t
(This polynomial happens to Ô¨Åt the data exactly.)
5.Use the identity
sinmtsinntD1
2¬åcos.mt nt/ cos.mtCnt/¬ç
7.Use the identity cos2ktD1Ccos2kt
2.
9.C2sintCsin2tC2
3sin3t[Hint: Save time by using
the results from Example 4.]
11.1
2 1
2cos2t(Why?)
13. Hint: Take functions fandginC¬å0; 2¬ç , and Ô¨Åx an integer
m0. Write the Fourier coefÔ¨Åcient of fCgthat involves
cosmt, and write the Fourier coefÔ¨Åcient that involves
sinmt.m > 0/ .
15. [M] The cubic curve is the graph of
g.t/D  :2685 C3:6095t C5:8576t2 :0477t3. The
velocity at tD4:5seconds is g0.4:5/D53:4 ft=sec. This is
about .7% faster than the estimate obtained in Exercise 13
in Section 6.6.
Chapter 6 Supplementary Exercises, page 392
1.a.F b.T c.T d.F e.F f. T
g.T h.T i.F j.T k.T l. F
m.T n.F o.F p.T q.T r. F
s.F
2.Hint: Iffv1;v2gis an orthonormal set and xDc1v1Cc2v2,
then the vectors c1v1andc2v2are orthogonal, and
kxk2D kc1v1Cc2v2k2D kc1v1k2C kc2v2k2
D.jc1jkv1k/2C.jc2jkv2k/2D jc1j2C jc2j2
(Explain why.) So the stated equality holds for pD2.
Suppose that the equality holds for pDk, with k2, let
fv1; : : : ; vkC1gbe an orthonormal set, and consider
xDc1v1C    C ckvkCckC1vkC1DukCckC1vkC1,
where ukDc1v1C    C ckvk.
3.Given xand an orthonormal set fv1; : : : ; vpginRn, letOxbe
the orthogonal projection of xonto the subspace spanned by
v1; : : : ; vp. By Theorem 10 in Section 6.3,
OxD.xv1/v1C    C .xvp/vp
By Exercise 2, kOxk2D jxv1j2C    C j xvpj2. Bessel‚Äôs
inequality follows from the fact that kOxk2 kxk2, noted
before the statement of the Cauchy‚ÄìSchwarz inequality, in
Section 6.7.5.Suppose .Ux/.Uy/Dxyfor all x,yinRn, and let
e1; : : : ; enbe the standard basis for Rn. For
jD1; : : : ; n; U ejis the jth column of U. Since
kUejk2D.Uej/.Uej/DejejD1, the columns of U
are unit vectors; since .Uej/.Uek/DejekD0for
j¬§k, the columns are pairwise orthogonal.
7.Hint: Compute QTQ, using the fact that
.uuT/TDuT TuTDuuT.
9.LetWDSpanfu;vg. Given zinRn, letOzDprojWz. Then
Ozis in Col A, where ADu v
, say,OzDAOxfor some Ox
inR2. SoOxis a least-squares solution of AxDz. The
normal equations can be solved to produce Ox, and then Ozis
found by computing AOx.
11.Hint: LetxD2
4x
y
¬¥3
5,bD2
4a
b
c3
5,vD2
41
 2
53
5, and
AD2
4vT
vT
vT3
5D2
41 2 5
1 2 5
1 2 53
5. The given set of
equations is AxDb, and the set of all least-squares
solutions coincides with the set of solutions of
ATAxDATb(Theorem 13 in Section 6.5). Study this
equation, and use the fact that .vvT/xDv.vTx/D.vTx/v,
because vTxis a scalar.
13. a.The row‚Äìcolumn calculation of Aushows that each row
ofAis orthogonal to every uin Nul A. So each row of
Ais in.NulA/?. Since .NulA/?is a subspace, it must
contain all linear combinations of the rows of A; hence
.NulA/?contains Row A.
b.If rank ADr, then dim Nul ADn r, by the Rank
Theorem. By Exercise 24(c) in Section 6.3,
dim Nul ACdim.NulA/?Dn
So dim .NulA/?must be r. But Row Ais an
r-dimensional subspace of .NulA/?, by the Rank
Theorem and part (a). Therefore, Row Amust coincide
with.NulA/?.
c.Replace AbyATin part (b) and conclude that Row AT
coincides with .NulAT/?. Since Row ATDColA, this
proves (c).
15. IfADURUTwithUorthogonal, then Ais similar to R
(because Uis invertible and UTDU 1/and so Ahas the
same eigenvalues as R(by Theorem 4 in Section 5.2),
namely, the nreal numbers on the diagonal of R.
17. [M]k¬Åxk
kxkD:4618 ,
cond.A/k¬Åbk
kbkD3363.1:548 10 4/D:5206 .
Observe that k¬Åxk=kxkalmost equals cond .A/times
k¬Åbk=kbk.
19. [M]k¬Åxk
kxkD7:178 10 8,k¬Åbk
kbkD2:832 10 4.
Observe that the relative change in xismuch smaller than
the relative change in b. In fact, since
SECOND REVISED PAGES


--- Page 559 ---
A48 Answers to Odd-Numbered Exercises
cond.A/k¬Åbk
kbkD23;683 .2:832 10 4/D6:707
the theoretical bound on the relative change in xis 6.707
(to four signiÔ¨Åcant Ô¨Ågures). This exercise shows that
even when a condition number is large, the relative error
in a solution need not be as large as you might expect.
Chapter 7
Section 7.1, page 401
1.Symmetric 3.Not symmetric 5.Symmetric
7.Orthogonal,:6 :8
:8 :6
9.Orthogonal, 4=5 3=5
3=5 4=5
11.Not orthogonal
13.PD"
1=p
2 1=p
2
1=p
2 1=p
2#
,DD4 0
0 2
15.PD"
 2=p
5 1=p
5
1=p
5 2=p
5#
,DD1 0
0 11
17.PD2
64 1=p
2 1=p
6 1=p
3
0  2=p
6 1=p
3
1=p
2 1=p
6 1=p
33
75,
DD2
4 4 0 0
0 4 0
0 0 73
5
19.PD2
4 1=p
5 4=p
45  2=3
2=p
5 2=p
45  1=3
0 5=p
45 2=33
5,
DD2
47 0 0
0 7 0
0 0  23
5
21.PD2
6640 1=p
2 1=2 1=2
0  1=p
2 1=2 1=2
1=p
2 0  1=2 1=2
 1=p
2 0  1=2 1=23
775,
DD2
6641 0 0 0
0 1 0 0
0 0 5 0
0 0 0 93
775
23.PD2
641=p
3 1=p
2 1=p
6
1=p
3 1=p
2 1=p
6
1=p
3 0 2=p
63
75,
DD2
42 0 0
0 5 0
0 0 53
5
25. See the Study Guide .27..Ax/yD.Ax/TyDxTATyDxTAyDx.Ay/, because
ATDA.
29. Hint: Use an orthogonal diagonalization of A, or appeal to
Theorem 2.
31. The Diagonalization Theorem in Section 5.3 says that the
columns of Pare (linearly independent) eigenvectors
corresponding to the eigenvalues of Alisted on the diagonal
ofD. SoPhas exactly kcolumns of eigenvectors
corresponding to . These kcolumns form a basis for the
eigenspace.
33.AD8u1uT
1C6u2uT
2C3u3uT
3
D82
41=2  1=2 0
 1=2 1=2 0
0 0 03
5
C62
41=6 1=6  2=6
1=6 1=6  2=6
 2=6  2=6 4=63
5
C32
41=3 1=3 1=3
1=3 1=3 1=3
1=3 1=3 1=33
5
35. Hint: .uuT/xDu.uTx/D.uTx/u, because uTxis a scalar.
37. [M]PD1
22
664 1 1 1 1
1 1 1  1
 1 1  1 1
1 1  1 13
775,
DD2
66419 0 0 0
0 11 0 0
0 0 5 0
0 0 0  113
775
39. [M]PD2
6641=p
2 3=p
50 2=5  2=5
0 4=p
50 1=5 4=5
0 4=p
50 4=5  1=5
1=p
2 3=p
50 2=5 2=53
775
DD2
664:75 0 0 0
0 :75 0 0
0 0 0 0
0 0 0  1:253
775
Section 7.2, page 408
1.a.5x2
1C2
3x1x2Cx2
2b.185 c.16
3.a.3 2
 2 5
b.3 1
1 0
5.a.2
43 3 4
 3 2  2
4 2 53
5 b.2
40 3 2
3 0  5
2 5 03
5
7.xDPy, where PD1p
21 1
1 1
,yTDyD6y2
1 4y2
2
SECOND REVISED PAGES


--- Page 560 ---
Section 7.4 A49
In Exercises 9‚Äì14, other answers (change of variables and new
quadratic form) are possible.
9.Positive deÔ¨Ånite; eigenvalues are 6 and 2
Change of variable: xDPy, with PD1p
2 1 1
1 1
New quadratic form: 6y2
1C2y2
2
11.IndeÔ¨Ånite; eigenvalues are 3 and  2
Change of variable: xDPy, with PD1p
5 2 1
1 2
New quadratic form: 3y2
1 2y2
2
13. Positive semideÔ¨Ånite; eigenvalues are 10 and 0
Change of variable: xDPy, with PD1p
101 3
 3 1
New quadratic form: 10y2
1
15. [M] Negative deÔ¨Ånite; eigenvalues are  13, 9, 7, 1
Change of variable: xDPy;
PD2
66640  1=2 0 3=p
12
0 1=2  2=p
6 1=p
12
 1=p
2 1=2 1=p
6 1=p
12
1=p
2 1=2 1=p
6 1=p
123
7775
New quadratic form:  13y2
1 9y2
2 7y2
3 y2
4
17. [M] Positive deÔ¨Ånite; eigenvalues are 1and21:
Change of variable: xDPy;
PD1p
502
6644 3 4  3
 5 0 5 0
3 4 3 4
0 5 0 53
775
New quadratic form: y2
1Cy2
2C21y2
3C21y2
4
19. 821.See the Study Guide.
23. Write the characteristic polynomial in two ways:
det.A I/Ddeta  b
b d  
D2 .aCd/Cad b2
and
. 1/. 2/D2 .1C2/C12
Equate coefÔ¨Åcients to obtain 1C2DaCdand
12Dad b2DdetA.
25. Exercise 28 in Section 7.1 showed that BTBis symmetric.
Also, xTBTBxD.Bx/TBxD kBxk20, so the quadratic
form is positive semideÔ¨Ånite, and we say that the matrix
BTBis positive semideÔ¨Ånite. Hint: To show that BTBis
positive deÔ¨Ånite when Bis square and invertible, suppose
thatxTBTBxD0and deduce that xD0.
27. Hint: Show that ACBis symmetric and the quadratic
form xT.ACB/xis positive deÔ¨Ånite.
Section 7.3, page 415
1.xDPy, where PD2
41=3 2=3  2=3
2=3 1=3 2=3
 2=3 2=3 1=33
53.a.9 b.2
41=3
2=3
 2=33
5 c.6
5.a.6 b."
1=p
2
 1=p
2#
c. 4
7.2
41=3
2=3
2=33
5 9.5Cp
5 11. 3
13. Hint: IfmDM, take D0in the formula for x. That is,
letxDun, and verify that xTAxDm. Ifm < M and if tis
a number between mandM, then 0t mM mand
0.t m/=.M  m/1. So let D.t m/=.M  m/.
Solve the expression for to see that tD.1 /mCM.
Asgoes from 0 to 1, tgoes from mtoM. Construct xas
in the statement of the exercise, and verify its properties.
15. [M]a.9 b.2
664 2=p
6
0
1=p
6
1=p
63
775c.3
17. [M]a.34 b.2
6641=2
1=2
1=2
1=23
775c.26
Section 7.4, page 425
1.3, 1 3.4, 1
The answers in Exercises 5‚Äì13 are not the only possibilities.
5. 1 0
0 12 0
0 01 0
0 1
7."
1=p
5 2=p
5
2=p
5 1=p
5#3 0
0 2
"
2=p
5 1=p
5
 1=p
5 2=p
5#
9.2
4 1 0 0
0 0 1
0 1 03
52
43p
2 0
0p
2
0 03
5
"
 1=p
2 1=p
2
1=p
2 1=p
2#
11.2
4 1=3 2=3 2=3
2=3  1=3 2=3
2=3 2=3  1=33
52
4p
90 0
0 0
0 03
5
"
3=p
10 1=p
10
1=p
10 3=p
10#
SECOND REVISED PAGES


--- Page 561 ---
A50 Answers to Odd-Numbered Exercises
13."
1=p
2 1=p
2
1=p
2 1=p
2#5 0 0
0 3 0
2
41=p
2 1=p
2 0
 1=p
18 1=p
18 4=p
18
 2=3 2=3 1=33
5
15. a.rankAD2
b.Basis for Col A:2
4:40
:37
 :843
5;2
4 :78
 :33
 :523
5
Basis for Nul A:2
4:58
 :58
:583
5
(Remember that VTappears in the SVD.)
17. IfUis an orthogonal matrix then det UD 1:If
ADU ¬ÜVTandAis square, then so are U,¬Ü, and V.
Hence det ADdetUdet¬ÜdetVT
D 1det¬ÜD 1  n
19. Hint: Since UandVare orthogonal,
ATAD.U ¬ÜVT/TU ¬ÜVTDV ¬ÜTUTU ¬ÜVT
DV.¬ÜT¬Ü/V 1
Thus Vdiagonalizes ATA. What does this tell you about V?
21. The right singular vector v1is an eigenvector for the largest
eigenvalue 1ofATA. By Theorem 7 in Section 7.3, the
largest eigenvalue, 2, is the maximum of xT.ATA/xover
all unit vectors orthogonal to v1. Since
xT.ATA/xD jjAxjj2, the square root of 2, which is the
second largest eigenvalue, is the maximum of jjAxjjover
all unit vector orthogonal to v1.
23. Hint: Use a column‚Äìrow expansion of .U ¬Ü/VT.
25. Hint: Consider the SVD for the standard matrix of T‚Äîsay,
ADU ¬ÜVTDU ¬ÜV 1. LetBD fv1; : : : ; vngand
CD fu1; : : : ; umgbe bases constructed from the columns of
VandU, respectively. Compute the matrix for Trelative to
BandC, as in Section 5.4. To do this, you must show that
V 1vjDej, thejth column of In.
27. [M]2
664 :57 :65 :42 :27
:63 :24 :68 :29
:07 :63 :53  :56
 :51 :34  :29 :733
775
2
66416:46 0 0 0 0
0 12:16 0 0 0
0 0 4:87 0 0
0 0 0 4:31 03
775
2
66664 :10 :61  :21 :52 :55
 :39 :29 :84  :14 :19
 :74 :27 :07 :38 :49
:41 :50 :45  :23 :58
 :36 :48 :19 :72 :293
77775
29. [M] 25.9343, 16.7554, 11.2917, 1.0785, .00037793;
1=5D68;622Section 7.5, page 432
1.MD12
10
;BD7 10  6 9 10 8
2 4 1 5 3  5
;
SD86 27
 27 16
3.:95
 :32
forD95:2,:32
:95
forD6:8
5.[M] (.130, .874, .468), 75.9% of the variance
7.y1D:95x 1 :32x 2;y1explains 93.3% of the variance.
9.c1D1=3,c2D2=3,c3D2=3; the variance of yis 9.
11.a.Ifwis the vector in RNwith a 1 in each position, then
X1   XN
wDX1C    C XND0
because the Xkare in mean-deviation form. Then
Y1   YN
w
DPTX1  PTXN
w By deÔ¨Ånition
DPTX1   XN
wDPT0D0
That is, Y1C    C YND0, so the Ykare in
mean-deviation form.
b.Hint: Because the Xjare in mean-deviation form, the
covariance matrix of the Xjis
1=.N  1/X1   XNX1   XNT
Compute the covariance matrix of the Yj, using part (a).
13. IfBDOX1  OXN
, then
SD1
N 1BBTD1
N 1OX1  OXn2
6664OXT
1:::
OXT
N3
7775
D1
N 1NX
1OXkOXT
kD1
N 1NX
1.Xk M/.Xk M/T
Chapter 7 Supplementary Exercises, page 434
1.a.T b.F c.T d.F e.F f. F
g.F h.T i.F j.F k.F l. F
m.T n.F o.T p.T q.F
3.If rank ADr, then dim Nul ADn r, by the Rank
Theorem. So 0 is an eigenvalue of multiplicity n r.
Hence, of the nterms in the spectral decomposition of A,
exactly n rare zero. The remaining rterms
(corresponding to the nonzero eigenvalues) are all rank 1
matrices, as mentioned in the discussion of the spectral
decomposition.
5.IfAvDvfor some nonzero , then
vD 1AvDA. 1v/, which shows that vis a linear
combination of the columns of A.
SECOND REVISED PAGES


--- Page 562 ---
Section 8.2 A51
7.Hint: IfADRTR, where Ris invertible, then Ais positive
deÔ¨Ånite, by Exercise 25 in Section 7.2. Conversely, suppose
thatAis positive deÔ¨Ånite. Then by Exercise 26 in Section
7.2,ADBTBfor some positive deÔ¨Ånite matrix B. Explain
whyBadmits a QR factorization, and use it to create the
Cholesky factorization of A.
9.IfAismnandxis inRn, then xTATAxD.Ax/T.Ax/D
kAxk20. Thus ATAis positive semideÔ¨Ånite. By
Exercise 22 in Section 6.5, rank ATADrankA.
11.Hint: Write an SVD of Ain the form ADU ¬ÜVTDPQ,
where PDU ¬ÜUTandQDUVT. Show that Pis
symmetric and has the same eigenvalues as ¬Ü. Explain why
Qis an orthogonal matrix.
13. a.IfbDAx, then xCDACbDACAx. By
Exercise 12(a), xCis the orthogonal projection of x
onto Row A.
b.From (a) and then Exercise 12(c),
AxCDA.ACAx/D.AACA/xDAxDb.
c.Since xCis the orthogonal projection onto Row A, the
Pythagorean Theorem shows that
kuk2D kxCk2C ku xCk2. Part (c) follows
immediately.
15. [M]ACD1
402
66664 2 14 13 13
 2 14 13 13
 2 6  7 7
2 6 7 7
4 12  6 63
77775,OxD2
66664:7
:7
 :8
:8
:63
77775
The reduced echelon form ofA
xT
is the same as the
reduced echelon form of A, except for an extra row of
zeros. So adding scalar multiples of the rows of AtoxTcan
produce the zero vector, which shows that xTis in Row A.
Basis for Nul A:2
66664 1
1
0
0
03
77775,2
666640
0
1
1
03
77775
Chapter 8
Section 8.1, page 444
1.Some possible answers: yD2v1 1:5v2C:5v3,
yD2v1 2v3Cv4,yD2v1C3v2 7v3C3v4
3.yD  3v1C2v2C2v3. The weights sum to 1, so this is an
afÔ¨Åne sum.
5.a.p1D3b1 b2 b32affSsince the coefÔ¨Åcients sum
to 1.
b.p2D2b1C0b2Cb3¬ÖaffSsince the coefÔ¨Åcients do
not sum to 1.
c.p3D  b1C2b2C0b32affSsince the coefÔ¨Åcients
sum to 1.7.a.p12SpanS, but p1¬ÖaffS
b.p22SpanS, and p22affS
c.p3¬ÖSpanS, sop3¬ÖaffS
9.v1D 3
0
andv2D1
 2
. Other answers are possible.
11.See the Study Guide .
13. Spanfv2 v1;v3 v1gis a plane if and only if
fv2 v1;v3 v1gis linearly independent. Suppose c2and
c3satisfy c2.v2 v1/Cc3.v3 v1/D0. Show that this
implies c2Dc3D0.
15. LetSD fxWAxDbg. To show that Sis afÔ¨Åne, it sufÔ¨Åces
to show that Sis a Ô¨Çat, by Theorem 3. Let
WD fxWAxD0g. Then Wis a subspace of Rn, by
Theorem 2 in Section 4.2 (or Theorem 12 in Section 2.8).
Since SDWCp, where psatisÔ¨Åes ApDb, by Theorem
6 in Section 1.5, Sis a translate of W, and hence Sis a Ô¨Çat.
17. A suitable set consists of any three vectors that are not
collinear and have 5 as their third entry. If 5 is their third
entry, they lie in the plane ¬¥D5. If the vectors are not
collinear, their afÔ¨Åne hull cannot be a line, so it must be the
plane.
19. Ifp;q2f .S/ , then there exist r;s2Ssuch that f .r/Dp
andf .s/Dq. Given any t2R, we must show that
zD.1 t/pCtqis inf .S/ . Now use deÔ¨Ånitions of pand
q, and the fact that fis linear. The complete proof is
presented in the Study Guide .
21. Since Bis afÔ¨Åne, Theorem 2 implies that Bcontains all
afÔ¨Åne combinations of points of B. Hence Bcontains all
afÔ¨Åne combinations of points of A. That is, aff AB.
23. Since A.A[B/, it follows from Exercise 22 that
affAaff.A[B/. Similarly, aff Baff.A[B/, so
¬åaffA[affB¬çaff.A[B/.
25. To show that DE\F, show that DEandDF.
The complete proof is presented in the Study Guide .
Section 8.2, page 454
1.AfÔ¨Ånely dependent and 2v1Cv2 3v3D0
3.The set is afÔ¨Ånely independent. If the points are called v1,
v2,v3, and v4, then fv1;v2;v3gis a basis for R3and
v4D16v1C5v2 3v3, but the weights in the linear
combination do not sum to 1.
5. 4v1C5v2 4v3C3v4D0
7.The barycentric coordinates are . 2; 4; 1/.
9.See the Study Guide .
11.When a set of Ô¨Åve points is translated by subtracting, say,
the Ô¨Årst point, the new set of four points must be linearly
dependent, by Theorem 8 in Section 1.7, because the four
points are in R3. By Theorem 5, the original set of Ô¨Åve
points is afÔ¨Ånely dependent.
SECOND REVISED PAGES


--- Page 563 ---
A52 Answers to Odd-Numbered Exercises
13. Iffv1;v2gis afÔ¨Ånely dependent, then there exist c1andc2,
not both zero, such that c1Cc2D0andc1v1Cc2v2D0.
Show that this implies v1Dv2. For the converse, suppose
v1Dv2and select speciÔ¨Åc c1andc2that show their afÔ¨Åne
dependence. The details are in the Study Guide .
15. a.The vectors v2 v1D1
2
andv3 v1D3
 2
are
not multiples and hence are linearly independent. By
Theorem 5, Sis afÔ¨Ånely independent.
b.p1$ 
 6
8;9
8;5
8
,p2$ 
0;1
2;1
2
,p3$ 14
8; 5
8; 1
8
,
p4$ 6
8; 5
8;7
8
,p5$ 1
4;1
8;5
8
c.p6is. ; ;C/,p7is.0;C; /, and p8is.C;C; /.
17. Suppose SD fb1; : : : ; bkgis an afÔ¨Ånely independent set.
Then equation (7) has a solution, because pis in aff S.
Hence equation (8) has a solution. By Theorem 5, the
homogeneous forms of the points in Sare linearly
independent. Thus (8) has a unique solution. Then (7) also
has a unique solution, because (8) encodes both equations
that appear in (7).
The following argument mimics the proof of Theorem 7 in
Section 4.4. If SD fb1; : : : ; bkgis an afÔ¨Ånely independent
set, then scalars c1; : : : ; c kexist that satisfy (7), by
deÔ¨Ånition of aff S. Suppose xalso has the representation
xDd1b1C    C dkbkand d1C    C dkD1 (7a)
for scalars d1; : : : ; d k. Then subtraction produces the
equation
0Dx xD.c1 d1/b1C    C .ck dk/bk (7b)
The weights in (7b) sum to 0 because the c‚Äôs and the d‚Äôs
separately sum to 1. This is impossible, unless each weight
in (8) is 0, because Sis an afÔ¨Ånely independent set. This
proves that ciDdiforiD1; : : : ; k .
19. Iffp1;p2;p3gis an afÔ¨Ånely dependent set, then there exist
scalars c1,c2, and c3, not all zero, such that
c1p1Cc2p2Cc3p3D0andc1Cc2Cc3D0. Now use
the linearity of f.
21. LetaDa1
a2
,bDb1
b2
, and cDc1
c2
. Then
det¬åQaQbQc¬çDdet2
4a1b1c1
a2b2c2
1 1 13
5D
det2
4a1a21
b1b21
c1c213
5, by the transpose property of the
determinant (Theorem 5 in Section 3.2). By Exercise 30 in
Section 3.3, this determinant equals 2 times the area of the
triangle with vertices at a,b, and c.
23. If¬åQaQbQc¬ç2
4r
s
t3
5DQp, then Cramer‚Äôs rule gives
rDdet¬åQpQbQc¬ç=det¬åQaQbQc¬ç. By Exercise 21, the
numerator of this quotient is twice the area of 4pbc, andthe denominator is twice the area of 4abc. This proves the
formula for r. The other formulas are proved using
Cramer‚Äôs rule for sandt.
25. The intersection point is x.4/D
 :12
41
3
 63
5C:62
47
3
 53
5C:52
43
9
 23
5D2
45:6
6:0
 3:43
5:
It is not inside the triangle.
Section 8.3, page 461
1.See the Study Guide .
3.None are in conv S.
5.p1D  1
6v1C1
3v2C2
3v3C1
6v4, sop1¬ÖconvS.
p2D1
3v1C1
3v2C1
6v3C1
6v4, sop22convS.
7.a.The barycentric coordinates of p1,p2,p3, and p4are,
respectively, 1
3;1
6;1
2
, 
0;1
2;1
2
, 1
2; 1
4;3
4
, and 1
2;3
4; 1
4
.
b.p3andp4are outside conv T.p1is inside conv T.
p2is on the edge v2v3of conv T.
9.p1andp3are outside the tetrahedron conv S.p2is on the
face containing the vertices v2,v3, and v4.p4is inside
convS.p5is on the edge between v1andv3.
11.See the Study Guide .
13. Ifp,q2f .S/ , then there exist r,s2Ssuch that f .r/Dp
andf .s/Dq. The goal is to show that the line segment
yD.1 t/pCtq, for0t1, is in f .S/ . Use the
linearity of fand the convexity of Sto show that
yDf .w/for some winS. This will show that yis inf .S/
and that f .S/ is convex.
15. pD1
6v1C1
2v2C1
3v4andpD1
2v1C1
6v2C1
3v3.
17. Suppose AB, where Bis convex. Then, since Bis
convex, Theorem 7 implies that Bcontains all convex
combinations of points of B. Hence Bcontains all convex
combinations of points of A. That is, conv AB.
19. a.Use Exercise 18 to show that conv Aand conv Bare
both subsets of conv .A[B/. This will imply that their
union is also a subset of conv .A[B/.
b.One possibility is to let Abe two adjacent corners of a
square and let Bbe the other two corners. Then what is
.convA/[.convB/, and what is conv .A[B/?
21.
p1
f0
p0gf1
p2(1
2)
(12) (12)
23. g.t/D.1 t/f0.t/Ctf1.t/
D.1 t/¬å.1 t/p0Ctp1¬çCt¬å.1 t/p1Ctp2¬ç
D.1 t/2p0C2t.1 t/p1Ct2p2:
SECOND REVISED PAGES


--- Page 564 ---
Section 8.5 A53
The sum of the weights in the linear combination for gis
.1 t/2C2t.1 t/Ct2, which equals
.1 2tCt2/C.2t 2t2/Ct2D1. The weights are each
between 0 and 1 when 0t1, sog.t/is in
convfp0;p1;p2g.
Section 8.4, page 469
1.f .x 1; x2/D3x1C4x2anddD13
3.a.Open b.Closed c.Neither
d.Closed e.Closed
5.a.Not compact, convex
b.Compact, convex
c.Not compact, convex
d.Not compact, not convex
e.Not compact, convex
7.a.nD2
40
2
33
5or a multiple
b.f .x/D2x2C3x3,dD11
9.a.nD2
6643
 1
2
13
775or a multiple
b.f .x/D3x1 x2C2x3Cx4,dD5
11.v2is on the same side as 0,v1is on the other side, and v3is
inH.
13. One possibility is pD2
66432
 14
0
03
775,v1D2
66410
 7
1
03
775,
v2D2
664 4
1
0
13
775.
15.f .x 1; x2; x3; x4/Dx1 3x2C4x3 2x4, and dD5
17.f .x 1; x2; x3/Dx1 2x2Cx3, and dD0
19.f .x 1; x2; x3/D  5x1C3x2Cx3, and dD0
21. See the Study Guide .
23.f .x 1; x2/D3x1 2x2withdsatisfying 9 < d < 10 is one
possibility.
25.f .x; y/ D4xCy. A natural choice for dis 12.75, which
equals f .3; :75/ . The point .3; :75/ is three-fourths of the
distance between the center of B.0; 3/and the center of
B.p; 1/.
27. Exercise 2(a) in Section 8.3 gives one possibility. Or let
SD f.x; y/ Wx2y2D1andy > 0g. Then conv Sis the
upper (open) half-plane.29. Letx,y2B.p; /and suppose zD.1 t/xCty, where
0t1. Then show that
kz pk D k ¬å.1 t/xCty¬ç pk
D k.1 t/.x p/Ct.y p/k< :
Section 8.5, page 481
1.a.mD1at the point p1 b.mD5at the point p2
c.mD5at the point p3
3.a.mD  3at the point p3
b.mD1on the set conv fp1;p3g
c.mD  3on the set conv fp1;p2g
5.0
0
;5
0
;4
3
;0
5
7.0
0
;7
0
;6
4
;0
6
9.The origin is an extreme point, but it is not a vertex. Explain
why.
11.One possibility is to let Sbe a square that includes part of
the boundary but not all of it. For example, include just two
adjacent edges. The convex hull of the proÔ¨Åle Pis a
triangular region.
S conv P =
13. a.f0.C5/D32,f1.C5/D80,f2.C5/D80,
f3.C5/D40,f4.C5/D10, and
32 80C80 40C10D2.
b.
f0f1f2f3f4
C12
C24 4
C38 12 6
C416 32 24 8
C532 80 80 40 10
For a general formula, see the Study Guide .
15. a.f0.Pn/Df0.Q/C1
b.fk.Pn/Dfk.Q/Cfk 1.Q/
c.fn 1.Pn/Dfn 2.Q/C1
SECOND REVISED PAGES


--- Page 565 ---
A54 Answers to Odd-Numbered Exercises
17. See the Study Guide .
19. LetSbe convex and let x2cSCdS, where c > 0 and
d > 0 . Then there exist s1ands2inSsuch that
xDcs1Cds2. But then
xDcs1Cds2D.cCd/c
cCds1Cd
cCds2
:
Now show that the expression on the right side is a member
of.cCd/S.
For the converse, pick a typical point in .cCd/S and
show it is in cSCdS.
21. Hint: Suppose AandBare convex. Let x,y2ACB.
Then there exist a,c2Aandb,d2Bsuch that xDaCb
andyDcCd. For any tsuch that 0t1, show that
wD.1 t/xCtyD.1 t/.aCb/Ct.cCd/
represents a point in ACB.
Section 8.6, page 492
1.The control points for x.t/Cbshould be p0Cb,p1Cb,
andp3Cb. Write the B√©zier curve through these points,
and show algebraically that this curve is x.t/Cb. See the
Study Guide .
3.a.x0.t/D. 3C6t 3t2/p0C.3 12tC9t2/p1C
.6t 9t2/p2C3t2p3, so
x0.0/D  3p0C3p1D3.p1 p0/, and
x0.1/D  3p2C3p3D3.p3 p2/. This shows that the
tangent vector x0.0/points in the direction from p0top1
and is three times the length of p1 p0. Likewise, x0.1/
points in the direction from p2top3and is three times
the length of p3 p2. In particular, x0.1/D0if and
only if p3Dp2.
b.x00.t/D.6 6t/p0C. 12C18t/p1
C.6 18t/p2C6tp3;so that
x00.0/D6p0 12p1C6p2D6.p0 p1/C6.p2 p1/
and
x00.1/D6p1 12p2C6p3D6.p1 p2/C6.p3 p2/
For a picture of x00.0/, construct a coordinate system
with the origin at p1, temporarily, label p0asp0 p1,
and label p2asp2 p1. Finally, construct a line from
this new origin through the sum of p0 p1andp2 p1,
extended out a bit. That line points in the direction of
x00.0/.
w = (p0 ‚Äì p1) + (p2 ‚Äì p1) =0 = p1 p2 ‚Äì p1
p0 ‚Äì p1 w
1
6x"(0)
5.a.From Exercise 3(a) or equation (9) in the text,
x0.1/D3.p3 p2/Use the formula for x0.0/, with the control points from
y.t/, and obtain
y0.0/D  3p3C3p4D3.p4 p3/
ForC1continuity, 3.p3 p2/D3.p4 p3/, so
p3D.p4Cp2/=2, and p3is the midpoint of the line
segment from p2top4.
b.Ifx0.1/Dy0.0/D0, then p2Dp3andp3Dp4. Thus,
the ‚Äúline segment‚Äù from p2top4is just the point p3.
[Note: In this case, the combined curve is still C1
continuous, by deÔ¨Ånition. However, some choices of the
other ‚Äúcontrol‚Äù points, p0,p1,p5, and p6, can produce a
curve with a visible corner at p3, in which case the
curve is not G1continuous at p3.]
7.Hint: Usex00.t/from Exercise 3 and adapt this for the
second curve to see that
y00.t/D6.1 t/p3C6. 2C3t/p4C6.1 3t/p5C6tp6
Then set x00.1/Dy00.0/. Since the curve is C1continuous
atp3, Exercise 5(a) says that the point p3is the midpoint of
the segment from p2top4. This implies that
p4 p3Dp3 p2. Use this substitution to show that p4
andp5are uniquely determined by p1,p2, and p3. Only p6
can be chosen arbitrarily.
9.Write a vector of the polynomial weights for x.t/, expand
the polynomial weights, and factor the vector as MBu.t/:
2
666641 4tC6t2 4t3Ct4
4t 12t2C12t3 4t4
6t2 12t3C6t4
4t3 4t4
t43
77775
D2
666641 4 6  4 1
0 4  12 12  4
0 0 6  12 6
0 0 0 4  4
0 0 0 0 13
777752
666641
t
t2
t3
t43
77775;
MBD2
666641 4 6  4 1
0 4  12 12  4
0 0 6  12 6
0 0 0 4  4
0 0 0 0 13
77775
11.See the Study Guide .
13. a.Hint: Use the fact that q0Dp0.
b.Multiply the Ô¨Årst and last parts of equation (13) by8
3
and solve for 8q2.
c.Use equation (8) to substitute for 8q3and then apply
part (a).
15. a.From equation (11), y0.1/D:5x0.:5/Dz0.0/.
b.Observe that y0.1/D3.q3 q2/. This follows from
equation (9), with y.t/and its control points in place of
x.t/and its control points. Similarly, for z.t/and its
control points, z0.0/D3.r1 r0/. By part (a),
SECOND REVISED PAGES


--- Page 566 ---
Section 8.6 A55
3.q3 q2/D3.r1 r0/. Replace r0byq3, and obtain
q3 q2Dr1 q3, and hence q3D.q2Cr1/=2.
c.Setq0Dp0andr3Dp3. Compute q1D.p0Cp1/=2
andr2D.p2Cp3/=2. Compute mD.p1Cp2/=2.
Compute q2D.q1Cm/=2andr1D.mCr2/=2.
Compute q3D.q2Cr1/=2and set r0Dq3.
17. a.r0Dp0,r1Dp0C2p1
3,r2D2p1Cp2
3,r3Dp2
b.Hint: Write the standard formula (7) in this section, with
riin place of piforiD0; : : : ; 3 , and then replace r0
andr3byp0andp2, respectively:
x.t/D.1 3tC3t2 t3/p0
C.3t 6t2C3t3/r1
C.3t2 3t3/r2Ct3p2(iii)
Use the formulas for r1andr2from part (a) to examine
the second and third terms in this expression for x.t/.
SECOND REVISED PAGES


--- Page 567 ---
This page intentionally left blank 

--- Page 568 ---
Index
Absolute value, complex number, A3
Accelerator-multiplier model, 253n
Adjoint, classical, 181
Adjugate matrix, 181
Adobe Illustrator, 483
AfÔ¨Åne combinations, 438‚Äì446
deÔ¨Ånition, 438
of points, 438‚Äì440, 443‚Äì444
AfÔ¨Åne coordinates. SeeBarycentric
coordinates
AfÔ¨Åne dependence, 446‚Äì456
deÔ¨Ånition, 446
linear dependence and, 447‚Äì448, 454
AfÔ¨Åne hull (afÔ¨Åne span), 439, 456
geometric view of, 443
of two points, 448
AfÔ¨Åne independence, 446‚Äì456
barycentric coordinates, 449‚Äì455
deÔ¨Ånition, 446
AfÔ¨Åne set, 441‚Äì443, 457
dimension of, 442
intersection of, 458
AfÔ¨Ånely dependent, 446
Aircraft design, 93‚Äì94
Algebraic multiplicity, eigenvalue, 278
Algorithms
change-of-coordinates matrix, 242
compute a B-matrix, 295
decouple a system, 317
diagonalization, 285‚Äì287
Gram‚ÄìSchmidt process, 356‚Äì362
inverse power method, 324‚Äì326
Jacobi‚Äôs method, 281
LU factorization, 127‚Äì129
QR algorithm, 326
reduction to Ô¨Årst-order system, 252
row‚Äìcolumn rule for computing
AB, 96
row reduction, 15‚Äì17
row‚Äìvector rule for computing Ax, 38
singular value decomposition, 419‚Äì422
solving a linear system, 21
steady-state vector, 259‚Äì262
writing solution set in parametric vector
form, 47
Ampere, 83
Analysis of variance, 364
Angles in R2andR3, 337‚Äì338
Area
approximating, 185
determinants as, 182‚Äì184
ellipse, 186
parallelogram, 183Argument, of a complex number, A5
Associative law, matrix multiplication, 99
Associative property, matrix addition, 96
Astronomy, barycentric coordinates in, 450n
Attractor, dynamical system, 306, 315‚Äì316
Augmented matrix, 4, 6‚Äì8, 18, 21,
38, 440
Auxiliary equation, 250‚Äì251
Average value, 383
Axioms
inner product space, 378
vector space, 192
B-coordinate vector, 218‚Äì220
B-matrix, 291‚Äì292, 294‚Äì295
B-splines, 486
Back-substitution, 19‚Äì20
Backward phase, row reduction algorithm, 17
Barycentric coordinates, 448‚Äì453
Basic variable, pivot column, 18
Basis
change of basis
overview, 241‚Äì243
Rn, 243‚Äì244
column space, 213‚Äì214
coordinate systems, 218‚Äì219
eigenspace, 270
fundamental set of solutions, 314
fundamental subspaces, 422‚Äì423
null space, 213‚Äì214, 233‚Äì234
orthogonal, 340‚Äì341
orthonormal, 344, 358‚Äì360, 399, 418
row space, 233‚Äì235
spanning set, 212
standard basis, 150, 211, 219, 344
subspace, 150‚Äì152, 158
two views, 214‚Äì215
Basis matrix, 487n
Basis Theorem, 229‚Äì230, 423, 467
Beam model, 106
Bessel‚Äôs inequality, 392
Best Approximation Theorem, 352‚Äì353
Best approximation
Fourier, 389
P4, 380‚Äì381
to y by elements of W, 352
B√©zier bicubic surface, 489, 491
B√©zier curves
approximations to, 489‚Äì490
connecting two curves, 485‚Äì487
matrix equations, 487‚Äì488
overview, 483‚Äì484
recursive subdivisions, 490‚Äì491B√©zier surfaces
approximations to, 489‚Äì490
overview, 488‚Äì489
recursive subdivisions, 490‚Äì491
B√©zier, Pierre, 483
Bidiagonal matrix, 133
Blending polynomials, 487n
Block diagonal matrix, 122
Block matrix. SeePartitioned matrix
Block multiplication, 120
Block upper triangular matrix, 121
Boeing, 93‚Äì94
Boundary condition, 254
Boundary point, 467
Bounded set, 467
Branch current, 83
Branch, network, 53
C (language), 39, 102
C, A2
Cn, 300‚Äì302
C3, 310
C1geometric continuity, 485
CAD. SeeComputer-aided design
Cambridge diet, 81
Capacitor, 314‚Äì315, 318
Caratheodory, Constantin, 459
Caratheodory‚Äôs theorem, 459
Casorati matrix, 247‚Äì248
Casoratian, 247
Cauchy‚ÄìSchwarz inequality,
381‚Äì382
Cayley‚ÄìHamilton theorem, 328
Center of projection, 144
Ceres, 376n
CFD. SeeComputational Ô¨Çuid dynamics
Change of basis, 241‚Äì244
Change of variable
dynamical system, 308
principal component analysis, 429
quadratic form, 404‚Äì405
Change-of-coordinates matrix, 221, 242
Characteristic equation, 278‚Äì279
Characteristic polynomial, 278, 281
Characterization of Linearly Dependent Sets
Theorem, 59
Chemical equation, balancing, 52
Cholesky factorization, 408
Classical adjoint, 181
Closed set, 467‚Äì468
Closed (subspace), 148
Codomain, matrix
transformation, 64
CONFIRMING PAGES
I1

--- Page 569 ---
I2 Index
CoefÔ¨Åcient
correlation coefÔ¨Åcient, 338
Ô¨Ålter coefÔ¨Åcient, 248
Fourier coefÔ¨Åcient, 389
of linear equation, 2
regression coefÔ¨Åcient, 371
trend coefÔ¨Åcient, 388
CoefÔ¨Åcient matrix, 4, 38, 136
Cofactor expansion, 168‚Äì169
Column
augmented, 110
determinants, 174
operations, 174
pivot column, 152, 157, 214
sum, 136
vector, 24
Column‚Äìrow expansion, 121
Column space
basis, 213‚Äì214
dimension, 230
null space contrast, 204‚Äì206
overview, 203‚Äì204
subspaces, 149, 151‚Äì152
Comet, orbit, 376
Comformable partitions, 120
Compact set, 467
Complex eigenvalue, 297‚Äì298, 300‚Äì301,
309‚Äì310, 317‚Äì319
Complex eigenvector, 297
Complex number, A2‚ÄìA6
absolute value, A3
argument of, A5
conjugate, A3
geometric interpretation, A4‚ÄìA5
powers of, A6
R2, A6
system, A2
Complex vector, 24n, 299‚Äì301
Complex vector space, 192n, 297, 310
Composite transformation, 141‚Äì142
Computational Ô¨Çuid dynamics (CFD), 93‚Äì94
Computer-aided design (CAD), 140, 489
Computer graphics
barycentric coordinates, 451‚Äì453
composite transformation, 141‚Äì142
homogeneous coordinates, 141‚Äì142
perspective projection, 144‚Äì146
three-dimensional graphics, 142‚Äì146
two-dimensional graphics, 140‚Äì142
Condition number, 118, 422
Conformable partition, 120
Conjugate, 300, A3
Consistent system of linear equations,
4, 7‚Äì8, 46‚Äì47
Constrained optimization problem, 410‚Äì415
Consumption matrix, Leontief input‚Äìoutput
model, 135‚Äì136
Contraction transformation, 67, 75
Control points, 490‚Äì491
Control system
control sequence, 266
controllable pair, 266Schur complement, 123
space shuttle, 189‚Äì190
state vector, 256, 266
steady-state response, 303
Controllability matrix, 266
Convergence, 137, 260
Convex combinations, 456‚Äì463
Convex hull, 458, 467, 474, 490
Convex set, 458‚Äì459
Coordinate mapping, 218‚Äì224
Coordinate systems
B-coordinate vector, 218‚Äì220
graphical interpretation of coordinates,
219‚Äì220
mapping, 221‚Äì224
Rnsubspace, 155‚Äì157, 220‚Äì221
unique representation theorem, 218
Coordinate vector, 156, 218‚Äì219
Correlation coefÔ¨Åcient, 338
Covariance, 429‚Äì430
Covariance matrix, 428
Cramer‚Äôs rule, 179‚Äì180
engineering application, 180
inverse formula, 181‚Äì182
Cray supercomputer, 122
Cross product, 466
Cross-product formula, 466
Crystallography, 219‚Äì220
Cubic curves
B√©zier curve, 484
Hermite cubic curve, 487
Current, 83‚Äì84
Curve Ô¨Åtting, 23, 373‚Äì374,
380‚Äì381
Curves. SeeB√©zier curves
D, 194
De Moivre‚Äôs Theorem, A6
Decomposition
eigenvector, 304, 321
force into component forces, 344
orthogonal, 341‚Äì342
polar, 434
singular value, 416‚Äì426
See also Factorization
Decoupled systems, 314, 317
DeÔ¨Çection vector, 106‚Äì107
Design matrix, 370
Determinant, 105
area, 182‚Äì184
cofactor expansion, 168‚Äì169
column operations, 174
Cramer‚Äôs rule, 179‚Äì180
eigenvalues and characteristic equation of a
square matrix, 276‚Äì278
linear transformation, 184‚Äì186
linearity property, 175‚Äì176
multiplicative property, 175‚Äì176
overview, 166‚Äì167
recursive deÔ¨Ånition, 167
row operations, 171‚Äì174
volume, 182‚Äì183Diagonal entries, 94
Diagonal matrix, 94, 122, 283‚Äì290, 417‚Äì419
Diagonal matrix Representation Theorem, 293
Diagonalization matrix
matrices whose eigenvalues are not
distinct, 287‚Äì288
orthogonal diagonalization,
420, 426
overview, 283‚Äì284
steps, 285‚Äì286
sufÔ¨Åcient conditions, 286‚Äì287
symmetric matrix, 397‚Äì399
theorem, 284
Diagonalization Theorem, 284
Diet, linear modeling of weight-loss diet,
81‚Äì83
Difference equation. SeeLinear difference
equation
Differential equation
decoupled systems, 314, 317
eigenfunction, 314‚Äì315
fundamental set of solutions, 314
kernel and range of linear
transformation, 207
Dilation transformation, 67, 73, 75
Dimension
column space, 230
null space, 230
R3subspace classiÔ¨Åcation, 228‚Äì229
subspace, 155, 157‚Äì158
vector space, 227‚Äì229
Dimension of a Ô¨Çat, 442
Dimension of a set, 442
Discrete linear dynamical system,
268, 303
Disjoint closed convex set, 468
Dodecahedron, 437
Domain, matrix transformation, 64
Dot product, 38, 332
Dusky-footed wood rat, 304
Dynamical system, 64, 267‚Äì268
attractor, 306, 315‚Äì316
decoupling, 317
discrete linear dynamical system,
268, 303
eigenvalue and eigenvector applications,
280‚Äì281, 305
evolution, 303
repeller, 306, 316
saddle point, 307‚Äì309, 316
spiral point, 319
trajectory, 305
Earth Satellite Corporation, 395
Echelon form, 13‚Äì15, 173, 238, 270
Echelon matrix, 13‚Äì14
Economics, linear system applications,
50‚Äì55
Edge, face of a polyhedron, 472
Effective rank, matrix, 419
Eigenfunction, differential equation, 314‚Äì315
Eigenspace, 270‚Äì271, 399
CONFIRMING PAGES


--- Page 570 ---
Index I3
Eigenvalue, 269
characteristic equation of a square
matrix, 276
characteristic polynomial, 279
determinants, 276‚Äì278
Ô¨Ånding, 278
complex eigenvalue, 297‚Äì298, 300‚Äì301,
309‚Äì310, 317‚Äì319
diagonalization. SeeDiagonalization,
matrix
differential equations. SeeDifferential
equations
dynamical system applications, 281
interactive estimates
inverse power method, 324‚Äì326
power method, 321‚Äì324
quadratic form, 407‚Äì408
similarity transformation, 279
triangular matrix, 271
Eigenvector, 269
complex eigenvector, 297
decomposition, 304
diagonalization. SeeDiagonalization,
matrix
difference equations, 273
differential equations. SeeDifferential
equations
dynamical system applications, 281
linear independence, 272
linear transformation
matrix of linear transformation,
291‚Äì292
Rn, 293‚Äì294
similarity of matrix representations,
294‚Äì295
from VintoV, 292
row reduction, 270
Eigenvector basis, 284
Election, Markov chain modeling of
outcomes, 257‚Äì258, 261
Electrical engineering
matrix factorization, 129‚Äì130
minimal realization, 131
Electrical networks, 2, 83‚Äì84
Elementary matrix, 108
inversion, 109‚Äì110
types, 108
Elementary reÔ¨Çector, 392
Elementary row operation, 6,
108‚Äì109
Ellipse, 406
area, 186
singular values, 417‚Äì419
sphere transformation onto ellipse in R2,
417‚Äì418
Equal vectors, in R2, 24
Equilibrium price, 50, 52
Equilibrium vector. SeeSteady-state vector
Equivalence relation, 295
Equivalent linear systems, 3
Euler, Leonard, 481
Euler‚Äôs formula, 481Evolution, dynamical system, 303
Existence
linear transformation, 73
matrix equation solutions, 37‚Äì38
matrix transformation, 65
system of linear equations, 7‚Äì9,
20‚Äì21
Existence and Uniqueness Theorem, 21
Extreme point, 472, 475
Faces of a polyhedron, 472
Facet, 472
Factorization
analysis of a dynamical system, 283
block matrices, 122
complex eigenvalue, 301
diagonal, 283, 294
dynamical system, 283
electrical engineering, 129‚Äì131
See also LU Factorization
Feasible set, 414
Feynman, Richard, 165
Filter coefÔ¨Åcient, 248
Filter, linear, 248‚Äì249
Final demand vector, Leontief input‚Äìoutput
model, 134
Finite set, 228
Finite-dimensional vector
space, 228
subspaces, 229‚Äì230
First principal component, 395
First-order difference equation. SeeLinear
difference equation
First-order equations, reduction to, 252
Flexibility matrix, 106
Flight control system, 191
Floating point arithmetic, 9
Flop, 20, 127
Forward phase, row reduction
algorithm, 17
Fourier approximation, 389‚Äì390
Fourier coefÔ¨Åcient, 389
Fourier series, 390
Free variable, pivot column, 18, 20
Fundamental set of solutions, 251
differential equations, 314
Fundamental subspace, 239, 337, 422‚Äì423
Gauss, Carl Friedrich, 12n, 376n
Gaussian elimination, 12n
General least-squares problem, 362‚Äì366
General linear model, 373
General solution, 18, 251‚Äì252
Geometric continuity, 485
Geometric descriptions
R2, 25‚Äì27
spanfu, vg, 30‚Äì31
spanfvg, 30‚Äì31
vector space, 193
Geometric interpretation
complex numbers, A4‚ÄìA5
orthogonal projection, 351Geometric point, 25
Geometry of vector space
afÔ¨Åne combinations, 438‚Äì446
afÔ¨Åne independence, 446‚Äì456
barycentric coordinates, 448‚Äì453
convex combinations, 456‚Äì463
curves and surfaces, 483‚Äì492
hyperplanes, 463‚Äì471
polytopes, 471‚Äì483
Geometry vector, 488
Given rotation, 91
Global Positioning System (GPS), 331‚Äì332
Gouraud shading, 489
GPS. SeeGlobal Positioning System
Gradient, 464
Gram matrix, 434
Gram‚ÄìSchmidt process
inner product, 379‚Äì380
orthonormal bases, 358
QR factorization, 358‚Äì360
steps, 356‚Äì358
Graphical interpretation, coordinates, 219‚Äì220
Gram‚ÄìSchmidt Process Theorem, 357
Halley‚Äôs Comet, 376
Hermite cubic curve, 487
Hermite polynomials, 231
High-end computer graphics boards, 146
Homogeneous coordinates
three-dimensional graphics, 143‚Äì144
two-dimensional graphics, 141‚Äì142
Homogeneous linear systems
applications, 50‚Äì52
linear difference equations, 248
solution, 43‚Äì45
Householder matrix, 392
Householder reÔ¨Çection, 163
Howard, Alan H., 81
Hypercube, 479‚Äì481
Hyperplane, 442, 463‚Äì471
Icosahedron, 437
Identity matrix, 39, 108
Identity for matrix multiplication, 99
(i; j /-cofactor, 167‚Äì168
Ill-conditioned equations, 366
Ill-conditioned matrix, 118
Imaginary axis, A4
Imaginary numbers, pure, A4
Imaginary part
complex number, A2
complex vector, 299‚Äì300
Inconsistent system of linear equations, 4, 40
IndeÔ¨Ånite quadratic form, 407
Indifference curve, 414
Inequality
Bessel‚Äôs, 392
Cauchy‚ÄìSchwarz, 381‚Äì382
triangle, 382
InÔ¨Ånite set, 227n
InÔ¨Ånite-dimensional vector, 228
Initial value problem, 314
CONFIRMING PAGES


--- Page 571 ---
I4 Index
Inner product
angles, 337
axioms, 378
C[a,b], 382‚Äì384
evaluation, 382
length, 335, 379
overview, 332‚Äì333, 378
properties, 333
Rn, 378‚Äì379
Inner product space, 378‚Äì380
best approximation in, 380‚Äì381
Cauchy‚ÄìSchwarz inequality in, 381‚Äì382
deÔ¨Ånition, 378
Fourier series, 389‚Äì390
Gram‚ÄìSchmidt process, 379‚Äì380
lengths in, 379
orthogonality in, 390
trend analysis, 387‚Äì388
triangle inequality in, 382
weighted least-squares, 385‚Äì387
Input sequence, 266
Inspection, linearly dependent vectors, 59‚Äì60
Interchange matrix, 175
Interior point, 467
Intermediate demand, Leontief input‚Äìoutput
model, 134‚Äì135
International Celestial Reference
System, 450n
Interpolated color, 451
Interpolated polynomial, 23, 162
Invariant plane, 302
Inverse, matrix, 104‚Äì105
algorithm for Ô¨Ånding A 1, 110
characterization, 113‚Äì115
Cramer‚Äôs rule, 181‚Äì182
elementary matrix, 109‚Äì110
Ô¨Çexibility matrix, 106
invertible matrix, 106‚Äì107
linear transformations, invertible, 115‚Äì116
Moore‚ÄìPenrose inverse, 424
partitioned matrix, 121‚Äì123
product of invertible matrices, 108
row reduction, 110‚Äì111
square matrix, 173
stiffness matrix, 106
Inverse power method, interactive estimates
for eigenvalues, 324‚Äì326
Invertible Matrix Theorem, 114‚Äì115, 122,
150, 158‚Äì159, 173, 176, 237,
276‚Äì277, 423
Isomorphic vector space, 222, 224
Isomorphism, 157, 222, 380n
Iterative methods
eigenspace, 322‚Äì324
eigenvalues, 279, 321‚Äì327
inverse power method, 324‚Äì326
Jacobi‚Äôs method, 281
power method, 321‚Äì323
QR algorithm, 281‚Äì282, 326
Jacobian matrix, 306n
Jacobi‚Äôs method, 281Jordan, Wilhem, 12n
Jordan form, 294
Junction, network, 53
k-face, 472
k-polytope, 472
k-pyramid, 482
Kernel, 205‚Äì207
Kirchhoff‚Äôs laws, 84, 130
Ladder network, 130
Laguerre polynomial, 231
Lamberson, R., 267‚Äì268
Landsat satellite, 395‚Äì396
LAPACK, 102, 122
Laplace transform, 180
Leading entry, 12, 14
Leading variable, 18n
Least-squares error, 365
Least-squares solution, 331
alternative calculations,
366‚Äì367
applications
curve Ô¨Åtting, 373‚Äì374
general linear model, 373
least-squares lines, 370‚Äì373
multiple regression, 374‚Äì375
general solution, 362‚Äì366
QR factorization, 366‚Äì367
singular value decomposition, 424
weighted least-squares, 385‚Äì387
Left distributive law, matrix multiplication, 99
Left-multiplication, 100, 108‚Äì109,
178, 360
Left singular vector, 419
Length, vector, 333‚Äì334, 379
Leontief, Wassily, 1, 50, 134, 139n
Leontief input‚Äìoutput model
column sum, 136
consumption matrix, 135‚Äì136
Ô¨Ånal demand vector, 134
(I C / 1
economic importance of entries, 137
formula for, 136‚Äì137
intermediate demand, 134‚Äì135
production vector, 134
unit consumption vector, 134
Level set, 464
Line segment, 456
Linear combinations
applications, 31
Ax, 35
vectors in Rn, 28‚Äì30
Linear dependence
characterization of linearly dependent sets,
59, 61
relation, 57‚Äì58, 210, 213
vector sets
one or two vectors, 58‚Äì59
overview, 57, 210
theorems, 59‚Äì61
two or more vectors, 59‚Äì60Linear difference equation, 85‚Äì86
discrete-time signals, 246‚Äì247
eigenvectors, 273
homogeneous equations, 248
nonhomogeneous equations, 248, 251‚Äì252
reduction to systems of Ô¨Årst-order
equations, 252
solution sets, 250‚Äì251
Linear equation, 2
Linear Ô¨Ålter, 248
Linear functional, 463, 474‚Äì475
Linear independence
eigenvector sets, 272
matrix columns, 58
spaceSof signals, 247‚Äì248
spanning set theorem, 212‚Äì213
standard basis, 211
vector sets
one or two vectors, 58‚Äì59
overview, 57, 210‚Äì211
two or more vectors, 59‚Äì60
Linear model, 1
applications
difference equations, 86‚Äì87
electrical networks, 83‚Äì85
weight loss diet, 81‚Äì83
general linear model, 373
Linear programming, 2
Linear regression coefÔ¨Åcient, 371
Linear system. SeeSystem of linear equations
Linear transformation, 63‚Äì64, 66‚Äì69, 72
contractions and expansions, 75
determinants, 184‚Äì186
eigenvectors and linear transformation
from VintoV, 292
matrix of linear transformation, 72,
291‚Äì292
similarity of matrix representations,
294‚Äì295
existence and uniqueness
questions, 73
geometric linear transformation
ofR2, 73
invertible, 115‚Äì116
one-to-one linear transformation, 76‚Äì78
projections, 76
range. SeeRange
reÔ¨Çections, 74
shear transformations, 75
See also Matrix of a linear transformation
Linear trend, 389
Loop current, 83‚Äì84
Low-pass Ô¨Ålter, 249
Lower triangular matrix, 117, 126‚Äì128
LU factorization, 129, 408
algorithm, 127‚Äì129
electrical engineering, 129‚Äì130
overview, 126‚Äì127
permuted LU factorization, 129
Macromedia Freehand, 483
Main diagonal, 94, 169
CONFIRMING PAGES


--- Page 572 ---
Index I5
Maple, 281, 326
Mapping. SeeTransformation
Marginal propensity to consume, 253
Mark II computer, 1
Markov chain, 281, 303
distant future prediction, 258‚Äì259
election outcomes, 257‚Äì258, 261
population modeling, 255‚Äì257,
259‚Äì260
steady-state vectors, 259‚Äì262
Mass‚Äìspring system, 198, 207, 216
Mathematica, 281
MATLAB, 23, 132, 187, 264, 281, 310, 324,
326, 361
Matrix, 4
algebra, 93‚Äì157
augmented matrix, 4, 6‚Äì8, 18, 21, 38
coefÔ¨Åcient matrix, 4, 38
determinant. SeeDeterminant
diagonalization. SeeDiagonalization,
matrix
echelon form, 13‚Äì14
equal matrices, 95
inverse. SeeInverse, matrix
linear independence of matrix columns, 58
mnmatrix, 4
notation, 95
partitioned. SeePartitioned matrix
pivot column, 14, 16
pivot position, 14‚Äì17
power, 101
rank. SeeRank, matrix
reduced echelon form, 13‚Äì14, 18‚Äì20
row equivalent matrices, 6‚Äì7
row equivalent, 6, 29n, A1
row operations, 6‚Äì7
row reduction, 12‚Äì18, 21
size, 4
solving, 4‚Äì7
symmetric. SeeSymmetric matrix
transformations, 64‚Äì66, 72
transpose, 101‚Äì102
Matrix equation, 2
AxDb, 35‚Äì36
computation of Ax, 38, 40
existence of solutions, 37‚Äì38
properties of Ax, 39‚Äì40
Matrix factorization, 94, 125‚Äì126
LU factorization
algorithm, 127‚Äì129
overview, 126‚Äì127
permuted LU factorization, 129
Matrix of a linear transformation,
71‚Äì73
Matrix multiplication, 96‚Äì99
composition of linear transformation
correspondence, 97
elementary matrix, 108‚Äì109
partitioned matrix, 120‚Äì121
properties, 99‚Äì100
row‚Äìcolumn rule, 98‚Äì99
warnings, 100Matrix of observations, 429
Matrix program, 23n
Matrix of the quadratic form, 403
Maximum of quadratic form,
410‚Äì413
Mean square error, 390
Mean-deviation form, 372, 428
Microchip, 119
Migration matrix, 86, 256, 281
Minimal realization, electrical
engineering, 131
Minimal representation, of a polytope, 473,
476‚Äì477
Modulus, complex number, A3
Moebius, A. F., 450
Molecular modeling, 142‚Äì143
Moore‚ÄìPenrose inverse, 424
Moving average, 254
Muir, Thomas, 165
Multichannel image, 395
Multiple regression, 373‚Äì375
Multiplicity of eigenvalue, 278
Multispectral image, 395, 427
Multivariate data, 426, 430‚Äì431
NAD. SeeNorth American Datum
National Geodetic Survey, 329
Natural cubic splines, 483
Negative deÔ¨Ånite quadratic form, 407
Negative semideÔ¨Ånite quadratic
form, 407
Network. SeeElectrical networks
Network Ô¨Çow, linear system applications,
53‚Äì54, 83
Node, network, 53
Nonhomogeneous linear systems
linear difference equations,
248, 251‚Äì252
solution, 45‚Äì47
Nonlinear dynamical system, 306n
Nonpivot column, A1
Nonsingular matrix, 105
Nontrivial solution, 44, 57‚Äì58
Nonzero entry, 12, 16
Nonzero linear functional, 463
Nonzero row, 12
Nonzero vector, 183
Nonzero vector, 205
Nonzero volume, 277
Norm, vector, 333‚Äì334, 379
Normal equation, 331, 363
Normalizing vectors, 334
North American Datum (NAD), 331‚Äì332
Null space, matrix
basis, 213‚Äì214
column space contrast, 204‚Äì206
dimension, 230, 235
explicit description, 202‚Äì203, 205
overview, 201‚Äì202
subspaces, 150‚Äì151
Nullility, 235
Nutrition model, 81‚Äì83Observation vector, 370, 429
Octahedron, 437
Ohm, 83‚Äì84, 314, 318
Ohms‚Äô law, 83‚Äì84, 130
Oil exploration, 1‚Äì2
One-to-one linear transformation,
76‚Äì78
Open ball, 467
Open set, 467
OpenGL, 483
Optimization, constrained. SeeConstrained
optimization problem
Orbit, 24
Order, polynomial, 389
Ordered n-tuples, 27
Ordered pairs, 24
Orthogonal basis, 341, 349, 356,
422‚Äì423
Orthogonal complement, 336‚Äì337
Orthogonal Decomposition Theorem, 350,
358, 363
Orthogonal diagonalization, 398, 404‚Äì405,
420, 426
Orthogonal matrix, 346
Orthogonal projection
Best Approximation Theorem, 352‚Äì353
Fourier series, 389
geometric interpretation, 351
overview, 342‚Äì344
properties, 352‚Äì354
Rn, 349‚Äì351
Orthogonal set, 340
Orthogonal vector, 335‚Äì336
Orthonormal basis, 344, 356, 358
Orthonormal column, 345‚Äì347
Orthonormal row, 346
Orthonormal set, 344‚Äì345
Over determined system, 23
Pn
standard basis, 211‚Äì212
vector space, 194
P2, 223
P3, 222
Parabola, 373
Parallel Ô¨Çats, 442
Parallel hyperplanes, 464
Parallelogram
area, 182‚Äì183
law, 339
rule for addition, 26, 28
Parameter vector, 370
Parametric
continuity, 485‚Äì486
descriptions of solution sets, 19
equations
line, 44, 69
plane, 44
vector form, 45, 47
Parametric descriptions, solution sets, 19
Parametric vector equation, 45
Partial pivoting, 17
CONFIRMING PAGES


--- Page 573 ---
I6 Index
Partitioned matrix, 93, 119
addition, 119‚Äì120
column‚Äìrow expansion, 121
inverse, 121‚Äì123
multiplication, 120‚Äì121
scalar multiplication, 119‚Äì120
Permuted lower triangular matrix, 128
Permuted LU factorization, 129
Perspective projection, 144‚Äì146
Pivot, 15, 277
column, 14, 16, 18, 152, 157, 214
partial pivoting, 17
position, 14‚Äì17
Pixel, 395
Plane
geometric description, 442
implicit equation, 463
Platonic solids, 437‚Äì438
Point mass, 33
Polar coordinates, A5
Polar decomposition, 434
Polygon, 437‚Äì438, 472
Polyhedron, 437, 472, 482
Polynomials
blending, 487n
characteristic, 278‚Äì279
degree, 194
Hermite, 231
interpolating, 23
Laguerre polynomial, 231
Legendre polynomial, 385
orthogonal, 380, 388
set, 194
trigonometric, 389
zero, 194
Polytopes, 471‚Äì483
Population
linear modeling, 85‚Äì86
Markov chain modeling, 255‚Äì257,
259‚Äì260
Positive deÔ¨Ånite matrix, 408
Positive deÔ¨Ånite quadratic form, 407
Positive semideÔ¨Ånite matrix, 408
Positive semideÔ¨Ånite quadratic
form, 407
PostScript fonts, 486‚Äì487
Power, matrix, 101
Powers, of a complex number, A6
Power method, interactive estimates for
eigenvalues, 321‚Äì324
Predator‚Äìprey model, 304‚Äì305
Predicted y-value, 371
Price, equilibrium, 49‚Äì51, 54
Principal axes
geometric view, 405‚Äì407
quadratic form, 405
Principal component analysis
covariance matrix, 428
Ô¨Årst principal component, 429
image processing, 395‚Äì396, 428‚Äì430
mean-deviation form, 428multivariate data dimension reduction,
430‚Äì431
sample mean, 427‚Äì428
second principal component, 429
total variance, 429
variable characterization, 431
Principal Axes Theorem, 405, 407
Principle of Mathematic Induction, 174
Probability vector, 256
Process control data, 426
Production vector, Leontief input‚Äìoutput
model, 134
ProÔ¨Åle, 472, 474
Projection
matrix, 400
transformation, 65, 75, 163
See also Orthogonal projection
Proper subset, 442n
Properties of Determinants
Theorem, 274
Pseudoinverse, 424
Public work schedules, 414‚Äì415
feasible set, 414
indifference curve, 414‚Äì415
utility, 412
Pure imaginary numbers, A4
Pythagorean Theorem, 381
QR algorithm, 281‚Äì282, 326
QR factorization
Cholesky factorization, 434
Gram‚ÄìSchmidt process, 358‚Äì360
least-squares solution, 366‚Äì367
QR Factorization Theorem, 359
Quadratic B√©zier curve, 484
Quadratic form, 403‚Äì404
change of variable in, 404‚Äì405
classiÔ¨Åcation, 407‚Äì408
constrained optimization, 410‚Äì415
eigenvalues, 407‚Äì408
matrix of, 403
principal axes, 405‚Äì407
Quadratic Forms and Eigenvalue Theorem,
407‚Äì408
Rn
algebraic properties, 27
change of basis, 243‚Äì244
dimension of a Ô¨Çat, 442
distance in, 334‚Äì335
eigenvector basis, 284
inner product, 378‚Äì379
linear functional, 463
linear transformations on, 293‚Äì294
orthogonal projection, 349‚Äì351
quadratic form. SeeQuadratic form
subspace
basis, 150‚Äì152, 158
column space, 149, 151‚Äì152
coordinate systems, 155‚Äì157, 220‚Äì221
dimension, 155, 157‚Äì158
lines, 149null space, 150‚Äì151
properties, 148
rank, 157‚Äì159
span, 149
transformation of RntoRm, 64, 71‚Äì72,
76‚Äì77
vectors in
inner product, 332‚Äì333
length, 333‚Äì334
linear combinations, 28‚Äì30
orthogonal vectors, 335‚Äì336
overview, 27
R2
angles in, 337‚Äì338
complex numbers, A6
geometric linear transformation, 73
polar coordinates in, A5
vectors in
geometric descriptions, 25‚Äì27
overview, 24‚Äì25
parallelogram rule for addition, 26
R3
angles in, 337‚Äì338
sphere transformation onto ellipse in R2,
417‚Äì418
subspace
classiÔ¨Åcation, 228‚Äì229
spanned by a set, 197
vectors in, 27
R4
polytope visualization, 477
subspace, 196‚Äì197
R40, 236
Range
matrix transformation, 64‚Äì65, 203
kernel and range of linear transformation,
205‚Äì207
Rank, matrix
algorithms, 238
estimation, 419n
Invertible Matrix Theorem. SeeInvertible
Matrix Theorem
overview, 157‚Äì159, 232‚Äì233
row space, 233‚Äì235
Rank of transformation, 63,
205‚Äì207, 265
Rank Theorem, 235‚Äì236
application to systems of
equations, 236
Ray-tracing, 451
Ray-triangle intersection, 452‚Äì453
Rayleigh quotient, 326, 393
Real axis, A4
Real part
complex number, A2
complex vector, 299‚Äì300
Real vector space, 192
Rectangular coordinate system, 25
Recurrence relation. SeeLinear difference
equation
Recursive description, 273
CONFIRMING PAGES


--- Page 574 ---
Index I7
Reduced echelon matrix, 13‚Äì14,
18‚Äì20, A1
Reduced LU factorization, 132
Reduced row echelon form, 13
ReÔ¨Çections, linear transformations,
74, 347‚Äì349
Regression
coefÔ¨Åcient, 371
line, 371
multiple, 372‚Äì374
orthogonal, 434
Regular polyhedron, 437, 482
Regular solid, 436
Regular stochastic matrix, 260
Relative error, 393
Rendering, computer graphics,
146, 489
Repeller, dynamical system, 306, 316
Residual vector, 373
Resistance, 83‚Äì84
Reversible row operations, 6
RGB coordinates, 451‚Äì453
Riemann sum, 383
Right distributive law, matrix
multiplication, 99
Right multiplication, 100
Right singular vector, 419
RLC circuit, 216
Rotation transformation,
68, 143‚Äì144, 146
Roundoff error, 9, 271, 419
Row equivalent matrices, 6‚Äì7,
18, 29n
Row operations, matrices, 6‚Äì7
Row reduced matrix, 13‚Äì14
Row reduction
algorithm, 14‚Äì17, 21, 127
matrix, 12‚Äì14, 110‚Äì111
Row replacement matrix, 175
Row vector, 233
Row‚Äìcolumn rule, matrix multiplication,
98‚Äì99, 120
Row‚Äìvector rule, computation of
Ax, 38
S, 193, 247‚Äì248
Saddle point, 307‚Äì309, 316
Sample covariance matrix, 428
Sample mean, 427‚Äì428
Samuelson, P. A., 253n
Scalar, 25, 192‚Äì193
Scalar multiple, 25, 95
Scale matrix, 175
Scatter plot, 427
Scene variance, 395
Schur complement, 123
Schur factorization, 393
Second principal component, 429
Series circuit, 130
Set
afÔ¨Åne, 441‚Äì443, 457‚Äì458
bounded, 467closed, 467‚Äì468
compact, 467‚Äì469
convex, 457‚Äì459
level, 464
open, 467
vector. SeeVector set
Shear transformation, 66, 75, 141
Shunt circuit, 130
Signal, space of, 193, 248‚Äì250
Similar matrices, 279, 294‚Äì295
Similarity transformation, 279
Simplex, 477‚Äì479
Singular matrix, 105, 115‚Äì116
Singular value decomposition (SVD),
416‚Äì417, 419‚Äì420
applications
bases for fundamental subspaces,
422‚Äì423
condition number, 422
least-squares solution, 424
reduced decomposition and
pseudoinverse, 424
internal structure, 420‚Äì422
R3sphere transformation onto ellipse in
R2, 417‚Äì418
singular values of a matrix, 418‚Äì419
Singular Value Decomposition
Theorem, 419
Sink, dynamical system, 316
Size, matrix, 4
Solids, Platonic, 437‚Äì438
Solution, 3‚Äì4
Solution set, 3, 18‚Äì21, 201, 250‚Äì251, 314
Source, dynamical system, 316
Space. SeeInner product; Vector space
Space shuttle, 191
Span, 30‚Äì31, 37
afÔ¨Åne, 437
linear independence, 59
orthogonal projection, 342
subspace, 149
subspace spanned by a set, 196‚Äì197
Span{u, v}
geometric description, 30‚Äì31
linear dependence, 59
solution set, 45
Span{v}, geometric description,
30‚Äì31
Spanning set, 196, 214
Spanning Set Theorem, 212‚Äì213, 229
Sparse matrix, 174
Spatial dimension, 427
Spectral decomposition, 400‚Äì401
Spectral factorization, 132
Spectral Theorem, 399
Spiral point, dynamical system, 319
Spline, 492
B-spline, 486‚Äì487, 492‚Äì493
natural cubic, 483
Spotted owl, 267‚Äì268, 303‚Äì304, 309‚Äì311
Stage-matrix model, 267, 310
Standard basis, 150, 211, 219Standard matrix, 290
Standard matrix of a linear transformation, 72
Standard position, 406
State-space design, 303
Steady-state response, 303
Steady-state vector, 259‚Äì262
Stiffness matrix, 106
Stochastic matrix, 256, 259‚Äì260
Strictly dominant eigenvalue, 321
Strictly separate hyperplane,
468‚Äì469
Submatrix, 266
Subset, proper, 442n
Subspace
Ô¨Ånite-dimensional space, 229‚Äì230
properties, 195‚Äì196
R3
classiÔ¨Åcation, 228‚Äì229
spanned by a set, 197
Rnvectors
basis, 150‚Äì152, 158
column space, 149, 151‚Äì152
coordinate systems, 155‚Äì157, 220‚Äì221
dimension, 155, 157‚Äì158
lines, 149
null space, 150‚Äì151
properties, 148
rank, 157‚Äì159
span, 149
spanned by a set, 196‚Äì197
Sum
matrices, 95
vectors, 25
Sum of squares for error, 385‚Äì386
Superposition principle, 84
Supported hyperplane, 472
Surface normal, 489
Surfaces. SeeB√©zier surfaces
SVD. SeeSingular value decomposition
Symbolic determinant, 466
Symmetric matrix, 397
diagonalization, 397‚Äì399
Spectral Theorem, 399
spectral decomposition, 400‚Äì401
System matrix, 124
System of linear equations
applications
economics, 50‚Äì52
chemical equation balancing, 52
network Ô¨Çow, 53‚Äì54
back-substitution, 19‚Äì20
consistent system, 4, 7‚Äì8
equivalent linear systems, 3
existence and uniqueness
questions, 7‚Äì9
inconsistent system, 4, 40
matrix notation, 4
overview, 1‚Äì3
solution
homogeneous linear systems, 43‚Äì45
nonhomogeneous linear systems, 45‚Äì47
nontrivial solution, 44
CONFIRMING PAGES


--- Page 575 ---
I8 Index
System of linear equations ( Continued )
overview, 3‚Äì4
parametric descriptions of solution
sets, 19
parametric vector form, 45, 47
row reduced matrix, 18‚Äì19
trivial solution, 44
Tangent vector, 484‚Äì485, 492‚Äì494
Tetrahedron, 187, 437
Three-moment equation, 254
Timaeus , 437
Total variance, 429
Trace, 429
Trajectory, dynamical system, 305
Transfer function, 124
Transfer matrix, 130‚Äì131
Transformation
matrices, 64‚Äì66
overview, 64
RntoRm, 64
shear transformation, 66, 75
See also Linear transformation
Translation, vector addition, 46
Transpose, 101‚Äì102
conjugate, 483n
inverse, 106
matrix cofactors, 181
product, 100
Trend analysis, 387‚Äì388
Trend coefÔ¨Åcients, 388
Trend function, 388
Trend surface, 374
Triangle, barycentric coordinates, 450‚Äì451
Triangle inequality, 382
Triangular determinant, 172, 174
Triangular form, 5, 8, 11, 13
Triangular matrix, 5, 169
determinants, 168
eigenvalues, 271
lower. SeeLower triangular matrix
upper. SeeUpper triangular matrixTridiagonal matrix, 133
Trigonometric polynomial, 389
Trivial solution, 44, 57‚Äì58
TrueType font, 494
Uncorrelated variable, 429, 431
Underdetermined system, 23
Uniform B-spline, 493
Unique Representation Theorem,
218, 449
Uniqueness
existence and uniqueness theorem, 21
linear transformation, 73
matrix transformation, 65
reduced echelon matrix, 13, A1
system of linear equations, 7‚Äì9, 20‚Äì21
Unit cell, 219‚Äì220
Unit consumption vector, Leontief
input‚Äìoutput model, 134
Unit lower triangular matrix, 126
Unit vector, 334, 379
Unstable equilibrium, 312
Upper triangular matrix, 117
Utility function, 414
Vandermonde matrix, 162
Variable, 18
leading, 18n
uncorrelated, 429
See also Change of variable
Variance, 364‚Äì365, 377, 386n, 428
sample, 432‚Äì433
scene, 395‚Äì396
total, 428
Variation-diminishing property, B√©zier
curves, 490
Vector, 24
geometric descriptions of span fvgand
spanfu, vg, 30‚Äì31
inner product. SeeInner product
length, 333‚Äì334, 379, 418
linear combinations in applications, 31matrix‚Äìvector product. SeeMatrix equation
Rn
linear combinations, 28‚Äì30
vectors in, 27
R2
geometric descriptions, 25‚Äì27
parallelogram rule for addition,
26, 28
vectors in, 24‚Äì25
R3, vectors in, 27
space, 191‚Äì194
subspace. SeeSubspace
subtraction, 27
sum, 24
Vector equation, 2, 44, 46, 56‚Äì57
Vector space
change of basis, 241‚Äì244
complex, 192n
dimension of vector space, 227‚Äì229
hyperplanes, 463‚Äì471
overview, 191‚Äì194
real, 192n
See also Geometry of vector space; Inner
product
Vertex, face of a polyhedron, 472
Very-large scale integrated
microchip, 119
Virtual reality, 143
V olt, 83
V olume
determinants as, 182‚Äì183
ellipsoid, 187
tetrahedron, 187
Weighted least-squares, 385‚Äì387
Weights, 28, 35, 203
Wire-frame approximation, 451
Zero functional, 463
Zero matrix, 94
Zero subspace, 149, 195
Zero vector, 60, 150, 195, 335
CONFIRMING PAGES


--- Page 576 ---
3KRWR&UHGLWV
&KDSWHU
3DJHWK2FWREHU :DVVLO\/HRQWLHI 5XVVLDQERUQ$PHULFDQZLQQHU RIWKH
1REHO3UL]HIRU(FRQRPLFV .H\VWRQH+XOWRQ$UFKLYH*HWW\,PDJHV
3DJH  (OHFWULF JULG QHWZRUN 2OLYLHU /H 4XHLQHF6KXWWHUVWRFN &RDO ORDG LQJ
$EXW\ULQ6KXWWHUVWRFN &1& /3* FXWWLQJVSDUNVFORVHXS 6DVLQ76KXW WHUVWRFN
3DJH :RPDQSD\LQJIRUJURFHULHVDWVXSHUPDUNHWFKHFNRXW 0RQNH\%XVLQ HVV
,PDJHV6KXWWHUVWRFN 3OXPEHU√Ä[LQJVLQNDWNLWFKHQ .XUKDQ6KXWWH UVWRFN
3DJH $HULDOYLHZRI0HWUR9DQFRXYHU -RVHI+DQXV6KXWWHUVWRFN $HULDO YLHZRI
$PHULFDQVXEXUEV *DU\%ODNHOH\6KXWWHUVWRFN
&KDSWHU
3DJH6XSHUKLJKUHVROXWLRQ%RHLQJEOXHSULQWUHQGHULQJ 6SRRN\ )RWROLD
3DJH%RHLQJEOHQGHGZLQJERG\ 1$6$
3DJH &RPSXWHU&LUFXLW%RDUG 5DGXE)RWROLD
3DJH 6SDFH3UREH 1$6$
3DJH 5HGWUDFWRUSORZLQJLQGXVN )RWRNRVWLF6KXWWHUVWRFN 3HRSOHEURZ VLQJFRQ
VXPHU HOHFWURQLFV UHWDLO VWRUH 'RWVKRFN6KXWWHUVWRFN :RPDQ FKHF NLQJ LQ DW D
KRWHO9LEH,PDJHV)RWROLD&DUSURGXFWLRQOLQHZLWKXQ√ÄQLVKHGFD UVLQDURZ5DLQHU
3OHQGO6KXWWHUVWRFN
3DJH 6FLHQWLVWZRUNLQJDWWKHODERUDWRU\ $OH[DQGHU5DWKV)RWROLD
&KDSWHU
3DJH 3K\VLFLVW5LFKDUG)H\QPDQ $3 ,PDJHV
&KDSWHU
3DJH 'U\GHQ2EVHUYHVVW$QQLYHUVDU\RI6760LVVLRQ 1$6$
3DJH )URQWYLHZRIODSWRSZLWKEODQNPRQLWRU ,IRQJ6KXWWHUVWRFN 6PDU WSKRQH
LVRODWHG 6KLP)RWROLD
3DJH  )URQWYLHZ &KLFDJR $UFKDQD %KDUWLDO6KXWWHUVWRFN )URQWYLHZ VXEXU EV
1RDK6WU\FNHU6KXWWHUVWRFN
&KDSWHU
3DJH 1RUWKHUQ6SRWWHG2ZO 'LJLWDOPHGLDIZVJRY
&KDSWHU3DJH 1RUWK$PHULFDQ'DWXP 'PLWU\.DOLQRYVN\6KXWWHUVWRFN
3DJH 0DUFHO&OHPHQV6KXWWHUVWRFN +DOOH\¬∑V&RPHW0DUFHO&OHPHQV6KXWWHU VWRFN
1

--- Page 577 ---
P2 Photo Credits
Chapter 7
Page 395 Landsat Satellite: Landsat Data/U.S. Geological Survey.
Page 396 Spectral band 1: Landsat Data/U.S. Geological Survey; Spectral band 4:
Landsat Data/U.S. Geological Survey; Spectral band 7: Landsat Data/U.S. Geologi-
cal Survey; Principal component 1: Landsat Data/U.S. Geological Survey; Principal
component 2: Landsat Data/U.S. Geological Survey; Principal component 3: Landsat
Data/U.S. Geological Survey.
Page 414 Small wood bridge with railings: Dejangasparin/Fotolia; Wheel loader machine
unloading sand: Dmitry Kalinovsky/Shutterstock; Young family having a picnic by
the river: Viki2win/Shutterstock.
Chapter 8
Page 437 School of Athens fresco: The Art Gallery Collection/Alamy.
CONFIRMING PAGES


--- Page 578 ---
References to Applications
WEB
indicates material on the Web site.
Biology and Ecology
Estimating systolic blood pressure, 376‚Äì377
Laboratory animal trials, 262
Molecular modeling, 142‚Äì143
Net primary production of nutrients, 373‚Äì374
Nutrition problems,
WEB 81‚Äì83, 87
Predator-prey system, 304‚Äì305, 312
Spotted owls and stage-matrix models,
WEB 267‚Äì268, 309‚Äì311
Business and Economics
Accelerator-multiplier model, 253
Average cost curve, 373‚Äì374
Car rental Ô¨Çeet, 88, 263
Cost vectors, 31
Equilibrium prices,
WEB 50‚Äì52, 55
Exchange table, 54‚Äì55
Feasible set, 414
Gross domestic product, 139
Indifference curves, 414‚Äì415
Intermediate demand, 134
Investment, 254
Leontief exchange model, 1,
WEB 50‚Äì52
Leontief input‚Äìoutput model, 1,
WEB 134‚Äì140
Linear programming,
WEB 2,
WEB 83‚Äì84, 122, 438, 471, 474
Loan amortization schedule, 254
Manufacturing operations, 31, 68‚Äì69
Marginal propensity to consume, 253
Markov chains,
WEB 255‚Äì264, 281
Maximizing utility subject to a budget constraint, 414‚Äì415
Population movement, 85‚Äì86, 88, 257, 263, 281
Price equation, 139
Total cost curve, 374
Value added vector, 139
Variable cost model, 376
Computers and Computer Science
B√©zier curves and surfaces, 462, 483‚Äì494
CAD, 489, 493
Color monitors, 147
Computer graphics,
WEB 94, 140‚Äì148, 451‚Äì453
Cray supercomputer, 122
Data storage, 40, 132
Error-detecting and error-correcting codes, 401, 424
Game theory, 471
High-end computer graphics boards, 146
Homogeneous coordinates, 141‚Äì142, 143
Parallel processing, 1, 102
Perspective projections,
WEB 144‚Äì145
Vector pipeline architecture, 122
Virtual reality, 143
VLSI microchips, 119
Wire-frame models, 93, 140Control Theory
Controllable system,
WEB 266
Control systems engineering, 124,
WEB 191‚Äì192
Decoupled system, 308, 314, 317
Deep space probe, 124
State-space model,
WEB 266, 303
Steady-state response, 303
Transfer function (matrix), 124, 130‚Äì131
Electrical Engineering
Branch and loop currents,
WEB 83‚Äì84
Circuit design,
WEB 2, 129‚Äì130
Current Ô¨Çow in networks,
WEB 83‚Äì84, 87‚Äì88
Discrete-time signals, 193‚Äì194, 246‚Äì247
Inductance-capacitance circuit, 207
Kirchhoff‚Äôs laws,
WEB 83‚Äì84
Ladder network, 130, 132‚Äì133
Laplace transforms, 124, 180
Linear Ô¨Ålters, 248‚Äì249, 254
Low-pass Ô¨Ålter, 249,
WEB 369
Minimal realization, 131
Ohm‚Äôs law,
WEB 83‚Äì84
RC circuit, 314‚Äì315
RLC circuit, 216, 318‚Äì319
Series and shunt circuits, 130
Transfer matrix, 130‚Äì131, 132‚Äì133
Engineering
Aircraft performance, 377, 391
Boeing Blended Wing Body,
WEB 94
Cantilevered beam, 254
CFD and aircraft design,
WEB 93‚Äì94
DeÔ¨Çection of an elastic beam, 106, 113
Deformation of a material, 434
Equilibrium temperatures, 11, 88,
WEB 133
Feedback controls, 471
Flexibility and stiffness matrices, 106, 113
Heat conduction, 133
Image processing,
WEB 395‚Äì396, 426‚Äì427, 432
LU factorization and airÔ¨Çow,
WEB 94
Moving average Ô¨Ålter, 254
Space shuttle control,
WEB 191‚Äì192
Superposition principle, 67, 84, 314
Surveying,
WEB 331‚Äì332
Mathematics
Area and volume,
WEB 165‚Äì166, 182‚Äì184, 277
Attractors/repellers in a dynamical system, 306, 309, 312,
315‚Äì316, 320
Bessel‚Äôs inequality, 392
Best approximation in function spaces, 380‚Äì381
Cauchy-Schwarz inequality, 381‚Äì382
Conic sections and quadratic surfaces,
WEB 407‚Äì408
Differential equations, 206‚Äì207, 313‚Äì321
Fourier series, 389‚Äì390
Hermite polynomials, 231
Hypercube, 479‚Äì481
FIRST PAGES


--- Page 579 ---
Interpolating polynomials,
WEB 23, 162
Isomorphism, 157, 222‚Äì223
Jacobian matrix, 306
Laguerre polynomials, 231
Laplace transforms, 124, 180
Legendre polynomials, 383
Linear transformations in calculus, 206,
WEB 292‚Äì294
Simplex, 477‚Äì479
Splines,
WEB 483‚Äì486, 492‚Äì493
Triangle inequality, 382
Trigonometric polynomials, 389
Numerical Linear Algebra
Band matrix, 133
Block diagonal matrix, 122, 124
Cholesky factorization,
WEB 408, 434
Companion matrix, 329
Condition number, 116, 118, 178, 393, 422
Effective rank,
WEB 238, 419
Floating point arithmetic, 9, 20, 187
Fundamental subspaces, 239, 337, 422‚Äì423
Givens rotation,
WEB 91
Gram matrix, 434
Gram‚ÄìSchmidt process,
WEB 361
Hilbert matrix, 118
Householder reÔ¨Çection, 163, 392
Ill-conditioned matrix (problem), 116, 366
Inverse power method, 324‚Äì326
Iterative methods, 321‚Äì327
Jacobi‚Äôs method for eigenvalues, 281
LAPACK, 102, 122
Large-scale problems, 91, 122,
WEB 331‚Äì332
LU factorization, 126‚Äì129, 131‚Äì132, 133, 434
Operation counts, 20,
WEB 111, 127,
WEB 129, 174
Outer products, 103, 121
Parallel processing, 1
Partial pivoting, 17,
WEB 129
Polar decomposition, 434
Power method, 321‚Äì324
Powers of a matrix,
WEB 101
Pseudoinverse, 424, 435
QR algorithm, 282, 326
QR factorization, 359‚Äì360,
WEB 361,
WEB 369, 392‚Äì393
Rank-revealing factorization, 132, 266, 434
Rank theorem,
WEB 235, 240
Rayleigh quotient, 326‚Äì327, 393
Relative error, 393
Schur complement, 124
Schur factorization, 393
Singular value decomposition, 132, 416‚Äì426
Sparse matrix, 93, 137, 174
Spectral decomposition, 400‚Äì401
Spectral factorization, 132
Tridiagonal matrix, 133
Vandermonde matrix, 162, 188, 329Vector pipeline architecture, 122
Physical Sciences
Cantilevered beam, 254
Center of gravity, 34
Chemical reactions, 52, 55
Crystal lattice, 220, 226
Decomposing a force, 344
Digitally recorded sound, 247
Gaussian elimination, 12
Hooke‚Äôs law, 106
Interpolating polynomial,
WEB 23, 162
Kepler‚Äôs Ô¨Årst law, 376
Landsat image,
WEB 395‚Äì396
Linear models in geology and geography, 374‚Äì375
Mass estimates for radioactive substances, 376
Mass-spring system, 198, 216
Model for glacial cirques, 374
Model for soil pH, 374
Pauli spin matrices, 162
Periodic motion, 297
Quadratic forms in physics, 403‚Äì410
Radar data, 124
Seismic data, 2
Space probe, 124
Steady-state heat Ô¨Çow, 11, 133
Superposition principle, 67, 84, 314
Three-moment equation, 254
TrafÔ¨Åc Ô¨Çow, 53‚Äì54, 56
Trend surface, 374
Weather, 263
Wind tunnel experiment, 23
Statistics
Analysis of variance, 364
Covariance, 427‚Äì429, 430, 431, 432
Full rank, 239
Least-squares error, 365
Least-squares line,
WEB 331,
WEB 369, 370‚Äì372
Linear model in statistics, 370‚Äì377
Markov chains,
WEB 255‚Äì264, 281
Mean-deviation form for data, 372, 428
Moore-Penrose inverse, 424
Multichannel image processing,
WEB 395‚Äì396, 426‚Äì434
Multiple regression, 374‚Äì375
Orthogonal polynomials, 381
Orthogonal regression, 433‚Äì434
Powers of a matrix,
WEB 101
Principal component analysis,
WEB 395‚Äì396, 429‚Äì430
Quadratic forms in statistics, 403
Readjusting the North American Datum,
WEB 331‚Äì332
Regression coefÔ¨Åcients, 371
Sums of squares (in regression), 377, 385‚Äì386
Trend analysis, 387‚Äì388
Variance, 377, 428‚Äì429
Weighted least-squares, 378, 385‚Äì387
FIRST PAGES